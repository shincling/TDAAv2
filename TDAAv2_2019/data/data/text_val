linked open datasets about scholarly publications enable the development and integration of sophisticated end user services however , richer datasets are still needed the first goal of this challenge was to investigate novel approaches to obtain such semantic data in particular , we were seeking methods and tools to extract information from scholarly publications , to publish it as lod , and to use queries over this lod to assess quality this year we focused on the quality of workshop proceedings , and of journal articles w r t their citation network a third , open task , asked to showcase how such semantic data could be exploited and how semantic web technologies could help in this emerging context
we extend the results of adcock , carlsson , and carlsson by constructing numeric invariants from the computation of a multidimensional persistence module as given by carlsson , singh , and zomorodian
in this paper , the sum capacity of the gaussian multiple input multiple output \( mimo \) cognitive radio channel \( mcc \) is expressed as a convex problem with finite number of linear constraints , allowing for polynomial time interior point techniques to find the solution in addition , a specialized class of sum power iterative waterfilling algorithms is determined that exploits the inherent structure of the sum capacity problem these algorithms not only determine the maximizing sum capacity value , but also the transmit policies that achieve this optimum the paper concludes by providing numerical results which demonstrate that the algorithm takes very few iterations to converge to the optimum
automatic text summarization is widely regarded as the highly difficult problem , partially because of the lack of large text summarization data set due to the great challenge of constructing the large scale summaries for full text , in this paper , we introduce a large corpus of chinese short text summarization dataset constructed from the chinese microblogging website sina weibo , which will be released to public soon this corpus consists of over 2 million real chinese short texts with short summaries given by the writer of each text we also manually tagged the relevance of 10 , 666 short summaries with their corresponding short texts based on the corpus , we introduce recurrent neural network for the summary generation and achieve promising results , which not only shows the usefulness of the proposed corpus for short text summarization research , but also provides a baseline for further research on this topic
our main technical result is that , in the coset leader graph of a linear binary code of block length n , the metric balls spanned by constant weight vectors grow exponentially slower than those in 0 , 1 n following the approach of friedman and tillich \( 2006 \) , we use this fact to improve on the first linear programming bound on the rate of ldpc codes , as the function of their minimal distance this improvement , combined with the techniques of ben haim and lytsin \( 2006 \) , improves the rate vs distance bounds for ldpc codes in a significant sub range of relative distances
in the problem of matrix compressed sensing we aim to recover a low rank matrix from few of its element wise linear projections in this contribution we analyze the asymptotic performance of a bayes optimal inference procedure for a model where the matrix to be recovered is a product of random matrices the results that we obtain using the replica method describe the state evolution of the recently introduced p big amp algorithm we show the existence of different types of phase transitions , their implications for the solvability of the problem , and we compare the results of the theoretical analysis to the performance reached by p big amp remarkably the asymptotic replica equations for matrix compressed sensing are the same as those for a related but formally different problem of matrix factorization
this paper proposes a new perspective on the problem of multidimensional spectral factorization , through helical mapping d dimensional \( d d \) data arrays are vectorized , processed by 1 d cepstral analysis and then remapped onto the original space partial differential equations \( pdes \) are the basic framework to describe the evolution of physical phenomena we observe that the minimum phase helical solution asymptotically converges to the d d semi causal solution , and allows to decouple the two solutions arising from pdes describing physical systems we prove this equivalence in the theoretical framework of cepstral analysis , and we also illustrate the validity of helical factorization through a 2 d wave propagation example and a 3 d application to helioseismology
linear arithmetic extended with free predicate symbols is undecidable , in general we show that the restriction of linear arithmetic inequations to simple bounds extended with the bernays sch onfinkel ramsey free first order fragment is decidable and nexptime complete the result is almost tight because the bernays sch onfinkel ramsey fragment is undecidable in combination with linear difference inequations , simple additive inequations , quotient inequations and multiplicative inequations
to ensure reliable communication in randomly varying and error prone channels , wireless systems use adaptive modulation and coding \( amc \) as well as hybrid arq \( harq \) in order to elucidate their compatibility and interaction , we compare the throughput provided by amc , harq , and their combination \( amc harq \) under two operational conditions in slow and fast block fading channels considering both , incremental redundancy harq \( harq ir \) and repetition redundancy harq \( harq rr \) we optimize the rate decision regions for amc harq and compare them in terms of attainable throughput under a fairly general model of the channel variation and the decoding functions , we conclude that i \) adding harq on top of amc may be counterproductive in the high average signal to noise ratio regime for fast fading channels , and ii \) harq is useful for slow fading channels , but it provides moderate throughput gains we provide explanations for these results which allow us to propose paths to improve amc harq systems
promise problems were mainly studied in quantum automata theory here we focus on state complexity of classical automata for promise problems first , it was known that there is a family of unary promise problems solvable by quantum automata by using a single qubit , but the number of states required by corresponding one way deterministic automata cannot be bounded by a constant for this family , we show that even two way nondeterminism does not help to save a single state by comparing this with the corresponding state complexity of alternating machines , we then get a tight exponential gap between two way nondeterministic and one way alternating automata solving unary promise problems second , despite of the existing quadratic gap between las vegas realtime probabilistic automata and one way deterministic automata for language recognition , we show that , by turning to promise problems , the tight gap becomes exponential last , we show that the situation is different for one way probabilistic automata with two sided bounded error we present a family of unary promise problems that is very easy for these machines solvable with only two states , but the number of states in two way alternating or any simpler automata is not limited by a constant moreover , we show that one way bounded error probabilistic automata can solve promise problems not solvable at all by any other classical model
deep neural networks are commonly trained using stochastic non convex optimization procedures , which are driven by gradient information estimated on fractions \( batches \) of the dataset while it is commonly accepted that batch size is an important parameter for offline tuning , the benefits of online selection of batches remain poorly understood we investigate online batch selection strategies for two state of the art methods of stochastic gradient based optimization , adadelta and adam as the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints , intuitively , datapoints with the greatest loss should be considered \( selected in a batch \) more frequently however , the limitations of this intuition and the proper control of the selection pressure over time are open questions we propose a simple strategy where all datapoints are ranked w r t their latest known loss value and the probability to be selected decays exponentially as a function of rank our experimental results on the mnist dataset suggest that selecting batches speeds up both adadelta and adam by a factor of about 5
mastermind is in essence a search problem in which a string of symbols that is kept secret must be found by sequentially playing strings that use the same alphabet , and using the responses that indicate how close are those other strings to the secret one as hints although it is commercialized as a game , it is a combinatorial problem of high complexity , with applications on fields that range from computer security to genomics as such a kind of problem , there are no exact solutions even exhaustive search methods rely on heuristics to choose , at every step , strings to get the best possible hint these methods mostly try to play the move that offers the best reduction in search space size in the next step this move is chosen according to an empirical score however , in this paper we will examine several state of the art exhaustive search methods and show that another factor , the presence of the actual solution among the candidate moves , or , in other words , the fact that the actual solution has the highest score , plays also a very important role using that , we will propose new exhaustive search approaches that obtain results which are comparable to the classic ones , and besides , are better suited as a basis for non exhaustive search strategies such as evolutionary algorithms , since their behavior in a series of key indicators is better than the classical algorithms
in order to approximate transandental functions , several algorithms were proposed historically , polynomial interpolation , infinite series , cdots and other , times , and based algorithms were studied for this purpose the cordic \( coordinate rotation digital computer \) introduced by jack e volder in 1959 , and generalized by j s walther a few years later , is a hardware based algorithmfor the approximation of trigonometric , hyperbolic andlogarithmic functions as a consequence , cordic is used for applications indiverse areas such as signal and image processing for these reasons , several modified versions were proposed in this article , we present anoverview of the cordic algorithm for the computation of the circular functions , essentially the scaling free version , and we will give a substential improvement to the commonly used one
the prosperity of location based social networking services enables geo social group queries for group based activity planning and marketing this paper proposes a new family of geo social group queries with minimum acquaintance constraint \( gsgqs \) , which are more appealing than existing geo social group queries in terms of producing a cohesive group that guarantees the worst case acquaintance level gsgqs , also specified with various spatial constraints , are more complex than conventional spatial queries particularly , those with a strict k nn spatial constraint are proved to be np hard for efficient processing of general gsgq queries on large location based social networks , we devise two social aware index structures , namely sar tree and sar tree the latter features a novel clustering technique that considers both spatial and social factors based on sar tree and sar tree , efficient algorithms are developed to process various gsgqs extensive experiments on real world gowalla and dianping datasets show that our proposed methods substantially outperform the baseline algorithms based on r tree
low density parity check \( ldpc \) codes are capable of achieving excellent performance and provide a useful alternative for high performance applications however , at medium to high signal to noise ratios \( snr \) , an observable error floor arises from the loss of independence of messages passed under iterative graph based decoding in this paper , the error floor performance of short block length codes is improved by use of a novel candidate selection metric in code graph construction the proposed multipath emd approach avoids harmful structures in the graph by evaluating certain properties of the cycles which may be introduced in each edge placement we present multipath emd based designs for several structured ldpc codes including quasi cyclic and irregular repeat accumulate codes in addition , an extended class of diversity achieving codes on the challenging block fading channel is proposed and considered with the multipath emd design this combined approach is demonstrated to provide gains in decoder convergence and error rate performance a simulation study evaluates the performance of the proposed and existing state of the art methods
in recent work on both generative and discriminative score to log likelihood ratio calibration , it was shown that linear transforms give good accuracy only for a limited range of operating points moreover , these methods required tailoring of the calibration training objective functions in order to target the desired region of best accuracy here , we generalize the linear recipes to non linear ones we experiment with a non linear , non parametric , discriminative pav solution , as well as parametric , generative , maximum likelihood solutions that use gaussian , student 's t and normal inverse gaussian score distributions experiments on nist sre'12 scores suggest that the non linear methods provide wider ranges of optimal accuracy and can be trained without having to resort to objective function tailoring
the problem of distributed controller synthesis for formation control of multi agent systems is considered the agents \( single integrators \) communicate over a communication graph and a decentralized linear feedback structure is assumed one of the agents is designated as the leader if the communication graph contains a directed spanning tree with the leader node as the root , then it is possible to place the poles of the ensemble system with purely local feedback controller gains given a desired formation , first one of the poles is placed at the origin then it is shown that the inter agent weights can be independently adjusted to assign an eigenvector corresponding to the formation positions , to the zero eigenvalue then , only the leader input is enough to bring the agents to the desired formation and keep it there with no further inputs moreover , given a formation , the computation of the inter agent weights that encode the formation information , can be calculated in a decentralized fashion using only local information
in this paper we present nlomj a natural language object model in java with english as the experiment language this modal describes the grammar elements of any permissible expression in a natural language and their complicated relations with each other with the concept object in oop \( object oriented programming \) directly mapped to the syntax and semantics of the natural language , it can be used in information retrieval as a linguistic method around the uml diagram of the nlomj the important classes \( sentence , clause and phrase \) and their sub classes are introduced and their syntactic and semantic meanings are explained
this article considers stochastic algorithms for efficiently solving a class of large scale non linear least squares \( nls \) problems which frequently arise in applications we propose eight variants of a practical randomized algorithm where the uncertainties in the major stochastic steps are quantified such stochastic steps involve approximating the nls objective function using monte carlo methods , and this is equivalent to the estimation of the trace of corresponding symmetric positive semi definite \( spsd \) matrices for the latter , we prove tight necessary and sufficient conditions on the sample size \( which translates to cost \) to satisfy the prescribed probabilistic accuracy we show that these conditions are practically computable and yield small sample sizes they are then incorporated in our stochastic algorithm to quantify the uncertainty in each randomized step the bounds we use are applications of more general results regarding extremal tail probabilities of linear combinations of gamma distributed random variables we derive and prove new results concerning the maximal and minimal tail probabilities of such linear combinations , which can be considered independently of the rest of this paper
in this paper we present a class of linear whitening filters termed linear extended whitening filters \( ewfs \) which are whitening filters that have desirable secondary properties and can be used for simplifying algorithms , or achieving desired side effects on given secondary matrices , random vectors or random processes further , we present an application of ewfs for simplification of qr decomposition based ml detection algorithm in wireless communication
this paper considers global optimization with a black box unknown objective function that can be non convex and non differentiable such a difficult optimization problem arises in many real world applications , such as parameter tuning in machine learning , engineering design problem , and planning with a complex physics simulator this paper proposes a new global optimization algorithm , called locally oriented global optimization \( logo \) , to aim for both fast convergence in practice and finite time error bound in theory the advantage and usage of the new algorithm are illustrated via theoretical analysis and an experiment conducted with 11 benchmark test functions further , we modify the logo algorithm to specifically solve a planning problem via policy search with continuous state action space and long time horizon while maintaining its finite time error bound we apply the proposed planning method to accident management of a nuclear power plant the result of the application study demonstrates the practical utility of our method
in this work we study the critical behavior of a three state \( 1 , 1 , 0 \) opinion model with independence each agent has a probability q to act as independent , i e , he she can choose his her opinion independently of the opinions of the other agents on the other hand , with the complementary probability 1 q the agent interacts with a randomly chosen individual through a kinetic exchange our analytical and numerical results show that the independence mechanism acts as a noise that induces an order disorder transition at critical points q c that depend on the individuals' flexibility for a special value of this flexibility the system undergoes a transition to an absorbing state with all opinions 0
vpsolver is a vector packing solver based on an arc flow formulation with graph compression in this paper , we present the algorithm introduced in vpsolver 3 0 0 for building compressed arc flow models for the multiple choice vector packing problem
in this paper , we analyze web downloaded data on people sharing their music library by attributing to each music group usual music genres \( rock , pop \) , and analysing correlations between music groups of different genres with percolation idea based methods , we probe the reality of these subdivisions and construct a music genre cartography , with a tree representation we also show the diversity of music genres with shannon entropy arguments , and discuss an alternative objective way to classify music , that is based on the complex structure of the groups audience finally , a link is drawn with the theory of hidden variables in complex networks
an information theoretic analysis of a multi keyhole channel , which includes a number of statistically independent keyholes with possibly different correlation matrices , is given when the number of keyholes or and the number of tx rx antennas is large , there is an equivalent rayleigh fading channel such that the outage capacities of both channels are asymptotically equal in the case of a large number of antennas and for a broad class of fading distributions , the instantaneous capacity is shown to be asymptotically gaussian in distribution , and compact , closed form expressions for the mean and variance are given motivated by the asymptotic analysis , a simple , full ordering scalar measure of spatial correlation and power imbalance in mimo channels is introduced , which quantifies the negative impact of these two factors on the outage capacity in a simple and well tractable way it does not require the eigenvalue decomposition , and has the full ordering property the size asymptotic results are used to prove telatar 's conjecture for semi correlated multi keyhole and rayleigh channels since the keyhole channel model approximates well the relay channel in the amplify and forward mode in certain scenarios , these results also apply to the latter
the damped gauss newton \( dgn \) algorithm for candecomp parafac \( cp \) decomposition can handle the challenges of collinearity of factors and different magnitudes of factors nevertheless , for factorization of an n d tensor of size i 1 times i n with rank r , the algorithm is computationally demanding due to construction of large approximate hessian of size \( rt times rt \) and its inversion where t sum n i n in this paper , we propose a fast implementation of the dgn algorithm which is based on novel expressions of the inverse approximate hessian in block form the new implementation has lower computational complexity , besides computation of the gradient \( this part is common to both methods \) , requiring the inversion of a matrix of size nr 2 times nr 2 , which is much smaller than the whole approximate hessian , if t gg nr in addition , the implementation has lower memory requirements , because neither the hessian nor its inverse never need to be stored in their entirety a variant of the algorithm working with complex valued data is proposed as well complexity and performance of the proposed algorithm is compared with those of dgn and als with line search on examples of difficult benchmark tensors
we consider the problem of reconstructing wideband frequency spectra from distributed , compressive measurements the measurements are made by a network of nodes , each independently mixing the ambient spectra with low frequency , random signals the reconstruction takes place via local transmissions between nodes , each performing simple statistical operations such as ridge regression and shrinkage
in this work , we propose a novel and efficient method for articulated human pose estimation in videos using a convolutional network architecture , which incorporates both color and motion features we propose a new human body pose dataset , flic motion , that extends the flic dataset with additional motion features we apply our architecture to this dataset and report significantly better performance than current state of the art pose detection systems
brlek et al conjectured in 2008 that any fixed point of a primitive morphism with finite palindromic defect is either periodic or its palindromic defect is zero bucci and vaslet disproved this conjecture in 2012 by a counterexample over ternary alphabet we prove that the conjecture is valid on binary alphabet we also describe a class of morphisms over multiliteral alphabet for which the conjecture still holds
let integers r ge 2 and d ge 3 be fixed let cal g d be the set of graphs with no induced path on d vertices we study the problem of packing k vertex disjoint copies of k 1 , r \( k ge 2 \) into a graph g from parameterized preprocessing , i e , kernelization , point of view we show that every graph g in cal g d can be reduced , in polynomial time , to a graph g' in cal g d with o \( k \) vertices such that g has at least k vertex disjoint copies of k 1 , r if and only if g' has such a result is known for arbitrary graphs g when r 2 and we conjecture that it holds for every r ge 2
we investigate a natural heyting algebra structure on the set of dyck paths of the same length we provide a geometrical description of the operations of pseudocomplement and relative pseudocomplement , as well as of regular elements we also find a logic theoretic interpretation of such heyting algebras , which we call dyck algebras , by showing that they are the algebraic counterpart of a certain fragment of a classical interval temporal logic \( also known as halpern shoham logic \) finally , we propose a generalization of our approach , suggesting a similar study of the heyting algebra arising from the poset of intervals of a finite poset using birkh off duality in order to illustrate this , we show how several combinatorial parameters of dyck paths can be expressed in terms of the heyting algebra structure of dyck algebras together with a certain total order on the set of atoms of each dyck algebra
we introduce a simple and effective method for regularizing large convolutional neural networks we replace the conventional deterministic pooling operations with a stochastic procedure , randomly picking the activation within each pooling region according to a multinomial distribution , given by the activities within the pooling region the approach is hyper parameter free and can be combined with other regularization approaches , such as dropout and data augmentation we achieve state of the art performance on four image datasets , relative to other approaches that do not utilize data augmentation
subspace codes were introduced by k otter and kschischang for error control in random linear network coding in this paper , a layered type of subspace codes is considered , which can be viewed as a superposition of multiple component subspace codes exploiting the layered structure , we develop two decoding algorithms for these codes the first algorithm operates by separately decoding each component code the second algorithm is similar to the successive interference cancellation \( sic \) algorithm for conventional superposition coding , and further permits an iterative version we show that both algorithms decode not only deterministically up to but also probabilistically beyond the error correction capability of the overall code finally we present possible applications of layered subspace codes in several network coding scenarios
the purpose of this paper is twofold first , we provide a novel analysis of a baseband time reversal \( tr \) beamforming system using two propagation models commonly used in indoor wireless communications this analysis applies to pico and femtocells in conventional wideband systems such as wifi networks we derive a new closed form approximation for the inter symbol interference \( isi \) power in such scenarios without using rate back off , which leads to a more accurate estimation of the probability of bit error compared to previous works we define performance parameters for the spatial focusing and time compression properties of tr beamforming and find closed form approximations for them second , we propose an equalized tr \( etr \) technique that mitigates the isi of conventional tr etr uses a zf pre equalizer at the transmitter in cascade configuration with the tr pre filter we derive theoretical performance bounds for etr and show that it greatly enhances the performance of conventional tr with minimal impact to its beamforming capability by means of numerical simulations , we verify our closed form approximations and show that the proposed etr technique outperforms conventional tr with respect to the ber under any snr , even though the total received power is greater for conventional tr
an epileptic seizure is a transient event of abnormal excessive neuronal discharge in the brain this unwanted event can be obstructed by detection of electrical changes in the brain that happen before the seizure takes place the automatic detection of seizures is necessary since the visual screening of eeg recordings is a time consuming task and requires experts to improve the diagnosis four linear least squares based preprocessing models are proposed to extract key features of an eeg signal in order to detect seizures the first two models are newly developed the original signal \( eeg \) is approximated by a sinusoidal curve its amplitude is formed by a polynomial function and compared with the pre developed spline function different statistical measures namely classification accuracy , true positive and negative rates , false positive and negative rates and precision are utilized to assess the performance of the proposed models these metrics are derived from confusion matrices obtained from classifiers different classifiers are used over the original dataset and the set of extracted features the proposed models significantly reduce the dimension of the classification problem and the computational time while the classification accuracy is improved in most cases the first and third models are promising feature extraction methods logistic , lazyib1 , lazyib5 and j48 are the best classifiers their true positive and negative rates are 1 while false positive and negative rates are zero and the corresponding precision values are 1 numerical results suggest that these models are robust and efficient for detecting epileptic seizure
a decidability proof for bisimulation equivalence of first order grammars \( finite sets of labelled rules for rewriting roots of first order terms \) is presented the equivalence generalizes the dpda \( deterministic pushdown automata \) equivalence , and the result corresponds to the result achieved by senizergues \( 1998 , 2005 \) in the framework of equational graphs , or of pda with restricted epsilon steps the framework of classical first order terms seems particularly useful for providing a proof that should be understandable for a wider audience we also discuss an extension to branching bisimilarity , announced by fu and yin \( 2014 \)
due to the short and bursty incoming messages , channel access activities in a wireless random access system are often fractional the lack of frequent data support consequently makes it difficult for the receiver to estimate and track the time varying channel states with high precision this paper investigates random multiple access communication over a compound wireless channel where channel realization is known neither at the transmitters nor at the receiver an achievable rate and error probability tradeoff bound is derived under the non asymptotic assumption of a finite codeword length the results are then extended to the random multiple access system where the receiver is only interested in decoding messages from a user subset
the joint base station \( bs \) association and beamforming problem has been studied extensively in recent years , yet the computational complexity for even the simplest siso case has not been fully characterized in this paper , we consider the problems for an uplink siso simo cellular network under the max min fairness criterion we first prove that the problems for both the siso and simo scenarios are polynomial time solvable secondly , we present a fixed point based binary search \( bs fp \) algorithm for both siso and simo scenarios whereby a qos \( quality of service \) constrained subproblem is solved at each step by a fixed point method thirdly , we propose a normalized fixed point \( nfp \) iterative algorithm to directly solve the original problem and prove its geometric convergence to global optima although it is not known whether the nfp algorithm is a polynomial time algorithm , empirically it converges to the global optima orders of magnitude faster than the polynomial time algorithms , making it suitable for applications in huge scale networks
combinatory categorial grammar \( ccg \) is a grammar formalism used for natural language parsing ccg assigns structured lexical categories to words and uses a small set of combinatory rules to combine these categories to parse a sentence in this work we propose and implement a new approach to ccg parsing that relies on a prominent knowledge representation formalism , answer set programming \( asp \) a declarative programming paradigm we formulate the task of ccg parsing as a planning problem and use an asp computational tool to compute solutions that correspond to valid parses compared to other approaches , there is no need to implement a specific parsing algorithm using such a declarative method our approach aims at producing all semantically distinct parse trees for a given sentence from this goal , normalization and efficiency issues arise , and we deal with them by combining and extending existing strategies we have implemented a ccg parsing tool kit aspccgtk that uses asp as its main computational means the c c supertagger can be used as a preprocessor within aspccgtk , which allows us to achieve wide coverage natural language parsing
let n 2m , m odd , e m , and p odd prime with p equiv1 mathrm mod 4 let d frac \( p m 1 \) 2 2 \( p e 1 \) in this paper , we study the cross correlation between a p ary m sequence s t of period p 2m 1 and its decimation s dt our result shows that the cross correlation function is six valued and that it takes the values in 1 , pm p m 1 , frac 1 pm p frac e 2 2 p m 1 , frac \( 1 p e \) 2 p m 1 also , the distribution of the cross correlation is completely determined
plausibility models are kripke models that agents use to reason about knowledge and belief , both of themselves and of each other such models are used to interpret the notions of conditional belief , degrees of belief , and safe belief the logic of conditional belief contains that modality and also the knowledge modality , and similarly for the logic of degrees of belief and the logic of safe belief with respect to these logics , plausibility models may contain too much information a proper notion of bisimulation is required that characterises them we define that notion of bisimulation and prove the required characterisations on the class of image finite and preimage finite models \( with respect to the plausibility relation \) , two pointed kripke models are modally equivalent in either of the three logics , if and only if they are bisimilar as a result , the information content of such a model can be similarly expressed in the logic of conditional belief , or the logic of degrees of belief , or that of safe belief this , we found a surprising result still , that does not mean that the logics are equally expressive the logics of conditional and degrees of belief are incomparable , the logics of degrees of belief and safe belief are incomparable , while the logic of safe belief is more expressive than the logic of conditional belief in view of the result on bisimulation characterisation , this is an equally surprising result we hope our insights may contribute to the growing community of formal epistemology and on the relation between qualitative and quantitative modelling
two party one way quantum communication has been extensively studied in the recent literature we target the size of minimal information that is necessary for a feasible party to finish a given combinatorial task , such as distinction of instances , using one way communication from another party this type of complexity measure has been studied under various names advice complexity , kolmogorov complexity , distinguishing complexity , and instance complexity we present a general framework focusing on underlying combinatorial takes to study these complexity measures using quantum information processing we introduce the key notions of relative hardness and quantum advantage , which provide the foundations for task based quantum minimal one way information complexity theory
this letter derives some new exponential bounds for discrete time , real valued , conditionally symmetric martingales with bounded jumps the new bounds are extended to conditionally symmetric sub supermartingales , and they are compared to some existing bounds
an accurate channel estimation is crucial for the novel time domain synchronous orthogonal frequency division multiplexing \( tds ofdm \) scheme in which pseudo noise \( pn \) sequences serve as both guard intervals \( gi \) for ofdm data symbols and training sequences for synchronization channel estimation this paper studies the channel estimation method based on the cross correlation of pn sequences a theoretical analysis of this estimator is conducted and several improved estimators are then proposed to reduce the estimation error floor encountered by the pn correlation based estimator it is shown through mathematical derivations and simulations that the new estimators approach or even achieve the cramer rao bound
a perfect matching in a 3 uniform hypergraph on n 3k vertices is a subset of frac n 3 disjoint edges we prove that if h is a 3 uniform hypergraph on n 3k vertices such that every vertex belongs to at least n 1 choose 2 2n 3 choose 2 1 edges then h contains a perfect matching we give a construction to show that this result is best possible
we explain how massive instances of scale free graphs following the barabasi albert model can be generated very quickly in an embarrassingly parallel way this makes this popular model available for studying big data graph problems as a demonstration , we generated a petaedge graph in less than an hour
there are both benefits and drawbacks to cultural diversity it can lead to friction and exacerbate differences however , as with biological diversity , cultural diversity is valuable in times of upheaval if a previously effective solution no longer works , it is good to have alternatives available what factors give rise to cultural diversity \? this paper describes a preliminary investigation of this question using a computational model of cultural evolution the model is composed of neural network based agents that evolve fitter ideas for actions by \( 1 \) inventing new ideas through modification of existing ones , and \( 2 \) imitating neighbors' ideas numerical simulations indicate that the diversity of ideas in a population is positively correlated with both the proportion of creators to imitators in the population , and the rate at which creators create this is the case for both minimum and peak diversity of actions over the duration of a run
the main objectives and instruments to develop belarusian educational and research web portal of nuclear knowledge are discussed draft structure of portal is presented
it is known that theta \( log n \) chords must be added to an n cycle to produce a pancyclic graph for vertex pancyclicity , where every vertex belongs to a cycle of every length , theta \( n \) chords are required a possibly `intermediate' variation is the following given k , 1 leq k leq n , how many chords must be added to ensure that there exist cycles of every length each of which passes exactly k chords \? for fixed k , we establish a lower bound of omega big \( n 1 k big \) on the growth rate
formal concept analysis fca is a data analysis method which enables to discover hidden knowledge existing in data a kind of hidden knowledge extracted from data is association rules different quality measures were reported in the literature to extract only relevant association rules given a dataset , the choice of a good quality measure remains a challenging task for a user given a quality measures evaluation matrix according to semantic properties , this paper describes how fca can highlight quality measures with similar behavior in order to help the user during his choice the aim of this article is the discovery of interestingness measures im clusters , able to validate those found due to the hierarchical and partitioning clustering methods ahc and k means then , based on the theoretical study of sixty one interestingness measures according to nineteen properties , proposed in a recent study , fca describes several groups of measures
this preprint has been withdrawn by the author for revision
deep neural networks \( dnn \) have achieved state of the art results in a wide range of tasks , with the best results obtained with large training sets and large models in the past , gpus enabled these breakthroughs because of their greater computational speed in the future , faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low power devices as a result , there is much interest in research and development of dedicated hardware for deep learning \( dl \) binary weights , i e , weights which are constrained to only two possible values \( e g 1 or 1 \) , would bring great benefits to specialized dl hardware by replacing many multiply accumulate operations by simple accumulations , as multipliers are the most space and power hungry components of the digital implementation of neural networks we introduce binaryconnect , a method which consists in training a dnn with binary weights during the forward and backward propagations , while retaining precision of the stored weights in which gradients are accumulated like other dropout schemes , we show that binaryconnect acts as regularizer and we obtain near state of the art results with binaryconnect on the permutation invariant mnist , cifar 10 and svhn
we derive an adaptive hierarchical method of estimating high dimensional probability density functions we call this method of density estimation the adaptive cluster expansion or ace for short we present an application of this approach , based on a multilayer topographic mapping network , that adaptively estimates the joint probability density function of the pixel values of an image , and presents this result as a probability image we apply this to the problem of identifying statistically anomalous regions in otherwise statistically homogeneous images
given the output of a data source taking values in a finite alphabet , we wish to detect change points , that is times when the statistical properties of the source change motivated by ideas of match lengths in information theory , we introduce a novel non parametric estimator which we call creche \( crossings enumeration change estimator \) we present simulation evidence that this estimator performs well , both for simulated sources and for real data formed by concatenating text sources for example , we show that we can accurately detect the point at which a source changes from a markov chain to an iid source with the same stationary distribution our estimator requires no assumptions about the form of the source distribution , and avoids the need to estimate its probabilities further , we establish consistency of the creche estimator under a related toy model , by establishing a fluid limit and using martingale arguments
using the recently developed framework of daniely et al , 2014 , we show that under a natural assumption on the complexity of refuting random k sat formulas , learning dnf formulas is hard furthermore , the same assumption implies the hardness of learning intersections of omega \( log \( n \) \) halfspaces , agnostically learning conjunctions , as well as virtually all \( distribution free \) learning problems that were previously shown hard \( under complexity assumptions \)
we give upper and lower bounds on the amount of quantum communication required to perform the task of quantum state redistribution in a one shot setting in quantum state redistribution as considered in luo and devetak \( 2009 \) and devetak and yard \( 2008 \) , there are 4 systems of interest the a system held by alice , the b system held by bob , the c system that is to be transmitted from alice to bob , and the r system that holds a purification of the state in the abc registers our bounds are in terms of smooth conditional min and max entropies , and the smooth max information the protocol for the upper bound has a clear structure , building on the work oppenheim \( 2008 \) it decomposes the quantum state redistribution task into two simpler quantum state merging tasks by introducing a coherent relay there remains a gap between our upper and lower bounds this gap vanishes in the independent and identical \( iid \) asymptotic limit and the remaining terms can then be rewritten as a quantum conditional mutual information , thus yielding an alternative proof of optimality of this communication rate for iid asymptotic quantum state redistribution
prolog server pages \( psp \) is a scripting language , based on prolog , than can be embedded in html documents to run psp applications one needs a web server , a web browser and a psp interpreter the code is executed , by the interpreter , on the server side \( web server \) and the output \( together with the html code in witch the psp code is embedded \) is sent to the client side \( browser \) the current implementation supports apache web server we implemented an apache web server module that handles psp files , and sends the result \( an html document \) to the client psp supports both get and post http requests it also provides methods for working with http cookies
a reply to the commentaries of yana \( 2013 \) , and some jots on information theory
in this paper , we propose a cross layer scheduling algorithm that achieves a throughput epsilon close to the optimal throughput in multi hop wireless networks with a tradeoff of o \( 1 epsilon \) in delay guarantees the algorithm aims to solve a joint congestion control , routing , and scheduling problem in a multi hop wireless network while satisfying per flow average end to end delay guarantees and minimum data rate requirements this problem has been solved for both backlogged as well as arbitrary arrival rate systems moreover , we discuss the design of a class of low complexity suboptimal algorithms , the effects of delayed feedback on the optimal algorithm , and the extensions of the proposed algorithm to different interference models with arbitrary link capacities
this paper considers the problem of secret key agreement with public discussion subject to a peak power constraint a on the channel input the optimal input distribution is proved to be discrete with finite support the result is obtained by first transforming the secret key channel model into an equivalent gaussian wiretap channel with better noise statistics at the legitimate receiver and then using the fact that the optimal distribution of the gaussian wiretap channel is discrete to overcome the computationally heavy search for the optimal discrete distribution , several suboptimal schemes are proposed and shown numerically to perform close to the capacity moreover , lower and upper bounds for the secret key capacity are provided and used to prove that the secret key capacity converges for asymptotic high values of a , to the secret key capacity with an average power constraint a 2 finally , when the amplitude constraint a is small \( a to 0 \) , the secret key capacity is proved to be asymptotically equal to the capacity of the legitimate user with an amplitude constraint a and no secrecy constraint
lookahead search is perhaps the most natural and widely used game playing strategy given the practical importance of the method , the aim of this paper is to provide a theoretical performance examination of lookahead search in a wide variety of applications to determine a strategy play using lookahead search , each agent predicts multiple levels of possible re actions to her move \( via the use of a search tree \) , and then chooses the play that optimizes her future payoff accounting for these re actions there are several choices of optimization function the agents can choose , where the most appropriate choice of function will depend on the specifics of the actual game we illustrate this in our examples furthermore , the type of search tree chosen by computationally constrained agent can vary we focus on the case where agents can evaluate only a bounded number , k , of moves into the future that is , we use depth k search trees and call this approach em k lookahead search we apply our method in five well known settings adword auctions industrial organization \( cournot 's model \) congestion games valid utility games and basic utility games cost sharing network design games we consider two questions first , what is the expected social quality of outcome when agents apply lookahead search \? second , what interactive behaviours can be exhibited when players use lookahead search \?
ontologies are built on systems that conceptually evolve over time in addition , techniques and languages for building ontologies evolve too this has led to numerous studies in the field of ontology versioning and ontology evolution this paper presents a new way to manage the lifecycle of an ontology incorporating both versioning tools and evolution process this solution , called versiongraph , is integrated in the source ontology since its creation in order to make it possible to evolve and to be versioned change management is strongly related to the model in which the ontology is represented therefore , we focus on the owl language in order to take into account the impact of the changes on the logical consistency of the ontology like specified in owl dl
the grundy number of a graph g is the maximum number k of colors used to color the vertices of g such that the coloring is proper and every vertex x colored with color i , is adjacent to \( i 1 \) vertices colored with each color j , in this paper we give bounds for the grundy number of some graphs and cartesian products of graphs in particular , we determine an exact value of this parameter for n dimensional meshes and some n dimensional toroidal meshes finally , we present an algorithm to generate all graphs for a given grundy number
the dialogue model learning environment supports an engineering oriented approach towards dialogue modelling for a spoken language interface major steps towards dialogue models is to know about the basic units that are used to construct a dialogue model and possible sequences in difference to many other approaches a set of dialogue acts is not predefined by any theory or manually during the engineering process , but is learned from data that are available in an avised spoken dialogue system the architecture is outlined and the approach is applied to the domain of appointment scheduling even though based on a word correctness of about 70 predictability of dialogue acts in dia mole turns out to be comparable to human assigned dialogue acts
cots based development is a component reuse approach promising to reduce costs and risks , and ensure higher quality the growing availability of cots components on the web has concretized the possibility of achieving these objectives in this multitude , a recurrent problem is the identification of the cots components that best satisfy the user requirements finding an adequate cots component implies searching among heterogeneous descriptions of the components within a broad search space thus , the use of search engines is required to make more efficient the cots components identification in this paper , we investigate , theoretically and empirically , the cots component search performance of eight software component search engines , nine semantic search engines and a conventional search engine \( google \) our empirical evaluation is conducted with respect to precision and normalized recall we defined ten queries for the assessed search engines these queries were carefully selected to evaluate the capability of each search engine for handling cots component identification
let g be a unit disk graph in the plane defined by n disks whose positions are known for the case when g is unweighted , we give a simple algorithm to compute a shortest path tree from a given source in o \( n log n \) time for the case when g is weighted , we show that a shortest path tree from a given source can be computed in o \( n 1 varepsilon \) time , improving the previous best time bound of o \( n 4 3 varepsilon \)
let \( g , t \) be an instance of the \( vertex \) multiway cut problem where g is a graph and t is a set of terminals for t in t , a set of nonterminal vertices separating t from t setminus t is called an emph isolating cut of t the largest among all the smallest isolating cuts is a natural lower bound for a multiway cut of \( g , t \) denote this lower bound by m and let k be an integer in this paper we propose an o \( kn k 3 \) algorithm that computes a multiway cut of \( g , t \) of size at most m k or reports that there is no such multiway cut the core of the proposed algorithm is the following combinatorial result let g be a graph and let x , y be two disjoint subsets of vertices of g let m be the smallest size of a vertex x y separator then , for the given integer k , the number of emph important x y separators cite marxtcs of size at most m k is at most sum i 0 k n choose i
in this work , two types of codes such that they both dominate and locate the vertices of a graph are studied those codes might be sets of detectors in a network or processors controlling a system whose set of responses should determine a malfunctioning processor or an intruder here , we present our more significant contributions on lambda codes and eta codes concerning concerning bounds , extremal values and realization theorems
we consider the task of generative dialogue modeling for movie scripts to this end , we extend the recently proposed hierarchical recurrent encoder decoder neural network and demonstrate that this model is competitive with state of the art neural language models and backoff n gram models we show that its performance can be improved considerably by bootstrapping the learning from a larger question answer pair corpus and from pretrained word embeddings
this work considers load balance control among the relays under the secure transmission protocol via relay cooperation in two hop wireless networks without the information of both eavesdropper channels and locations the available two hop secure transmission protocols in physical layer secrecy framework cannot provide a flexible load balance control , which may significantly limit their application scopes this paper proposes a secure transmission protocol in case that the path loss is identical between all pairs of nodes , in which the relay is randomly selected from the first k preferable assistant relays this protocol enables load balance among relays to be flexibly controlled by a proper setting of the parameter k , and covers the available works as special cases , like ones with the optimal relay selection \( k 1 \) and ones with the random relay selection \( k n , i e the number of system nodes \) the theoretic analysis is further provided to determine the maximum number of eavesdroppers one network can tolerate by applying the proposed protocol to ensure a desired performance in terms of the secrecy outage probability and transmission outage probability
this paper presents an information theoretic approach to address the phasor measurement unit \( pmu \) placement problem in electric power systems different from the conventional 'topological observability' based approaches , this paper advocates a much more refined , information theoretic criterion , namely the mutual information \( mi \) between the pmu measurements and the power system states the proposed mi criterion can not only include the full system observability as a special case , but also can rigorously model the remaining uncertainties in the power system states with pmu measurements , so as to generate highly informative pmu configurations further , the mi criterion can facilitate robust pmu placement by explicitly modeling probabilistic pmu outages we propose a greedy pmu placement algorithm , and show that it achieves an approximation ratio of \( 1 1 e \) for any pmu placement budget we further show that the performance is the best that one can achieve in practice , in the sense that it is np hard to achieve any approximation ratio beyond \( 1 1 e \) such performance guarantee makes the greedy algorithm very attractive in the practical scenario of multi stage installations for utilities with limited budgets finally , simulation results demonstrate near optimal performance of the proposed pmu placement algorithm
while the efficiency of mimo transmissions in a rich scattering environment has been demonstrated , less is known about the situation where the fading matrix coefficients come from a line of sight model in this paper , we study in detail how this line of sight assumption affects the performance of distributed mimo transmissions between far away clusters of nodes in a wireless network our analysis pertains to the study of a new class of random matrices
the problem of quickest detection of an anomalous process among m processes is considered at each time , a subset of the processes can be observed , and the observations from each chosen process follow two different distributions , depending on whether the process is normal or abnormal the objective is a sequential search strategy that minimizes the expected detection time subject to an error probability constraint this problem can be considered as a special case of active hypothesis testing first considered by chernoff in 1959 where a randomized strategy , referred to as the chernoff test , was proposed and shown to be asymptotically \( as the error probability approaches zero \) optimal for the special case considered in this paper , we show that a simple deterministic test achieves asymptotic optimality and offers better performance in the finite regime we further extend the problem to the case where multiple anomalous processes are present in particular , we examine the case where only an upper bound on the number of anomalous processes is known
in this paper we develop a statistical mechanical interpretation of the noiseless source coding scheme based on an absolutely optimal instantaneous code the notions in statistical mechanics such as statistical mechanical entropy , temperature , and thermal equilibrium are translated into the context of noiseless source coding especially , it is discovered that the temperature 1 corresponds to the average codeword length of an instantaneous code in this statistical mechanical interpretation of noiseless source coding scheme this correspondence is also verified by the investigation using box counting dimension using the notion of temperature and statistical mechanical arguments , some information theoretic relations can be derived in the manner which appeals to intuition
this paper briefly characterizes the field of cognitive computing as an exemplification , the field of natural language question answering is introduced together with its specific challenges a possibility to master these challenges is illustrated by a detailed presentation of the loganswer system , which is a successful representative of the field of natural language question answering
a new lower bound on the minimum hamming distance of linear quasi cyclic codes over finite fields is proposed it is based on spectral analysis and generalizes the semenov trifonov bound in a similar way as the hartmann tzeng bound extends the bch approach for cyclic codes furthermore , a syndrome based algebraic decoding algorithm is given
most of the complex social , technological and biological networks have a significant community structure therefore the community structure of complex networks has to be considered as a universal property , together with the much explored small world and scale free properties of these networks despite the large interest in characterizing the community structures of real networks , not enough attention has been devoted to the detection of universal mechanisms able to spontaneously generate networks with communities triadic closure is a natural mechanism to make new connections , especially in social networks here we show that models of network growth based on simple triadic closure naturally lead to the emergence of community structure , together with fat tailed distributions of node degree , high clustering coefficients communities emerge from the initial stochastic heterogeneity in the concentration of links , followed by a cycle of growth and fragmentation communities are the more pronounced , the sparser the graph , and disappear for high values of link density and randomness in the attachment procedure by introducing a fitness based link attractivity for the nodes , we find a novel phase transition , where communities disappear for high heterogeneity of the fitness distribution , but a new mesoscopic organization of the nodes emerges , with groups of nodes being shared between just a few superhubs , which attract most of the links of the system
we address an important problem in sequence to sequence \( seq2seq \) learning referred to as copying , in which certain segments in the input sequence are selectively replicated in the output sequence a similar phenomenon is observable in human language communication for example , humans tend to repeat entity names or even long phrases in conversation the challenge with regard to copying in seq2seq is that new machinery is needed to decide when to perform the operation in this paper , we incorporate copying into neural network based seq2seq learning and propose a new model called copynet with encoder decoder structure copynet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub sequences in the input sequence and put them at proper places in the output sequence our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of copynet for example , copynet can outperform regular rnn based model with remarkable margins on text summarization tasks
we study the problem of eliminating recursion from monadic datalog programs on trees with an infinite set of labels we show that the boundedness problem , i e , determining whether a datalog program is equivalent to some nonrecursive one is undecidable but the decidability is regained if the descendant relation is disallowed under similar restrictions we obtain decidability of the problem of equivalence to a given nonrecursive program we investigate the connection between these two problems in more detail
the exact average complexity analysis of the basic sphere decoder for general space time codes applied to multiple input multiple output \( mimo \) wireless channel is known to be difficult in this work , we shed the light on the computational complexity of sphere decoding for the quasi static , lattice space time \( last \) coded mimo channel specifically , we drive an upper bound of the tail distribution of the decoder 's computational complexity we show that , when the computational complexity exceeds a certain limit , this upper bound becomes dominated by the outage probability achieved by last coding and sphere decoding schemes we then calculate the minimum average computational complexity that is required by the decoder to achieve near optimal performance in terms of the system parameters our results indicate that there exists a cut off rate \( multiplexing gain \) for which the average complexity remains bounded
we study quantum algorithms for testing bipartiteness and expansion of bounded degree graphs we give quantum algorithms that solve these problems in time o \( n \( 1 3 \) \) , beating the omega \( sqrt \( n \) \) classical lower bound for testing expansion , we also prove an omega \( n \( 1 4 \) \) quantum query lower bound , thus ruling out the possibility of an exponential quantum speedup our quantum algorithms follow from a combination of classical property testing techniques due to goldreich and ron , derandomization , and the quantum algorithm for element distinctness the quantum lower bound is obtained by the polynomial method , using novel algebraic techniques and combinatorial analysis to accommodate the graph structure
let a and b be two finite sets of points with total cardinality n , the many to many point matching with demands and capacities matches each point ai in a to at least alpha'i and at most alphai points in b , and each point bj in b to at least betaj and at most beta'j points in a for all 1 i s and 1 j t in this paper , we present an upper bound for this problem using our new polynomial time algorithm
previous studies on the invulnerability of scale free networks under edge attacks supported the conclusion that scale free networks would be fragile under selective attacks however , these studies are based on qualitative methods with obscure definitions on the robustness this paper therefore employs a quantitative method to analyze the invulnerability of the scale free networks , and uses four scale free networks as the experimental group and four random networks as the control group the experimental results show that some scale free networks are robust under selective edge attacks , different to previous studies thus , this paper analyzes the difference between the experimental results and previous studies , and suggests reasonable explanations
a new algorithm for calculating intermolecular pair forces in molecular dynamics \( md \) simulations on a distributed parallel computer is presented the arbitrary interacting cells algorithm \( aica \) is designed to operate on geometrical domains defined by an unstructured , arbitrary polyhedral mesh , which has been spatially decomposed into irregular portions for parallelisation it is intended for nano scale fluid mechanics simulation by md in complex geometries , and to provide the md component of a hybrid md continuum simulation aica has been implemented in the open source computational toolbox openfoam , and verified against a published md code
software defined networking \( sdn \) eases network management by centralizing the control plane and separating it from the data plane the separation of planes in sdn , however , introduces new vulnerabilities in sdn networks since the difference in processing packets at each plane allows an adversary to fingerprint the network 's packet forwarding logic in this paper , we study the feasibility of fingerprinting the controller switch interactions by a remote adversary , whose aim is to acquire knowledge about specific flow rules that are installed at the switches this knowledge empowers the adversary with a better understanding of the network 's packet forwarding logic and exposes the network to a number of threats in our study , we collect measurements from hosts located across the globe using a realistic sdn network comprising of openflow hardware and software switches we show that , by leveraging information from the rtt and packet pair dispersion of the exchanged packets , fingerprinting attacks on sdn networks succeed with overwhelming probability we also show that these attacks are not restricted to active adversaries , but can be equally mounted by passive adversaries that only monitor traffic exchanged with the sdn network finally , we discuss the implications of these attacks on the security of sdn networks , and we present and evaluate an efficient countermeasure to strengthen sdn networks against fingerprinting our results demonstrate the effectiveness of our countermeasure in deterring fingerprinting attacks on sdn networks
this work considers the multiple access multicast error correction scenario over a packetized network with z malicious edge adversaries the network has min cut m and packets of length ell , and each sink demands all information from the set of sources sources the capacity region is characterized for both a side channel model \( where sources and sinks share some random bits that are secret from the adversary \) and an omniscient adversarial model \( where no limitations on the adversary 's knowledge are assumed \) in the side channel adversarial model , the use of a secret channel allows higher rates to be achieved compared to the omniscient adversarial model , and a polynomial complexity capacity achieving code is provided for the omniscient adversarial model , two capacity achieving constructions are given the first is based on random subspace code design and has complexity exponential in ell m , while the second uses a novel multiple field extension technique and has o \( ell m sources \) complexity , which is polynomial in the network size our code constructions are end to end in that all nodes except the sources and sinks are oblivious to the adversaries and may simply implement predesigned linear network codes \( random or otherwise \) also , the sources act independently without knowledge of the data from other sources
orthogonal frequency division multiplexing \( ofdm \) is an attractive modulation and multiple access techniques for channels with a nonflat frequency response , as it saves the need for complex equalizers it can offer high quality performance in terms of bandwidth efficiency , robustness against multipath fading and cost effective implementation however , its main disadvantage is the high peak to average power ratio \( papr \) of the output signal as a result , a linear behavior of the system over a large dynamic range is needed and therefore the efficiency of the output amplifier is reduced in this paper , we investigate the effect of some of these sets of time waveforms on the ofdm system performance in terms of bit error rate \( ber \) we evaluate the system performance in awgn channels the obtained results indicate that the reduction in papr of the investigated methods is associated with considerable improvement in ber performance of the system , in multipath channels , as compared to conventional ofdm these promising results indicate that pulse shaping with reduced papr is an attractive solution for an ofdm system
one of the most tedious tasks in the application of machine learning is model selection , i e hyperparameter selection fortunately , recent progress has been made in the automation of this process , through the use of sequential model based optimization \( smbo \) methods this can be used to optimize a cross validation performance of a learning algorithm over the value of its hyperparameters however , it is well known that ensembles of learned models almost consistently outperform a single model , even if properly selected in this paper , we thus propose an extension of smbo methods that automatically constructs such ensembles this method builds on a recently proposed ensemble construction paradigm known as agnostic bayesian learning in experiments on 22 regression and 39 classification data sets , we confirm the success of this proposed approach , which is able to outperform model selection with smbo
a new algorithm for solving large scale convex optimization problems with a separable objective function is proposed the basic idea is to combine three techniques lagrangian dual decomposition , excessive gap and smoothing the main advantage of this algorithm is that it dynamically updates the smoothness parameters which leads to numerically robust performance the convergence of the algorithm is proved under weak conditions imposed on the original problem the rate of convergence is o \( frac 1 k \) , where k is the iteration counter in the second part of the paper , the algorithm is coupled with a dual scheme to construct a switching variant of the dual decomposition we discuss implementation issues and make a theoretical comparison numerical examples confirm the theoretical results
deep gaussian processes \( dgps \) are multi layer hierarchical generalisations of gaussian processes \( gps \) and are formally equivalent to neural networks with multiple , infinitely wide hidden layers dgps are nonparametric probabilistic models and as such are arguably more flexible , have a greater capacity to generalise , and provide better calibrated uncertainty estimates than alternative deep models this paper develops a new approximate bayesian learning scheme that enables dgps to be applied to a range of medium to large scale regression problems for the first time the new method uses an approximate expectation propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning we evaluate the new method for non linear regression on eleven real world datasets , showing that it always outperforms gp regression and is almost always better than state of the art deterministic and sampling based approximate inference methods for bayesian neural networks as a by product , this work provides a comprehensive analysis of six approximate bayesian methods for training neural networks
attribute based access control \( abac \) extends traditional access control by considering an access request as a set of pairs attribute name value , making it particularly useful in the context of open and distributed systems , where security relevant information can be collected from different sources however , abac enables attribute hiding attacks , allowing an attacker to gain some access by withholding information in this paper , we first introduce the notion of policy resistance to attribute hiding attacks we then propose the tool atrap \( automatic term rewriting for authorisation policies \) , based on the recent formal abac language ptacl , which first automatically searches for resistance counter examples using maude , and then automatically searches for an isabelle proof of resistance we illustrate our approach with two simple examples of policies and propose an evaluation of atrap performances
we investigate connections between information theoretic and estimation theoretic quantities in vector poisson channel models in particular , we generalize the gradient of mutual information with respect to key system parameters from the scalar to the vector poisson channel model we also propose , as another contribution , a generalization of the classical bregman divergence that offers a means to encapsulate under a unifying framework the gradient of mutual information results for scalar and vector poisson and gaussian channel models the so called generalized bregman divergence is also shown to exhibit various properties akin to the properties of the classical version the vector poisson channel model is drawing considerable attention in view of its application in various domains as an example , the availability of the gradient of mutual information can be used in conjunction with gradient descent methods to effect compressive sensing projection designs in emerging x ray and document classification applications
evolutionary computation algorithms are increasingly being used to solve optimization problems as they have many advantages over traditional optimization algorithms in this paper we use evolutionary computation to study the trade off between pleiotropy and redundancy in a client server based network pleiotropy is a term used to describe components that perform multiple tasks , while redundancy refers to multiple components performing one same task pleiotropy reduces cost but lacks robustness , while redundancy increases network reliability but is more costly , as together , pleiotropy and redundancy build flexibility and robustness into systems therefore it is desirable to have a network that contains a balance between pleiotropy and redundancy we explore how factors such as link failure probability , repair rates , and the size of the network influence the design choices that we explore using genetic algorithms
this paper presents two control algorithms enabling a uav to circumnavigate an unknown target using range and range rate \( i e , the derivative of range \) measurements given a prescribed orbit radius , both control algorithms \( i \) tend to drive the uav toward the tangent of prescribed orbit when the uav is outside or on the orbit , and \( ii \) apply zero control input if the uav is inside the desired orbit the algorithms differ in that , the first algorithm is smooth and unsaturated while the second algorithm is non smooth and saturated by analyzing properties associated with the bearing angle of the uav relative to the target and through proper design of lyapunov functions , it is shown that both algorithms produce the desired orbit for an arbitrary initial state three examples are provided as a proof of concept
a conceptual framework for measuring the usability characteristics of mobile learning \( m learning \) application has been developed furthermore , a software prototype for smartphones to assess usability issues of m learning applications has also been designed and implemented this prototype has been developed , using java language and the android software development kit , based on the recommended guidelines of the proposed conceptual framework the usability of the proposed model was compared to a generally available similar mobile application \( based on the blackboard \) by conducting a questionnairebased survey at western university the two models were evaluated in terms of ease of use , user satisfaction , attractiveness , and learnability the results of the questionnaire showed that the participants considered the user interface based on our proposed framework more user friendly as compared to the blackboard based user interface
reliability is one of the important measures of how well the system meets its design objective , and mathematically is the probability that a system will perform satisfactorily for at least a given period of time when the system is described by a connected network of n components \( nodes \) and their l connection \( links \) , the reliability of the system becomes a difficult network design problem which solutions are of great practical interest in science and engineering this paper discusses the numerical method of finding the most reliable network for a given n and l using genetic algorithm for a given topology of the network , the reliability is numerically computed using adjacency matrix for a search in the space of all possible topologies of the connected network with n nodes and l links , genetic operators such as mutation and crossover are applied to the adjacency matrix through a string representation in the context of graphs , the mutation of strings in genetic algorithm corresponds to the rewiring of graphs , while crossover corresponds to the interchange of the sub graphs for small networks where the most reliable network can be found by exhaustive search , genetic algorithm is very efficient for larger networks , our results not only demonstrate the efficiency of our algorithm , but also suggest that the most reliable network will have high symmetry
the similarity of the mathematical description of random field spin systems to orthogonal frequency division multiplexing \( ofdm \) scheme for wireless communication is exploited in an intercarrier interference \( ici \) canceller used in the demodulation of ofdm the translational symmetry in the fourier domain generically concentrates the major contribution of ici from each subcarrier in the subcarrier 's neighborhood this observation in conjunction with mean field approach leads to a development of an ici canceller whose necessary cost of computation scales linearly with respect to the number of subcarriers it is also shown that the dynamics of the mean field canceller are well captured by a discrete map of a single macroscopic variable , without taking the spatial and time correlations of estimated variables into account
we consider graph properties that can be checked from labels , i e , bit sequences , of logarithmic length attached to vertices we prove that there exists such a labeling for checking a first order formula with free set variables in the graphs of every class that is emph nicely locally cwd decomposable this notion generalizes that of a emph nicely locally tree decomposable class the graphs of such classes can be covered by graphs of bounded emph clique width with limited overlaps we also consider such labelings for emph bounded first order formulas on graph classes of emph bounded expansion some of these results are extended to counting queries
computer infections such as viruses and worms spread over networks of contacts between computers , with different types of networks being exploited by different types of infections here we analyze the structures of several of these networks , exploring their implications for modes of spread and the control of infection we argue that vaccination strategies that focus on a limited number of network nodes , whether targeted or randomly chosen , are in many cases unlikely to be effective an alternative dynamic mechanism for the control of contagion , called throttling , is introduced and argued to be effective under a range of conditions
drawing on a large database of publicly announced r d alliances , we empirically investigate the evolution of r d networks and the process of alliance formation in several manufacturing sectors over a 24 year period \( 1986 2009 \) our goal is to empirically evaluate the temporal and sectoral robustness of a large set of network indicators , thus providing a more complete description of r d networks with respect to the existing literature we find that most network properties are not only invariant across sectors , but also independent of the scale of aggregation at which they are observed , and we highlight the presence of core periphery architectures in explaining some properties emphasized in previous empirical studies \( e g asymmetric degree distributions and small worlds \) in addition , we show that many properties of r d networks are characterized by a rise and fall dynamics with a peak in the mid nineties we find that such dynamics is driven by mechanisms of accumulative advantage , structural homophily and multiconnectivity in particular , the change from the rise to the fall phase is associated to a structural break in the importance of multiconnectivity
multidimensional combinatorial substitutions are rules that replace symbols by finite patterns of symbols in mathbb z d we focus on the case where the patterns are not necessarily rectangular , which requires a specific description of the way they are glued together in the image by a substitution two problems can arise when defining a substitution in such a way it can fail to be consistent , and the patterns in an image by the substitution might overlap we prove that it is undecidable whether a two dimensional substitution is consistent or overlapping , and we provide practical algorithms to decide these properties in some particular cases
we consider a novel group testing procedure , termed semi quantitative group testing , motivated by a class of problems arising in genome sequence processing semi quantitative group testing \( sqgt \) is a non binary pooling scheme that may be viewed as a combination of an adder model followed by a quantizer for the new testing scheme we define the capacity and evaluate the capacity for some special choices of parameters using information theoretic methods we also define a new class of disjunct codes suitable for sqgt , termed sq disjunct codes we also provide both explicit and probabilistic code construction methods for sqgt with simple decoding algorithms
the paper introduces scholarly information retrieval \( ir \) as a further dimension that should be considered in the science modeling debate the ir use case is seen as a validation model of the adequacy of science models in representing and predicting structure and dynamics in science particular conceptualizations of scholarly activity and structures in science are used as value added search services to improve retrieval quality a co word model depicting the cognitive structure of a field \( used for query expansion \) , the bradford law of information concentration , and a model of co authorship networks \( both used for re ranking search results \) an evaluation of the retrieval quality when science model driven services are used turned out that the models proposed actually provide beneficial effects to retrieval quality from an ir perspective , the models studied are therefore verified as expressive conceptualizations of central phenomena in science thus , it could be shown that the ir perspective can significantly contribute to a better understanding of scholarly structures and activities
while previous sentiment analysis research has concentrated on the interpretation of explicitly stated opinions and attitudes , this work initiates the computational study of a type of opinion implicature \( i e , opinion oriented inference \) in text this paper described a rule based framework for representing and analyzing opinion implicatures which we hope will contribute to deeper automatic interpretation of subjective language in the course of understanding implicatures , the system recognizes implicit sentiments \( and beliefs \) toward various events and entities in the sentence , often attributed to different sources \( holders \) and of mixed polarities thus , it produces a richer interpretation than is typical in opinion analysis
compared to machines , humans are extremely good at classifying images into categories , especially when they possess prior knowledge of the categories at hand if this prior information is not available , supervision in the form of teaching images is required to learn categories more quickly , people should see important and representative images first , followed by less important images later or not at all however , image importance is individual specific , i e a teaching image is important to a student if it changes their overall ability to discriminate between classes further , students keep learning , so while image importance depends on their current knowledge , it also varies with time in this work we propose an interactive machine teaching algorithm that enables a computer to teach challenging visual concepts to a human our adaptive algorithm chooses , online , which labeled images from a teaching set should be shown to the student as they learn we show that a teaching strategy that probabilistically models the student 's ability and progress , based on their correct and incorrect answers , produces better 'experts' we present results using real human participants across several varied and challenging real world datasets
simdialog is a visual editor for dialog in computer games this paper presents the design of simdialog , illustrating how script writers and non programmers can easily create dialog for video games with complex branching structures and dynamic response characteristics the system creates dialog as a directed graph this allows for play using the dialog with a state based cause and effect system that controls selection of non player character responses and can provide a basic scoring mechanism for games
multidimensional lattice constellations which present signal space diversity \( ssd \) have been extensively studied for single antenna transmission over fading channels , with focus on their optimal design for achieving high diversity gain in this two part series of papers we present a novel combinatorial geometrical approach based on parallelotope geometry , for the performance evaluation of multidimensional finite lattice constellations with arbitrary structure , dimension and rank in part i , we present an analytical expression for the exact symbol error probability \( sep \) of multidimensional signal sets , and two novel closed form bounds , named multiple sphere lower bound \( mlsb \) and multiple sphere upper bound \( msub \) part ii extends the analysis to the transmission over fading channels , where multidimensional signal sets are commonly used to combat fading degradation numerical and simulation results show that the proposed geometrical approach leads to accurate and tight expressions , which can be efficiently used for the performance evaluation and the design of multidimensional lattice constellations , both in additive white gaussian noise \( awgn \) and fading channels
we address the problem of identifying a graph structure from the observation of signals defined on its nodes fundamentally , the unknown graph encodes direct relationships between signal elements , which we aim to recover from observable indirect relationships generated by a diffusion process on the graph the fresh look advocated here permeates benefits from convex optimization and stationarity of graph signals , in order to identify the graph shift operator \( a matrix representation of the graph \) given only its eigenvectors these spectral templates can be obtained , e g , from the sample covariance of independent graph signals diffused on the sought network the novel idea is to find a graph shift that , while being consistent with the provided spectral information , endows the network with certain desired properties such as sparsity to that end we develop efficient inference algorithms stemming from provably tight convex relaxations of natural nonconvex criteria , particularizing the results for two shifts the adjacency matrix and the normalized laplacian algorithms and theoretical recovery conditions are developed not only when the templates are perfectly known , but also when the eigenvectors are noisy or when only a subset of them are given numerical tests showcase the effectiveness of the proposed algorithms in recovering social , brain , and amino acid networks
this paper addresses the problem of non bayesian learning over multi agent networks , where agents repeatedly collect partially informative observations about an unknown state of the world , and try to collaboratively learn the true state we focus on the impact of the adversarial agents on the performance of consensus based non bayesian learning , where non faulty agents combine local learning updates with consensus primitives in particular , we consider the scenario where an unknown subset of agents suffer byzantine faults agents suffering byzantine faults behave arbitrarily two different learning rules are proposed
we study synthesis of controllers for real time systems , where the objective is to stay in a given safe set the problem is solved by obtaining winning strategies in concurrent two player emph timed automaton games with safety objectives to prevent a player from winning by blocking time , we restrict each player to strategies that ensure that the player cannot be responsible for causing a zeno run we construct winning strategies for the controller which require access only to \( 1 \) the system clocks \( thus , controllers which require their own internal infinitely precise clocks are not necessary \) , and \( 2 \) a linear \( in the number of clocks \) number of memory bits precisely , we show that a memory of size big \( 3 cdot c 1 lg \( c 1 \) big \) bits suffices for winning controller strategies for safety objectives , where c is the set of clocks of the timed automaton game , significantly improving the previous known exponential bound we also settle the open question of whether emph region strategies for controllers require memory for safety objectives by showing with an example that region strategies do require memory for safety objectives
in this paper , we show how to use stochastic approximation to compute hitting time of a stochastic process , based on the study of the time for a fluid approximation of this process to be at distance 1 n of its fixed point this approach is developed to study a generalized version of the coupon collector problem the system is composed by n independent identical markov chains at each time step , one markov chain is picked at random and performs one transition we show that the time at which all chains have hit the same state is bounded by a n log n b n log log n o \( n \) where a and b are two constants depending on eigenvalues of the markov chain
this paper addresses the assignment of transmission and sleep time slots between interfering transmitters with the objective of minimal power consumption in particular , we address the constructive alignment of discontinuous transmission \( dtx \) time slots under link rate constraints due to the complexity of the combinatorial optimization problem at hand , we resort to heuristic assignment strategies we derive four time slot alignment solutions \( sequential alignment , random alignment , p persistent ranking and dtx alignment with memory \) and identify trade offs one solution , dtx alignment with memory , addresses trade offs of the other three by maintaining memory of past alignment and channel quality to buffer short term changes in channel quality all strategies are found to exhibit similar convergence behavior , but different power consumption and retransmission probabilities dtx alignment with memory is shown to achieve up to 40 savings in power consumption and more than 20 lower retransmission probability than the state of the art
we consider the problem of regularized regression in a network of communication constrained devices each node has local data and objectives , and the goal is for the nodes to optimize a global objective we develop a distributed optimization algorithm that is based on recent work on semi stochastic proximal gradient methods our algorithm employs iteratively refined quantization to limit message size we present theoretical analysis and conditions for the algorithm to achieve a linear convergence rate finally , we demonstrate the performance of our algorithm through numerical simulations
we consider concurrent games played on graphs at every round of the game , each player simultaneously and independently selects a move the moves jointly determine the transition to a successor state two basic objectives are the safety objective ``stay forever in a set f of states'' , and its dual , the reachability objective , ``reach a set r of states'' we present in this paper a strategy improvement algorithm for computing the value of a concurrent safety game , that is , the maximal probability with which player 1 can enforce the safety objective the algorithm yields a sequence of player 1 strategies which ensure probabilities of winning that converge monotonically to the value of the safety game the significance of the result is twofold first , while strategy improvement algorithms were known for markov decision processes and turn based games , as well as for concurrent reachability games , this is the first strategy improvement algorithm for concurrent safety games second , and most importantly , the improvement algorithm provides a way to approximate the value of a concurrent safety game from below \( the known value iteration algorithms approximate the value from above \) thus , when used together with value iteration algorithms , or with strategy improvement algorithms for reachability games , our algorithm leads to the first practical algorithm for computing converging upper and lower bounds for the value of reachability and safety games
business process deviance refers to the phenomenon whereby a subset of the executions of a business process deviate , in a negative or positive way , with respect to its expected or desirable outcomes deviant executions of a business process include those that violate compliance rules , or executions that undershoot or exceed performance targets deviance mining is concerned with uncovering the reasons for deviant executions by analyzing business process event logs this article provides a systematic review and comparative evaluation of deviance mining approaches based on a family of data mining techniques known as sequence classification using real life logs from multiple domains , we evaluate a range of feature types and classification methods in terms of their ability to accurately discriminate between normal and deviant executions of a process we also analyze the interestingness of the rule sets extracted using different methods we observe that feature sets extracted using pattern mining techniques only slightly outperform simpler feature sets based on counts of individual activity occurrences in a trace
scoop is a programming model and language that allows concurrent programming at a high level of abstraction several approaches to verifying scoop programs have been proposed in the past , but none of them operate directly on the source code without modifications or annotations we propose a fully automatic approach to verifying \( a subset of \) scoop programs by translation to graph based models first , we present a graph transformation based semantics for scoop we present an implementation of the model in the state of the art model checker groove , which can be used to simulate programs and verify concurrency and consistency properties , such as the impossibility of deadlocks occurring or the absence of postcondition violations second , we present a translation tool that operates on scoop program code and generates input for the model we evaluate our approach by inspecting a number of programs in the form of case studies
researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature we show how we can use citations to produce automatically generated , readily consumable , technical extractive summaries we first propose c lexrank , a model for summarizing single scientific articles based on citations , which employs community detection and extracts salient information rich sentences next , we further extend our experiments to summarize a set of papers , which cover the same scientific topic we generate extractive summaries of a set of question answering \( qa \) and dependency parsing \( dp \) papers , their abstracts , and their citation sentences and show that citations have unique information amenable to creating a summary
with inspiration from random forests \( rf \) in the context of classification , a new clustering ensemble method cluster forests \( cf \) is proposed geometrically , cf randomly probes a high dimensional data cloud to obtain good local clusterings and then aggregates via spectral clustering to obtain cluster assignments for the whole dataset the search for good local clusterings is guided by a cluster quality measure kappa cf progressively improves each local clustering in a fashion that resembles the tree growth in rf empirical studies on several real world datasets under two different performance metrics show that cf compares favorably to its competitors theoretical analysis reveals that the kappa measure makes it possible to grow the local clustering in a desirable way it is noise resistant a closed form expression is obtained for the mis clustering rate of spectral clustering under a perturbation model , which yields new insights into some aspects of spectral clustering
we introduce two models of inclusion hierarchies random graph hierarchy \( rgh \) and limited random graph hierarchy \( lrgh \) in both models a set of nodes at a given hierarchy level is connected randomly , as in the erd h o s r ' e nyi random graph , with a fixed average degree equal to a system parameter c clusters of the resulting network are treated as nodes at the next hierarchy level and they are connected again at this level and so on , until the process cannot continue in the rgh model we use all clusters , including those of size 1 , when building the next hierarchy level , while in the lrgh model clusters of size 1 stop participating in further steps we find that in both models the number of nodes at a given hierarchy level h decreases approximately exponentially with h the height of the hierarchy h , i e the number of all hierarchy levels , increases logarithmically with the system size n , i e with the number of nodes at the first level the height h decreases monotonically with the connectivity parameter c in the rgh model and it reaches a maximum for a certain c max in the lrgh model the distribution of separate cluster sizes in the lrgh model is a power law with an exponent about 1 25 the above results follow from approximate analytical calculations and have been confirmed by numerical simulations
the wisdom of crowds is a phenomenon described in social science that suggests four criteria applicable to groups of people it is claimed that , if these criteria are satisfied , then the aggregate decisions made by a group will often be better than those of its individual members inspired by this concept , we present a novel feedback framework for the cluster ensemble problem , which we call wisdom of crowds cluster ensemble \( wocce \) although many conventional cluster ensemble methods focusing on diversity have recently been proposed , wocce analyzes the conditions necessary for a crowd to exhibit this collective wisdom these include decentralization criteria for generating primary results , independence criteria for the base algorithms , and diversity criteria for the ensemble members we suggest appropriate procedures for evaluating these measures , and propose a new measure to assess the diversity we evaluate the performance of wocce against some other traditional base algorithms as well as state of the art ensemble methods the results demonstrate the efficiency of wocce 's aggregate decision making compared to other algorithms
the paper introduces a new technique for compressing binary decision diagrams in those cases where random access is not required using this technique , compression and decompression can be done in linear time in the size of the bdd and compression will in many cases reduce the size of the bdd to 1 2 bits per node empirical results for our compression technique are presented , including comparisons with previously introduced techniques , showing that the new technique dominate on all tested instances
logics for knowledge representation suffer from over specialization while each logic may provide an ideal representation formalism for some problems , it is less than optimal for others a solution to this problem is to choose from several logics and , when necessary , combine the representations in general , such an approach results in a very difficult problem of combination however , if we can choose the logics from a uniform framework then the problem of combining them is greatly simplified in this paper , we develop such a framework for defeasible logics it supports all defeasible logics that satisfy a strong negation principle we use logic meta programs as the basis for the framework
we analyze the dispersions of distributed lossless source coding \( the slepian wolf problem \) , the multiple access channel and the asymmetric broadcast channel for the two encoder slepian wolf problem , we introduce a quantity known as the entropy dispersion matrix , which is analogous to the scalar dispersions that have gained interest recently we prove a global dispersion result that can be expressed in terms of this entropy dispersion matrix and provides intuition on the approximate rate losses at a given blocklength and error probability to gain better intuition about the rate at which the non asymptotic rate region converges to the slepian wolf boundary , we define and characterize two operational dispersions the local dispersion and the weighted sum rate dispersion the former represents the rate of convergence to a point on the slepian wolf boundary while the latter represents the fastest rate for which a weighted sum of the two rates converges to its asymptotic fundamental limit interestingly , when we approach either of the two corner points , the local dispersion is characterized not by a univariate gaussian but a bivariate one as well as a subset of off diagonal elements of the aforementioned entropy dispersion matrix finally , we demonstrate the versatility of our achievability proof technique by providing inner bounds for the multiple access channel and the asymmetric broadcast channel in terms of dispersion matrices all our proofs are unified a so called vector rate redundancy theorem which is proved using the multidimensional berry esseen theorem
tse and zdancewic have formalized the notion of noninterference for abadi et al 's dcc in terms of logical relations and given a proof of noninterference by reduction to parametricity of system f unfortunately , their proof contains errors in a key lemma that their translation from dcc to system f preserves the logical relations defined for both calculi in fact , we have found a counterexample for it in this article , instead of dcc , we prove noninterference for sealing calculus , a new variant of dcc , by reduction to the basic lemma of a logical relation for the simply typed lambda calculus , using a fully complete translation to the simply typed lambda calculus full completeness plays an important role in showing preservation of the two logical relations through the translation also , we investigate relationship among sealing calculus , dcc , and an extension of dcc by tse and zdancewic and show that the first and the last of the three are equivalent
the classical sampling nyquist shannon kotelnikov theorem states that a band limited continuous time function is uniquely defined by infinite two sided sampling series taken with a sufficient frequency the paper shows that these band limited functions allows an arbitrarily close uniform approximation by functions that are uniquely defined by their extremely sparse subsamples representing arbitrarily small fractions of one sided equidistant sample series with fixed oversampling parameter in particular , an arbitrarily small adjustment of a band limited underlying function makes redundant every \( m 1 \) members of any set of m samples for an arbitrarily large m this allows to bypass , in a certain sense , the restriction on the sampling rate defined by the critical nyquist rate
we present an unsupervised model for inducing signed social networks from the content exchanged across network edges inference in this model solves three problems simultaneously \( 1 \) identifying the sign of each edge \( 2 \) characterizing the distribution over content for each edge type \( 3 \) estimating weights for triadic features that map directly to theoretical models such as structural balance to avoid the intractable sum over possible labelings of each network , we employ noise contrastive estimation , thus obtaining the structural parameters efficiently we apply this model to a dataset of movie scripts , with the goal of learning the social function of address terms such as 'madame' , 'comrade' , and 'dude' working without any labeled data , our system offers a coherent clustering of address terms , while at the same time making intuitively plausible judgments of the formality of social relations in each film
in this pair of papers \( part i and part ii in this issue \) , we investigate the issue of power control and subcarrier assignment in a sectorized two cell downlink ofdma system impaired by multicell interference as recommended for wimax , we assume that the first part of the available bandwidth is likely to be reused by different base stations \( and is thus subject to multicell interference \) and that the second part of the bandwidth is shared in an orthogonal way between the different base stations \( and is thus protected from multicell interference \) although the problem of multicell resource allocation is nonconvex in this scenario , we provide in part i the general form of the global solution in particular , the optimal resource allocation turns out to be binary in the sense that , except for at most one pivot user in each cell , any user receives data either in the reused bandwidth or in the protected bandwidth , but not in both the determination of the optimal resource allocation essentially reduces to the determination of the latter pivot position
we train a number of neural networks to play games bowling , breakout and seaquest using information stored in the memory of a video game console atari 2600 we consider four models of neural networks which differ in size and architecture two networks which use only information contained in the ram and two mixed networks which use both information in the ram and information from the screen as the benchmark we used the convolutional model proposed in nips and received comparable results in all considered games quite surprisingly , in the case of seaquest we were able to train ram only agents which behave better than the benchmark screen only agent mixing screen and ram did not lead to an improved performance comparing to screen only and ram only agents
this paper argues that one of the most important decisions in designing and deploying censorship resistance systems is whether one set of system options should be selected \( the best \) , or whether there should be several sets of good ones we model the problem of choosing these options as a cat and mouse game and show that the best strategy depends on the value the censor associates with total system censorship versus partial , and the tolerance of false positives if the censor has a low tolerance to false positives then choosing one censorship resistance system is best otherwise choosing several systems is the better choice , but the way traffic should be distributed over the systems depends on the tolerance of the censor to false negatives we demonstrate that establishing the censor 's utility function is critical to discovering the best strategy for censorship resistance
we derive the density evolution equations for non binary low density parity check \( ldpc \) ensembles when transmission takes place over the binary erasure channel we introduce ensembles defined with respect to the general linear group over the binary field for these ensembles the density evolution equations can be written compactly the density evolution for the general linear group helps us in understanding the density evolution for codes defined with respect to finite fields we compute thresholds for different alphabet sizes for various ldpc ensembles surprisingly , the threshold is not a monotonic function of the alphabet size we state the stability condition for non binary ldpc ensembles over any binary memoryless symmetric channel we also give upper bounds on the map thresholds for various non binary ensembles based on exit curves and the area theorem
we consider the classical two encoder multiterminal source coding problem where distortion is measured under logarithmic loss we provide a single letter characterization of the achievable rate distortion region for arbitrarily correlated sources with finite alphabets in doing so , we also give the rate distortion region for the m encoder ceo problem \( also under logarithmic loss \) several applications and examples are given
a genoid is a category of two objects such that one is the product of itself with the other a genoid may be viewed as an abstract substitution algebra it is a remarkable fact that such a simple concept can be applied to present a unified algebraic approach to lambda calculus and first order logic
we study the performance of estimators of a sparse nonrandom vector based on an observation which is linearly transformed and corrupted by additive white gaussian noise using the reproducing kernel hilbert space framework , we derive a new lower bound on the estimator variance for a given differentiable bias function \( including the unbiased case \) and an almost arbitrary transformation matrix \( including the underdetermined case considered in compressed sensing theory \) for the special case of a sparse vector corrupted by white gaussian noise i e , without a linear transformation and unbiased estimation , our lower bound improves on previously proposed bounds
we give the definition of lazard and hall sets in the context of transitive factorizations of free monoids the equivalence of the two properties is proved this allows to build new effective bases of free partially commutative lie algebras the commutation graphs for which such sets exist are completely characterized and we explicit , in this context , the classical pbw rewriting process
if a graph has no induced subgraph isomorphic to h 1 or h 2 then it is said to be \( h 1 , h 2 \) free dabrowski and paulusma found 13 open cases for the question whether the clique width of \( h 1 , h 2 \) free graphs is bounded one of them is the class of \( s 1 , 2 , 2 , triangle \) free graphs in this paper we show that these graphs have bounded clique width thus , also \( p 1 2p 2 , triangle \) free graphs have bounded clique width which solves another open problem of dabrowski and paulusma
a fundamental problem in any communication system is given a communication channel between a transmitter and a receiver , how many independent signals can be exchanged between them \? arbitrary communication channels that can be described by linear compact channel operators mapping between normed spaces are examined in this paper the \( well known \) notions of degrees of freedom at level epsilon and essential dimension of such channels are developed in this general setting we argue that the degrees of freedom at level epsilon and the essential dimension fundamentally limit the number of independent signals that can be exchanged between the transmitter and the receiver we also generalise the concept of singular values of compact operators to be applicable to compact operators defined on arbitrary normed spaces which do not necessarily carry a hilbert space structure we show how these generalised singular values can be used to calculate the degrees of freedom at level epsilon and the essential dimension of compact operators that describe communication channels we describe physically realistic channels that require such general channel models
overfitting is the bane of data analysts , even when data are plentiful formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures yet the practice of data analysis is an inherently interactive and adaptive process new analyses and hypotheses are proposed after seeing the results of previous ones , parameters are tuned on the basis of obtained results , and datasets are shared and reused an investigation of this gap has recently been initiated by the authors in \( dwork et al , 2014 \) , where we focused on the problem of estimating expectations of adaptively chosen functions in this paper , we give a simple and practical method for reusing a holdout \( or testing \) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses , while provably avoiding overfitting we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment we also formalize and address the general problem of data reuse in adaptive data analysis we show how the differential privacy based approach given in \( dwork et al , 2014 \) is applicable much more broadly to adaptive data analysis we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings finally , we demonstrate that these incomparable approaches can be unified via the notion of approximate max information that we introduce
a sensor network is a collection of wireless devices that are able to monitor physical or environmental conditions these devices \( nodes \) are expected to operate autonomously , be battery powered and have very limited computational capabilities this makes the task of protecting a sensor network against misbehavior or possible malfunction a challenging problem in this document we discuss performance of artificial immune systems \( ais \) when used as the mechanism for detecting misbehavior we show that \( i \) mechanism of the ais have to be carefully applied in order to avoid security weaknesses , \( ii \) the choice of genes and their interaction have a profound influence on the performance of the ais , \( iii \) randomly created detectors do not comply with limitations imposed by communications protocols and \( iv \) the data traffic pattern seems not to impact significantly the overall performance we identified a specific mac layer based gene that showed to be especially useful for detection genes measure a network 's performance from a node 's viewpoint furthermore , we identified an interesting complementarity property of genes this property exploits the local nature of sensor networks and moves the burden of excessive communication from normally behaving nodes to misbehaving nodes these results have a direct impact on the design of ais for sensor networks and on engineering of sensor networks
we consider the problem of optimizing a nonlinear objective function over a weighted independence system presented by a linear optimization oracle we provide a polynomial time algorithm that determines an r best solution for nonlinear functions of the total weight of an independent set , where r is a constant that depends on certain frobenius numbers of the individual weights and is independent of the size of the ground set in contrast , we show that finding an optimal \( 0 best \) solution requires exponential time even in a very special case of the problem
emph couplings are a powerful mathematical tool for reasoning about pairs of probabilistic processes recent developments in formal verification identify a close connection between couplings and prhl , a relational program logic motivated by applications to provable security , enabling formal construction of couplings from the probability theory literature however , existing work using prhlmerely shows existence of a coupling and does not give a way to prove quantitative properties about the coupling , which are need to reason about mixing and convergence of probabilistic processes furthermore , prhl is inherently incomplete , and is not able to capture some advanced forms of couplings such as shift couplings we address both problems as follows first , we define an extension of prhl , called xprhl , which explicitly constructs the coupling in a prhl derivation in the form of a probabilistic product program that simulates two correlated runs of the original program existing verification tools for probabilistic programs can then be directly applied to the probabilistic product to prove quantitative properties of the coupling second , we equip prhl with a new rule for whileloops , where reasoning can freely mix synchronized and unsynchronized loop iterations our proof rule can capture examples of emph shift couplings , and the logic is relatively complete for deterministic programs we show soundness of xprhl and use it to analyze two classes of examples first , we verify rapid mixing using different tools from coupling standard coupling , shift coupling , and emph path coupling , a compositional principle for combining local couplings into a global coupling second , we verify \( approximate \) equivalence between a source and an optimized program for several instances of loop optimizations from the literature
we analyze fading interference relay networks where m single antenna source destination terminal pairs communicate concurrently and in the same frequency band through a set of k single antenna relays using half duplex two hop relaying assuming that the relays have channel state information \( csi \) , it is shown that in the large m limit , provided k grows fast enough as a function of m , the network decouples in the sense that the individual source destination terminal pair capacities are strictly positive the corresponding required rate of growth of k as a function of m is found to be sufficient to also make the individual source destination fading links converge to nonfading links we say that the network crystallizes as it breaks up into a set of effectively isolated wires in the air a large deviations analysis is performed to characterize the crystallization rate , i e , the rate \( as a function of m , k \) at which the decoupled links converge to nonfading links in the course of this analysis , we develop a new technique for characterizing the large deviations behavior of certain sums of dependent random variables for the case of no csi at the relay level , assuming amplify and forward relaying , we compute the per source destination terminal pair capacity for m , k converging to infinity , with k m staying fixed , using tools from large random matrix theory
in 1975 , carleial presented a special case of an interference channel in which the interference does not reduce the capacity of the constituent point to point gaussian channels in this work , we show that if the inequalities in the conditions that carleial stated are strict , the dispersions are similarly unaffected more precisely , in this work , we characterize the second order coding rates of the gaussian interference channel in the strictly very strong interference regime in other words , we characterize the speed of convergence of rates of optimal block codes towards a boundary point of the \( rectangular \) capacity region these second order rates are expressed in terms of the average probability of error and variances of some modified information densities which coincide with the dispersion of the \( single user \) gaussian channel we thus conclude that the dispersions are unaffected by interference in this channel model
in this paper , we study factors that influence tag reuse behavior in social tagging systems our work is guided by the activation equation of the cognitive model act r , which states that the usefulness of information in human memory depends on the three factors usage frequency , recency and semantic context it is our aim to shed light on the influence of these factors on tag reuse in our experiments , we utilize six datasets from the social tagging systems flickr , citeulike , bibsonomy , delicious , lastfm and movielens , covering a range of various tagging settings our results confirm that frequency , recency and semantic context positively influence the reuse probability of tags however , the extent to which each factor individually influences tag reuse strongly depends on the type of folksonomy present in a social tagging system our work can serve as guideline for researchers and developers of tag based recommender systems when designing algorithms for social tagging environments
we show that the best nonnegative rank r approximation of a nonnegative tensor is almost always unique and that nonnegative tensors with nonunique best nonnegative rank r approximation form a semialgebraic set contained in an algebraic hypersurface we then establish a singular vector variant of the perron frobenius theorem for positive tensors that may be used for answering various questions involving nonnegative tensor approximations in particular , we apply it to show that the best nonnegative rank 1 approximation of a positive tensor is always unique and that a best nonnegative rank r approximation of a positive tensor can almost never be obtained by deflation
in a em locally recoverable or em repairable code , any symbol of a codeword can be recovered by reading only a small \( constant \) number of other symbols the notion of local recoverability is important in the area of distributed storage where a most frequent error event is a single storage node failure \( erasure \) a common objective is to repair the node by downloading data from as few other storage node as possible in this paper , we bound the minimum distance of a code in terms of its length , size and locality unlike previous bounds , our bound follows from a significantly simple analysis and depends on the size of the alphabet being used it turns out that the binary simplex codes satisfy our bound with equality hence the simplex codes are the first example of a optimal binary locally repairable code family we also provide achievability results based on random coding and concatenated codes that are numerically verified to be close to our bounds
the performance of second order statistics \( sos \) based semi blind channel estimation in long code ds cdma systems is analyzed the covariance matrix of sos estimates is obtained in the large system limit , and is used to analyze the large sample performance of two sos based semi blind channel estimation algorithms a notion of blind estimation efficiency is also defined and is examined via simulation results
in this paper , we explore salient questions about user interests , conversations and friendships in the facebook social network , using a novel latent space model that integrates several data types a key challenge of studying facebook 's data is the wide range of data modalities such as text , network links , and categorical labels our latent space model seamlessly combines all three data modalities over millions of users , allowing us to study the interplay between user friendships , interests , and higher order network wide social trends on facebook the recovered insights not only answer our initial questions , but also reveal surprising facts about user interests in the context of facebook 's ecosystem we also confirm that our results are significant with respect to evidential information from the study subjects
an important enabler towards the successful deployment of any new element feature to the cellular network is the investigation and characterization of the operational conditions where its introduction will enhance performance even though there has been significant research activity on the potential of device to device \( d2d \) communications , there are currently no clear indications of whether d2d communications are actually able to provide benefits for a wide range of operational conditions , thus justifying their introduction to the system this paper attempts to fill this gap by taking a stochastic geometry approach on characterizing the set \( region \) of operational conditions for which d2d communications enhance performance in terms of average user rate for the practically interesting case of a heavy loaded network , the operational region is provided in closed form as a function of a variety of parameters such as maximum d2d link distances and user densities , reflecting a wide range of operational conditions \( points \) it is shown that under the appropriate deployment scheme , d2d communications can indeed be beneficial not only for the usually considered regime of proximal communications but to a wide range of operational conditions that include d2d link distances comparable to the distance to the cellular access point and considerably large user densities
consider r sensors , each one intends to send a function x i \( e g a signal or image \) to a receiver common to all r sensors before transmission , each x i is multiplied by an encoding matrix a i during transmission each a ix i gets convolved with a function h i the receiver records the function y , given by the sum of all these convolved signals assume that the receiver knowns all the a i , but does neither know the x i nor the h i when and under which conditions is it possible to recover the individual signals x i and the channels h i from just one received signal y \? this challenging problem , which intertwines blind deconvolution with blind demixing , appears in a variety of applications , such as audio processing , image processing , neuroscience , spectroscopy , and astronomy it is also expected to play a central role in connection with the future internet of things we will prove that under reasonable and practical assumptions , it is possible to solve this otherwise highly ill posed problem and recover the r transmitted functions x i and the impulse responses h i in a robust , reliable , and efficient manner from just one single received function y by solving a semidefinite program we derive explicit bounds on the number of measurements needed for successful recovery and prove that our method is robust in presence of noise our theory is actually a bit pessimistic , since numerical experiments demonstrate that , quite remarkably , recovery is still possible if the number of measurements is close to the number of degrees of freedom
we consider the problem of differentially private query release through a synthetic database approach departing from the existing approaches that require the query set to be specified in advance , we advocate to devise query set independent mechanisms , with an ambitious goal of providing accurate answers , while meeting the privacy constraints , for all queries in a general query class specifically , a differentially private mechanism is constructed to encode rich stochastic structure into the synthetic database , and customized companion estimators are then derived to provide accurate answers by making use of all available information , including the mechanism \( which is public information \) and the query functions accordingly , the distortion under the best of this kind of mechanisms at the worst case query in a general query class , so called the minimax distortion , provides a fundamental characterization of differentially private query release for the general class of statistical queries , we prove that with the squared error distortion measure , the minimax distortion is o \( 1 n \) by deriving asymptotically tight upper and lower bounds in the regime that the database size n goes to infinity the upper bound is achievable by a mechanism mathcal e and its corresponding companion estimators , which points directly to the feasibility of the proposed approach in large databases we further evaluate the mechanism mathcal e and the companion estimators through experiments on real datasets from netflix and facebook experimental results show improvement over the state of art mwem algorithm and verify the scaling behavior o \( 1 n \) of the minimax distortion
we show some new examples how limit theory can help understanding other combinatorial structures first , we generalize the manickam mikl 'os singhi conjecture , using limit theory then we introduce two limit problems of alpern 's caching game , which are good approximations of the finite game when some parameters tend to infinity with the use of these limit problems , we show a surprising result which disproves some conjectures about the finite problem
artificial immune systems have been successfully applied to a number of problem domains including fault tolerance and data mining , but have been shown to scale poorly when applied to computer intrusion detec tion despite the fact that the biological immune system is a very effective anomaly detector this may be because ais algorithms have previously been based on the adaptive immune system and biologically naive mod els this paper focuses on describing and testing a more complex and biologically authentic ais model , inspired by the interactions between the innate and adaptive immune systems its performance on a realistic process anomaly detection problem is shown to be better than standard ais methods \( negative selection \) , policy based anomaly detection methods \( systrace \) , and an alternative innate ais approach \( the dca \) in addition , it is shown that runtime information can be used in combination with system call information to enhance detection capability
the capacity of caching networks has received considerable attention in the past few years a particularly studied setting is the case of a single server \( e g , a base station \) and multiple users , each of which caches segments of files in a finite library each user requests one \( whole \) file in the library and the server sends a common coded multicast message to satisfy all users at once the problem consists of finding the smallest possible codeword length to satisfy such requests in this paper we consider the generalization to the case where each user places l geq 1 requests the obvious naive scheme consists of applying l times the order optimal scheme for a single request , obtaining a linear in l scaling of the multicast codeword length we propose a new achievable scheme based on multiple groupcast index coding that achieves a significant gain over the naive scheme furthermore , through an information theoretic converse we find that the proposed scheme is approximately optimal within a constant factor of \( at most \) 18
this paper presents a model for end to end learning of task oriented dialog systems the main component of the model is a recurrent neural network \( an lstm \) , which maps from raw dialog history directly to a distribution over system actions the lstm automatically infers a representation of dialog history , which relieves the system developer of much of the manual feature engineering of dialog state in addition , the developer can provide software that expresses business rules and provides access to programmatic apis , enabling the lstm to take actions in the real world on behalf of the user the lstm can be optimized using supervised learning \( sl \) , where a domain expert provides example dialogs which the lstm should imitate or using reinforcement learning \( rl \) , where the system improves by interacting directly with end users experiments show that sl and rl are complementary sl alone can derive a reasonable initial policy from a small number of training dialogs and starting rl optimization with a policy trained with sl substantially accelerates the learning rate of rl
this article examines wireless communications from a source to a destination in the presence of an eavesdropper from a security reliability tradeoff \( srt \) perspective explicitly , the security is quantified in terms of the intercept probability experienced at the eavesdropper , while the outage probability encountered at the destination is used to measure the transmission reliability we characterize the srt of conventional direct transmission from the source to the destination and show that if the outage probability is increased , the intercept probability decreases , and vice versa we first demonstrate that the employment of relay nodes for assisting the source destination transmissions is capable of defending against eavesdropping , followed by quantifying the benefits of single relay selection \( srs \) as well as of multi relay selection \( mrs \) schemes more specifically , in the srs scheme , only the single best relay is selected for forwarding the source signal to the destination , whereas the mrs scheme allows multiple relays to participate in this process it is illustrated that both the srs and mrs methods achieve a better srt than the conventional direct transmission , especially upon increasing the number of relays numerical results also show that as expected , the mrs outperforms the srs in terms of its srt additionally , we present some open challenges and future directions for wireless relay aided physical layer security
the research question this report addresses is how , and to what extent , those directly involved with the design , development and employment of a specific black box algorithm can be certain that it is not unlawfully discriminating \( directly and or indirectly \) against particular persons with protected characteristics \( e g gender , race and ethnicity \) \?
we consider communication over the binary erasure channel \( bec \) using low density parity check \( ldpc \) codes and belief propagation \( bp \) decoding for fixed numbers of bp iterations , the bit error probability approaches a limit as blocklength tends to infinity , and the limit is obtained via density evolution on the other hand , the difference between the bit error probability of codes with blocklength n and that in the large blocklength limit is asymptotically alpha \( epsilon , t \) n theta \( n 2 \) where alpha \( epsilon , t \) denotes a specific constant determined by the code ensemble considered , the number t of iterations , and the erasure probability epsilon of the bec in this paper , we derive a set of recursive formulas which allows evaluation of the constant alpha \( epsilon , t \) for standard irregular ensembles the dominant difference alpha \( epsilon , t \) n can be considered as effects of cycle free and single cycle structures of local graphs furthermore , it is confirmed via numerical simulations that estimation of the bit error probability using alpha \( epsilon , t \) is accurate even for small blocklengths
landmines , specifically anti tank mines , cluster bombs , and unexploded ordnance form a serious problem in many countries several landmine sweeping techniques are used for minesweeping this paper presents the design and the implementation of the vision system of an autonomous robot for landmines localization the proposed work develops state of the art techniques in digital image processing for pre processing captured images of the contaminated area after enhancement , artificial neural network \( ann \) is used in order to identify , recognize and classify the landmines' make and model the back propagation algorithm is used for training the network the proposed work proved to be able to identify and classify different types of landmines under various conditions \( rotated landmine , partially covered landmine \) with a success rate of up to 90
we consider the problem of learning distributed representations for documents in data streams the documents are represented as low dimensional vectors and are jointly learned with distributed vector representations of word tokens using a hierarchical framework with two embedded neural language models in particular , we exploit the context of documents in streams and use one of the language models to model the document sequences , and the other to model word sequences within them the models learn continuous vector representations for both word tokens and documents such that semantically similar documents and words are close in a common vector space we discuss extensions to our model , which can be applied to personalized recommendation and social relationship mining by adding further user layers to the hierarchy , thus learning user specific vectors to represent individual preferences we validated the learned representations on a public movie rating data set from movielens , as well as on a large scale yahoo news data comprising three months of user activity logs collected on yahoo servers the results indicate that the proposed model can learn useful representations of both documents and word tokens , outperforming the current state of the art by a large margin
electronic geometry textbook is a knowledge management system that manages geometric textbook knowledge to enable users to construct and share dynamic geometry textbooks interactively and efficiently based on a knowledge base organizing and storing the knowledge represented in specific languages , the system implements interfaces for maintaining the data representing that knowledge as well as relations among those data , for automatically generating readable documents for viewing or printing , and for automatically discovering the relations among knowledge data an interface has been developed for users to create geometry textbooks with automatic checking , in real time , of the consistency of the structure of each resulting textbook by integrating an external geometric theorem prover and an external dynamic geometry software package , the system offers the facilities for automatically proving theorems and generating dynamic figures in the created textbooks this paper provides a comprehensive account of the current version of electronic geometry textbook
next generation cellular networks will consist of multiple tiers of cells and users associated with different network tiers may have different priorities \( e g , macrocell picocell femtocell networks with macro tier prioritized over pico tier , which is again prioritized over femto tier \) designing efficient joint power and admission control \( jpac \) algorithms for such networks under a co channel deployment \( i e , underlay \) scenario is of significant importance feasibility checking of a given target signal to noise plus interference ratio \( sinr \) vector is generally the most significant contributor to the complexity of jpac algorithms in single multi tier underlay cellular networks this is generally accomplished through iterative strategies whose complexity is either unpredictable or of o \( m 3 \) , when the well known relationship between the sinr vector and the power vector is used , where m is the number of users links in this paper , we derive a novel relationship between a given sinr vector and its corresponding uplink downlink power vector based on which the feasibility checking can be performed with a complexity of o \( b 3 m b \) , where b is the number of base stations this is significantly less compared to o \( m 3 \) in many cellular wireless networks since the number of base stations is generally much lower than the number of users links in such networks the developed novel relationship between the sinr and power vector not only substantially reduces the complexity of designing jpac algorithms , but also provides insights into developing efficient but low complexity power update strategies for prioritized multi tier cellular networks we propose two such algorithms and through simulations , we show that our proposed algorithms outperform the existing ones in prioritized cellular networks
this paper presents a hmm based speech recognition engine and its integration into direct manipulation interfaces for korean document editor speech recognition can reduce typical tedious and repetitive actions which are inevitable in standard guis \( graphic user interfaces \) our system consists of general speech recognition engine called abrain auditory brain and speech commandable document editor called she simple hearing editor abrain is a phoneme based speech recognition engine which shows up to 97 of discrete command recognition rate she is a eurobridge widget based document editor that supports speech commands as well as direct manipulation interfaces
we propose to classify the power of algorithms by the complexity of the problems that they can be used to solve instead of restricting to the problem a particular algorithm was designed to solve explicitly , however , we include problems that , with polynomial overhead , can be solved 'implicitly' during the algorithm 's execution for example , we allow to solve a decision problem by suitably transforming the input , executing the algorithm , and observing whether a specific bit in its internal configuration ever switches during the execution we show that the simplex method , the network simplex method \( both with dantzig 's original pivot rule \) , and the successive shortest path algorithm are np mighty , that is , each of these algorithms can be used to solve any problem in np this result casts a more favorable light on these algorithms' exponential worst case running times furthermore , as a consequence of our approach , we obtain several novel hardness results for example , for a given input to the simplex algorithm , deciding whether a given variable ever enters the basis during the algorithm 's execution and determining the number of iterations needed are both np hard problems finally , we close a long standing open problem in the area of network flows over time by showing that earliest arrival flows are np hard to obtain
we study the stochastic versions of a broad class of combinatorial problems where the weights of the elements in the input dataset are uncertain the class of problems that we study includes shortest paths , minimum weight spanning trees , and minimum weight matchings , and other combinatorial problems like knapsack we observe that the expected value is inadequate in capturing different types of em risk averse or em risk prone behaviors , and instead we consider a more general objective which is to maximize the em expected utility of the solution for some given utility function , rather than the expected weight \( expected weight becomes a special case \) under the assumption that there is a pseudopolynomial time algorithm for the em exact version of the problem \( this is true for the problems mentioned above \) , we can obtain the following approximation results for several important classes of utility functions \( 1 \) if the utility function uti is continuous , upper bounded by a constant and lim x rightarrow infty uti \( x \) 0 , we show that we can obtain a polynomial time approximation algorithm with an em additive error epsilon for any constant epsilon 0 \( 2 \) if the utility function uti is a concave increasing function , we can obtain a polynomial time approximation scheme \( ptas \) \( 3 \) if the utility function uti is increasing and has a bounded derivative , we can obtain a polynomial time approximation scheme our results recover or generalize several prior results on stochastic shortest path , stochastic spanning tree , and stochastic knapsack our algorithm for utility maximization makes use of the separability of exponential utility and a technique to decompose a general utility function into exponential utility functions , which may be useful in other stochastic optimization problems
this paper proposes gprop , a deep reinforcement learning algorithm for continuous policies with compatible function approximation the algorithm is based on two innovations firstly , we present a temporal difference based method for learning the gradient of the value function secondly , we present the deviator actor critic \( dac \) model , which comprises three neural networks that estimate the value function , its gradient , and determine the actor 's policy respectively we evaluate gprop on two challenging tasks a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients and the octopus arm , a challenging reinforcement learning benchmark gprop is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm
in this paper we propose two new algorithms based on biclustering analysis , which can be used at the basis of a recommender system for educational orientation of russian school graduates the first algorithm was designed to help students make a choice between different university faculties when some of their preferences are known the second algorithm was developed for the special situation when nothing is known about their preferences the final version of this recommender system will be used by higher school of economics
every open source project needs to decide on an open source license this decision is of high economic relevance just which license is the best one to help the project grow and attract a community \? the most common question is should the project choose a restrictive \( reciprocal \) license or a more permissive one \? as an important step towards answering this question , this paper analyses actual license choice and correlated project growth from ten years of open source projects it provides closed analytical models and finds that around 2001 a reversal in license choice occurred from restrictive towards permissive licenses
we study a simple question when are dynamic relaying strategies essential in optimizing the diversity multiplexing tradeoff \( dmt \) in half duplex wireless relay networks \? this is motivated by apparently two contrasting results even for a simple 3 node network , with a single half duplex relay when all channels are assumed to be i i d fading , a static schedule where the relay listens half the time and transmits half the time combined with quantize map forward \( qmf \) relaying is known to achieve the full duplex performance however , when there is no direct link between source and destination , a dynamic decode forward \( ddf \) strategy is needed to achieve the optimal tradeoff in this case , a static schedule is strictly suboptimal and the optimal tradeoff is significantly worse than the full duplex performance in this paper we study the general case when the direct link is neither as strong as the other links nor fully non existent , and identify regimes where dynamic schedules are necessary and those where static schedules are enough we identify 4 qualitatively different regimes for the single relay channel where the tradeoff between diversity and multiplexing is significantly different we show that in all these regimes one of the above two strategies is sufficient to achieve the optimal tradeoff by developing a new upper bound on the best achievable tradeoff under channel state information available only at the receivers a natural next question is whether these two strategies are sufficient to achieve the dmt of more general half duplex wireless networks we propose a generalization of the two existing schemes through a dynamic qmf \( dqmf \) strategy , where the relay listens for a fraction of time depending on received csi but not long enough to be able to decode we show that such a dqmf strategy is needed to achieve the optimal dmt in a parallel channel with two relays
the large majority of commercially available multiple input multiple output \( mimo \) radio channel measurement devices \( sounders \) is based on time division multiplexed switching \( tdms \) of a single transmit receive radio frequency chain into the elements of a transmit receive antenna array while being cost effective , such a solution can cause significant measurement errors due to phase noise and frequency offset in the local oscillators in this paper , we systematically analyze the resulting errors and show that , in practice , overestimation of channel capacity by several hundred percent can occur overestimation is caused by phase noise \( and to a lesser extent frequency offset \) leading to an increase of the mimo channel rank our analysis furthermore reveals that the impact of phase errors is , in general , most pronounced if the physical channel has low rank \( typical for line of sight or poor scattering scenarios \) the extreme case of a rank 1 physical channel is analyzed in detail finally , we present measurement results obtained from a commercially employed tdms based mimo channel sounder in the light of the findings of this paper , the results obtained through mimo channel measurement campaigns using tdms based channel sounders should be interpreted with great care
we tackle the problem of constructing increasing chord graphs spanning point sets we prove that , for every point set p with n points , there exists an increasing chord planar graph with o \( n \) steiner points spanning p further , we prove that , for every convex point set p with n points , there exists an increasing chord graph with o \( n log n \) edges \( and with no steiner points \) spanning p
in this paper , we study band allocation of mathcal m s buffered secondary users \( sus \) to mathcal m p orthogonal primary licensed bands , where each primary band is assigned to one primary user \( pu \) each su is assigned to one of the available primary bands with a certain probability designed to satisfy some specified quality of service \( qos \) requirements for the sus in the proposed system , only one su is assigned to a particular band the optimization problem used to obtain the stability region 's envelope \( closure \) is shown to be a linear program we compare the stability region of the proposed system with that of a system where each su chooses a band randomly with some assignment probability we also compare with a fixed \( deterministic \) assignment system , where only one su is assigned to one of the primary bands all the time we prove the advantage of the proposed system over the other systems
in this paper , we present an epistemic logic approach to the compositionality of several privacy related informationhiding disclosure properties the properties considered here are anonymity , privacy , onymity , and identity our initial observation reveals that anonymity and privacy are not necessarily sequentially compositional this means that even though a system comprising several sequential phases satisfies a certain unlinkability property in each phase , the entire system does not always enjoy a desired unlinkability property we show that the compositionality can be guaranteed provided that the phases of the system satisfy what we call the independence assumptions more specifically , we develop a series of theoretical case studies of what assumptions are sufficient to guarantee the sequential compositionality of various degrees of anonymity , privacy , onymity , and or identity properties similar results for parallel composition are also discussed
we consider the problem of rational secret sharing introduced by halpern and teague 1 , where the players involved in secret sharing play only if it is to their advantage this can be characterized in the form of preferences players would prefer to get the secret than to not get it and secondly with lesser preference , they would like as few other players to get the secret as possible several positive results have already been published to efficiently solve the problem of rational secret sharing but only a handful of papers have touched upon the use of an asynchronous broadcast channel 2 used cryptographic primitives , 3 used an interactive dealer , and 4 used an honest minority of players in order to handle an asynchronous broadcast channel in our paper , we propose an m out of n rational secret sharing scheme which can function over an asynchronous broadcast channel without the use of cryptographic primitives and with a non interactive dealer this is possible because our scheme uses a small number , k 1 , of honest players the protocol is resilient to coalitions of size up to k and furthermore it is epsilon resilient to coalitions of size up to and including m 1 the protocol will have a strict nash equilibrium with probability pr \( \( k 1 \) n \) and an epsilon nash equilibrium with probability pr \( \( n k 1 \) n \) furthermore , our protocol is immune to backward induction later on in the paper , we extend our results to include malicious players as well we also show that our protocol handles the possibility of a player deviating in order to force another player to get a wrong value in what we believe to be a more time efficient manner than was done in asharov and lindell 5
invariance to geometric transformations is a highly desirable property of automatic classifiers in many image recognition tasks nevertheless , it is unclear to which extent state of the art classifiers are invariant to basic transformations such as rotations and translations this is mainly due to the lack of general methods that properly measure such an invariance in this paper , we propose a rigorous and systematic approach for quantifying the invariance to geometric transformations of any classifier our key idea is to cast the problem of assessing a classifier 's invariance as the computation of geodesics along the manifold of transformed images we propose the manitest method , built on the efficient fast marching algorithm to compute the invariance of classifiers our new method quantifies in particular the importance of data augmentation for learning invariance from data , and the increased invariance of convolutional neural networks with depth we foresee that the proposed generic tool for measuring invariance to a large class of geometric transformations and arbitrary classifiers will have many applications for evaluating and comparing classifiers based on their invariance , and help improving the invariance of existing classifiers
this paper examines a general class of noisy matrix completion tasks where the goal is to estimate a matrix from observations obtained at a subset of its entries , each of which is subject to random noise or corruption our specific focus is on settings where the matrix to be estimated is well approximated by a product of two \( a priori unknown \) matrices , one of which is sparse such structural models referred to here as sparse factor models have been widely used , for example , in subspace clustering applications , as well as in contemporary sparse modeling and dictionary learning tasks our main theoretical contributions are estimation error bounds for sparsity regularized maximum likelihood estimators for problems of this form , which are applicable to a number of different observation noise or corruption models several specific implications are examined , including scenarios where observations are corrupted by additive gaussian noise or additive heavier tailed \( laplace \) noise , poisson distributed observations , and highly quantized \( e g , one bit \) observations we also propose a simple algorithmic approach based on the alternating direction method of multipliers for these tasks , and provide experimental evidence to support our error analyses
the models of statistical physics used to study collective phenomena in some interdisciplinary contexts , such as social dynamics and opinion spreading , do not consider the effects of the memory on individual decision processes on the contrary , in the naming game , a recently proposed model of language formation , each agent chooses a particular state , or opinion , by means of a memory based negotiation process , during which a variable number of states is collected and kept in memory in this perspective , the statistical features of the number of states collected by the agents becomes a relevant quantity to understand the dynamics of the model , and the influence of topological properties on memory based models by means of a master equation approach , we analyze the internal agent dynamics of naming game in populations embedded on networks , finding that it strongly depends on very general topological properties of the system \( e g average and fluctuations of the degree \) however , the influence of topological properties on the microscopic individual dynamics is a general phenomenon that should characterize all those social interactions that can be modeled by memory based negotiation processes
network coding is a packet encoding technique which has recently been shown to improve network performance \( by reducing delays and increasing throughput \) in broadcast and multicast communications the cost for such an improvement comes in the form of increased decoding complexity \( and thus delay \) at the receivers end before delivering the file to higher layers , the receiver should first decode those packets in our work we consider the broadcast transmission of a large file to n wireless users the file is segmented into a number of blocks \( each containing k packets the coding window size \) the packets of each block are encoded using random linear network coding \( rlnc \) we obtain the minimum coding window size so that the completion time of the file transmission is upper bounded by a used defined delay constraint
we present a complete classification of quantum stabilizer gates in terms of the functions they generate assuming the ability to swap qubits and use ancillary workspace because we view these stabilizer circuits as subroutines of some general quantum computation , we insist that any ancilla qubits used during the computation must not change in an input dependent manner this is the first attempt at a quantum extension of the classification of reversible classical gates introduced by aaronson et al , another part of an ambitious program to classify all quantum gate sets the classification uses , at its center , a reinterpretation of the tableau representation of stabilizer gates to give circuit decompositions , from which elementary generators can easily be extracted there are a total of 57 different stabilizer classes generated in this way , 30 of which arise from the single qubit subgroups of the clifford group at a high level , the remaining classes are arranged according to the bases they preserve for instance , the cnot gate preserves the x and z bases because it maps x basis elements to x basis elements and z basis elements to z basis elements the remaining classes are characterized by more subtle tableau invariants for instance , the t 4 and phase gate generate a proper subclass of z preserving gates
given the present state of work in natural language processing , this address argues first , that advance in both science and applications requires a revival of concern about what language is about , broadly speaking the world and second , that an attack on the summarising task , which is made ever more important by the growth of electronic text resources and requires an understanding of the role of large scale discourse structure in marking important text content , is a good way forward
in this paper , we study a fast approximation method for it large scale high dimensional sparse least squares regression problem by exploiting the johnson lindenstrauss \( jl \) transforms , which embed a set of high dimensional vectors into a low dimensional space in particular , we propose to apply the jl transforms to the data matrix and the target vector and then to solve a sparse least squares problem on the compressed data with a it slightly larger regularization parameter theoretically , we establish the optimization error bound of the learned model for two different sparsity inducing regularizers , i e , the elastic net and the ell 1 norm compared with previous relevant work , our analysis is it non asymptotic and exhibits more insights on the bound , the sample complexity and the regularization as an illustration , we also provide an error bound of the it dantzig selector under jl transforms
this paper studies the problem of testing if an input \( gamma , \) , where gamma is a finite set of unknown size and is a binary operation over gamma given as an oracle , is close to a specified class of groups friedl et al efficient testing of groups , stoc'05 have constructed an efficient tester using poly \( log gamma \) queries for the class of abelian groups we focus in this paper on subclasses of abelian groups , and show that these problems are much harder omega \( gamma 1 6 \) queries are necessary to test if the input is close to a cyclic group , and omega \( gamma c \) queries for some constant c are necessary to test more generally if the input is close to an abelian group generated by k elements , for any fixed integer k 0 we also show that knowledge of the size of the ground set gamma helps only for k 1 , in which case we construct an efficient tester using poly \( log gamma \) queries for any other value k 1 the query complexity remains omega \( gamma c \) all our upper and lower bounds hold for both the edit distance and the hamming distance these are , to the best of our knowledge , the first nontrivial lower bounds for such group theoretic problems in the property testing model and , in particular , they imply the first exponential separations between the classical and quantum query complexities of testing closeness to classes of groups
we study the networks formed by the directors of the most important swiss boards and the boards themselves for the year 2009 the networks are obtained by projection from the original bipartite graph we highlight a number of important statistical features of those networks such as degree distribution , weight distribution , and several centrality measures as well as their interrelationships while similar statistics were already known for other board systems , and are comparable here , we have extended the study with a careful investigation of director and board centrality , a k core analysis , and a simulation of the speed of information propagation and its relationships with the topological aspects of the network such as clustering and link weight and betweenness the overall picture that emerges is one in which the topological structure of the swiss board and director networks has evolved in such a way that special actors and links between actors play a fundamental role in the flow of information among distant parts of the network this is shown in particular by the centrality measures and by the simulation of a simple epidemic process on the directors network
let f v rightarrow 1 , ldots , k be a labeling of the vertices of a graph g \( v , e \) and denote with f \( n \( v \) \) the sum of the labels of all vertices adjacent to v the least value k for which a graph g admits a labeling satisfying f \( n \( u \) \) neq f \( n \( v \) \) for all \( u , v \) in e is called emph additive chromatic number of g and denoted eta \( g \) it was first presented by czerwi 'nski , grytczuk and zelazny who also proposed a conjecture that for every graph g , eta \( g \) leq chi \( g \) , where chi \( g \) is the chromatic number of g bounds of eta \( g \) are known for very few families of graphs in this work , we show that the conjecture holds for split graphs by giving an upper bound of the additive chromatic number and we present exact formulas for computing eta \( g \) when g is a fan , windmill , circuit , wheel , complete split , headless spider , cycle wheel complete sun , regular bipartite or complete multipartite observing that the conjecture is satisfied in all cases in addition , we propose an integer programming formulation which is used for checking the conjecture over all connected graphs up to 10 vertices
we present , in easily reproducible terms , a simple transformation for offline parsable grammars which results in a provably terminating parsing program directly top down interpretable in prolog the transformation consists in two steps \( 1 \) removal of empty productions , followed by \( 2 \) left recursion elimination it is related both to left corner parsing \( where the grammar is compiled , rather than interpreted through a parsing program , and with the advantage of guaranteed termination in the presence of empty productions \) and to the generalized greibach normal form for dcgs \( with the advantage of implementation simplicity \)
this workshop brings together experts of communities which often have been perceived as different once bibliometrics scientometrics informetrics on the one side and information retrieval on the other our motivation as organizers of the workshop started from the observation that main discourses in both fields are different , that communities are only partly overlapping and from the belief that a knowledge transfer would be profitable for both sides bibliometric techniques are not yet widely used to enhance retrieval processes in digital libraries , although they offer value added effects for users on the other side , more and more information professionals , working in libraries and archives are confronted with applying bibliometric techniques in their services this way knowledge exchange becomes more urgent the first workshop set the research agenda , by introducing in each other methods , reporting about current research problems and brainstorming about common interests this follow up workshop continues the overall communication , but also puts one problem into the focus in particular , we will explore how statistical modelling of scholarship can improve retrieval services for specific communities , as well as for large , cross domain collections like mendeley or researchgate this second bir workshop continues to raise awareness of the missing link between information retrieval \( ir \) and bibliometrics and contributes to create a common ground for the incorporation of bibliometric enhanced services into retrieval at the scholarly search engine interface
the asynchronous push pull protocol , a randomized distributed algorithm for spreading a rumour in a graph g , works as follows independent poisson clocks of rate 1 are associated with the vertices of g initially , one vertex of g knows the rumour whenever the clock of a vertex x rings , it calls a random neighbour y if x knows the rumour and y does not , then x tells y the rumour \( a push operation \) , and if x does not know the rumour and y knows it , y tells x the rumour \( a pull operation \) the average spread time of g is the expected time it takes for all vertices to know the rumour , and the guaranteed spread time of g is the smallest time t such that with probability at least 1 1 n , after time t all vertices know the rumour the synchronous variant of this protocol , in which each clock rings precisely at times 1 , 2 , dots , has been studied extensively we prove the following results for any n vertex graph in either version , the average spread time is at most linear even if only the pull operation is used , and the guaranteed spread time is within a logarithmic factor of the average spread time , so it is o \( n log n \) in the asynchronous version , both the average and guaranteed spread times are omega \( log n \) we give examples of graphs illustrating that these bounds are best possible up to constant factors we also prove theoretical relationships between the guaranteed spread times in the two versions firstly , in all graphs the guaranteed spread time in the asynchronous version is within an o \( log n \) factor of that in the synchronous version , and this is tight next , we find examples of graphs whose asynchronous spread times are logarithmic , but the synchronous versions are polynomially large finally , we show for any graph that the ratio of the synchronous spread time to the asynchronous spread time is o \( n 2 3 \)
in the context of sparse recovery , it is known that most of existing regularizers such as ell 1 suffer from some bias incurred by some leading entries \( in magnitude \) of the associated vector to neutralize this bias , we propose a class of models with partial regularizers for recovering a sparse solution of a linear system we show that every local minimizer of these models is sufficiently sparse or the magnitude of all its nonzero entries is above a uniform constant depending only on the data of the linear system moreover , for a class of partial regularizers , any global minimizer of these models is a sparsest solution to the linear system we also establish some sufficient conditions for local or global recovery of the sparsest solution to the linear system , among which one of the conditions is weaker than the best known restricted isometry property \( rip \) condition for sparse recovery by ell 1 in addition , a first order feasible augmented lagrangian \( fal \) method is proposed for solving these models , in which each subproblem is solved by a nonmonotone proximal gradient \( npg \) method despite the complication of the partial regularizers , we show that each proximal subproblem in npg can be solved as a certain number of one dimensional optimization problems , which usually have a closed form solution we also show that any accumulation point of the sequence generated by fal is a first order stationary point of the models numerical results on compressed sensing and sparse logistic regression demonstrate that the proposed models substantially outperform the widely used ones in the literature in terms of solution quality
we propose an algorithm with expected complexity of bigo \( n log n \) arithmetic operations to solve a special shortest vector problem arising in computer and forward design , where n is the dimension of the channel vector this algorithm is more efficient than the best known algorithms with proved complexity
in this paper , we develop an approach to recursively estimate the quadratic risk for matrix recovery problems regularized with spectral functions toward this end , in the spirit of the sure theory , a key step is to compute the \( weak \) derivative and divergence of a solution with respect to the observations as such a solution is not available in closed form , but rather through a proximal splitting algorithm , we propose to recursively compute the divergence from the sequence of iterates a second challenge that we unlocked is the computation of the \( weak \) derivative of the proximity operator of a spectral function to show the potential applicability of our approach , we exemplify it on a matrix completion problem to objectively and automatically select the regularization parameter
in many global optimization problems , it is required to evaluate a global point \( min or max \) in large space that calculation effort is very high in this paper is presented new approach for optimization problem with subdivision labeling method \( slm \) but in this method for higher dimensional has high computational slm genetic algorithm \( slmga \) in optimization problems is one of the solutions of this problem in proposed algorithm the initial population is crossing points and subdividing in each step is according to mutation rslmga is compared with other well known algorithms de , pga , grefensstette and eshelman and numerical results show that rslmga achieve global optimal point with more decision by smaller generations
in this paper , anew algorithm which is based on geometrical moments and local binary patterns \( lbp \) for content based image retrieval \( cbir \) is proposed in geometrical moments , each vector is compared with the all other vectors for edge map generation the same concept is utilized at lbp calculation which is generating nine lbp patterns from a given 3x3 pattern finally , nine lbp histograms are calculated which are used as a feature vector for image retrieval moments are important features used in recognition of different types of images two experiments have been carried out for proving the worth of our algorithm the results after being investigated shows a significant improvement in terms of their evaluation measures as compared to lbp and other existing transform domain techniques
component selection is considered one of hard tasks in component based software engineering \( cbse \) it is difficult to find the optimal component selection cbse is an approach that is used to develop a software system from pre existing software components appropriate software component selection plays an important role in cbse many approaches were suggested to solve component selection problem in this paper the component selection is done by improving the integrated component selection framework by including the pliability metric pliability is a flexible measure that assesses software quality in terms of its components quality the validation of this proposed solution is done through collecting a sample of people who answer an electronic questionnaire that composed of 20 questions the questionnaire is distributed through social sites such as twitter , facebook and emails the result of the validation showed that using the integrated component selection framework with pliability metric is suitable for component selection
this paper focusses on the sparse estimation in the situation where both the the sensing matrix and the measurement vector are corrupted by additive gaussian noises the performance bound of sparse estimation is analyzed and discussed in depth two types of lower bounds , the constrained cram ' e r rao bound \( ccrb \) and the hammersley chapman robbins bound \( hcrb \) , are discussed it is shown that the situation with sensing matrix perturbation is more complex than the one with only measurement noise for the ccrb , its closed form expression is deduced it demonstrates a gap between the maximal and nonmaximal support cases it is also revealed that a gap lies between the ccrb and the mse of the oracle pseudoinverse estimator , but it approaches zero asymptotically when the problem dimensions tend to infinity for a tighter bound , the hcrb , despite of the difficulty in obtaining a simple expression for general sensing matrix , a closed form expression in the unit sensing matrix case is derived for a qualitative study of the performance bound it is shown that the gap between the maximal and nonmaximal cases is eliminated for the hcrb numerical simulations are performed to verify the theoretical results in this paper
traditional approaches to automatic and parallelization of logic programs rely on some static analysis to identify independent goals that can be safely and efficiently run in parallel in any possible execution in this paper , we present a novel technique for generating annotations for independent and parallelism that is based on partial evaluation basically , we augment a simple partial evaluation procedure with \( run time \) groundness and variable sharing information so that parallel conjunctions are added to the residual clauses when the conditions for independence are met in contrast to previous approaches , our partial evaluator is able to transform the source program in order to expose more opportunities for parallelism to the best of our knowledge , we present the first approach to a parallelizing partial evaluator
energy cost of cellular networks is ever increasing to match the surge of wireless data traffic , and the saving of this cost is important to reduce the operational expenditure \( opex \) of wireless operators in future the recent advancements of renewable energy integration and two way energy flow in smart grid provide potential new solutions to save the cost however , they also impose challenges , especially on how to use the stochastically and spatially distributed renewable energy harvested at cellular base stations \( bss \) to reliably supply time and space varying wireless traffic over cellular networks to overcome these challenges , in this article we present three approaches , namely , emph energy cooperation , communication cooperation , and joint energy and communication cooperation , in which different bss bidirectionally trade or share energy via the aggregator in smart grid , and or share wireless resources and shift loads with each other to reduce the total energy cost
we explore the use of neural networks trained with dropout in predicting epileptic seizures from electroencephalographic data \( scalp eeg \) the input to the neural network is a set of 9 pre defined features extracted from 1 second non overlapping windows in each of 14 channels per patient selected for the experiment the models in our experiments achieve high sensitivity and specificity on patient records not used in the training process this is demonstrated using leave one out cross validation across patient records
we focus on the problem of query rewriting for sponsored search we base rewrites on a historical click graph that records the ads that have been clicked on in response to past user queries given a query q , we first consider simrank as a way to identify queries similar to q , i e , queries whose ads a user may be interested in we argue that simrank fails to properly identify query similarities in our application , and we present two enhanced version of simrank one that exploits weights on click graph edges and another that exploits ``evidence '' we experimentally evaluate our new schemes against simrank , using actual click graphs and queries form yahoo ! , and using a variety of metrics our results show that the enhanced methods can yield more and better query rewrites
we give a simple deterministic constant round algorithm in the congested clique model for reducing the number of edges in a graph to n 1 varepsilon while preserving the minimum spanning forest , where varepsilon 0 is any constant this implies that in the congested clique model , it is sufficient to improve mst and other connectivity algorithms on graphs with slightly superlinear number of edges to obtain a general improvement as a byproduct , we also obtain a simple alternative proof showing that mst can be computed deterministically in o \( log log n \) rounds
in this paper , first order logic is interpreted in the framework of universal algebra , using the clone theory developed in three previous papers we first define the free clone t \( l , c \) of terms of a first order language l over a set c of parameters in a standard way the free right algebra f \( l , c \) of formulas over t \( l , c \) is then generated by atomic formulas structures for l over c are represented as perfect valuations of f \( l , c \) , and theories of l are represented as filters of f \( l \) finally godel 's completeness theorem and first incompleteness theorem are stated as expected
wireless industry nowadays is facing two major challenges 1 \) how to support the vertical industry applications so that to expand the wireless industry market and 2 \) how to further enhance device capability and user experience in this paper , we propose a technology framework to address these challenges the proposed technology framework is based on end to end vertical and horizontal slicing , where vertical slicing enables vertical industry and services and horizontal slicing improves system capacity and user experience the technology development on vertical slicing has already started in late 4g and early 5g and is mostly focused on slicing the core network we envision this trend to continue with the development of vertical slicing in the radio access network and the air interface moving beyond vertical slicing , we propose to horizontally slice the computation and communication resources to form virtual computation platforms for solving the network capacity scaling problem and enhancing device capability and user experience in this paper , we explain the concept of vertical and horizontal slicing and illustrate the slicing techniques in the air interface , the radio access network , the core network and the computation platform this paper aims to initiate the discussion on the long range technology roadmap and spur development on the solutions for e2e network slicing in 5g and beyond
since its introduction by valiant in 1984 , pac learning of dnf expressions remains one of the central problems in learning theory we consider this problem in the setting where the underlying distribution is uniform , or more generally , a product distribution kalai , samorodnitsky and teng \( 2009 \) showed that in this setting a dnf expression can be efficiently approximated from its heavy low degree fourier coefficients alone this is in contrast to previous approaches where boosting was used and thus fourier coefficients of the target function modified by various distributions were needed this property is crucial for learning of dnf expressions over smoothed product distributions , a learning model introduced by kalai et al \( 2009 \) and inspired by the seminal smoothed analysis model of spielman and teng \( 2001 \) we introduce a new approach to learning \( or approximating \) a polynomial threshold functions which is based on creating a function with range 1 , 1 that approximately agrees with the unknown function on low degree fourier coefficients we then describe conditions under which this is sufficient for learning polynomial threshold functions our approach yields a new , simple algorithm for approximating any polynomial size dnf expression from its heavy low degree fourier coefficients alone our algorithm greatly simplifies the proof of learnability of dnf expressions over smoothed product distributions we also describe an application of our algorithm to learning monotone dnf expressions over product distributions building on the work of servedio \( 2001 \) , we give an algorithm that runs in time poly \( \( s cdot log \( s eps \) \) log \( s eps \) , n \) , where s is the size of the target dnf expression and eps is the accuracy this improves on poly \( \( s cdot log \( ns eps \) \) log \( s eps \) cdot log \( 1 eps \) , n \) bound of servedio \( 2001 \)
the proliferation of smart meters enables load serving entities to aggregate customers according to their consumption patterns we demonstrate a method for constructing groups of customers who will be the cheapest to service at wholesale market prices using smart meter data from a region in california , we show that by aggregating more of these customers together , their consumption can be forecasted more accurately , which allows an lse to mitigate financial risks in its wholesale market transactions we observe that the consumption of aggregates of customers with similar consumption patterns can be forecasted more accurately than that of random aggregates of customers
mobile p2p technology provides a scalable approach to content delivery to a large number of users on their mobile devices in this work , we study the dissemination of a emph single content \( e g , an item of news , a song or a video clip \) among a population of mobile nodes each node in the population is either a emph destination \( interested in the content \) or a potential emph relay \( not yet interested in the content \) there is an interest evolution process by which nodes not yet interested in the content \( i e , relays \) can become interested \( i e , become destinations \) on learning about the popularity of the content \( i e , the number of already interested nodes \) in our work , the interest in the content evolves under the emph linear threshold model the content is copied between nodes when they make random contact for this we employ a controlled epidemic spread model we model the joint evolution of the copying process and the interest evolution process , and derive the joint fluid limit ordinary differential equations we then study the selection of the parameters under the content provider 's control , for the optimization of various objective functions that aim at maximizing content popularity and efficient content delivery
we present a new platform , regulus lite , which supports rapid development and web deployment of several types of phrasal speech translation systems using a minimal formalism a distinguishing feature is that most development work can be performed directly by domain experts we motivate the need for platforms of this type and discuss three specific cases medical speech translation , speech to sign language translation and voice questionnaires we briefly describe initial experiences in developing practical systems
code division multiple access \( cdma \) is the most promising candidate for wideband data access this is due to the advantage of soft limit on the number of active mobile devices many wireless mesh systems impose an upper bound on the ber performance which restricts the increase in number of mobile users capacity is further reduced in multipath fading environment \( mfe \) this paper presents an effective method of improving the capacity of a cdma based mesh network by managing the transmitted powers of the mobile devices and using mmse based multiuser detection \( mud \) the proposed scheme improves the capacity two times as compared to the conventional cdma based mesh network simulation results have been presented to demonstrate the effectiveness of the proposed scheme
developers spend a significant amount of time searching their local codebase to help them search efficiently , researchers have proposed novel tools that apply state of the art information retrieval algorithms to retrieve relevant code snippets from the local codebase however , these tools still rely on the developer to craft an effective query , which requires that the developer is familiar with the terms contained in the related code snippets our empirical data from a state of the art local code search tool , called sando , suggests that developers are sometimes unacquainted with their local codebase in order to bridge the gap between developers and their ever increasing local codebase , in this paper we demonstrate the recommendation techniques integrated in sando
this paper considers the problem of defining a measure of redundant information that quantifies how much common information two or more random variables specify about a target random variable we discussed desired properties of such a measure , and propose new measures with some desirable properties
the oscar \( octagonal selection and clustering algorithm for regression \) regularizer consists of a l 1 norm plus a pair wise l inf norm \( responsible for its grouping behavior \) and was proposed to encourage group sparsity in scenarios where the groups are a priori unknown the oscar regularizer has a non trivial proximity operator , which limits its applicability we reformulate this regularizer as a weighted sorted l 1 norm , and propose its grouping proximity operator \( gpo \) and approximate proximity operator \( apo \) , thus making state of the art proximal splitting algorithms \( psas \) available to solve inverse problems with oscar regularization the gpo is in fact the apo followed by additional grouping and averaging operations , which are costly in time and storage , explaining the reason why algorithms with apo are much faster than that with gpo the convergences of psas with gpo are guaranteed since gpo is an exact proximity operator although convergence of psas with apo is may not be guaranteed , we have experimentally found that apo behaves similarly to gpo when the regularization parameter of the pair wise l inf norm is set to an appropriately small value experiments on recovery of group sparse signals \( with unknown groups \) show that psas with apo are very fast and accurate
this paper presents comparison of several stochastic optimization algorithms developed by authors in their previous works for the solution of some problems arising in civil engineering the introduced optimization methods are the integer augmented simulated annealing \( iasa \) , the real coded augmented simulated annealing \( rasa \) , the differential evolution \( de \) in its original fashion developed by r storn and k price and simplified real coded differential genetic algorithm \( sade \) each of these methods was developed for some specific optimization problem namely the chebychev trial polynomial problem , the so called type 0 function and two engineering problems the reinforced concrete beam layout and the periodic unit cell problem respectively detailed and extensive numerical tests were performed to examine the stability and efficiency of proposed algorithms the results of our experiments suggest that the performance and robustness of rasa , iasa and sade methods are comparable , while the de algorithm performs slightly worse this fact together with a small number of internal parameters promotes the sade method as the most robust for practical use
wyner 's wiretap channel is extended to parallel broadcast channels and fading channels with multiple receivers in the first part of the paper , we consider the setup of parallel broadcast channels with one sender , multiple intended receivers , and one eavesdropper we study the situations where the sender broadcasts either a common message or independent messages to the intended receivers we derive upper and lower bounds on the common message secrecy capacity , which coincide when the users are reversely degraded for the case of independent messages we establish the secrecy sum capacity when the users are reversely degraded in the second part of the paper we apply our results to fading channels perfect channel state information of all intended receivers is known globally , whereas the eavesdropper channel is known only to her for the common message case , a somewhat surprising result is proven a positive rate can be achieved independently of the number of intended receivers for independent messages , an opportunistic transmission scheme is presented that achieves the secrecy sum capacity in the limit of large number of receivers our results are stated for a fast fading channel model extensions to the block fading model are also discussed
we study the throughput capacity region of the gaussian multi access \( mac \) fading channel with perfect channel state information \( csi \) at the receiver and at the transmitters , at low power regime we show that it has a multidimensional rectangle structure and thus is simply characterized by single user capacity points more specifically , we show that at low power regime , the boundary surface of the capacity region shrinks to a single point corresponding to the sum rate maximizer and that the coordinates of this point coincide with single user capacity bounds inspired by this result , we propose an on off scheme , compute its achievable rate , and show that this scheme achieves single user capacity bounds of the mac channel for a wide class of fading channels at asymptotically low power regime we argue that this class of fading encompasses all known wireless channels for which the capacity region of the mac channel has even a simpler expression in terms of users' average power constraints only using the duality of gaussian mac and broadcast channels \( bc \) , we deduce a simple characterization of the bc capacity region at low power regime and show that for a class of fading channels \( including rayleigh fading \) , time sharing is asymptotically optimal
model checking has been used to verify the correctness of digital circuits , security protocols , communication protocols , as they can be modelled by means of finite state transition model however , modelling the behaviour of hybrid systems like uavs in a kripke model is challenging this work is aimed at capturing the behaviour of an uav performing cooperative search mission into a kripke model , so as to verify it against the temporal properties expressed in computation tree logic \( ctl \) smv model checker is used for the purpose of model checking
this paper explores commonalities and differences between dachs , a variant of dependency grammar , and lexical functional grammar dachs is based on traditional linguistic insights , but on modern mathematical tools , aiming to integrate different knowledge systems \( from syntax and semantics \) via their coupling to an abstract syntactic primitive , the dependency relation these knowledge systems correspond rather closely to projections in lfg we will investigate commonalities arising from the usage of the projection approach in both theories , and point out differences due to the incompatible linguistic premises the main difference to lfg lies in the motivation and status of the dimensions , and the information coded there we will argue that lfg confounds different information in one projection , preventing it to achieve a good separation of alternatives and calling the motivation of the projection into question
we introduce online learning algorithms which are independent of feature scales , proving regret bounds dependent on the ratio of scales existent in the data rather than the absolute scale this has several useful effects there is no need to pre normalize data , the test time and test space complexity are reduced , and the algorithms are more robust
a malleable coding scheme considers not only compression efficiency but also the ease of alteration , thus encouraging some form of recycling of an old compressed version in the formation of a new one malleability cost is the difficulty of synchronizing compressed versions , and malleable codes are of particular interest when representing information and modifying the representation are both expensive we examine the trade off between compression efficiency and malleability cost under a malleability metric defined with respect to a string edit distance this problem introduces a metric topology to the compressed domain we characterize the achievable rates and malleability as the solution of a subgraph isomorphism problem this can be used to argue that allowing conditional entropy of the edited message given the original message to grow linearly with block length creates an exponential increase in code length
modern cellular networks in traditional frequency bands are notoriously interference limited especially in urban areas , where base stations are deployed in close proximity to one another the latest releases of long term evolution \( lte \) incorporate features for coordinating downlink transmissions as an efficient means of managing interference recent field trial results and theoretical studies of the performance of joint transmission \( jt \) coordinated multi point \( comp \) schemes revealed , however , that their gains are not as high as initially expected , despite the large coordination overhead these schemes are known to be very sensitive to defects in synchronization or information exchange between coordinating bases stations as well as uncoordinated interference in this article , we review recent advanced coordinated beamforming \( cb \) schemes as alternatives , requiring less overhead than jt comp while achieving good performance in realistic conditions by stipulating that , in certain lte scenarios of increasing interest , uncoordinated interference constitutes a major factor in the performance of comp techniques at large , we hereby assess the resilience of the state of the art cb to uncoordinated interference we also describe how these techniques can leverage the latest specifications of current cellular networks , and how they may perform when we consider standardized feedback and coordination this allows us to identify some key roadblocks and research directions to address as lte evolves towards the future of mobile communications
a simple construction is presented which allows computing the transition amplitude of a quantum circuit to be encoded as computing the permanent of a matrix which is of size proportional to the number of quantum gates in the circuit this opens up some interesting classical monte carlo algorithms for approximating quantum circuits
this paper explores the process of validation for the abstract syntax of a graphical notation we define an unified specification for five of the uml diagrams used by the discovery method and , in this document , we illustrate how diagrams can be represented in alloy and checked against our specification in order to know if these are valid under the discovery notation
we propose a distributed sequential algorithm for quick detection of spectral holes in a cognitive radio set up two or more local nodes make decisions and inform the fusion centre \( fc \) over a reporting multiple access channel \( mac \) , which then makes the final decision the local nodes use energy detection and the fc uses mean change detection in the presence of heavy tailed electromagnetic interference \( emi \) and outliers different nonparametric sequential algorithms are compared to choose appropriate algorithms to be used at the local nodes and the fc a recently developed entropy test is selected for the local nodes for energy detection , and a modified version of a well known t test at the fusion centre for mean change detection we show via simulations and analysis that the nonparametric distributed algorithms developed performs well in the presence of emi and outliers the algorithm is iterative in nature , unlike t test and rank test or their robust versions
this paper presents and extends our type theoretical framework for a compositional treatment of natural language semantics with some lexical features like coercions \( e g of a town into a football club \) and copredication \( e g on a town as a set of people and as a location \) the second order typed lambda calculus was shown to be a good framework , and here we discuss how to introduced predefined types and coercive subtyping which are much more natural than internally coded similar constructs linguistic applications of these new features are also exemplified
in order to perform quantum cryptography procedures it is often essencial to ensure that the parties of the communication are authentic such task is accomplished by quantum authentication protocols which are distributed algorithms based on the intrinsic properties of quantum mechanics the choice of an authentication protocol must consider that quantum states are very delicate and that the channel is subject to eavesdropping however , even in face of the various existing definitions of quantum authentication protocols in the literature , little is known about them in this perspective , and this lack of knowledge may unfavor comparisons and wise choices in the attempt to overcome this limitation , in the present work we aim at showing an approach to evaluate quantum authentication protocols based on the determination of their quantum communication complexity based on our investigation , no similar methods to analyze quantum authentication protocols were found in the literature pursuing this further , our approach has advantages that need to be highlighted it characterizes a systematic procedure to evaluate quantum authentication protocols its evaluation is intuitive , based only on the protocol execution the resulting measure is a concise notation of what resources a quantum authentication protocol demands and how many communications are performed it allows comparisons between protocols it makes possible to analyze the communication effort when an eavesdropping occurs and , lastly , it is likely to be applied in almost any quantum authentication protocol to illustrate the proposed approach , we also bring results about its application in ten existing quantum authentication protocols \( data origin authentication and identity authentication \) such evaluations increase the knowledge about the existing protocols , presenting its advantages , limitations and contrasts
in a variety of disciplines such as social sciences , psychology , medicine and economics , the recorded data are considered to be noisy measurements of latent variables connected by some causal structure this corresponds to a family of graphical models known as the structural equation model with latent variables while linear non gaussian variants have been well studied , inference in nonparametric structural equation models is still underdeveloped we introduce a sparse gaussian process parameterization that defines a non linear structure connecting latent variables , unlike common formulations of gaussian process latent variable models the sparse parameterization is given a full bayesian treatment without compromising markov chain monte carlo efficiency we compare the stability of the sampling procedure and the predictive ability of the model against the current practice
we live in the information age , and information has become a critically important component of our life the success of the internet made huge amounts of it easily available and accessible to everyone to keep the flow of this information manageable , means for its faultless circulation and effective handling have become urgently required considerable research efforts are dedicated today to address this necessity , but they are seriously hampered by the lack of a common agreement about what is information \? in particular , what is visual information human 's primary input from the surrounding world the problem is further aggravated by a long lasting stance borrowed from the biological vision research that assumes human like information processing as an enigmatic mix of perceptual and cognitive vision faculties i am trying to find a remedy for this bizarre situation relying on a new definition of information , which can be derived from kolmogorov 's compexity theory and chaitin 's notion of algorithmic information , i propose a unifying framework for visual information processing , which explicitly accounts for the perceptual and cognitive image processing peculiarities i believe that this framework will be useful to overcome the difficulties that are impeding our attempts to develop the right model of human like intelligent image processing
living innovation laboratory \( lil \) is an open and recyclable way for multidisciplinary researchers to remote control resources and co develop user centered projects in the past few years , there were several papers about lil published and trying to discuss and define the model and architecture of lil people all acknowledge about the three characteristics of lil user centered , co creation , and context aware , which make it distinguished from test platform and other innovation approaches its existing model consists of five phases initialization , preparation , formation , development , and evaluation goal net is a goal oriented methodology to formularize a progress in this thesis , goal net is adopted to subtract a detailed and systemic methodology for lil lil goal net model breaks the five phases of lil into more detailed steps big data , crowd sourcing , crowd funding and crowd testing take place in suitable steps to realize uui , mcc and pca throughout the innovation process in lil 2 0 it would become a guideline for any company or organization to develop a project in the form of an lil 2 0 project to prove the feasibility of lil goal net model , it was applied to two real cases one project is a kinect game and the other one is an internet product they were both transformed to lil 2 0 successfully , based on lil goal net based methodology the two projects were evaluated by phenomenography , which was a qualitative research method to study human experiences and their relations in hope of finding the better way to improve human experiences through phenomenographic study , the positive evaluation results showed that the new generation of lil had more advantages in terms of effectiveness and efficiency
password authentication is a very important system security procedure to gain access to user resources in the traditional password authentication methods a server has check the authenticity of the users in our proposed method users can freely select their passwords from a predefined character set they can also use a graphical image as password the password may be a character or an image it will be converted into binary form and the binary values will be normalized associative memories have been used recently for password authentication in order to overcome drawbacks of the traditional password authentication methods in this paper we proposed a method using bidirectional associative memory algorithm for both alphanumeric \( text \) and graphical password by doing so the amount of security what we provide for the user can be enhanced this paper along with test results show that converting user password in to probabilistic values and giving them as input for bam improves the security of the system
we investigate issues related to two hard problems related to voting , the optimal weighted lobbying problem and the winner problem for dodgson elections regarding the former , christian et al cfrs06 showed that optimal lobbying is intractable in the sense of parameterized complexity we provide an efficient greedy algorithm that achieves a logarithmic approximation ratio for this problem and even for a more general variant optimal weighted lobbying we prove that essentially no better approximation ratio than ours can be proven for this greedy algorithm the problem of determining dodgson winners is known to be complete for parallel access to np hhr97 homan and hemaspaandra hh06 proposed an efficient greedy heuristic for finding dodgson winners with a guaranteed frequency of success , and their heuristic is a ``frequently self knowingly correct algorithm '' we prove that every distributional problem solvable in polynomial time on the average with respect to the uniform distribution has a frequently self knowingly correct polynomial time algorithm furthermore , we study some features of probability weight of correctness with respect to procaccia and rosenschein 's junta distributions pr07
in this paper , we consider the problem of energy efficient uplink scheduling with delay constraint for a multi user wireless system we address this problem within the framework of constrained markov decision processes \( cmdps \) wherein one seeks to minimize one cost \( average power \) subject to a hard constraint on another \( average delay \) we do not assume the arrival and channel statistics to be known to handle state space explosion and informational constraints , we split the problem into individual cmdps for the users , coupled through their lagrange multipliers and a user selection problem at the base station to address the issue of unknown channel and arrival statistics , we propose a reinforcement learning algorithm the users use this learning algorithm to determine the rate at which they wish to transmit in a slot and communicate this to the base station the base station then schedules the user with the highest rate in a slot we analyze convergence , stability and optimality properties of the algorithm we also demonstrate the efficacy of the algorithm through simulations within ieee 802 16 system
we investigate labeling schemes supporting adjacency , ancestry , sibling , and connectivity queries in forests in the course of more than 20 years , the existence of log n o \( log log \) labeling schemes supporting each of these functions was proven , with the most recent being ancestry fraigniaud and korman , stoc '10 several multi functional labeling schemes also enjoy lower or upper bounds of log n omega \( log log n \) or log n o \( log log n \) respectively notably an upper bound of log n 5 log log n for adjacency siblings and a lower bound of log n log log n for each of the functions siblings , ancestry , and connectivity alstrup et al , soda '03 we improve the constants hidden in the o notation in particular we show a log n 2 log log n lower bound for connectivity ancestry and connectivity siblings , as well as an upper bound of log n 3 log log n o \( log log log n \) for connectivity adjacency siblings by altering existing methods in the context of dynamic labeling schemes it is known that ancestry requires omega \( n \) bits cohen , et al pods '02 in contrast , we show upper and lower bounds on the label size for adjacency , siblings , and connectivity of 2 log n bits , and 3 log n to support all three functions there exist efficient adjacency labeling schemes for planar , bounded treewidth , bounded arboricity and interval graphs in a dynamic setting , we show a lower bound of omega \( n \) for each of those families
many algorithms have been proposed for fitting network models with communities , but most of them do not scale well to large networks , and often fail on sparse networks here we propose a new fast pseudo likelihood method for fitting the stochastic block model for networks , as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees we show that the algorithms perform well under a range of settings , including on very sparse networks , and illustrate on the example of a network of political blogs we also propose spectral clustering with perturbations , a method of independent interest , which works well on sparse networks where regular spectral clustering fails , and use it to provide an initial value for pseudo likelihood we prove that pseudo likelihood provides consistent estimates of the communities under a mild condition on the starting value , for the case of a block model with two communities
in the past decade , blogging web sites have become more sophisticated and influential than ever much of this sophistication and influence follows from their network organization blogging social networks \( bsns \) allow individual bloggers to form contact lists , subscribe to other blogs , comment on blog posts , declare interests , and participate in collective blogs thus , a bsn is a bimodal venue , where users can engage in publishing \( post \) as well as in social \( make friends \) activities in this paper , we study the co evolution of both activities we observed a significant positive correlation between blogging and socializing in addition , we identified a number of user archetypes that correspond to mainly bloggers , mainly socializers , etc we analyzed a bsn at the level of individual posts and changes in contact lists and at the level of trajectories in the friendship publishing space both approaches produced consistent results the majority of bsn users are passive readers publishing is the dominant active behavior in a bsn and social activities complement blogging , rather than compete with it
the 1 bit compressed sensing framework enables the recovery of a sparse vector x from the sign information of each entry of its linear transformation discarding the amplitude information can significantly reduce the amount of data , which is highly beneficial in practical applications in this paper , we present a bayesian approach to signal reconstruction for 1 bit compressed sensing , and analyze its typical performance using statistical mechanics utilizing the replica method , we show that the bayesian approach enables better reconstruction than the l1 norm minimization approach , asymptotically saturating the performance obtained when the non zero entries positions of the signal are known we also test a message passing algorithm for signal reconstruction on the basis of belief propagation the results of numerical experiments are consistent with those of the theoretical analysis
an interleaver is a critical component for the channel coding performance of turbo codes algebraic constructions are of particular interest because they admit analytical designs and simple , practical hardware implementation contention free interleavers have been recently shown to be suitable for parallel decoding of turbo codes in this correspondence , it is shown that permutation polynomials generate maximum contention free interleavers , i e , every factor of the interleaver length becomes a possible degree of parallel processing of the decoder further , it is shown by computer simulations that turbo codes using these interleavers perform very well for the 3rd generation partnership project \( 3gpp \) standard
in this paper , we generalize spencer 's hyperbolic cosine algorithm to the matrix valued setting we apply the proposed algorithm to several problems by analyzing its computational efficiency under two special cases of matrices one in which the matrices have a group structure and an other in which they have rank one as an application of the former case , we present a deterministic algorithm that , given the multiplication table of a finite group of size n , it constructs an expanding cayley graph of logarithmic degree in near optimal o \( n 2 log 3 n \) time for the latter case , we present a fast deterministic algorithm for spectral sparsification of positive semi definite matrices , which implies an improved deterministic algorithm for spectral graph sparsification of dense graphs in addition , we give an elementary connection between spectral sparsification of positive semi definite matrices and element wise matrix sparsification as a consequence , we obtain improved element wise sparsification algorithms for diagonally dominant like matrices
study of patterns on images is recognized as an important step in characterization and classification of image the ability to efficiently analyze and describe image patterns is thus of fundamental importance the study of syntactic methods of describing pictures has been of interest for researchers array grammars can be used to represent and recognize connected patterns in any image the patterns are recognized using connected patterns it is very difficult to represent all connected patterns \( cp \) even on a small 3 x 3 neighborhood in a pictorial way the present paper proposes the model of array grammar capable of generating any kind of simple or complex pattern and derivation of connected pattern in an image neighborhood using the proposed grammar is discussed
the word2vec software of tomas mikolov and colleagues \( https code google com p word2vec \) has gained a lot of traction lately , and provides state of the art word embeddings the learning models behind the software are described in two research papers we found the description of the models in these papers to be somewhat cryptic and hard to follow while the motivations and presentation may be obvious to the neural networks language modeling crowd , we had to struggle quite a bit to figure out the rationale behind the equations this note is an attempt to explain equation \( 4 \) \( negative sampling \) in distributed representations of words and phrases and their compositionality by tomas mikolov , ilya sutskever , kai chen , greg corrado and jeffrey dean
unicast communication over a network of m parallel relays in the presence of an eavesdropper is considered the relay nodes , operating under individual power constraints , amplify and forward the signals received at their inputs the problem of the maximum secrecy rate achievable with af relaying is addressed previous work on this problem provides iterative algorithms based on semidefinite relaxation however , those algorithms result in suboptimal performance without any performance and convergence guarantees we address this problem for three specific network models , with real valued channel gains we propose a novel transformation that leads to convex optimization problems our analysis leads to \( i \) a polynomial time algorithm to compute the optimal secure af rate for two of the models and \( ii \) a closed form expression for the optimal secure rate for the other
automatic speech recognition \( asr \) by machine is an attractive research topic in signal processing domain and has attracted many researchers to contribute in this area in recent year , there have been many advances in automatic speech reading system with the inclusion of audio and visual speech features to recognize words under noisy conditions the objective of audio visual speech recognition system is to improve recognition accuracy in this paper we computed visual features using zernike moments and audio feature using mel frequency cepstral coefficients \( mfcc \) on vviswa \( visual vocabulary of independent standard words \) dataset which contains collection of isolated set of city names of 10 speakers the visual features were normalized and dimension of features set was reduced by principal component analysis \( pca \) in order to recognize the isolated word utterance on pca space the performance of recognition of isolated words based on visual only and audio only features results in 63 88 and 100 respectively
in this paper we study the quantization stage that is implicit in any compressed sensing signal acquisition paradigm we propose using sigma delta quantization and a subsequent reconstruction scheme based on convex optimization we prove that the reconstruction error due to quantization decays polynomially in the number of measurements our results apply to arbitrary signals , including compressible ones , and account for measurement noise additionally , they hold for sub gaussian \( including gaussian and bernoulli \) random compressed sensing measurements , as well as for both high bit depth and coarse quantizers , and they extend to 1 bit quantization in the noise free case , when the signal is strictly sparse we prove that by optimizing the order of the quantization scheme one can obtain root exponential decay in the reconstruction error due to quantization
this paper introduces the new robot programming language lightrocks \( light weight robot coding for skills \) , a domain specific language \( dsl \) for robot programming the language offers three different level of abstraction for robot programming on lowest level skills are coded by domain experts on a more abstract level these skills are supposed to be combined by shop floor workers or technicians to define tasks the language is designed to allow as much flexibility as necessary on the lowest level of abstraction and is kept as simple as possible with the more abstract layers a statechart like model is used to describe the different levels of detail for this we apply the uml p and the language workbench monticore to this end we are able to generate code while hiding controller specific implementation details in addition the development in lightrocks is supported by a generic graphical editor implemented as an eclipse plugin
we say that a permutation p is 'merged' from permutations q and r , if we can color the elements of p red and blue so that the red elements are order isomorphic to q and the blue ones to r a 'permutation class' is a set of permutations closed under taking subpermutations a permutation class c is 'splittable' if it has two proper subclasses a and b such that every element of c can be obtained by merging an element of a with an element of b several recent papers use splittability as a tool in deriving enumerative results for specific permutation classes the goal of this paper is to study splittability systematically as our main results , we show that if q is a sum decomposable permutation of order at least four , then the class av \( q \) of all q avoiding permutations is splittable , while if q is a simple permutation , then av \( q \) is unsplittable we also show that there is a close connection between splittings of certain permutation classes and colorings of circle graphs of bounded clique size indeed , our splittability results can be interpreted as a generalization of a theorem of gy 'arf 'as stating that circle graphs of bounded clique size have bounded chromatic number
most research on adaptive decision making takes a strategy first approach , proposing a method of solving a problem and then examining whether it can be implemented in the brain and in what environments it succeeds we present a method for studying strategy development based on computational evolution that takes the opposite approach , allowing strategies to develop in response to the decision making environment via darwinian evolution we apply this approach to a dynamic decision making problem where artificial agents make decisions about the source of incoming information in doing so , we show that the complexity of the brains and strategies of evolved agents are a function of the environment in which they develop more difficult environments lead to larger brains and more information use , resulting in strategies resembling a sequential sampling approach less difficult environments drive evolution toward smaller brains and less information use , resulting in simpler heuristic like strategies
an overview of game theoretic approaches to energy efficient resource allocation in wireless networks is presented focusing on multiple access networks , it is demonstrated that game theory can be used as an effective tool to study resource allocation in wireless networks with quality of service \( qos \) constraints a family of non cooperative \( distributed \) games is presented in which each user seeks to choose a strategy that maximizes its own utility while satisfying its qos requirements the utility function considered here measures the number of reliable bits that are transmitted per joule of energy consumed and , hence , is particulary suitable for energy constrained networks the actions available to each user in trying to maximize its own utility are at least the choice of the transmit power and , depending on the situation , the user may also be able to choose its transmission rate , modulation , packet size , multiuser receiver , multi antenna processing algorithm , or carrier allocation strategy the best response strategy and nash equilibrium for each game is presented using this game theoretic framework , the effects of power control , rate control , modulation , temporal and spatial signal processing , carrier allocation strategy and delay qos constraints on energy efficiency and network capacity are quantified
in this paper , we study the downlink performance of two important 5g network architectures , i e massive multiple input multiple output \( m mimo \) and small cell densification we propose a comparative modeling for the two systems , where the user and antenna base station \( bs \) locations are distributed according to poisson point processes \( ppps \) we then leverage both the stochastic geometry results and large system analytical tool to study the sir distribution and the average shannon and outage rates of each network by comparing these results , we observe that for user average spectral efficiency , small cell densification is favorable in crowded areas with moderate to high user density and massive mimo with low user density however , small cell systems outperform m mimo in all cases when the performance metric is the energy efficiency the results of this paper are useful for the optimal design of practical 5g networks
phase retrieval arises in various fields of science and engineering and it is well studied in a finite dimensional setting in this paper , we consider an infinite dimensional phase retrieval problem to reconstruct real valued signals living in a shift invariant space from its phaseless samples taken either on the whole line or on a set with finite sampling rate we find the equivalence between nonseparability of signals in a linear space and its phase retrievability with phaseless samples taken on the whole line for a spline signal of order n , we show that it can be well approximated , up to a sign , from its noisy phaseless samples taken on a set with sampling rate 2n 1 we propose an algorithm to reconstruct nonseparable signals in a shift invariant space generated by a compactly supported continuous function the proposed algorithm is robust against bounded sampling noise and it could be implemented in a distributed manner
distinguishability takes a crucial rule in studying observability of hybrid system such as switched system recently , for two linear systems , lou and si gave a condition not only necessary but also sufficient to the distinguishability of linear systems however , the condition is not easy enough to verify this paper will give a new equivalent condition which is relatively easy to verify
in this work we present a novel end to end framework for tracking and classifying a robot 's surroundings in complex , dynamic and only partially observable real world environments the approach deploys a recurrent neural network to filter an input stream of raw laser measurements in order to directly infer object locations , along with their identity in both visible and occluded areas to achieve this we first train the network using unsupervised deep tracking , a recently proposed theoretical framework for end to end space occupancy prediction we show that by learning to track on a large amount of unsupervised data , the network creates a rich internal representation of its environment which we in turn exploit through the principle of inductive transfer of knowledge to perform the task of it 's semantic classification as a result , we show that only a small amount of labelled data suffices to steer the network towards mastering this additional task furthermore we propose a novel recurrent neural network architecture specifically tailored to tracking and semantic classification in real world robotics applications we demonstrate the tracking and classification performance of the method on real world data collected at a busy road junction our evaluation shows that the proposed end to end framework compares favourably to a state of the art , model free tracking solution and that it outperforms a conventional one shot training scheme for semantic classification
in social networks , control of rumor spread is an active area of research sir model is generally used to study the rumor dynamics in network while considering the rumor as an epidemic in disease spreading model , epidemic is controlled by removing central nodes in the network full network information is needed for such removal to have the information of complete network is difficult proposition as a consequence , the search of an algorithm that may control epidemic without needing global information is a matter of great interest in this paper , an immunization strategy is proposed that uses only local information available at a node , viz degree of the node and average degree of its neighbour nodes proposed algorithm has been evaluated for scale free network using sir model numerical results show that proposed method has less complexity and gives significantly better results in comparison with other strategies while using only local information
there is a compelling demand for the data integration and exploitation of heterogeneous biomedical information for improved clinical practice , medical research , and personalised healthcare across the eu the area of paediatric information integration is particularly challenging since the patients physiology changes with growth and different aspects of health being regularly monitored over extended periods of time paediatricians require access to heterogeneous data sets , often collected in different locations with different apparatus and over extended timescales using a grid platform originally developed for physics at cern and a novel integrated semantic data model the health e child project has developed an integrated healthcare platform for european paediatrics , providing seamless integration of traditional and emerging sources of biomedical data the long term goal of the project was to provide uninhibited access to universal biomedical knowledge repositories for personalised and preventive healthcare , large scale information based biomedical research and training , and informed policy making the project built a grid enabled european network of leading clinical centres that can share and annotate paediatric data , can validate systems clinically , and diffuse clinical excellence across europe by setting up new technologies , clinical workflows , and standards the health e child project highlights data management challenges for the future of european paediatric healthcare and is the subject of this chapter
there has been much effort on studying how social media sites , such as twitter , help propagate information in different situations , including spreading alerts and sos messages in an emergency however , existing work has not addressed how to actively identify and engage the right strangers at the right time on social media to help effectively propagate intended information within a desired time frame to address this problem , we have developed two models \( i \) a feature based model that leverages peoples' exhibited social behavior , including the content of their tweets and social interactions , to characterize their willingness and readiness to propagate information on twitter via the act of retweeting and \( ii \) a wait time model based on a user 's previous retweeting wait times to predict her next retweeting time when asked based on these two models , we build a recommender system that predicts the likelihood of a stranger to retweet information when asked , within a specific time window , and recommends the top n qualified strangers to engage with our experiments , including live studies in the real world , demonstrate the effectiveness of our work
we study the problem of learning high dimensional regression models regularized by a structured sparsity inducing penalty that encodes prior structural information on either input or output sides we consider two widely adopted types of such penalties as our motivating examples 1 \) overlapping group lasso penalty , based on the l1 l2 mixed norm penalty , and 2 \) graph guided fusion penalty for both types of penalties , due to their non separability , developing an efficient optimization method has remained a challenging problem in this paper , we propose a general optimization approach , called smoothing proximal gradient method , which can solve the structured sparse regression problems with a smooth convex loss and a wide spectrum of structured sparsity inducing penalties our approach is based on a general smoothing technique of nesterov it achieves a convergence rate faster than the standard first order method , subgradient method , and is much more scalable than the most widely used interior point method numerical results are reported to demonstrate the efficiency and scalability of the proposed method
the ans family of arithmetic coders developed by jarek duda has the unique property that encoder and decoder are completely symmetric in the sense that a decoder reading bits will be in the exact same state that the encoder was in when writing those bits all buffering of information is explicitly part of the coder state and identical between encoder and decoder as a consequence , the output from multiple abs ans coders can be interleaved into the same bitstream without any additional metadata this allows for very efficient encoding and decoding on cpus supporting superscalar execution or simd instructions , as well as gpu implementations we also show how interleaving without additional metadata can be implemented for any entropy coder , at some increase in encoder complexity
in this paper we initiate the study of optimization of bandit type problems in scenarios where the feedback of a play is not immediately known this arises naturally in allocation problems which have been studied extensively in the literature , albeit in the absence of delays in the feedback we study this problem in the bayesian setting in presence of delays , no solution with provable guarantees is known to exist with sub exponential running time we show that bandit problems with delayed feedback that arise in allocation settings can be forced to have significant structure , with a slight loss in optimality this structure gives us the ability to reason about the relationship of single arm policies to the entangled optimum policy , and eventually leads to a o \( 1 \) approximation for a significantly general class of priors the structural insights we develop are of key interest and carry over to the setting where the feedback of an action is available instantaneously , and we improve all previous results in this setting as well
this paper studies emulation of induction by coinduction in a call by name language with control operators since it is known that call by name programming languages with control operators cannot have general initial algebras , interaction of induction and control operators is often restricted to effect free functions we show that some class of such restricted inductive types can be derived from full coinductive types by the power of control operators as a typical example of our results , the type of natural numbers is represented by the type of streams the underlying idea is a counterpart of the fact that some coinductive types can be expressed by inductive types in call by name pure language without side effects
previous work identifying depth optimal n channel sorting networks for 9 leq n leq 16 is based on exploiting symmetries of the first two layers however , the naive generate and test approach typically applied does not scale this paper revisits the problem of generating two layer prefixes modulo symmetries an improved notion of symmetry is provided and a novel technique based on regular languages and graph isomorphism is shown to generate the set of non symmetric representations an empirical evaluation demonstrates that the new method outperforms the generate and test approach by orders of magnitude and easily scales until n 40
in this paper , an efficient distributed approach for implementing the approximate message passing \( amp \) algorithm , named distributed amp \( damp \) , is developed for compressed sensing \( cs \) recovery in sensor networks with the sparsity k unknown in the proposed damp , distributed sensors do not have to use or know the entire global sensing matrix , and the burden of computation and storage for each sensor is reduced to reduce communications among the sensors , a new data query algorithm , called global computation for amp \( gcamp \) , is proposed the proposed gcamp based damp approach has exactly the same recovery solution as the centralized amp algorithm , which is proved theoretically in the paper the performance of the damp approach is evaluated in terms of the communication cost saved by using gcamp for comparison purpose , thresholding algorithm \( ta \) , a well known distributed top k algorithm , is modified so that it also leads to the same recovery solution as the centralized amp numerical results demonstrate that the gcamp based damp outperforms the modified ta based damp , and reduces the communication cost significantly
this paper explores the use of a bayesian non parametric topic modeling technique for the purpose of anomaly detection in video data we present results from two experiments the first experiment shows that the proposed technique is automatically able characterize the underlying terrain , and detect anomalous flora in image data collected by an underwater robot the second experiment shows that the same technique can be used on images from a static camera in a dynamic unstructured environment the second dataset consisting of video data from a static seafloor camera , capturing images of a busy coral reef the proposed technique was able to detect all three instances of an underwater vehicle passing in front of the camera , amongst many other observations of fishes , debris , lighting changes due to surface waves , and benthic flora
polar codes were recently introduced by ar i kan they achieve the capacity of arbitrary symmetric binary input discrete memoryless channels under a low complexity successive cancellation decoding strategy the original polar code construction is closely related to the recursive construction of reed muller codes and is based on the 2 times 2 matrix bigl 1 0 1 1 bigr it was shown by ar i kan and telatar that this construction achieves an error exponent of frac12 , i e , that for sufficiently large blocklengths the error probability decays exponentially in the square root of the length it was already mentioned by ar i kan that in principle larger matrices can be used to construct polar codes a fundamental question then is to see whether there exist matrices with exponent exceeding frac12 we first show that any ell times ell matrix none of whose column permutations is upper triangular polarizes symmetric channels we then characterize the exponent of a given square matrix and derive upper and lower bounds on achievable exponents using these bounds we show that there are no matrices of size less than 15 with exponents exceeding frac12 further , we give a general construction based on bch codes which for large n achieves exponents arbitrarily close to 1 and which exceeds frac12 for size 16
tetravex is a widely played one person computer game in which you are given n 2 unit tiles , each edge of which is labelled with a number the objective is to place each tile within a n by n square such that all neighbouring edges are labelled with an identical number unfortunately , playing tetravex is computationally hard more precisely , we prove that deciding if there is a tiling of the tetravex board is np complete deciding where to place the tiles is therefore np hard this may help to explain why tetravex is a good puzzle this result compliments a number of similar results for one person games involving tiling for example , np completeness results have been shown for the offline version of tetris , kplumber \( which involves rotating tiles containing drawings of pipes to make a connected network \) , and shortest sliding puzzle problems it raises a number of open questions for example , is the infinite version turing complete \? how do we generate tetravex problems which are truly puzzling as random np complete problems are often surprising easy to solve \? can we observe phase transition behaviour \? what about the complexity of the problem when it is guaranteed to have an unique solution \? how do we generate puzzles with unique solutions \?
this paper addresses the task of zero shot image classification the key contribution of the proposed approach is to control the semantic embedding of images one of the main ingredients of zero shot learning by formulating it as a metric learning problem the optimized empirical criterion associates two types of sub task constraints metric discriminating capacity and accurate attribute prediction this results in a novel expression of zero shot learning not requiring the notion of class in the training phase only pairs of image attributes , augmented with a consistency indicator , are given as ground truth at test time , the learned model can predict the consistency of a test image with a given set of attributes , allowing flexible ways to produce recognition inferences despite its simplicity , the proposed approach gives state of the art results on four challenging datasets used for zero shot recognition evaluation
a popular method in combinatorial optimization is to express polytopes p , which may potentially have exponentially many facets , as solutions of linear programs that use few extra variables to reduce the number of constraints down to a polynomial after two decades of standstill , recent years have brought amazing progress in showing lower bounds for the so called extension complexity , which for a polytope p denotes the smallest number of inequalities necessary to describe a higher dimensional polytope q that can be linearly projected on p however , the central question in this field remained wide open can the perfect matching polytope be written as an lp with polynomially many constraints \? we answer this question negatively in fact , the extension complexity of the perfect matching polytope in a complete n node graph is 2 omega \( n \) by a known reduction this also improves the lower bound on the extension complexity for the tsp polytope from 2 omega \( n 1 2 \) to 2 omega \( n \)
in this paper , we consider physical layer security provisioning in multi cell massive multiple input multiple output \( mimo \) systems specifically , we consider secure downlink transmission in a multi cell massive mimo system with matched filter precoding and artificial noise \( an \) generation at the base station \( bs \) in the presence of a passive multi antenna eavesdropper we investigate the resulting achievable ergodic secrecy rate and the secrecy outage probability for the cases of perfect training and pilot contamination thereby , we consider two different an shaping matrices , namely , the conventional an shaping matrix , where the an is transmitted in the null space of the matrix formed by all user channels , and a random an shaping matrix , which avoids the complexity associated with finding the null space of a large matrix our analytical and numerical results reveal that in multi cell massive mimo systems employing matched filter precoding \( 1 \) an generation is required to achieve a positive ergodic secrecy rate if the user and the eavesdropper experience the same path loss , \( 2 \) even with an generation secure transmission may not be possible if the number of eavesdropper antennas is too large and not enough power is allocated to channel estimation , \( 3 \) for a given fraction of power allocated to an and a given number of users , in case of pilot contamination , the ergodic secrecy rate is not a monotonically increasing function of the number of bs antennas , and \( 4 \) random an shaping matrices provide a favourable performance complexity tradeoff and are an attractive alternative to conventional an shaping matrices
the cooperative motion algorithm is an e \? cient lattice method to simulate dense polymer systems and is often used with two di \? erent criteria to generate a markov chain in the con \? guration space while the \? rst method is the well established metropolis algorithm , the other one is an heuristic algorithm which needs justi \? cation as an introductory step towards justi \? cation for the 3d lattice polymers , we study a simple system which is the binary equimolar uid on a 2d triangular lattice since all lattice sites are occupied only selected type of motions are considered , such the vacancy movements , swapping neighboring lattice sites \( kawasaki dynamics \) and cooperative loops we compare both methods , calculating the energy as well as heat capacity as a function of temperature the critical temperature , which was determined using the binder cumulant , was the same for all methods with the simulation accuracy and in agreement with the exact critical temperature for the ising model on the 2d triangular lattice in order to achieve reliable results at low temperatures we employ the parallel tempering algorithm which enables simultaneous simulations of replicas of the system in a wide range of temperatures
in this note we study the greedy algorithm for combinatorial auctions with submodular bidders it is well known that this algorithm provides an approximation ratio of 2 for every order of the items we show that if the valuations are vertex cover functions and the order is random then the expected approximation ratio imrpoves to frac 7 4
we consider a basic content distribution scenario consisting of a single origin server connected through a shared bottleneck link to a number of users each equipped with a cache of finite memory the users issue a sequence of content requests from a set of popular files , and the goal is to operate the caches as well as the server such that these requests are satisfied with the minimum number of bits sent over the shared link assuming a basic markov model for renewing the set of popular files , we characterize approximately the optimal long term average rate of the shared link we further prove that the optimal online scheme has approximately the same performance as the optimal offline scheme , in which the cache contents can be updated based on the entire set of popular files before each new request to support these theoretical results , we propose an online coded caching scheme termed coded least recently sent \( lrs \) and simulate it for a demand time series derived from the dataset made available by netflix for the netflix prize for this time series , we show that the proposed coded lrs algorithm significantly outperforms the popular least recently used \( lru \) caching algorithm
we study the problem of a seller dynamically pricing d distinct types of goods , when faced with the online arrival of buyers drawn independently from an unknown distribution the seller observes only the bundle of goods purchased at each day , but nothing else about the buyer 's valuation function when buyers have strongly concave , holder continuous valuation functions , we give a pricing scheme that finds a pricing that optimizes welfare \( including the seller 's cost of production \) in time and number of rounds that are polynomial in d and the accuracy parameter we are able to do this despite the fact that \( i \) welfare is a non concave function of the prices , and \( ii \) the welfare is not observable to the seller we also extend our results to a limited supply setting
one class classifiers offer valuable tools to assess the presence of outliers in data in this paper , we propose a design methodology for one class classifiers based on entropic spanning graphs the spanning graph is learned on the embedded input data , with the aim to generate a partition of the vertices the final partition is derived by exploiting a criterion based on mutual information minimization here , we compute the mutual information by using a convenient formulation provided in terms of the alpha jensen difference once training is completed , in order to associate a confidence level with the classifier decision , a graph based fuzzy model is constructed the fuzzification process is based only on topological information of the vertices of the entropic spanning graph as such , the proposed one class classifier is suitable also for datasets with complex geometric structures we provide experiments on well known benchmarking datasets containing both feature vectors and labeled graphs in addition , we apply the method on the problem of protein solubility recognition by considering several data representations for the samples experimental results demonstrate the effectiveness and versatility of the proposed method with respect to other state of the art approaches
inferring the coupling structure of complex systems from time series data in general by means of statistical and information theoretic techniques is a challenging problem in applied science the reliability of statistical inferences requires the construction of suitable information theoretic measures that take into account both direct and indirect influences , manifest in the form of information flows , between the components within the system in this work , we present an application of the optimal causation entropy \( ocse \) principle to identify the coupling structure of a synthetic biological system , the repressilator specifically , when the system reaches an equilibrium state , we use a stochastic perturbation approach to extract time series data that approximate a linear stochastic process then , we present and jointly apply the aggregative discovery and progressive removal algorithms based on the ocse principle to infer the coupling structure of the system from the measured data finally , we show that the success rate of our coupling inferences not only improves with the amount of available data , but it also increases with a higher frequency of sampling and is especially immune to false positives
human computation is a computing approach that draws upon human cognitive abilities to solve computational tasks for which there are so far no satisfactory fully automated solutions even when using the most advanced computing technologies available human computation for citizen science projects consists in designing systems that allow large crowds of volunteers to contribute to scientific research by executing human computation tasks examples of successful projects are galaxy zoo and foldit a key feature of this kind of project is its capacity to engage volunteers an important requirement for the proposal and evaluation of new engagement strategies is having a clear understanding of the typical engagement of the volunteers however , even though several projects of this kind have already been completed , little is known about this issue in this paper , we investigate the engagement pattern of the volunteers in their interactions in human computation for citizen science projects , how they differ among themselves in terms of engagement , and how those volunteer engagement features should be taken into account for establishing the engagement encouragement strategies that should be brought into play in a given project to this end , we define four quantitative engagement metrics to measure different aspects of volunteer engagement , and use data mining algorithms to identify the different volunteer profiles in terms of the engagement metrics our study is based on data collected from two projects galaxy zoo and the milky way project the results show that the volunteers in such projects can be grouped into five distinct engagement profiles that we label as follows hardworking , spasmodic , persistent , lasting , and moderate the analysis of these profiles provides a deeper understanding of the nature of volunteers' engagement in human computation for citizen science projects
direct quantile regression involves estimating a given quantile of a response variable as a function of input variables we present a new framework for direct quantile regression where a gaussian process model is learned , minimising the expected tilted loss function the integration required in learning is not analytically tractable so to speed up the learning we employ the expectation propagation algorithm we describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets the method is shown to be competitive with state of the art methods whilst allowing for the leverage of the full gaussian process probabilistic framework
this paper investigates the interplay between cooperation and achievable rates in multi terminal networks cooperation refers to the process of nodes working together to relay data toward the destination there is an inherent tradeoff between achievable information transmission rates and the level of cooperation , which is determined by how many nodes are involved and how the nodes encode decode the data we illustrate this trade off by studying information theoretic decode forward based coding strategies for data transmission in multi terminal networks decode forward strategies are usually discussed in the context of omniscient coding , in which all nodes in the network fully cooperate with each other , both in encoding and decoding in this paper , we investigate myopic coding , in which each node cooperates with only a few neighboring nodes we show that achievable rates of myopic decode forward can be as large as that of omniscient decode forward in the low snr regime we also show that when each node has only a few cooperating neighbors , adding one node into the cooperation increases the transmission rate significantly furthermore , we show that myopic decode forward can achieve non zero rates as the network size grows without bound
propositional dynamic logic or pdl was invented as a logic for reasoning about regular programming constructs we propose a new perspective on pdl as a multi agent strategic logic \( masl \) this logic for strategic reasoning has group strategies as first class citizens , and brings game logic closer to standard modal logic we demonstrate that masl can express key notions of game theory , social choice theory and voting theory in a natural way , we give a sound and complete proof system for masl , and we show that masl encodes coalition logic next , we extend the language to epistemic multi agent strategic logic \( emasl \) , we give examples of what it can express , we propose to use it for posing new questions in epistemic social choice theory , and we give a calculus for reasoning about a natural class of epistemic game models we end by listing avenues for future research and by tracing connections to a number of other logics for reasoning about strategies
motivated by the rank modulation scheme , a recent work by sala and dolecek explored the study of constraint codes for permutations the constraint studied by them is inherited by the inter cell interference phenomenon in flash memories , where high level cells can inadvertently increase the level of low level cells in this paper , the model studied by sala and dolecek is extended into two constraints a permutation sigma in s n satisfies the emph two neighbor k constraint if for all 2 leq i leq n 1 either sigma \( i 1 \) sigma \( i \) leq k or sigma \( i \) sigma \( i 1 \) leq k , and it satisfies the emph asymmetric two neighbor k constraint if for all 2 leq i leq n 1 , either sigma \( i 1 \) sigma \( i \) k or sigma \( i 1 \) sigma \( i \) k we show that the capacity of the first constraint is \( 1 epsilon \) 2 in case that k theta \( n epsilon \) and the capacity of the second constraint is 1 regardless to the value of k we also extend our results and study the capacity of these two constraints combined with error correction codes in the kendall 's tau metric
we propose a graphical model for representing networks of stochastic processes , the minimal generative model graph it is based on reduced factorizations of the joint distribution over time we show that under appropriate conditions , it is unique and consistent with another type of graphical model , the directed information graph , which is based on a generalization of granger causality we demonstrate how directed information quantifies granger causality in a particular sequential prediction setting we also develop efficient methods to estimate the topological structure from data that obviate estimating the joint statistics one algorithm assumes upper bounds on the degrees and uses the minimal dimension statistics necessary in the event that the upper bounds are not valid , the resulting graph is nonetheless an optimal approximation another algorithm uses near minimal dimension statistics when no bounds are known but the distribution satisfies a certain criterion analogous to how structure learning algorithms for undirected graphical models use mutual information estimates , these algorithms use directed information estimates we characterize the sample complexity of two plug in directed information estimators and obtain confidence intervals for the setting when point estimates are unreliable , we propose an algorithm that uses confidence intervals to identify the best approximation that is robust to estimation error lastly , we demonstrate the effectiveness of the proposed algorithms through analysis of both synthetic data and real data from the twitter network in the latter case , we identify which news sources influence users in the network by merely analyzing tweet times
hyperspectral instruments \( hsis \) measure the electromagnetic energy emitted by materials at high resolution \( hundreds to thousands of channels \) enabling material identification through spectroscopic analysis laser induced breakdown spectroscopy \( libs \) is used by the chemcam instrument on the curiosity rover to measure the emission spectra of surface materials on mars from orbit , hyperspectral instruments \( hsis \) on the crism instrument of the mars reconnaissance orbiter \( mro \) measure the electromagnetic energy emitted by materials at high resolution \( hundreds to thousands of channels \) enabling material identification through spectroscopic analysis the data received are noisy , high dimensional , and largely unlabeled the ability to accurately predict elemental and material compositions of surface samples as well as to simulate spectra from hypothetical compositions , collectively known as hyperspectral unmixing , is invaluable to the exploration process the nature of the problem allows us to construct deep \( semi supervised \) generative models to accomplish both these tasks while making use of a large unlabeled dataset our main technical contribution is an invertibility trick where we train our model in reverse
a well known method for convergence acceleration of continued fraction k \( a n b n \) is to use the modified approximants s n \( omega n \) in place of the classical approximants s n \( 0 \) , where omega n are close to tails f \( n \) of continued fraction recently , author proposed a method of iterative character producing tail approximations whose asymptotic expansion 's accuracy is improving in each step this method can be applied to continued fractions k \( a n b n \) , where a n , b n are polynomials in n \( deg a n 2 , deg b n leq 1 \) for sufficiently large n the purpose of this paper is to extend this idea for the class of continued fractions k \( a n b n a n' b n' \) , where a n , a n' , b n , b n' are polynomials in n \( deg a n deg a n' , deg b n deg b n' \) we give examples involving such continued fraction expansions of some mathematical constants , as well as elementary and special functions
we describe how we connected three programs that compute groebner bases to coq , to do automated proofs on algebraic , geometrical and arithmetical expressions the result is a set of coq tactics and a certificate mechanism \( downloadable at http www sop inria fr marelle loic pottier gb keappa tgz \) the programs are f4 , gb , and gbcoq f4 and gb are the fastest \( up to our knowledge \) available programs that compute groebner bases gbcoq is slow in general but is proved to be correct \( in coq \) , and we adapted it to our specific problem to be efficient the automated proofs concern equalities and non equalities on polynomials with coefficients and indeterminates in r or z , and are done by reducing to groebner computation , via hilbert 's nullstellensatz we adapted also the results of harrison , to allow to prove some theorems about modular arithmetics the connection between coq and the programs that compute groebner bases is done using the external tactic of coq that allows to call arbitrary programs accepting xml inputs and outputs we also produce certificates in order to make the proof scripts independant from the external programs
standard belief change assumes an underlying logic containing full classical propositional logic however , there are good reasons for considering belief change in less expressive logics as well in this paper we build on recent investigations by delgrande on contraction for horn logic we show that the standard basic form of contraction , partial meet , is too strong in the horn case this result stands in contrast to delgrande 's conjecture that orderly maxichoice is the appropriate form of contraction for horn logic we then define a more appropriate notion of basic contraction for the horn case , influenced by the convexity property holding for full propositional logic and which we refer to as infra contraction the main contribution of this work is a result which shows that the construction method for horn contraction for belief sets based on our infra remainder sets corresponds exactly to hansson 's classical kernel contraction for belief sets , when restricted to horn logic this result is obtained via a detour through contraction for belief bases we prove that kernel contraction for belief bases produces precisely the same results as the belief base version of infra contraction the use of belief bases to obtain this result provides evidence for the conjecture that horn belief change is best viewed as a hybrid version of belief set change and belief base change one of the consequences of the link with base contraction is the provision of a representation result for horn contraction for belief sets in which a version of the core retainment postulate features
in maxsat , we are given a cnf formula f with n variables and m clauses and asked to find a truth assignment satisfying the maximum number of clauses let r 1 , , r m be the number of literals in the clauses of f then asat \( f \) sum i 1 m \( 1 2 r i \) is the expected number of clauses satisfied by a random truth assignment \( the truth values to the variables are distributed uniformly and independently \) it is well known that , in polynomial time , one can find a truth assignment satisfying at least asat \( f \) clauses in the parameterized problem maxsat aa , we are to decide whether there is a truth assignment satisfying at least asat \( f \) k clauses , where k is the parameter we prove that maxsat aa is para np complete and , thus , maxsat aa is not fixed parameter tractable unless p np this is in sharp contrast to maxlin2 aa which was recently proved to be fixed parameter tractable by crowston et al \( arxiv 1104 1135v3 \) in fact , we consider a more refined version of sc maxsat aa , sc max r \( n \) sat aa , where r j le r \( n \) for each j alon em et al \( soda 2010 \) proved that if r r \( n \) is a constant , then sc max r sat aa is fixed parameter tractable we prove that sc max r \( n \) sat aa is para np complete for r \( n \) lceil log n rceil we also prove that assuming the exponential time hypothesis , sc max r \( n \) sat aa is not in xp already for any r \( n \) ge log log n phi \( n \) , where phi \( n \) is any unbounded strictly increasing function this lower bound on r \( n \) cannot be decreased much further as we prove that sc max r \( n \) sat aa is \( i \) in xp for any r \( n \) le log log n log log log n and \( ii \) fixed parameter tractable for any r \( n \) le log log n log log log n phi \( n \) , where phi \( n \) is any unbounded strictly increasing function the proof uses some results on sc maxlin2 aa
the quality of life of many people could be improved by autonomous humanoid robots in the home to function in the human world , a humanoid household robot must be able to locate itself and perceive the environment like a human scene perception , object detection and segmentation , and object spatial localization in 3d are fundamental capabilities for such humanoid robots this paper presents a 3d multi class object detection and segmentation method the contributions are twofold firstly , we present a multi class detection method , where a minimal joint codebook is learned in a principled manner secondly , we incorporate depth information using rgb d imagery , which increases the robustness of the method and gives the 3d location of objects necessary since the robot reasons in 3d space experiments show that the multi class extension improves the detection efficiency with respect to the number of classes and the depth extension improves the detection robustness and give sufficient natural 3d location of the objects
model based compressed sensing refers to compressed sensing with extra structure about the underlying sparse signal known a priori recent work has demonstrated that both for deterministic and probabilistic models imposed on the signal , this extra information can be successfully exploited to enhance recovery performance in particular , weighted ell 1 minimization with suitable choice of weights has been shown to improve performance in the so called non uniform sparse model of signals in this paper , we consider a full generalization of the non uniform sparse model with very mild assumptions we prove that when the measurements are obtained using a matrix with i i d gaussian entries , weighted ell 1 minimization successfully recovers the sparse signal from its measurements with overwhelming probability we also provide a method to choose these weights for any general signal model from the non uniform sparse class of signal models
this paper presents a new method for pre training neural networks that can decrease the total training time for a neural network while maintaining the final performance , which motivates its use on deep neural networks by partitioning the training task in multiple training subtasks with sub models , which can be performed independently and in parallel , it is shown that the size of the sub models reduces almost quadratically with the number of subtasks created , quickly scaling down the sub models used for the pre training the sub models are then merged to provide a pre trained initial set of weights for the original model the proposed method is independent of the other aspects of the training , such as architecture of the neural network , training method , and objective , making it compatible with a wide range of existing approaches the speedup without loss of performance is validated experimentally on mnist and on cifar10 data sets , also showing that even performing the subtasks sequentially can decrease the training time moreover , we show that larger models may present higher speedups and conjecture about the benefits of the method in distributed learning systems
this paper investigates spectrum power trading be tween a small cell \( sc \) and a macro cell \( mc \) , where the sc consumes power to serve the macro cell users \( mus \) in exchange for some bandwidth from the mc our goal is to maximize the system energy efficiency \( ee \) of the sc while guaranteeing the quality of service \( qos \) of each mu as well as small cell users \( sus \) specifically , given the minimum data rate requirement and the bandwidth provided by the mc , the sc jointly optimizes mu selection , bandwidth allocation , and power allocation while guaranteeing its own minimum required system data rate the problem is challenging due to the binary mu selection variables and the fractional form objective function we first show that in order to achieve the maximum system ee , the bandwidth of an mu is shared with at most one su in the sc then , for a given mu selection , the optimal bandwidth and power allocations are obtained by exploiting the fractional programming to perform mu selection , we first introduce the concept of trading ee then , we reveal a sufficient and necessary condition for serving an mu without considering the total power constraint and the minimum data rate constraint based on this insight , we propose a low computational complexity mu selection algorithm simulation results demonstrate the effectiveness of the proposed algorithms
i motivate a variation \( due to k szlach ' a nyi \) of monoidal categories called skew monoidal categories where the unital and associativity laws are not required to be isomorphisms , only natural transformations coherence has to be formulated differently than in the well known monoidal case in my \( to my knowledge new \) version , it becomes a statement of uniqueness of normalizing rewrites i present a proof of this coherence theorem and also formalize it fully in the dependently typed programming language agda
in this paper , we prove a conjecture published in 1989 and also partially address an open problem announced at the conference on learning theory \( colt \) 2015 for an expected loss function of a deep nonlinear neural network , we prove the following statements under the independence assumption adopted from recent work 1 \) the function is non convex and non concave , 2 \) every local minimum is a global minimum , 3 \) every critical point that is not a global minimum is a saddle point , and 4 \) the property of saddle points differs for shallow networks \( with three layers \) and deeper networks \( with more than three layers \) moreover , we prove that the same four statements hold for deep linear neural networks with any depth , any widths and no unrealistic assumptions as a result , we present an instance , for which we can answer to the following question how difficult to directly train a deep model in theory \? it is more difficult than the classical machine learning models \( because of the non convexity \) , but not too difficult \( because of the nonexistence of poor local minima and the property of the saddle points \) we note that even though we have advanced the theoretical foundations of deep learning , there is still a gap between theory and practice
computability logic \( see http www cis upenn edu giorgi cl html \) is a long term project for redeveloping logic on the basis of a constructive game semantics , with games seen as abstract models of interactive computational problems among the fragments of this logic successfully axiomatized so far is cl12 a conservative extension of classical first order logic , whose language augments that of classical logic with the so called choice sorts of quantifiers and connectives this system has already found fruitful applications as a logical basis for constructive and complexity oriented versions of peano arithmetic , such as arithmetics for polynomial time computability , polynomial space computability , and beyond the present paper introduces a third , indispensable complexity measure for interactive computations termed amplitude complexity , and establishes the adequacy of cl12 with respect to a amplitude , s space and t time computability under certain minimal conditions on the triples \( a , s , t \) of function classes this result very substantially broadens the potential application areas of cl12 the paper is self contained , and targets readers with no prior familiarity with the subject
in this paper , we present a general review of hash functions in a cryptographic sense we give special emphasis on some particular topics such as cipher block chaining message authentication code \( cbc mac \) and its variants this paper also broadens the information given in some well known surveys , by including more details on block cipher based hash functions and security of different hash schemes
we describe here a rudimentary sage implementation of the bhattacharya mesner hypermatrix algebra package
with a view to provide a user friendly interface for designing , training and developing deep learning frameworks , we have developed expresso , a gui tool written in python expresso is built atop caffe , the open source , prize winning framework popularly used to develop convolutional neural networks expresso provides a convenient wizard like graphical interface which guides the user through various common scenarios data import , construction and training of deep networks , performing various experiments , analyzing and visualizing the results of these experiments the multi threaded nature of expresso enables concurrent execution and notification of events related to the aforementioned scenarios the gui sub components and inter component interfaces in expresso have been designed with extensibility in mind we believe expresso 's flexibility and ease of use will come in handy to researchers , newcomers and seasoned alike , in their explorations related to deep learning
a poisson binomial distribution over n variables is the distribution of the sum of n independent bernoullis we provide a sample near optimal algorithm for testing whether a distribution p supported on 0 , , n to which we have sample access is a poisson binomial distribution , or far from all poisson binomial distributions the sample complexity of our algorithm is o \( n 1 4 \) to which we provide a matching lower bound we note that our sample complexity improves quadratically upon that of the naive learn followed by tolerant test approach , while instance optimal identity testing vv14 is not applicable since we are looking to simultaneously test against a whole family of distributions
disjunctive answer set programming \( asp \) is a powerful declarative programming paradigm whose main decision problems are located on the second level of the polynomial hierarchy identifying tractable fragments and developing efficient algorithms for such fragments are thus important objectives in order to complement the sophisticated asp systems available to date hard problems can become tractable if some problem parameter is bounded by a fixed constant such problems are then called fixed parameter tractable \( fpt \) while several fpt results for asp exist , parameters that relate to directed or signed graphs representing the program at hand have been neglected so far in this paper , we first give some negative observations showing that directed width measures on the dependency graph of a program do not lead to fpt results we then consider the graph parameter of signed clique width and present a novel dynamic programming algorithm that is fpt w r t this parameter clique width is more general than the well known treewidth , and , to the best of our knowledge , ours is the first fpt algorithm for bounded clique width for reasoning problems beyond sat
we consider the design of optimal localized feedback gains for one dimensional formations in which vehicles only use information from their immediate neighbors the control objective is to enhance coherence of the formation by making it behave like a rigid lattice for the single integrator model with symmetric gains , we establish convexity , implying that the globally optimal controller can be computed efficiently we also identify a class of convex problems for double integrators by restricting the controller to symmetric position and uniform diagonal velocity gains to obtain the optimal non symmetric gains for both the single and the double integrator models , we solve a parameterized family of optimal control problems ranging from an easily solvable problem to the problem of interest as the underlying parameter increases when this parameter is kept small , we employ perturbation analysis to decouple the matrix equations that result from the optimality conditions , thereby rendering the unique optimal feedback gain this solution is used to initialize a homotopy based newton 's method to find the optimal localized gain to investigate the performance of localized controllers , we examine how the coherence of large scale stochastically forced formations scales with the number of vehicles we establish several explicit scaling relationships and show that the best performance is achieved by a localized controller that is both non symmetric and spatially varying
the coalitional manipulation \( cm \) problem has been studied extensively in the literature for many voting rules the cm problem , however , has been studied only in the complete information setting , that is , when the manipulators know the votes of the non manipulators a more realistic scenario is an incomplete information setting where the manipulators do not know the exact votes of the non manipulators but may have some partial knowledge of the votes in this paper , we study a setting where the manipulators know a partial order for each voter that is consistent with the vote of that voter in this setting , we introduce and study two natural computational problems \( 1 \) weak manipulation \( wm \) problem where the manipulators wish to vote in a way that makes their preferred candidate win in at least one extension of the partial votes of the non manipulators \( 2 \) strong manipulation \( sm \) problem where the manipulators wish to vote in a way that makes their preferred candidate win in all possible extensions of the partial votes of the non manipulators we study the computational complexity of the wm and the sm problems for commonly used voting rules such as plurality , veto , k approval , k veto , maximin , copeland , and bucklin our key finding is that , barring a few exceptions , manipulation becomes a significantly harder problem in the setting of incomplete votes
let g \( v 1 \( g \) , v 2 \( g \) , e \( g \) \) be a bipartite multigraph , and r subseteq v 1 \( g \) cup v 2 \( g \) a proper coloring of edges of g with the colors 1 , ldots , t is called interval \( respectively , continuous \) on r , if each color is used for at least one edge and the edges incident with each vertex x in r are colored by d \( x \) consecutive colors \( respectively , by the colors 1 , ldots , d \( x \) \) , where d \( x \) is a degree of the vertex x we denote by w 1 \( g \) and w 1 \( g \) , respectively , the least and the greatest values of t , for which there exists an interval on v 1 \( g \) coloring of the multigraph g with the colors 1 , ldots , t in the paper the following basic results are obtained textbf theorem 2 for an arbitrary k , w 1 \( g \) leq k leq w 1 \( g \) , there is an interval on v 1 \( g \) coloring of the multigraph g with the colors 1 , ldots , k textbf theorem 3 the problem of recognition of the existence of a continuous on v 1 \( g \) coloring of the multigraph g is np complete textbf theorem 4 if for any edge \( x , y \) in e \( g \) , where x in v 1 \( g \) , the inequality d \( x \) geq d \( y \) holds then there is a continuous on v 1 \( g \) coloring of the multigraph g textbf theorem 1 if g has no multiple edges and triangles , and there is an interval on v \( g \) coloring of the graph g with the colors 1 , ldots , k , then k leq v \( g \) 1
we study sigma delta \( sigma delta \) quantization of oversampled bandlimited functions we prove that digitally integrating blocks of bits and then down sampling , a process known as decimation , can efficiently encode the associated sigma delta bit stream it allows a large reduction in the bit rate while still permitting good approximation of the underlying bandlimited function via an appropriate reconstruction kernel specifically , in the case of stable r th order sigma delta schemes we show that the reconstruction error decays exponentially in the bit rate for example , this result applies to the 1 bit , greedy , first order sigma delta scheme
the reference scenario in this paper is a single cell in a long term evolution advanced \( lte a \) system , where multiple user equipments \( ues \) aim at uploading some data to a central server or to the cloud the traditional uploading technique used in cellular systems , i e , with separate links from each ue to the enodeb , is compared to innovative textit relay based schemes that exploit device to device \( d2d \) communications between two \( or more \) ues in proximity to each other differences in the channel quality experienced by the ues offer an opportunity to develop d2d based solutions , where textit \( i \) the ue with a poor direct link to the enodeb will forward data to a nearby ue over a high quality d2d link and textit \( ii \) the receiving ue then uploads its own generated data and the relayed data to the enodeb over a good uplink channel a straightforward gain in the data uploading time can be obtained for the first ue to extend the benefits , also to the relaying ue , enhanced d2d based solutions are proposed that decrease the uploading time of this ue based on the cooperative sharing of the resources allocated by the enodeb to the cooperating devices finally , preliminary results are also presented for a multihop study case , where a chain of devices exploits d2d communications to upload data to the enodeb
we develop novel data dissemination and collection algorithms for wireless sensor networks \( wsns \) in which we consider n sensor nodes distributed randomly in a certain field to measure a physical phenomena such sensors have limited energy , shortage coverage range , bandwidth and memory constraints we desire to disseminate nodes' data throughout the network such that a base station will be able to collect the sensed data by querying a small number of nodes we propose two data dissemination and collection algorithms \( dca 's \) to solve this problem data dissemination is achieved through dynamical selection of some nodes the selected nodes will be changed after a time slot t and may be repeated after a period t
given a graph g and an integer k , the feedback vertex set \( fvs \) problem asks if there is a vertex set t of size at most k that hits all cycles in the graph the fixed parameter tractability status of fvs in directed graphs was a long standing open problem until chen et al \( stoc '08 \) showed that it is fpt by giving a 4 k k ! n o \( 1 \) time algorithm in the subset versions of this problems , we are given an additional subset s of vertices \( resp , edges \) and we want to hit all cycles passing through a vertex of s \( resp an edge of s \) recently , the subset feedback vertex set in undirected graphs was shown to be fpt by cygan et al \( icalp '11 \) and independently by kakimura et al \( soda '12 \) we generalize the result of chen et al \( stoc '08 \) by showing that subset feedback vertex set in directed graphs can be solved in time 2 o \( k 3 \) n o \( 1 \) by our result , we complete the picture for feedback vertex set problems and their subset versions in undirected and directed graphs besides proving the fixed parameter tractability of directed subset feedback vertex set , we reformulate the random sampling of important separators technique in an abstract way that can be used for a general family of transversal problems moreover , we modify the probability distribution used in the technique to achieve better running time in particular , this gives an improvement from 2 2 o \( k \) to 2 o \( k 2 \) in the parameter dependence of the directed multiway cut algorithm of chitnis et al \( soda '12 \)
we propose an information theoretic framework for the secure two party function computation \( sfc \) problem and introduce the notion of sfc capacity we study and extend string oblivious transfer \( ot \) to sample wise ot we propose an efficient , perfectly private ot protocol utilizing the binary erasure channel or source we also propose the bootstrap string ot protocol which provides disjoint \( weakened \) privacy while achieving a multiplicative increase in rate , thus trading off security for rate finally , leveraging our ot protocol , we construct a protocol for sfc and establish a general lower bound on sfc capacity of the binary erasure channel and source
recently li and xia have proposed a transmission scheme for wireless relay networks based on the alamouti space time code and orthogonal frequency division multiplexing to combat the effect of timing errors at the relay nodes this transmission scheme is amazingly simple and achieves a diversity order of two for any number of relays motivated by its simplicity , this scheme is extended to a more general transmission scheme that can achieve full cooperative diversity for any number of relays the conditions on the distributed space time code \( dstc \) structure that admit its application in the proposed transmission scheme are identified and it is pointed out that the recently proposed full diversity four group decodable dstcs from precoded co ordinate interleaved orthogonal designs and extended clifford algebras satisfy these conditions it is then shown how differential encoding at the source can be combined with the proposed transmission scheme to arrive at a new transmission scheme that can achieve full cooperative diversity in asynchronous wireless relay networks with no channel information and also no timing error knowledge at the destination node finally , four group decodable distributed differential space time codes applicable in this new transmission scheme for power of two number of relays are also provided
this paper addresses the problem of distributed detection in multi agent networks agents receive private signals about an unknown state of the world the underlying state is globally identifiable , yet informative signals may be dispersed throughout the network using an optimization based framework , we develop an iterative local strategy for updating individual beliefs in contrast to the existing literature which focuses on asymptotic learning , we provide a finite time analysis furthermore , we introduce a kullback leibler cost to compare the efficiency of the algorithm to its centralized counterpart our bounds on the cost are expressed in terms of network size , spectral gap , centrality of each agent and relative entropy of agents' signal structures a key observation is that distributing more informative signals to central agents results in a faster learning rate furthermore , optimizing the weights , we can speed up learning by improving the spectral gap we also quantify the effect of link failures on learning speed in symmetric networks we finally provide numerical simulations which verify our theoretical results
a theoretical framework that supports automated construction of dynamic prime models purely from experimental time series data has been invented and developed , which can automatically generate \( construct \) data driven models of any time series data in seconds this has resulted in the formulation and formalisation of new reverse engineering and dynamic methods for automated systems modelling of complex systems , including complex biological , financial , control , and artificial neural network systems the systems model theory behind the invention has been formalised as a new , effective and robust system identification strategy complementary to process based modelling the proposed dynamic modelling and network inference solutions often involve tackling extremely difficult parameter estimation challenges , inferring unknown underlying network structures , and unsupervised formulation and construction of smart and intelligent ode models of complex systems in underdetermined conditions , i e , cases of dealing with how best to instantaneously and rapidly construct data consistent prime models of unknown \( or well studied \) complex system from small sized time series data , inference of unknown underlying network of interaction is more challenging this article reports a robust step by step mathematical and computational analysis of the entire prime model construction process that determines a model from data in less than a minute
treating modern firewall policy languages as imperative , special purpose programming languages , in this article we will try to apply static code analysis techniques for the purpose of anomaly detection we will first abstract a policy in common firewall policy language into an intermediate language , and then we will try to apply anomaly detection algorithms to it the contributions made by this work are 1 an analysis of various control flow instructions in popular firewall policy languages 2 introduction of an intermediate firewall policy language , with emphasis on control flow constructs 3 application of textit static code analysis to detect anomalies in firewall policy , expressed in intermediate firewall policy language 4 sample implementation of textit static code analysis of firewall policies , expressed in our abstract language using datalog language
we present foopar , an extension for highly efficient parallel computing in the multi paradigm programming language scala scala offers concise and clean syntax and integrates functional programming features our framework foopar combines these features with parallel computing techniques foopar is designed modular and supports easy access to different communication backends for distributed memory architectures as well as high performance math libraries in this article we use it to parallelize matrix matrix multiplication and show its scalability by a isoefficiency analysis in addition , results based on a empirical analysis on two supercomputers are given we achieve close to optimal performance wrt theoretical peak performance based on this result we conclude that foopar allows to fully access scala 's design features without suffering from performance drops when compared to implementations purely based on c and mpi
we study optimization of finite sums of emph geodesically smooth functions on riemannian manifolds although variance reduction techniques for optimizing finite sum problems have witnessed a huge surge of interest in recent years , all existing work is limited to vector space problems we introduce emph riemannian svrg , a new variance reduced riemannian optimization method we analyze this method for both geodesically smooth emph convex and emph nonconvex functions our analysis reveals that riemannian svrg comes with advantages of the usual svrg method , but with factors depending on manifold curvature that influence its convergence to the best of our knowledge , ours is the first emph fast stochastic riemannian method moreover , our work offers the first non asymptotic complexity analysis for nonconvex riemannian optimization \( even for the batch setting \) our results have several implications for instance , they offer a riemannian perspective on variance reduced pca , which promises a short , transparent convergence analysis
this paper reports a variant of the three stage quantum cryptography protocol which can be used in low intensity laser output regimes the variant , which tracks the intensity of the laser beam at the intermediate stages , makes the task of the eavesdropper harder than the standard k06 protocol the constraints on the iaqc protocol are much less than those on bb84 and in principle it can not only be used for key distribution but also for direct bitwise encryption of data the iaqc protocol is an improvement on the k06 protocol in that it makes it harder for the eavesdropper to monitor the channel
we study 1 way quantum finite automata \( qfas \) first , we compare them with their classical counterparts we show that , if an automaton is required to give the correct answer with a large probability \( over 0 98 \) , then the power of 1 way qfas is equal to the power of 1 way reversible automata however , quantum automata giving the correct answer with smaller probabilities are more powerful than reversible automata second , we show that 1 way qfas can be very space efficient namely , we construct a 1 way qfa which is exponentially smaller than any equivalent classical \( even randomized \) finite automaton this construction may be useful for design of other space efficient quantum algorithms third , we consider several generalizations of 1 way qfas here , our goal is to find a model which is more powerful than 1 way qfas keeping the quantum part as simple as possible
we present a superposition coding scheme for communication over a network , which combines partial decode and forward and noisy network coding this hybrid scheme is termed as superposition noisy network coding the scheme is designed and analyzed for single relay channel , single source multicast network and multiple source multicast network the achievable rate region is determined for each case the special cases of gaussian single relay channel and two way relay channel are analyzed for superposition noisy network coding the achievable rate of the proposed scheme is higher than the existing schemes of noisy network coding and compress forward
in this paper , the transmission of an improper complex second order stationary data sequence is considered over a strictly band limited frequency selective channel it is assumed that the transmitter employs linear modulation and that the channel output is corrupted by additive proper complex cyclostationary noise under the average transmit power constraint , the problem of minimizing the mean squared error at the output of a widely linear receiver is formulated in the time domain to find the optimal transmit and receive waveforms the optimization problem is converted into a frequency domain problem by using the vectorized fourier transform technique and put into the form of a double minimization first , the widely linear receiver is optimized that requires , unlike the linear receiver design with only one waveform , the design of two receive waveforms then , the optimal transmit waveform for the linear modulator is derived by introducing the notion of the impropriety frequency function of a discrete time random process and by performing a line search combined with an iterative algorithm the optimal solution shows that both the periodic spectral correlation due to the cyclostationarity and the symmetric spectral correlation about the origin due to the impropriety are well exploited
wireless cellular networks evolve towards a heterogeneous infrastructure , featuring multiple types of base stations \( bss \) , such as femto bss \( fbss \) and macro bss \( mbss \) a wireless device observes multiple points \( bss \) through which it can access the infrastructure and it may choose to receive the downlink \( dl \) traffic from one bs and send uplink \( ul \) traffic through another bs such a situation is referred to as decoupled dl ul access using the framework of stochastic geometry , we derive the association probability for dl ul in order to maximize the average received power , as the relative density of fbss initially increases , a large fraction of devices chooses decoupled access , i e receive from a mbs in dl and transmit through a fbs in ul we analyze the impact that this type of association has on the average throughput in the system
in computability theory and computable analysis , finite programs can compute infinite objects presenting a computable object via any program for it , provides at least as much information as presenting the object itself , written on an infinite tape what additional information do programs provide \? we characterize this additional information to be any upper bound on the kolmogorov complexity of the object hence we identify the exact relationship between markov computability and type 2 computability we then use this relationship to obtain several results characterizing the computational and topological structure of markov semidecidable sets
we present a methodology for the automated verification of quantum protocols using mcmas , a symbolic model checker for multi agent systems the method is based on the logical framework developed by d'hondt and panangaden for investigating epistemic and temporal properties , built on the model for distributed measurement based quantum computation \( dmc \) , an extension of the measurement calculus to distributed quantum systems we describe the translation map from dmc to interpreted systems , the typical formalism for reasoning about time and knowledge in multi agent systems then , we introduce dmc2ispl , a compiler into the input language of the mcmas model checker we demonstrate the technique by verifying the quantum teleportation protocol , and discuss the performance of the tool
this paper describes work performed withing the crater \( em c orpus em r esources em a nd em t erminology em e xt em r action , mlap 93 20 \) project , funded by the commission of the european communities in particular , it addresses the issue of adapting the xerox tagger to spanish in order to tag the spanish version of the itu \( international telecommunications union \) corpus the model implemented by this tagger is briefly presented along with some modifications performed on it in order to use some parameters not probabilistically estimated initial decisions , like the tagset , the lexicon and the training corpus are also discussed finally , results are presented and the benefits of the em mixed model justified
this paper presents a spermwhale' localization architecture using jointly a bag of features \( bof \) approach and machine learning framework bof methods are known , especially in computer vision , to produce from a collection of local features a global representation invariant to principal signal transformations our idea is to regress supervisely from these local features two rough estimates of the distance and azimuth thanks to some datasets where both acoustic events and ground truth position are now available furthermore , these estimates can feed a particle filter system in order to obtain a precise spermwhale' position even in mono hydrophone configuration anti collision system and whale watching are considered applications of this work
we generalize shimizu et al 's \( 2006 \) ica based approach for discovering linear non gaussian acyclic \( lingam \) structural equation models \( sems \) from causally sufficient , continuous valued observational data by relaxing the assumption that the generating sem 's graph is acyclic , we solve the more general problem of linear non gaussian \( ling \) sem discovery ling discovery algorithms output the distribution equivalence class of sems which , in the large sample limit , represents the population distribution we apply a ling discovery algorithm to simulated data finally , we give sufficient conditions under which only one of the sems in the output class is 'stable'
while machine learning is currently very successful in several application domains , we are still very far from a real artificial intelligence in this paper , we study basic sequence prediction problems that are beyond the scope of what is learnable with popular methods such as recurrent networks we show that simple algorithms can be learnt from sequential data with a recurrent network associated with trainable stacks we focus our study on algorithmically generated sequences such as a n b n , that can only be learnt by models which have the capacity to count our study highlights certain topics in machine learning that deserve more attention , such as addressing the shortcomings of purely gradient based training of non convex models we achieve progress in this direction by incorporating search based strategy once trained , we show that our method is able generalize to sequences up to an arbitrary size
upper and lower bounds on the capacity of gaussian multicast relay channels are shown to be quasi concave in the receiver signal to noise ratios and the transmit correlation coefficient the bounds considered are the cut set bound , decode forward \( df \) rates , and quantize forward rates the df rates are shown to be quasi concave in the relay position and this property is used to optimize the relay position for several networks
kalman filters and observers are two main classes of dynamic state estimation \( dse \) routines power system dse has been implemented by various kalman filters , such as the extended kalman filter \( ekf \) and the unscented kalman filter \( ukf \) in this paper , we discuss two challenges for an effective power system dse \( a \) model uncertainty and \( b \) potential cyber attacks to address this , the cubature kalman filter \( ckf \) and a nonlinear observer are introduced and implemented various kalman filters and the observer are then tested on the 16 machine , 68 bus system given realistic scenarios under model uncertainty and different types of cyber attacks against synchrophasor measurements it is shown that ckf and the observer are more robust to model uncertainty and cyber attacks than their counterparts based on the tests , a thorough qualitative comparison is also performed for kalman filter routines and observers
an interesting research problem in our age of big data is that of determining provenance granular evaluation of provenance of physical goods e g tracking ingredients of a pharmaceutical or demonstrating authenticity of luxury goods has often not been possible with today 's items that are produced and transported in complex , inter organizational , often internationally spanning supply chains recent adoption of internet of things and blockchain technologies give promise at better supply chain provenance we are particularly interested in the blockchain as many favoured use cases of blockchain are for provenance tracking we are also interested in applying ontologies as there has been some work done on knowledge provenance , traceability , and food provenance using ontologies in this paper , we make a case for why ontologies can contribute to blockchain design to support this case , we analyze a traceability ontology and translate some of its representations to smart contracts that execute a provenance trace and enforce traceability constraints on the ethereum blockchain platform
in this paper , we develop a framework for information theoretic learning based on infinitely divisible matrices we formulate an entropy like functional on positive definite matrices based on renyi 's axiomatic definition of entropy and examine some key properties of this functional that lead to the concept of infinite divisibility the proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel hilbert spaces as an application example , we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art
hough transform \( ht \) has been the most common method for circle detection , exhibiting robustness , but adversely demanding considerable computational effort and large memory requirements alternative approaches include heuristic methods that employ iterative optimization procedures for detecting multiple circles since only one circle can be marked at each optimization cycle , multiple executions must be enforced in order to achieve multi detection this paper presents an algorithm for automatic detection of multiple circular shapes that considers the overall process as a multi modal optimization problem the approach is based on the artificial bee colony \( abc \) algorithm , a swarm optimization algorithm inspired by the intelligent foraging behavior of honey bees unlike the original abc algorithm , the proposed approach presents the addition of a memory for discarded solutions such memory allows holding important information regarding other local optima which might have emerged during the optimization process the detector uses a combination of three non collinear edge points as parameters to determine circle candidates a matching function \( nectar amount \) determines if such circle candidates \( bee food sources \) are actually present in the image guided by the values of such matching functions , the set of encoded candidate circles are evolved through the abc algorithm so that the best candidate \( global optimum \) can be fitted into an actual circle within the edge only image then , an analysis of the incorporated memory is executed in order to identify potential local optima , i e , other circles
some new results are derived concerning random coding error exponents and expurgated exponents for list decoding with a deterministic list size l two asymptotic regimes are considered , the fixed list size regime , where l is fixed independently of the block length n , and the exponential list size , where l grows exponentially with n we first derive a general upper bound on the list decoding average error probability , which is suitable for both regimes this bound leads to more specific bounds in the two regimes in the fixed list size regime , the bound is related to known bounds and we establish its exponential tightness in the exponential list size regime , we establish the achievability of the well known sphere packing lower bound relations to guessing exponents are also provided an immediate byproduct of our analysis in both regimes is the universality of the maximum mutual information \( mmi \) list decoder in the error exponent sense finally , we consider expurgated bounds at low rates , both using gallager 's approach and the csisz 'ar k orner marton approach , which is , in general better \( at least for l 1 \) the latter expurgated bound , which involves the notion of it multi information , is also modified to apply to continuous alphabet channels , and in particular , to the gaussian memoryless channel , where the expression of the expurgated bound becomes quite explicit
in this paper we present a multi user cooperative protocol for wireless networks two sources transmit simultaneously their information blocks and relays employ opportunistically successive interference cancellation \( sic \) in an effort to decode them an adaptive decode amplify and forward scheme is applied at the relays to the decoded blocks or their sufficient statistic if decoding fails the main feature of the protocol is that sic is exploited in a network since more opportunities arise for each block to be decoded as the number of used relays nru is increased this feature leads to benefits in terms of diversity and multiplexing gains that are proven with the help of an analytical outage model and a diversity multiplexing tradeoff \( dmt \) analysis the performance improvements are achieved without any network synchronization and coordination in the final part of this work the closed form outage probability model is used by a novel approach for offline pre selection of the nru relays , that have the best sic performance , from a larger number of nr nodes the analytical results are corroborated with extensive simulations , while the protocol is compared with orthogonal and multi user protocols reported in the literature
network intrusion detection sensors are usually built around low level models of network traffic this means that their output is of a similarly low level and as a consequence , is difficult to analyze intrusion alert correlation is the task of automating some of this analysis by grouping related alerts together attack graphs provide an intuitive model for such analysis unfortunately alert flooding attacks can still cause a loss of service on sensors , and when performing attack graph correlation , there can be a large number of extraneous alerts included in the output graph this obscures the fine structure of genuine attacks and makes them more difficult for human operators to discern this paper explores modified correlation algorithms which attempt to minimize the impact of this attack
chemicals released in the air can be extremely dangerous for human beings and the environment hyperspectral images can be used to identify chemical plumes , however the task can be extremely challenging assuming we know a priori that some chemical plume , with a known frequency spectrum , has been photographed using a hyperspectral sensor , we can use standard techniques like the so called matched filter or adaptive cosine estimator , plus a properly chosen threshold value , to identify the position of the chemical plume however , due to noise and sensors fault , the accurate identification of chemical pixels is not easy even in this apparently simple situation in this paper we present a post processing tool that , in a completely adaptive and data driven fashion , allows to improve the performance of any classification methods in identifying the boundaries of a plume this is done using the multidimensional iterative filtering \( mif \) algorithm \( arxiv 1411 6051 , arxiv 1507 07173 \) , which is a non stationary signal decomposition method like the pioneering empirical mode decomposition \( emd \) method moreover , based on the mif technique , we propose also a pre processing method that allows to decorrelate and mean center a hyperspectral dataset the cosine similarity measure , which often fails in practice , appears to become a successful and outperforming classifier when equipped with such pre processing method we show some examples of the proposed methods when applied to real life problems
we present a framework for representing and modeling data on graphs based on this framework , we study three typical classes of graph signals smooth graph signals , piecewise constant graph signals , and piecewise smooth graph signals for each class , we provide an explicit definition of the graph signals and construct a corresponding graph dictionary with desirable properties we then study how such graph dictionary works in two standard tasks approximation and sampling followed with recovery , both from theoretical as well as algorithmic perspectives finally , for each class , we present a case study of a real world problem by using the proposed methodology
two skew cyclic codes can be equivalent for the hamming metric only if they have the same length , and only the zero code is degenerate the situation is completely different for the rank metric , where lengths of codes correspond to the number of outgoing links from the source when applying the code on a network we study rank equivalences between skew cyclic codes of different lengths and , with the aim of finding the skew cyclic code of smallest length that is rank equivalent to a given one , we define different types of length for a given skew cyclic code , relate them and compute them in most cases we give different characterizations of rank degenerate skew cyclic codes using conventional polynomials and linearized polynomials some known results on the rank weight hierarchy of cyclic codes for some lengths are obtained as particular cases and extended to all lengths and to all skew cyclic codes finally , we prove that the smallest length of a linear code that is rank equivalent to a given skew cyclic code can be attained by a pseudo skew cyclic code throughout the paper , we find new relations between linear skew cyclic codes and their galois closures
in this paper , we describe a new corpus based approach to prepositional phrase attachment disambiguation , and present results comparing performance of this algorithm with other corpus based approaches to this problem
manifold learning is a class of algorithms seeking a low dimensional non linear representation of high dimensional data thus manifold learning algorithms are , at least in theory , most applicable to high dimensional data and sample sizes to enable accurate estimation of the manifold despite this , most existing manifold learning implementations are not particularly scalable here we present a python package that implements a variety of manifold learning algorithms in a modular and scalable fashion , using fast approximate neighbors searches and fast sparse eigendecompositions the package incorporates theoretical advances in manifold learning , such as the unbiased laplacian estimator and the estimation of the embedding distortion by the riemannian metric method in benchmarks , even on a single core desktop computer , our code embeds millions of data points in minutes , and takes just 200 minutes to embed the main sample of galaxy spectra from the sloan digital sky survey consisting of 0 6 million samples in 3750 dimensions a task which has not previously been possible
this paper presents a solution to autonomous underwater vehicles \( auvs \) large scale route planning and task assignment joint problem given a set of constraints \( e g , time \) and a set of task priority values , the goal is to find the optimal route for underwater mission that maximizes the sum of the priorities and minimizes the total risk percentage while meeting the given constraints making use of the heuristic nature of genetic and swarm intelligence algorithms in solving np hard graph problems , particle swarm optimization \( pso \) and genetic algorithm \( ga \) are employed to find the optimum solution , where each individual in the population is a candidate solution \( route \) to evaluate the robustness of the proposed methods , the performance of the all ps and ga algorithms are examined and compared for a number of monte carlo runs simulation results suggest that the routes generated by both algorithms are feasible and reliable enough , and applicable for underwater motion planning however , the ga based route planner produces superior results comparing to the results obtained from the pso based route planner
in this paper , a bayesian inference technique based on taylor series approximation of the logarithm of the likelihood function is presented the proposed approximation is devised for the case , where the prior distribution belongs to the exponential family of distributions the logarithm of the likelihood function is linearized with respect to the sufficient statistic of the prior distribution in exponential family such that the posterior obtains the same exponential family form as the prior similarities between the proposed method and the extended kalman filter for nonlinear filtering are illustrated furthermore , an extended target measurement update for target models where the target extent is represented by a random matrix having an inverse wishart distribution is derived the approximate update covers the important case where the spread of measurement is due to the target extent as well as the measurement noise in the sensor
we introduce weak oddness omega textrm w , a new measure of uncolourability of cubic graphs , defined as the least number of odd components in an even factor for every bridgeless cubic graph g , rho \( g \) le omega textrm w \( g \) le omega \( g \) , where rho \( g \) denotes the resistance of g and omega \( g \) denotes the oddness of g , so this new measure is an approximation of both oddness and resistance we demonstrate that there are graphs g satisfying rho \( g \) omega textrm w \( g \) omega \( g \) , and that the difference between any two of those three measures can be arbitrarily large the construction implies that if we replace a vertex of a cubic graph with a triangle , then its oddness can decrease by an arbitrarily large amount
in orthogonal frequency division multiplexing \( ofdm \) systems , carrier and sampling frequency offsets \( cfo and sfo , respectively \) can destroy the orthogonality of the subcarriers and degrade system performance in the literature , nguyen le , le ngoc , and ko proposed a simple maximum likelihood \( ml \) scheme using two long training symbols for estimating the initial cfo and sfo of a recursive least squares \( rls \) estimation scheme however , the results of nguyen le 's ml estimation show poor performance relative to the cramer rao bound \( crb \) in this paper , we extend moose 's cfo estimation algorithm to joint ml estimation of cfo and sfo using two long training symbols in particular , we derive crbs for the mean square errors \( mses \) of cfo and sfo estimation simulation results show that the proposed ml scheme provides better performance than nguyen le 's ml scheme
in this paper , a novel cluster based approach for optimizing the energy efficiency of wireless small cell networks is proposed a dynamic mechanism based on the spectral clustering technique is proposed to dynamically form clusters of small cell base stations such clustering enables intra cluster coordination among the base stations for optimizing the downlink performance through load balancing , while satisfying users' quality of service requirements in the proposed approach , the clusters use an opportunistic base station sleep wake switching mechanism to strike a balance between delay and energy consumption the inter cluster interference affects the performance of the clusters and their choices of active or sleep state due to the lack of inter cluster communications , the clusters have to compete with each other to make decisions on improving the energy efficiency this competition is formulated as a noncooperative game among the clusters that seek to minimize a cost function which captures the tradeoff between energy expenditure and load to solve this game , a distributed learning algorithm is proposed using which the clusters autonomously choose their optimal transmission strategies simulation results show that the proposed approach yields significant performance gains in terms of reduced energy expenditures up to 40 and reduced load up to 23 compared to conventional approaches
in order to create effective storytelling agents three fundamental questions must be answered first , is a physically embodied agent preferable to a virtual agent or a voice only narration \? second , does a human voice have an advantage over a synthesised voice \? third , how should the emotional trajectory of the different characters in a story be related to a storyteller 's facial expressions during storytelling time , and how does this correlate with the apparent emotions on the faces of the listeners \? the results of two specially designed studies indicate that the physically embodied robot produces more attention to the listener as compared to a virtual embodiment , that a human voice is preferable over the current state of the art of text to speech , and that there is a complex yet interesting relation between the emotion lines of the story , the facial expressions of the narrating agent , and the emotions of the listener , and that the empathising of the listener is evident through its facial expressions this work constitutes an important step towards emotional storytelling robots that can observe their listeners and adapt their style in order to maximise their effectiveness
we give a new approach to the dictionary learning \( also known as sparse coding \) problem of recovering an unknown n times m matrix a \( for m geq n \) from examples of the form y ax e , where x is a random vector in mathbb r m with at most tau m nonzero coordinates , and e is a random noise vector in mathbb r n with bounded magnitude for the case m o \( n \) , our algorithm recovers every column of a within arbitrarily good constant accuracy in time m o \( log m log \( tau 1 \) \) , in particular achieving polynomial time if tau m delta for any delta 0 , and time m o \( log m \) if tau is \( a sufficiently small \) constant prior algorithms with comparable assumptions on the distribution required the vector x to be much sparser at most sqrt n nonzero coordinates and there were intrinsic barriers preventing these algorithms from applying for denser x we achieve this by designing an algorithm for noisy tensor decomposition that can recover , under quite general conditions , an approximate rank one decomposition of a tensor t , given access to a tensor t' that is tau close to t in the spectral norm \( when considered as a matrix \) to our knowledge , this is the first algorithm for tensor decomposition that works in the constant spectral norm noise regime , where there is no guarantee that the local optima of t and t' have similar structures our algorithm is based on a novel approach to using and analyzing the sum of squares semidefinite programming hierarchy \( parrilo 2000 , lasserre 2001 \) , and it can be viewed as an indication of the utility of this very general and powerful tool for unsupervised learning problems
in this work we explore possibilities for coding when information worlds have different \( semantic \) values we introduce a loss function that expresses the overall performance of a coding scheme for discrete channels and exchange the usual goal of minimizing the error probability to that of minimizing the expected loss in this environment we explore the possibilities of using poset decoders to make a message wise unequal error protection \( uep \) , where the most valuable information is protected by placing in its proximity information words that differ by small valued information similar definitions and results are shortly presented also for signal constellations in euclidean space
this paper characterizes the capacity of a class of modulo additive noise relay channels , in which the relay observes a corrupted version of the noise and has a separate channel to the destination the capacity is shown to be strictly below the cut set bound in general and achievable using a quantize and forward strategy at the relay this result confirms a conjecture by ahlswede and han about the capacity of channels with rate limited state information at the destination for this particular class of channels
urban environments develop complex , non obvious structures that are often hard to represent in the form of maps or guides finding the right place to go often requires intimate familiarity with the location in question and cannot easily be deduced by visitors in this work , we exploit large scale samples of usage information , in the form of mobile phone traces and geo tagged twitter messages in order to automatically explore and annotate city maps via kernel density estimation our experiments are based on one year 's worth of mobile phone activity collected by nokia 's mobile data challenge \( mdc \) we show that usage information can be a strong predictor of semantic place categories , allowing us to automatically annotate maps based on the behavior of the local user base
this paper is concerned with the recognition of approximate graph products with respect to the cartesian product most graphs are prime , although they can have a rich product like structure the proposed algorithms are based on a local approach that covers a graph by small subgraphs , so called partial star products , and then utilizes this information to derive the global factors and an embedding of the graph under investigation into cartesian product graphs
we consider the task of obtaining the maximum a posteriori estimate of discrete pairwise random fields with arbitrary unary potentials and semimetric pairwise potentials for this problem , we propose an accurate hierarchical move making strategy where each move is computed efficiently by solving an st mincut problem unlike previous move making approaches , e g the widely used a expansion algorithm , our method obtains the guarantees of the standard linear programming \( lp \) relaxation for the important special case of metric labeling unlike the existing lp relaxation solvers , e g interior point algorithms or tree reweighted message passing , our method is significantly faster as it uses only the efficient st mincut algorithm in its design using both synthetic and real data experiments , we show that our technique outperforms several commonly used algorithms
this paper describes a blended learning implementation and experience supported with intelligent learning environments included in a learning management system \( lms \) called ku uzem the blended learning model is realized as a combination of face to face education and e learning the intelligent learning environments consist of two applications named ctutor , itest in addition to standard e learning tools , students can use ctutor to resolve c programming exercises ctutor is a problem solving environment , which diagnoses students' knowledge level but also gives feedbacks and tips to help them to understand the course subject , overcome their misconceptions and reinforce learnt concepts itest provides an assessment environment in which students can take quizzes that were prepared according to their learning levels the realized model was used for two terms in the c programming course given at afyon kocatepe university a survey was conducted at the end of the course to find out to what extent the students were accepting the blended learning model supported with ku uzem and to discover students' attitude towards intelligent learning environments additionally , an experiment formed with an experimental group who took an active part in the realized model and a control group who only took the face to face education was performed during the first term of the course according to the results , students were satisfied with intelligent learning environments and the realized learning model furthermore , the use of intelligent learning environments improved the students' knowledge about c programming
the symbol error rate of the minimum distance detector for an arbitrary multi dimensional constellation impaired by additive white gaussian noise is characterized as the product of a completely monotone function with a non negative power of the signal to noise ratio this representation is also shown to apply to cases when the impairing noise is compound gaussian using this general result , it is proved that the symbol error rate is completely monotone if the rank of its constellation matrix is either one or two further , a necessary and sufficient condition for the complete monotonicity of the symbol error rate of a constellation of any dimension is also obtained applications to stochastic ordering of wireless system performance are also discussed
design space exploration of multiprocessor systems involves the optimization of cost performance functions over a large number of design parameters , most of which are discrete valued this optimization is non trivial because the evaluation of cost performance functions is computationally expensive , typically involving simulation of long benchmark programs on a cycle accurate model of the system further , algorithms for optimization over discrete parameters do not scale well with the number of parameters we describe a new approach to this optimization problem , based on embedding the discrete parameter space into an extended continuous space optimization is then carried out over the extended continuous space using standard descent based continuous optimization schemes the embedding is performed using a novel simulation based ergodic interpolation method that produces the interpolated value in a single simulation run the post embedding performance function is continuous , and observed to be piecewise smooth we demonstrate the approach by considering a multiprocessor design exploration problem with 31 discrete parameters where the objective function is a weighted sum of cost and performance metrics , and cost performance tradeoff curves are obtained by varying the weights we use the cobyla implementation from the python scipy library to perform the optimization on the extended continuous space near optimal solutions are obtained within three hundred simulation runs , and we observe improvements in the objective function ranging from 1 3x to 12 2x \( for randomly chosen initial parameter values \) cost performance trade off curves generated from these optimization runs provide clear indicators for the optimal system configuration thus , continuous embeddings of discrete parameter optimization problems offer an effective mechanism for the design space exploration of multiprocessor systems
arithmetic coding \( ac \) is widely used for the entropy coding of text and video data it involves recursive partitioning of the range 0 , 1 \) in accordance with the relative probabilities of occurrence of the input symbols a data \( image or video \) encryption scheme based on arithmetic coding called as chaotic arithmetic coding \( cac \) has been presented in previous works in cac , a large number of chaotic maps can be used to perform coding , each achieving shannon optimal compression performance the exact choice of map is governed by a key cac has the effect of scrambling the intervals without making any changes to the width of interval in which the codeword must lie , thereby allowing encryption without sacrificing any coding efficiency in this paper , we use a redundancy in cac procedure for secure multicast of videos where multiple users are distributed with different keys to decode same encrypted file by encrypting once , we can generate multiple keys , either of which can be used to decrypt the encoded file this is very suitable for video distribution over internet where a single video can be distributed to multiple clients in a privacy preserving manner
autonomous robots need to be able to adapt to unforeseen situations and to acquire new skills through trial and error reinforcement learning in principle offers a suitable methodological framework for this kind of autonomous learning however current computational reinforcement learning agents mostly learn each individual skill entirely from scratch how can we enable artificial agents , such as robots , to acquire some form of generic knowledge , which they could leverage for the learning of new skills \? this paper argues that , like the brain , the cognitive system of artificial agents has to develop a world model to support adaptive behavior and learning inspiration is taken from two recent developments in the cognitive science literature predictive processing theories of cognition , and the sensorimotor contingencies theory of perception based on these , a hypothesis is formulated about what the content of information might be that is encoded in an internal world model , and how an agent could autonomously acquire it a computational model is described to formalize this hypothesis , and is evaluated in a series of simulation experiments
estimating the probability that a sum of random variables \( rvs \) exceeds a given threshold is a well known challenging problem closed form expression of the sum distribution is usually intractable and presents an open problem a crude monte carlo \( mc \) simulation is the standard technique for the estimation of this type of probability however , this approach is computationally expensive especially when dealing with rare events \( i e events with very small probabilities \) importance sampling \( is \) is an alternative approach which effectively improves the computational efficiency of the mc simulation in this paper , we develop a general framework based on is approach for the efficient estimation of the probability that the sum of independent and not necessarily identically distributed heavy tailed rvs exceeds a given threshold the proposed is approach is based on constructing a new sampling distribution by twisting the hazard rate of the original underlying distribution of each component in the summation a minmax approach is carried out for the determination of the twisting parameter , for any given threshold moreover , using this minmax optimal choice , the estimation of the probability of interest is shown to be asymptotically optimal as the threshold goes to infinity we also offer some selected simulation results illustrating first the efficiency of the proposed is approach compared to the naive mc simulation the near optimality of the minmax approach is then numerically analyzed
central to robot exploration and mapping is the task of persistent localization in environmental fields characterized by spatially correlated measurements this paper presents a gaussian process localization \( gp localize \) algorithm that , in contrast to existing works , can exploit the spatially correlated field measurements taken during a robot 's exploration \( instead of relying on prior training data \) for efficiently and scalably learning the gp observation model online through our proposed novel online sparse gp as a result , gp localize is capable of achieving constant time and memory \( i e , independent of the size of the data \) per filtering step , which demonstrates the practical feasibility of using gps for persistent robot localization and autonomy empirical evaluation via simulated experiments with real world datasets and a real robot experiment shows that gp localize outperforms existing gp localization algorithms
singular value decomposition \( svd \) has been used successfully in recent years in the area of recommender systems in this paper we present how this model can be extended to consider both user ratings and information from wikipedia by mapping items to wikipedia pages and quantifying their similarity , we are able to use this information in order to improve recommendation accuracy , especially when the sparsity is high another advantage of the proposed approach is the fact that it can be easily integrated into any other svd implementation , regardless of additional parameters that may have been added to it preliminary experimental results on the movielens dataset are encouraging
this paper shows how to decode errors and erasures with gabidulin codes in sub quadratic time in the code length , improving previous algorithms which had at least quadratic complexity the complexity reduction is achieved by accelerating operations on linearized polynomials in particular , we present fast algorithms for division , multi point evaluation and interpolation of linearized polynomials and show how to efficiently compute minimal subspace polynomials
we consider the problem of computing a maximal independent set \( mis \) in an extremely harsh broadcast model that relies only on carrier sensing the model consists of an anonymous broadcast network in which nodes have no knowledge about the topology of the network or even an upper bound on its size furthermore , it is assumed that an adversary chooses at which time slot each node wakes up at each time slot a node can either beep , that is , emit a signal , or be silent at a particular time slot , beeping nodes receive no feedback , while silent nodes can only differentiate between none of its neighbors beeping , or at least one of its neighbors beeping we start by proving a lower bound that shows that in this model , it is not possible to locally converge to an mis in sub polynomial time we then study four different relaxations of the model which allow us to circumvent the lower bound and find an mis in polylogarithmic time first , we show that if a polynomial upper bound on the network size is known , it is possible to find an mis in o \( log 3 n \) time second , if we assume sleeping nodes are awoken by neighboring beeps , then we can also find an mis in o \( log 3 n \) time third , if in addition to this wakeup assumption we allow sender side collision detection , that is , beeping nodes can distinguish whether at least one neighboring node is beeping concurrently or not , we can find an mis in o \( log 2 n \) time finally , if instead we endow nodes with synchronous clocks , it is also possible to find an mis in o \( log 2 n \) time
this paper studies the problem of recovering a signal vector and the corrupted noise vector from a collection of corrupted linear measurements through the solution of a l1 minimization , where the sensing matrix is a partial fourier matrix whose rows are selected randomly and uniformly from rows of a full fourier matrix after choosing the parameter in the l1 minimization appropriately , we show that the recovery can be successful even when a constant fraction of the measurements are arbitrarily corrupted , moreover , the proportion of corrupted measurement can grows arbitrarily close to 1 , provided that the signal vector is sparse enough the upper bound on the sparsity of the signal vector required in this paper is asymptotically optimal and is better than those achieved by recent literatures 1 , 2 by a ln \( n \) factor furthermore , the assumptions we impose on the signal vector and the corrupted noise vector are loosest comparing to the existing literatures 1 3 , which lenders our recovery guarantees are more applicable extensive numerical experiments based on synthesis as well as real world data are presented to verify the conclusion of the proposed theorem and to demonstrate the potential of the l1 minimization framework
a quantum communication channel can be put to many uses it can transmit classical information , private classical information , or quantum information it can be used alone , with shared entanglement , or together with other channels for each of these settings there is a capacity that quantifies a channel 's potential for communication in this short review , i summarize what is known about the various capacities of a quantum channel , including a discussion of the relevant additivity questions i also give some indication of potentially interesting directions for future research
we study synthetic aperture radar \( sar \) imaging and motion estimation of complex scenes consisting of stationary and moving targets we use the classic sar setup with a single antenna emitting signals and receiving the echoes from the scene the known motion estimation methods for such setups work only in simple cases , with one or a few targets in the same motion we propose to extend the applicability of these methods to complex scenes , by complementing them with a data pre processing step intended to separate the echoes from the stationary targets and the moving ones we present two approaches the first is an iteration designed to subtract the echoes from the stationary targets one by one it estimates the location of each stationary target from a preliminary image , and then uses it to define a filter that removes its echo from the data the second approach is based on the robust principle component analysis \( pca \) method the key observation is that with appropriate pre processing and windowing , the discrete samples of the stationary target echoes form a low rank matrix , whereas the samples of a few moving target echoes form a high rank sparse matrix the robust pca method is designed to separate the low rank from the sparse part , and thus can be used for the sar data separation we present a brief analysis of the two methods and explain how they can be combined to improve the data separation for extended and complex imaging scenes we also assess the performance of the methods with extensive numerical simulations
we consider binary systematic network codes and investigate their capability of decoding a source message either in full or in part we carry out a probability analysis , derive closed form expressions for the decoding probability and show that systematic network coding outperforms conventional network coding we also develop an algorithm based on gaussian elimination that allows progressive decoding of source packets simulation results show that the proposed decoding algorithm can achieve the theoretical optimal performance furthermore , we demonstrate that systematic network codes equipped with the proposed algorithm are good candidates for progressive packet recovery owing to their overall decoding delay characteristics
atomizing various web activities by replacing human to human interactions on the internet has been made indispensable due to its enormous growth however , bots also known as web bots which have a malicious intend and pretending to be humans pose a severe threat to various services on the internet that implicitly assume a human interaction accordingly , web service providers before allowing access to such services use various human interaction proof 's \( hips \) to authenticate that the user is a human and not a bot completely automated public turing test to tell computers and humans apart \( captcha \) is a class of hips tests and are based on artificial intelligence these tests are easier for humans to qualify and tough for bots to simulate several web services use captchas as a defensive mechanism against automated web bots in this paper , we review the existing captcha schemes that have been proposed or are being used to protect various web services we classify them in groups and compare them with each other in terms of security and usability we present general method used to generate and break text based and image based captchas further , we discuss various security and usability issues in captcha design and provide guidelines for improving their robustness and usability
a discourse strategy is a strategy for communicating with another agent designing effective dialogue systems requires designing agents that can choose among discourse strategies we claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation the proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics
a majority logic decoder made of unreliable logic gates , whose failures are transient and datadependent , is analyzed based on a combinatorial representation of fault configurations a closed form expression for the average bit error rate for an one step majority logic decoder is derived , for a regular low density parity check \( ldpc \) code ensemble and the proposed failure model the presented analysis framework is then used to establish bounds on the one step majority logic decoder performance under the simplified probabilistic gate output switching model based on the expander property of tanner graphs of ldpc codes , it is proven that a version of the faulty parallel bit flipping decoder can correct a fixed fraction of channel errors in the presence of data dependent gate failures the results are illustrated with numerical examples of finite geometry codes
we study an extension of duncan watts' 2002 model of information cascades in social networks where edge weights are taken to be random , an innovation motivated by recent applications of cascade analysis to systemic risk in financial networks the main result is a probabilistic analysis that characterizes the cascade in an infinite network as the fixed point of a vector valued mapping , explicit in terms of convolution integrals that can be efficiently evaluated numerically using the fast fourier transform algorithm a second result gives an approximate probabilistic analysis of cascades on real world networks , finite networks based on a fixed deterministic graph extensive cross testing with monte carlo estimates shows that this approximate analysis performs surprisingly well , and provides a flexible microscope that can be used to investigate properties of information cascades in real world networks over a wide range of model parameters
we develop a new notion of security against timing attacks where the attacker is able to simultaneously observe the execution time of a program and the probability of the values of low variables we then show how to measure the security of a program with respect to this notion via a computable estimate of the timing leakage and use this estimate for cost optimisation
in the context of csps , a strong backdoor is a subset of variables such that every complete assignment yields a residual instance guaranteed to have a specified property if the property allows efficient solving , then a small strong backdoor provides a reasonable decomposition of the original instance into easy instances an important challenge is the design of algorithms that can find quickly a small strong backdoor if one exists we present a systematic study of the parameterized complexity of backdoor detection when the target property is a restricted type of constraint language defined by means of a family of polymorphisms in particular , we show that under the weak assumption that the polymorphisms are idempotent , the problem is unlikely to be fpt when the parameter is either r \( the constraint arity \) or k \( the size of the backdoor \) unless p np or fpt w 2 when the parameter is k r , however , we are able to identify large classes of languages for which the problem of finding a small backdoor is fpt
the following textit network computing problem is considered source nodes in a directed acyclic network generate independent messages and a single receiver node computes a target function f of the messages the objective is to maximize the average number of times f can be computed per network usage , i e , the ``computing capacity'' the textit network coding problem for a single receiver network is a special case of the network computing problem in which all of the source messages must be reproduced at the receiver for network coding with a single receiver , routing is known to achieve the capacity by achieving the network textit min cut upper bound we extend the definition of min cut to the network computing problem and show that the min cut is still an upper bound on the maximum achievable rate and is tight for computing \( using coding \) any target function in multi edge tree networks and for computing linear target functions in any network we also study the bound 's tightness for different classes of target functions in particular , we give a lower bound on the computing capacity in terms of the steiner tree packing number and a different bound for symmetric functions we also show that for certain networks and target functions , the computing capacity can be less than an arbitrarily small fraction of the min cut bound
in this paper , a tutorial software to learn information theory basics in a practical way is reported the software , called it tutor uv , makes use of a modern existing spanish corpus for the modeling of the source both the source and the channel coding are also included in this educational tool as part of the learning experience entropy values of the spanish language obtained with the it tutor uv are discussed and compared to others that were previously calculated under limited conditions
the `security index' of a discrete time lti system under sensor attacks is introduced as a quantitative measure on the security of an observable system we derive ideas from error control coding theory to provide sufficient conditions for attack detection and correction
this paper studies a layered erasure model for two user interference channels , which can be viewed as a simplified version of gaussian fading interference channel it is assumed that channel state information \( csi \) is only available at receivers but not at transmitters under such assumption , an outer bound is derived for the capacity region of such interference channel the new outer bound is tight in many circumstances for the remaining open cases , the outer bound extends previous results
over the last few years , more and more heuristic decision making techniques have been inspired by nature , e g evolutionary algorithms , ant colony optimisation and simulated annealing more recently , a novel computational intelligence technique inspired by immunology has emerged , called artificial immune systems \( ais \) this immune system inspired technique has already been useful in solving some computational problems in this keynote , we will very briefly describe the immune system metaphors that are relevant to ais we will then give some illustrative real world problems suitable for ais use and show a step by step algorithm walkthrough a comparison of ais to other well known algorithms and areas for future work will round this keynote off it should be noted that as ais is still a young and evolving field , there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from the examples given here
let w be a word in alphabet x , d with m x 's and n d 's interpreting x as multiplication by x , and d as differentiation with respect to x , the identity wf \( x \) x m n sum k s w \( k \) x k d k f \( x \) , valid for any smooth function f \( x \) , defines a sequence \( s w \( k \) \) k , the terms of which we refer to as the em stirling numbers \( of the second kind \) of w the nomenclature comes from the fact that when w \( xd \) n , we have s w \( k \) n brace k , the ordinary stirling number of the second kind explicit expressions for , and identities satisfied by , the s w \( k \) have been obtained by numerous authors , and combinatorial interpretations have been presented here we provide a new combinatorial interpretation that retains the spirit of the familiar interpretation of n brace k as a count of partitions specifically , we associate to each w a quasi threshold graph g w , and we show that s w \( k \) enumerates partitions of the vertex set of g w into classes that do not span an edge of g w we also discuss some relatives of , and consequences of , our interpretation , including q analogs and bijections between families of labelled forests and sets of restricted partitions
this paper presents an algorithm for selecting an appropriate classifier word for a noun in thai language , it frequently happens that there is fluctuation in the choice of classifier for a given concrete noun , both from the point of view of the whole spe ech community and individual speakers basically , there is no exect rule for classifier selection as far as we can do in the rule based approach is to give a default rule to pick up a corresponding classifier of each noun registration of classifier for each noun is limited to the type of unit classifier because other types are open due to the meaning of representation we propose a corpus based method \( biber , 1993 nagao , 1993 smadja , 1993 \) which generates noun classifier associations \( nca \) to overcome the problems in classifier assignment and semantic construction of noun phrase the nca is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences
we study a model where two opposing provers debate over the membership status of a given string in a language , trying to convince a weak verifier whose coins are visible to all we show that the incorporation of just two qubits to an otherwise classical constant space verifier raises the class of debatable languages from at most mathsf np to the collection of all turing decidable languages \( recursive languages \) when the verifier is further constrained to make the correct decision with probability 1 , the corresponding class goes up from the regular languages up to at least mathsf e we also show that the quantum model outperforms its classical counterpart when restricted to run in polynomial time , and demonstrate some non context free languages which have such short debates with quantum verifiers
the capacity region of the interference channel in which one transmitter non causally knows the message of the other , termed the cognitive interference channel , has remained open since its inception in 2005 a number of subtly differing achievable rate regions and outer bounds have been derived , some of which are tight under specific conditions in this work we present a new unified inner bound for the discrete memoryless cognitive interference channel we show explicitly how it encompasses all known discrete memoryless achievable rate regions as special cases the presented achievable region was recently used in deriving the capacity region of the general deterministic cognitive interference channel , and thus also the linear high snr deterministic approximation of the gaussian cognitive interference channel the high snr deterministic approximation was then used to obtain the capacity of the gaussian cognitive interference channel to within 1 87 bits
we introduce and analyze a model for decentral ized control the model is broad enough to include problems such as formation control , decentralization of the power grid and flocking the objective of this paper is twofold first , we show how the issue of decentralization goes beyond having agents know only part of the state of the system in fact , we argue that a complete theory of decentralization should take into account the fact that agents can be made aware of only part of the global objective of the ensemble a second contribution of this paper is the introduction of a rigorous definition of information flow for a decentralized system we show how to attach to a general nonlinear decentralized system a unique information flow graph that is an invariant of the system in order to address some finer issues in decentralized system , such as the existence of so called information loops , we further refine the information flow graph to a simplicial complex more precisely , a whitney complex we illustrate the main results on a variety of examples
in this paper we investigate the achievable rate of a system that includes a nomadic transmitter with several antennas , which is received by multiple agents , exhibiting independent channel gains and additive circular symmetric complex gaussian noise in the nomadic regime , we assume that the agents do not have any decoding ability these agents process their channel observations and forward them to the final destination through lossless links with a fixed capacity we propose new achievable rates based on elementary compression and also on a wyner ziv \( ceo like \) processing , for both fast fading and block fading channels , as well as for general discrete channels the simpler two agents scheme is solved , up to an implicit equation with a single variable limiting the nomadic transmitter to a circular symmetric complex gaussian signalling , new upper bounds are derived for both fast and block fading , based on the vector version of the entropy power inequality these bounds are then compared to the achievable rates in several extreme scenarios the asymptotic setting with numbers of agents and transmitter 's antennas taken to infinity is analyzed in addition , the upper bounds are analytically shown to be tight in several examples , while numerical calculations reveal a rather small gap in a finite 2 times2 setting the advantage of the wyner ziv approach over elementary compression is shown where only the former can achieve the full diversity multiplexing tradeoff we also consider the non nomadic setting , with agents that can decode here we give an achievable rate , over fast fading channel , which combines broadcast with dirty paper coding and the decentralized reception , which was introduced for the nomadic setting
a transmitter without channel state information \( csi \) wishes to send a delay limited gaussian source over a slowly fading channel the source is coded in superimposed layers , with each layer successively refining the description in the previous one the receiver decodes the layers that are supported by the channel realization and reconstructs the source up to a distortion the expected distortion is minimized by optimally allocating the transmit power among the source layers for two source layers , the allocation is optimal when power is first assigned to the higher layer up to a power ceiling that depends only on the channel fading distribution all remaining power , if any , is allocated to the lower layer for convex distortion cost functions with convex constraints , the minimization is formulated as a convex optimization problem in the limit of a continuum of infinite layers , the minimum expected distortion is given by the solution to a set of linear differential equations in terms of the density of the fading distribution as the bandwidth ratio b \( channel uses per source symbol \) tends to zero , the power distribution that minimizes expected distortion converges to the one that maximizes expected capacity while expected distortion can be improved by acquiring csi at the transmitter \( csit \) or by increasing diversity from the realization of independent fading paths , at high snr the performance benefit from diversity exceeds that from csit , especially when b is large
with the emergence of social networking services , researchers enjoy the increasing availability of large scale heterogenous datasets capturing online user interactions and behaviors traditional analysis of techno social systems data has focused mainly on describing either the dynamics of social interactions , or the attributes and behaviors of the users however , overwhelming empirical evidence suggests that the two dimensions affect one another , and therefore they should be jointly modeled and analyzed in a multi modal framework the benefits of such an approach include the ability to build better predictive models , leveraging social network information as well as user behavioral signals to this purpose , here we propose the constrained latent space model \( clsm \) , a generalized framework that combines mixed membership stochastic blockmodels \( mmsb \) and latent dirichlet allocation \( lda \) incorporating a constraint that forces the latent space to concurrently describe the multiple data modalities we derive an efficient inference algorithm based on variational expectation maximization that has a computational cost linear in the size of the network , thus making it feasible to analyze massive social datasets we validate the proposed framework on two problems prediction of social interactions from user attributes and behaviors , and behavior prediction exploiting network information we perform experiments with a variety of multi modal social systems , spanning location based social networks \( gowalla \) , social media services \( instagram , orkut \) , e commerce and review sites \( amazon , ciao \) , and finally citation networks \( cora \) the results indicate significant improvement in prediction accuracy over state of the art methods , and demonstrate the flexibility of the proposed approach for addressing a variety of different learning problems commonly occurring with multi modal social data
uniqueness of normal forms \( un \) is an important property of term rewrite systems un is decidable for ground \( i e , variable free \) systems and undecidable in general recently it was shown to be decidable for linear , shallow systems we generalize this previous result and show that this property is decidable for shallow rewrite systems , in contrast to confluence , reachability and other properties , which are all undecidable for flat systems our result is also optimal in some sense , since we prove that the un property is undecidable for two classes of linear rewrite systems left flat systems in which right hand sides are of depth at most two and right flat systems in which left hand sides are of depth at most two
graph pattern matching involves finding exact or approximate matches for a query subgraph in a larger graph it has been studied extensively and has strong applications in domains such as computer vision , computational biology , social networks , security and finance the problem of exact graph pattern matching is often described in terms of subgraph isomorphism which is np complete the exponential growth in streaming data from online social networks , news and video streams and the continual need for situational awareness motivates a solution for finding patterns in streaming updates this is also the prime driver for the real time analytics market development of incremental algorithms for graph pattern matching on streaming inputs to a continually evolving graph is a nascent area of research some of the challenges associated with this problem are the same as found in continuous query \( cq \) evaluation on streaming databases this paper reviews some of the representative work from the exhaustively researched field of cq systems and identifies important semantics , constraints and architectural features that are also appropriate for hpc systems performing real time graph analytics for each of these features we present a brief discussion of the challenge encountered in the database realm , the approach to the solution and state their relevance in a high performance , streaming graph processing framework
this paper introduces a new scheme of lt codes , named multiple configurations in multiple configurations lt codes \( mc lt codes \) , multiple sets of output symbols are simultaneously provided to receivers for recovering the source data each receiver , without the need to send information back to the sender , is capable of receiving the output symbols generated by some configuration chosen according to its own decoding phase aiming at the broadcasting scenarios without feedback channels , the proposed mc lt codes are shown to outperform the optimal pure lt codes at the cost of encoding and transmitting units in this paper , the inspiration of mc lt codes is presented , how mc lt codes work is described by giving examples , in which the optimal pure lt codes are outperformed , and a practical design of mc lt codes , which is analytically proved to have at least the same performance bound as the pure lt codes , is proposed the results of numerical simulation experiments demonstrate that the proposed practical design of mc lt codes can deliver better performance than the lt codes in comparison in summary , this paper creates new potential research directions for lt codes , and mc lt codes are a promising variant of lt codes , especially for broadcasting scenarios
knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future we improve upon the attention model of bahdanau et al \( 2014 \) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word this architecture easily captures informative features , such as fertility and regularities in relative distortion in experiments , we show our parameterization of attention improves translation quality
in this paper , we propose novel state based algorithms which dynamically control the random access network based on its current state such as channel states of wireless links and backlog states of the queues after formulating the problem , corresponding algorithms with diverse control functions are proposed consequently , it will be shown that the proposed state based schemes for control of the random access networks , results in significant performance gains in comparison with previously proposed control algorithms in order to select an appropriate control function , performances of the state based control algorithms are compared for a wide range of traffic scenarios it is also shown that even an approximate knowledge of network statistics helps in selecting the proper state dependent control function
we investigate quantitative extensions of modal logic and the modal mu calculus , and study the question whether the tight connection between logic and games can be lifted from the qualitative logics to their quantitative counterparts it turns out that , if the quantitative mu calculus is defined in an appropriate way respecting the duality properties between the logical operators , then its model checking problem can indeed be characterised by a quantitative variant of parity games however , these quantitative games have quite different properties than their classical counterparts , in particular they are , in general , not positionally determined the correspondence between the logic and the games goes both ways the value of a formula on a quantitative transition system coincides with the value of the associated quantitative game , and conversely , the values of quantitative parity games are definable in the quantitative mu calculus
this work studies the generalized moran process , as introduced by lieberman et al nature , 433 312 316 , 2005 , where the individuals of a population reside on the vertices of an undirected connected graph the initial population has a single mutant of a fitness value r , residing at some vertex v , while every other individual has initially fitness 1 the main quantity of interest is the fixation probability , i e the probability that eventually the whole graph is occupied by descendants of the mutant in this work we concentrate on the fixation probability when the mutant is initially on a specific vertex v we then aim at finding graphs that have many strong starts \( or many weak starts \) for the mutant thus we introduce a parameterized notion of selective amplifiers suppressors , i e graphs with at least some h \( n \) vertices \( starting points of the mutant \) , which fixate the graph with large \( resp small \) probability we prove the existence of strong selective amplifiers and of quite strong selective suppressors regarding the traditional notion of fixation probability from a random start , we provide the first non trivial upper and lower bounds first we demonstrate the non existence of strong universal amplifiers , i e we prove that for any graph the fixation probability from a random start is at most 1 frac c \( r \) n 3 4 then we prove the thermal theorem , stating that for any graph , when the mutant starts at vertex v , the fixation probability is at least \( r 1 \) \( r frac deg v deg min \) this implies the first nontrivial lower bound for the usual notion of fixation probability , which is almost tight this theorem extends the isothermal theorem of lieberman et al for regular graphs our proof techniques are original and are based on new domination arguments which may be of general interest in markov processes that are of the general birth death type
in this paper , we study a parallel version of galton watson processes for the random generation of tree shaped structures random trees are useful in many situations \( testing , binary search , simulation of physics phenomena , \) as attests more than 49000 citations on google scholar using standard analytic combinatorics , we first give a theoretical , average case study of the random process in order to evaluate how parallelism can be extracted from this process , and we deduce a parallel generation algorithm then we present how it can be implemented in a task based parallel paradigm for shared memory \( here , intel cilk \) this implementation faces several challenges , among which efficient , thread safe random bit generation , memory management and algorithmic modifications for small grain parallelism finally , we evaluate the performance of our implementation and the impact of different choices and parameters we obtain a significant efficiency improvement for the generation of big trees we also conduct empirical and theoretical studies of the average behaviour of our algorithm
we present a simple and fast geometric method for modeling data by a union of affine subspaces the method begins by forming a collection of local best fit affine subspaces , i e , subspaces approximating the data in local neighborhoods the correct sizes of the local neighborhoods are determined automatically by the jones' beta 2 numbers \( we prove under certain geometric conditions that our method finds the optimal local neighborhoods \) the collection of subspaces is further processed by a greedy selection procedure or a spectral method to generate the final model we discuss applications to tracking based motion segmentation and clustering of faces under different illuminating conditions we give extensive experimental evidence demonstrating the state of the art accuracy and speed of the suggested algorithms on these problems and also on synthetic hybrid linear data as well as the mnist handwritten digits data and we demonstrate how to use our algorithms for fast determination of the number of affine subspaces
information divergence functions play a critical role in statistics and information theory in this paper we introduce a divergence function between distributions and describe a number of properties that make it appealing for classification applications based on an extension of a multivariate two sample test , we identify a nonparametric estimator of the divergence that does not impose strong assumptions on the data distribution furthermore , we show that this measure bounds the minimum binary classification error for the case when the training and test data are drawn from the same distribution and for the case where there exists some mismatch between training and test distributions we confirm the theoretical results by designing feature selection algorithms using the criteria from these bounds and evaluating the algorithms on a series of pathological speech classification tasks
the compressed sensing paradigm allows to efficiently represent sparse signals by means of their linear measurements however , the problem of transmitting these measurements to a receiver over a channel potentially prone to packet losses has received little attention so far in this paper , we propose novel methods to generate multiple descriptions from compressed sensing measurements to increase the robustness over unreliable channels in particular , we exploit the democracy property of compressive measurements to generate descriptions in a simple manner by partitioning the measurement vector and properly allocating bit rate , outperforming classical methods like the multiple description scalar quantizer in addition , we propose a modified version of the basis pursuit denoising recovery procedure that is specifically tailored to the proposed methods experimental results show significant performance gains with respect to existing methods
this paper studies multiple proof quantum merlin arthur \( qma \) proof systems in the setting when the completeness soundness gap is small small means that we only lower bound the gap with an inverse exponential function of the input length , or with an even smaller function using the protocol of blier and tapp arxiv 0709 0738 , we show that in this case the proof system has the same expressive power as non deterministic exponential time \( nexp \) since single proof qma proof systems , with the same bound on the gap , have expressive power at most exponential time \( exp \) , we get a separation between single and multi prover proof systems in the 'small gap setting' , under the assumption that exp is not equal to nexp this implies , among others , the nonexistence of certain operators called disentanglers \( defined by aaronson et al arxiv 0804 0802 \) , with good approximation parameters we also show that in this setting the proof system has the same expressive power if we restrict the verifier to be able to perform only bell measurements , i e , using a bellqma verifier this is not known to hold in the usual setting , when the gap is bounded by an inverse polynomial function of the input length to show this we use the protocol of chen and drucker arxiv 1011 0716 the only caveat here is that we need at least a linear amount of proofs to achieve the power of nexp , while in the previous setting two proofs were enough we also study the case when the proof lengths are only logarithmic in the input length and observe that in some cases the expressive power decreases however , we show that it does n't decrease further if we make the proof lengths to be even shorter
input sensitive profiling is a recent performance analysis technique that makes it possible to estimate the empirical cost function of individual routines of a program , helping developers understand how performance scales to larger inputs and pinpoint asymptotic bottlenecks in the code a current limitation of input sensitive profilers is that they specifically target sequential computations , ignoring any communication between threads in this paper we show how to overcome this limitation , extending the range of applicability of the original approach to multithreaded applications and to applications that operate on i o streams we develop new metrics for automatically estimating the size of the input given to each routine activation , addressing input produced by non deterministic memory stores performed by other threads as well as by the os kernel \( e g , in response to i o or network operations \) we provide real case studies , showing that our extension allows it to characterize the behavior of complex applications more precisely than previous approaches an extensive experimental investigation on a variety of benchmark suites \( including the spec omp2012 and the parsec benchmarks \) shows that our valgrind based input sensitive profiler incurs an overhead comparable to other prominent heavyweight analysis tools , while collecting significantly more performance points from each profiling session and correctly characterizing both thread induced and external input
now a day 's students have a large set of data having precious information hidden data mining technique can help to find this hidden information in this paper , data mining techniques name byes classification method is used on these data to help an institution institutions can find those students who are consistently perform well this study will help to institution reduce the drop put ratio to a significant level and improve the performance level of the institution
the profusion of online news articles makes it difficult to find interesting articles , a problem that can be assuaged by using a recommender system to bring the most relevant news stories to readers however , news recommendation is challenging because the most relevant articles are often new content seen by few users in addition , they are subject to trends and preference changes over time , and in many cases we do not have sufficient information to profile the reader in this paper , we introduce a class of news recommendation systems based on context trees they can provide high quality news recommendation to anonymous visitors based on present browsing behaviour we show that context tree recommender systems provide good prediction accuracy and recommendation novelty , and they are sufficiently flexible to capture the unique properties of news articles
we present an algorithm that identifies the reasoning patterns of agents in a game , by iteratively examining the graph structure of its multi agent influence diagram \( maid \) representation if the decision of an agent participates in no reasoning patterns , then we can effectively ignore that decision for the purpose of calculating a nash equilibrium for the game in some cases , this can lead to exponential time savings in the process of equilibrium calculation moreover , our algorithm can be used to enumerate the reasoning patterns in a game , which can be useful for constructing more effective computerized agents interacting with humans
we consider a two user state dependent multiaccess channel in which the states of the channel are known non causally to one of the encoders and only strictly causally to the other encoder both encoders transmit a common message and , in addition , the encoder that knows the states non causally transmits an individual message we find explicit characterizations of the capacity region of this communication model in both discrete memoryless and memoryless gaussian cases the analysis also reveals optimal ways of exploiting the knowledge of the state only strictly causally at the encoder that sends only the common message when such a knowledge is beneficial the encoders collaborate to convey to the decoder a lossy version of the state , in addition to transmitting the information messages through a generalized gel'fand pinsker binning particularly important in this problem are the questions of 1 \) optimal ways of performing the state compression and 2 \) whether or not the compression indices should be decoded uniquely we show that both compression `a la noisy network coding , i e , with no binning , and compression using wyner ziv binning are optimal the scheme that uses wyner ziv binning shares elements with cover and el gamal original compress and forward , but differs from it mainly in that backward decoding is employed instead of forward decoding and the compression indices are not decoded uniquely finally , by exploring the properties of our outer bound , we show that , although not required in general , the compression indices can in fact be decoded uniquely essentially without altering the capacity region , but at the expense of larger alphabets sizes for the auxiliary random variables
a serious problem in learning probabilistic models is the presence of hidden variables these variables are not observed , yet interact with several of the observed variables detecting hidden variables poses two problems determining the relations to other variables in the model and determining the number of states of the hidden variable in this paper , we address the latter problem in the context of bayesian networks we describe an approach that utilizes a score based agglomerative state clustering as we show , this approach allows us to efficiently evaluate models with a range of cardinalities for the hidden variable we show how to extend this procedure to deal with multiple interacting hidden variables we demonstrate the effectiveness of this approach by evaluating it on synthetic and real life data we show that our approach learns models with hidden variables that generalize better and have better structure than previous approaches
redundant transfer of resources is a critical issue for compromising the performance of mobile web applications \( a k a , apps \) in terms of data traffic , load time , and even energy consumption evidence shows that the current cache mechanisms are far from satisfactory with lessons learned from how native apps manage their resources , in this paper , we propose the rewap approach to fundamentally reducing redundant transfers by restructuring the resource loading of mobile web apps rewap is based on an efficient mechanism of resource packaging where stable resources are encapsulated and maintained into a package , and such a package shall be loaded always from the local storage and updated by explicitly refreshing by retrieving and analyzing the update of resources , rewap maintains resource packages that can accurately identify which resources can be loaded from the local storage for a considerably long period rewap also provides a wrapper for mobile web apps to enable loading and updating resource packages in the local storage as well as loading resources from resource packages rewap can be easily and seamlessly deployed into existing mobile web architectures with minimal modifications , and is transparent to end users we evaluate rewap based on continuous 15 day access traces of 50 mobile web apps that suffer heavily from the problem of redundant transfers compared to the original mobile web apps with cache enabled , rewap can significantly reduce the data traffic , with the median saving up to 51 in addition , rewap can incur only very minor runtime overhead of the client side browsers
we show that learning a convex body in rr d , given random samples from the body , requires 2 omega \( sqrt d eps \) samples by learning a convex body we mean finding a set having at most eps relative symmetric difference with the input body to prove the lower bound we construct a hard to learn family of convex bodies our construction of this family is very simple and based on error correcting codes
in this work we address the problem of approximating high dimensional data with a low dimensional representation we make the following contributions we propose an inverse regression method which exchanges the roles of input and response , such that the low dimensional variable becomes the regressor , and which is tractable we introduce a mixture of locally linear probabilistic mapping model that starts with estimating the parameters of inverse regression , and follows with inferring closed form solutions for the forward parameters of the high dimensional regression problem of interest moreover , we introduce a partially latent paradigm , such that the vector valued response variable is composed of both observed and latent entries , thus being able to deal with data contaminated by experimental artifacts that cannot be explained with noise models the proposed probabilistic formulation could be viewed as a latent variable augmentation of regression we devise expectation maximization \( em \) procedures based on a data augmentation strategy which facilitates the maximum likelihood search over the model parameters we propose two augmentation schemes and we describe in detail the associated em inference procedures that may well be viewed as generalizations of a number of em regression , dimension reduction , and factor analysis algorithms the proposed framework is validated with both synthetic and real data we provide experimental evidence that our method outperforms several existing regression techniques
this paper gives the linear complexity of binary ding helleseth generalized cyclotomic sequences of any order
generic matrix multiplication \( gemm \) and one dimensional convolution cross correlation \( conv \) kernels often constitute the bulk of the compute and memory intensive processing within image audio recognition and matching systems we propose a novel method to scale the energy and processing throughput of gemm and conv kernels for such error tolerant multimedia applications by adjusting the precision of computation our technique employs linear projections to the input matrix or signal data during the top level gemm and conv blocking and reordering the gemm and conv kernel processing then uses the projected inputs and the results are accumulated to form the final outputs throughput and energy scaling takes place by changing the number of projections computed by each kernel , which in turn produces approximate results , i e changes the precision of the performed computation results derived from a voltage and frequency scaled arm cortex a15 processor running face recognition and music matching algorithms demonstrate that the proposed approach allows for 280 440 increase of processing throughput and 75 80 decrease of energy consumption against optimized gemm and conv kernels without any impact in the obtained recognition or matching accuracy even higher gains can be obtained if one is willing to tolerate some reduction in the accuracy of the recognition and matching applications
we consider a communication problem in which an update of the source message needs to be conveyed to one or more distant receivers that are interested in maintaining specific linear functions of the source message the setting is one in which the updates are sparse in nature , and where neither the source nor the receiver \( s \) is aware of the exact em difference vector , but only know the amount of sparsity that is present in the difference vector under this setting , we are interested in devising linear encoding and decoding schemes that minimize the communication cost involved we show that the optimal solution to this problem is closely related to the notion of maximally recoverable codes \( mrcs \) , which were originally introduced in the context of coding for storage systems in the context of storage , mrcs guarantee optimal erasure protection when the system is partially constrained to have local parity relations among the storage nodes in our problem , we show that optimal solutions exist if and only if mrcs of certain kind \( identified by the desired linear functions \) exist we consider point to point and broadcast versions of the problem , and identify connections to mrcs under both these settings
we study the robustness the invariance under definition changes of the cluster class cl p hhkw05 this class contains each p function that is computed by a balanced turing machine whose accepting paths always form a cluster with respect to some length respecting total order with efficient adjacency checks the definition of cl p is heavily influenced by the defining paper 's focus on \( global \) orders in contrast , we define a cluster class , clu p , to capture what seems to us a more natural model of cluster computing we prove that the naturalness is costless cl p clu p then we exploit the more natural , flexible features of clu p to prove new robustness results for cl p and to expand what is known about the closure properties of cl p the complexity of recognizing edges of an ordered collection of computation paths or of a cluster of accepting computation paths is central to this study most particularly , our proofs exploit the power of unique discovery of edges the ability of nondeterministic functions to , in certain settings , discover on exactly one \( in some cases , on at most one \) computation path a critical piece of information regarding edges of orderings or clusters
a directed acyclic graph \( dag \) partially represents the conditional independence structure among observations of a system if the local markov condition holds , that is , if every variable is independent of its non descendants given its parents in general , there is a whole class of dags that represents a given set of conditional independence relations we are interested in properties of this class that can be derived from observations of a subsystem only to this end , we prove an information theoretic inequality that allows for the inference of common ancestors of observed parts in any dag representing some unknown larger system more explicitly , we show that a large amount of dependence in terms of mutual information among the observations implies the existence of a common ancestor that distributes this information within the causal interpretation of dags our result can be seen as a quantitative extension of reichenbach 's principle of common cause to more than two variables our conclusions are valid also for non probabilistic observations such as binary strings , since we state the proof for an axiomatized notion of mutual information that includes the stochastic as well as the algorithmic version
the problem of constrained finite impulse response \( fir \) filter design is central to signal processing and arises in a variety of disciplines this paper surveys the design of such filters using projection onto convex sets \( pocs \) and discusses certain commonly encountered time and frequency domain constraints we study in particular the design of nyquist filters and propose a simple extension to the work carried out by haddad , stark , and galatsanos in 1 the flexibility and the ease that this design method provides in terms of accommodating constraints is one of its outstanding features
we consider one hop communication in wireless networks with random connections in the random connection model , the channel powers between different nodes are drawn from a common distribution in an i i d manner an scheme achieving the throughput scaling of order n 1 3 delta , for any delta 0 , is proposed , where n is the number of nodes such achievable throughput , along with the order n 1 3 upper bound derived by cui et al , characterizes the throughput capacity of one hop schemes for the class of connection models with finite mean and variance
we study the outage probability of opportunistic relay selection in decode and forward relaying with secrecy constraints we derive the closed form expression for the outage probability based on the analytical result , the asymptotic performance is then investigated the accuracy of our performance analysis is verified by the simulation results
in this paper we extend our previous and only study on the dynamics of the chilean web this new study doubles the time period and to the best of our knowledge is the only study of its type known about any country in the web the new results corroborate the trends found before , in particular the exponential growth of the web , and reinforce the conclusion that the web is more chaotic than we would like hence , modeling most web characteristics is not trivial
we further study the keyless authentication problem in a noisy model in our previous work , where no secret setup is available for sender alice and receiver bob while there is dmc w 1 from alice to bob and a two way noiseless but insecure channel between them we propose a construction such that the message length over dmc w 1 does not depend on the size of the source space if the source space is cal s and the number of channel w 1 uses is n , then our protocol only has a round complexity of log cal s log n 4 in addition , we show that the round complexity of any secure protocol in our model is lower bounded by log cal s log n 5 we also obtain a lower bound on the success probability when the message size on dmc w 1 is given finally , we derive the capacity for a non interactive authentication protocol under general dmcs , which extends the result under bscs in our previous work
as computer clusters become more common and the size of the problems encountered in the field of ai grows , there is an increasing demand for efficient parallel inference algorithms we consider the problem of parallel inference on large factor graphs in the distributed memory setting of computer clusters we develop a new efficient parallel inference algorithm , dbrsplash , which incorporates over segmented graph partitioning , belief residual scheduling , and uniform work splash operations we empirically evaluate the dbrsplash algorithm on a 120 processor cluster and demonstrate linear to super linear performance gains on large factor graph models
in this paper , designs and analyses of compressive recognition systems are discussed , and also a method of establishing a dual connection between designs of good communication codes and designs of recognition systems is presented pattern recognition systems based on compressed patterns and compressed sensor measurements can be designed using low density matrices we examine truncation encoding where a subset of the patterns and measurements are stored perfectly while the rest is discarded we also examine the use of ldpc parity check matrices for compressing measurements and patterns we show how more general ensembles of good linear codes can be used as the basis for pattern recognition system design , yielding system design strategies for more general noise models
in this paper , the filter and forward \( ff \) relay design for orthogonal frequency division multiplexing \( ofdm \) transmission systems is considered to improve the system performance over simple amplify and forward \( af \) relaying unlike conventional ofdm relays performing ofdm demodulation and remodulation , to reduce processing complexity , the proposed ff relay directly filters the incoming signal in time domain with a finite impulse response \( fir \) and forwards the filtered signal to the destination three design criteria are considered to optimize the relay filter the first criterion is the minimization of the relay transmit power subject to per subcarrier signal to noise ratio \( snr \) constraints , the second is the maximization of the worst subcarrier channel snr subject to source and relay transmit power constraints , and the third is the maximization of data rate subject to source and relay transmit power constraints it is shown that the first problem reduces to a semi definite programming \( sdp \) problem by semi definite relaxation and the solution to the relaxed sdp problem has rank one under a mild condition for the latter two problems , the problem of joint source power allocation and relay filter design is considered and an efficient algorithm is proposed for each problem based on alternating optimization and the projected gradient method \( pgm \) numerical results show that the proposed ff relay significantly outperforms simple af relays with insignificant increase in complexity thus , the proposed ff relay provides a practical alternative to the af relaying scheme for ofdm transmission
in this paper , we propose a quality centric congestion control for multimedia streaming over ip networks , which we refer to as media tcp unlike existing congestion control schemes that adapt a user 's sending rate merely to the network condition , our solution adapts the sending rate to both the network condition and the application characteristics by explicitly considering the distortion impacts , delay deadlines , and interdependencies of different video packet classes hence , our media aware solution is able to provide differential services for transmitting various packet classes and thereby , further improves the multimedia streaming quality we model this problem using a finite horizon markov decision process \( fhmdp \) and determine the optimal congestion control policy that maximizes the long term multimedia quality , while adhering to the horizon tcp friendliness constraint , which ensures long term fairness with existing tcp applications we show that the fhmdp problem can be decomposed into multiple optimal stopping problems , which admit a low complexity threshold based solution moreover , unlike existing congestion control approaches , which focus on maintaining throughput based fairness among users , the proposed media tcp aims to achieve quality based fairness among multimedia users we also derive sufficient conditions for multiple multimedia users to achieve quality based fairness using media tcp congestion control our simulation results show that the proposed media tcp achieves more than 3db improvement in terms of psnr over the conventional tcp congestion control approaches , with the largest improvements observed for real time streaming applications requiring stringent playback delays
in support of art investigation , we propose a new source separation method that unmixes a single x ray scan acquired from double sided paintings in this problem , the x ray signals to be separated have similar morphological characteristics , which brings previous source separation methods to their limits our solution is to use photographs taken from the front and back side of the panel to drive the separation process the crux of our approach relies on the coupling of the two imaging modalities \( photographs and x rays \) using a novel coupled dictionary learning framework able to capture both common and disparate features across the modalities using parsimonious representations the common component models features shared by the multi modal images , whereas the innovation component captures modality specific information as such , our model enables the formulation of appropriately regularized convex optimization procedures that lead to the accurate separation of the x rays our dictionary learning framework can be tailored both to a single and a multi scale framework , with the latter leading to a significant performance improvement moreover , to improve further on the visual quality of the separated images , we propose to train coupled dictionaries that ignore certain parts of the painting corresponding to craquelure experimentation on synthetic and real data taken from digital acquisition of the ghent altarpiece \( 1432 \) confirms the superiority of our method against the state of the art morphological component analysis technique that uses either fixed or trained dictionaries to perform image separation
we survey the emerging area of compression based , parameter free , similarity distance measures useful in data mining , pattern recognition , learning and automatic semantics extraction given a family of distances on a set of objects , a distance is universal up to a certain precision for that family if it minorizes every distance in the family between every two objects in the set , up to the stated precision \( we do not require the universal distance to be an element of the family \) we consider similarity distances for two types of objects literal objects that as such contain all of their meaning , like genomes or books , and names for objects the latter may have literal embodyments like the first type , but may also be abstract like ``red'' or ``christianity '' for the first type we consider a family of computable distance measures corresponding to parameters expressing similarity according to particular featuresdistances generated by web users corresponding to particular semantic relations between the \( names for \) the designated objects for both families we give universal similarity distance measures , incorporating all particular distance measures in the family in the first case the universal distance is based on compression and in the second case it is based on google page counts related to search terms in both cases experiments on a massive scale give evidence of the viability of the approaches between pairs of literal objects for the second type we consider similarity
the computation of the global minimum energy conformation \( gmec \) is an important and challenging topic in structure based computational protein design in this paper , we propose a new protein design algorithm based on the and or branch and bound \( aobb \) search , which is a variant of the traditional branch and bound search algorithm , to solve this combinatorial optimization problem by integrating with a powerful heuristic function , aobb is able to fully exploit the graph structure of the underlying residue interaction network of a backbone template to significantly accelerate the design process tests on real protein data show that our new protein design algorithm is able to solve many prob lems that were previously unsolvable by the traditional exact search algorithms , and for the problems that can be solved with traditional provable algorithms , our new method can provide a large speedup by several orders of magnitude while still guaranteeing to find the global minimum energy conformation \( gmec \) solution
it is commonly believed that increasing the interpretability of a machine learning model may decrease its predictive power however , inspecting input output relationships of those models using visual analytics , while treating them as black box , can help to understand the reasoning behind outcomes without sacrificing predictive quality we identify a space of possible solutions and provide two examples of where such techniques have been successfully used in practice
we consider the problem of optimally allocating a given total storage budget in a distributed storage system a source has a data object which it can code and store over a set of storage nodes it is allowed to store any amount of coded data in each node , as long as the total amount of storage used does not exceed the given budget a data collector subsequently attempts to recover the original data object by accessing each of the nodes independently with some constant probability by using an appropriate code , successful recovery occurs when the total amount of data in the accessed nodes is at least the size of the original data object the goal is to find an optimal storage allocation that maximizes the probability of successful recovery this optimization problem is challenging because of its discrete nature and nonconvexity , despite its simple formulation symmetric allocations \( in which all nonempty nodes store the same amount of data \) , though intuitive , may be suboptimal the problem is nontrivial even if we optimize over only symmetric allocations our main result shows that the symmetric allocation that spreads the budget maximally over all nodes is asymptotically optimal in a regime of interest specifically , we derive an upper bound for the suboptimality of this allocation and show that the performance gap vanishes asymptotically in the specified regime further , we explicitly find the optimal symmetric allocation for a variety of cases our results can be applied to distributed storage systems and other problems dealing with reliability under uncertainty , including delay tolerant networks \( dtns \) and content delivery networks \( cdns \)
data quantization learns encoding results of data with certain requirements , and provides a broad perspective of many real world applications to data handling nevertheless , the results of encoder is usually limited to multivariate inputs with the random mapping , and side information of binary codes are hardly to mostly depict the original data patterns as possible in the literature , cosine based random quantization has attracted much attentions due to its intrinsic bounded results nevertheless , it usually suffers from the uncertain outputs , and information of original data fails to be fully preserved in the reduced codes in this work , a novel binary embedding method , termed adaptive training quantization \( atq \) , is proposed to learn the ideal transform of random encoder , where the limitation of cosine random mapping is tackled as an adaptive learning idea , the reduced mapping is adaptively calculated with idea of data group , while the bias of random transform is to be improved to hold most matching information experimental results show that the proposed method is able to obtain outstanding performance compared with other random quantization methods
currently the dempster shafer based algorithm and uniform random probability based algorithm are the preferred method of resolving security games , in which defenders are able to identify attackers and only strategy remained ambiguous however this model is inefficient in situations where resources are limited and both the identity of the attackers and their strategies are ambiguous the intent of this study is to find a more effective algorithm to guide the defenders in choosing which outside agents with which to cooperate given both ambiguities we designed an experiment where defenders were compelled to engage with outside agents in order to maximize protection of their targets we introduced two important notions the behavior of each agent in target protection and the tolerance threshold in the target protection process from these , we proposed an algorithm that was applied by each defender to determine the best potential assistant \( s \) with which to cooperate our results showed that our proposed algorithm is safer than the dempster shafer based algorithm
we study the phase diagram and the algorithmic hardness of the random `locked' constraint satisfaction problems , and compare them to the commonly studied 'non locked' problems like satisfiability of boolean formulas or graph coloring the special property of the locked problems is that clusters of solutions are isolated points this simplifies significantly the determination of the phase diagram , which makes the locked problems particularly appealing from the mathematical point of view on the other hand we show empirically that the clustered phase of these problems is extremely hard from the algorithmic point of view the best known algorithms all fail to find solutions our results suggest that the easy hard transition \( for currently known algorithms \) in the locked problems coincides with the clustering transition these should thus be regarded as new benchmarks of really hard constraint satisfaction problems
cardinality potentials are a generally useful class of high order potential that affect probabilities based on how many of d binary variables are active maximum a posteriori \( map \) inference for cardinality potential models is well understood , with efficient computations taking o \( dlogd \) time yet efficient marginalization and sampling have not been addressed as thoroughly in the machine learning community we show that there exists a simple algorithm for computing marginal probabilities and drawing exact joint samples that runs in o \( dlog2 d \) time , and we show how to frame the algorithm as efficient belief propagation in a low order tree structured model that includes additional auxiliary variables we then develop a new , more general class of models , termed recursive cardinality models , which take advantage of this efficiency finally , we show how to do efficient exact inference in models composed of a tree structure and a cardinality potential we explore the expressive power of recursive cardinality models and empirically demonstrate their utility
this paper suggests the use of intelligent network aware processing agents in wireless local area network drivers to generate metrics for bandwidth estimation based on real time channel statistics to enable wireless multimedia application adaptation various configurations in the wireless digital home are studied and the experimental results with performance variations are presented
an isogeometric boundary element method for problems in elasticity is presented , which is based on an independent approximation for the geometry , traction and displacement field this enables a flexible choice of refinement strategies , permits an efficient evaluation of geometry related information , a mixed collocation scheme which deals with discontinuous tractions along non smooth boundaries and a significant reduction of the right hand side of the system of equations for common boundary conditions all these benefits are achieved without any loss of accuracy compared to conventional isogeometric formulations the system matrices are approximated by means of hierarchical matrices to reduce the computational complexity for large scale analysis for the required geometrical bisection of the domain , a strategy for the evaluation of bounding boxes containing the supports of nurbs basis functions is presented the versatility and accuracy of the proposed methodology is demonstrated by convergence studies showing optimal rates and real world examples in two and three dimensions
we study a cooperation model where the positions of base stations follow a poisson point process distribution and where voronoi cells define the planar areas associated with them for the service of each user , either one or two base stations are involved if two , these cooperate by exchange of user data and reduced channel information \( channel phase , second neighbour interference \) with conferencing over some backhaul link the total user transmission power is split between them and a common message is encoded , which is coherently transmitted by the stations the decision for a user to choose service with or without cooperation is directed by a family of geometric policies the suggested policies further control the shape of coverage contours in favor of cell edge areas analytic expressions based on stochastic geometry are derived for the coverage probability in the network their numerical evaluation shows benefits from cooperation , which are enhanced when dirty paper coding is applied to eliminate the second neighbour interference
we present the construction of a new family of erasure correcting codes for distributed storage that yield low repair bandwidth and low repair complexity the construction is based on two classes of parity symbols the primary goal of the first class of symbols is to provide good erasure correcting capability , while the second class facilitates node repair , reducing the repair bandwidth and the repair complexity we compare the proposed codes with other codes proposed in the literature
this paper introduces a new hybrid memory analysis , structural analysis , which combines an expressive shape analysis style abstract domain with efficient and simple points to style transfer functions using data from empirical studies on the runtime heap structures and the programmatic idioms used in modern object oriented languages we construct a heap analysis with the following characteristics \( 1 \) it can express a rich set of structural , shape , and sharing properties which are not provided by a classic points to analysis and that are useful for optimization and error detection applications \( 2 \) it uses efficient , weakly updating , set based transfer functions which enable the analysis to be more robust and scalable than a shape analysis and \( 3 \) it can be used as the basis for a scalable interprocedural analysis that produces precise results in practice the analysis has been implemented for net bytecode and using this implementation we evaluate both the runtime cost and the precision of the results on a number of well known benchmarks and real world programs our experimental evaluations show that the domain defined in this paper is capable of precisely expressing the majority of the connectivity , shape , and sharing properties that occur in practice and , despite the use of weak updates , the static analysis is able to precisely approximate the ideal results the analysis is capable of analyzing large real world programs \( over 30k bytecodes \) in less than 65 seconds and using less than 130mb of memory in summary this work presents a new type of memory analysis that advances the state of the art with respect to expressive power , precision , and scalability and represents a new area of study on the relationships between and combination of concepts from shape and points to analyses
generic programming is an effective methodology for developing reusable software libraries many programming languages provide generics and have features for describing interfaces , but none completely support the idioms used in generic programming to address this need we developed the language g the central feature of g is the concept , a mechanism for organizing constraints on generics that is inspired by the needs of modern c libraries g provides modular type checking and separate compilation \( even of generics \) these characteristics support modular software development , especially the smooth integration of independently developed components in this article we present the rationale for the design of g and demonstrate the expressiveness of g with two case studies porting the standard template library and the boost graph library from c to g the design of g shares much in common with the concept extension proposed for the next c standard \( the authors participated in its design \) but there are important differences described in this article
timing side channels in two user schedulers are studied when two users share a scheduler , one user may learn the other user 's behavior from patterns of service timings we measure the information leakage of the resulting timing side channel in schedulers serving a legitimate user and a malicious attacker , using a privacy metric defined as the shannon equivocation of the user 's job density we show that the commonly used first come first serve \( fcfs \) scheduler provides no privacy as the attacker is able to to learn the user 's job pattern completely furthermore , we introduce an scheduling policy , accumulate and serve scheduler , which services jobs from the user and attacker in batches after buffering them the information leakage in this scheduler is mitigated at the price of service delays , and the maximum privacy is achievable when large delays are added
it has long been conjectured that hypothesis spaces suitable for data that is compositional in nature , such as text or images , may be more efficiently represented with deep hierarchical architectures than with shallow ones despite the vast empirical evidence , formal arguments to date are limited and do not capture the kind of networks used in practice using tensor factorization , we derive a universal hypothesis space implemented by an arithmetic circuit over functions applied to local data structures \( e g image patches \) the resulting networks first pass the input through a representation layer , and then proceed with a sequence of layers comprising sum followed by product pooling , where sum corresponds to the widely used convolution operator the hierarchical structure of networks is born from factorizations of tensors based on the linear weights of the arithmetic circuits we show that a shallow network corresponds to a rank 1 decomposition , whereas a deep network corresponds to a hierarchical tucker \( ht \) decomposition log space computation for numerical stability transforms the networks into simnets in its basic form , our main theoretical result shows that the set of polynomially sized rank 1 decomposable tensors has measure zero in the parameter space of polynomially sized ht decomposable tensors in deep learning terminology , this amounts to saying that besides a negligible set , all functions that can be implemented by a deep network of polynomial size , require an exponential size if one wishes to implement \( or approximate \) them with a shallow network our construction and theory shed new light on various practices and ideas employed by the deep learning community , and in that sense bear a paradigmatic contribution as well
orthogonal frequency division multiplexing \( ofdm \) is an emerging research field of wireless communication it is one of the most proficient multi carrier transmission techniques widely used today as broadband wired wireless applications having several attributes such as provides greater immunity to multipath fading impulse noise , eliminating inter symbol interference \( isi \) , inter carrier interference \( ici \) the need for equalizers ofdm signals have a general problem of high peak to average power ratio \( papr \) which is defined as the ratio of the peak power to the average power of the ofdm signal the drawback of high papr is that the dynamic range of the power amplifier \( pa \) and digital to analog converter \( dac \) in this paper , an improved scheme of amplitude clipping filtering method is proposed and implemented which shows the significant improvement in case of papr reduction while increasing slight ber compare to an existing method also , the comparative studies of different parameters will be covered
past experiences under the designation of swarm paintings conducted in 2001 , not only confirmed the possibility of realizing an artificial art \( thus non human \) , as introduced into the process the questioning of creative migration , specifically from the computer monitors to the canvas via a robotic harm in more recent self organized based research we seek to develop and profound the initial ideas by using a swarm of autonomous robots \( artsbot project 2002 03 \) , that live avoiding the purpose of being merely a simple perpetrator of order streams coming from an external computer , but instead , that actually co evolve within the canvas space , acting \( that is , laying ink \) according to simple inner threshold stimulus response functions , reacting simultaneously to the chromatic stimulus present in the canvas environment done by the passage of their team mates , as well as by the distributed feedback , affecting their future collective behaviour in parallel , and in what respects to certain types of collective systems , we seek to confirm , in a physically embedded way , that the emergence of order \( even as a concept \) seems to be found at a lower level of complexity , based on simple and basic interchange of information , and on the local dynamic of parts , who , by self organizing mechanisms tend to form an lived whole , innovative and adapting , allowing for emergent open ended creative and distributed production keywords artsbots project , swarm intelligence , stigmergy , unmanned art , symbiotic art , swarm paintings , robot paintings , non human art , painting emergence and cooperation , art and complexity , artbots the robot talent show
learning structured models using maximum margin techniques has become an indispensable tool for com puter vision researchers , as many computer vision applications can be cast naturally as an image labeling problem pixel based or superpixel based conditional random fields are particularly popular examples typ ically , neighborhood graphs , which contain a large number of cycles , are used as exact inference in loopy graphs is np hard in general , learning these models without approximations is usually deemed infeasible in this work we show that , despite the theoretical hardness , it is possible to learn loopy models exactly in practical applications to this end , we analyze the use of multiple approximate inference techniques together with cutting plane training of structural svms we show that our proposed method yields exact solutions with an optimality guarantees in a computer vision application , for little additional computational cost we also propose a dynamic caching scheme to accelerate training further , yielding runtimes that are comparable with approximate methods we hope that this insight can lead to a reconsideration of the tractability of loopy models in computer vision
this thesis presents important insights and concepts related to the topic of the extraction of geometric primitives from the edge contours of digital images three specific problems related to this topic have been studied , viz , polygonal approximation of digital curves , tangent estimation of digital curves , and ellipse fitting anddetection from digital curves for the problem of polygonal approximation , two fundamental problems have been addressed first , the nature of the performance evaluation metrics in relation to the local and global fitting characteristics has been studied second , an explicit error bound of the error introduced by digitizing a continuous line segment has been derived and used to propose a generic non heuristic parameter independent framework which can be used in several dominant point detection methods for the problem of tangent estimation for digital curves , a simple method of tangent estimation has been proposed it is shown that the method has a definite upper bound of the error for conic digital curves it has been shown that the method performs better than almost all \( seventy two \) existing tangent estimation methods for conic as well as several non conic digital curves for the problem of fitting ellipses on digital curves , a geometric distance minimization model has been considered an unconstrained , linear , non iterative , and numerically stable ellipse fitting method has been proposed and it has been shown that the proposed method has better selectivity for elliptic digital curves \( high true positive and low false positive \) as compared to several other ellipse fitting methods for the problem of detecting ellipses in a set of digital curves , several innovative and fast pre processing , grouping , and hypotheses evaluation concepts applicable for digital curves have been proposed and combined to form an ellipse detection method
influence maximization is the problem of finding a set of influential users in a social network such that the expected spread of influence under a certain propagation model is maximized much of the previous work has neglected the important distinction between social influence and actual product adoption however , as recognized in the management science literature , an individual who gets influenced by social acquaintances may not necessarily adopt a product \( or technology \) , due , e g , to monetary concerns in this work , we distinguish between influence and adoption by explicitly modeling the states of being influenced and of adopting a product we extend the classical linear threshold \( lt \) model to incorporate prices and valuations , and factor them into users' decision making process of adopting a product we show that the expected profit function under our proposed model maintains submodularity under certain conditions , but no longer exhibits monotonicity , unlike the expected influence spread function to maximize the expected profit under our extended lt model , we employ an unbudgeted greedy framework to propose three profit maximization algorithms the results of our detailed experimental study on three real world datasets demonstrate that of the three algorithms , textsf page , which assigns prices dynamically based on the profit potential of each candidate seed , has the best performance both in the expected profit achieved and in running time
in our basic model , we study a stationary poisson pattern of nodes on a line embedded in an independent planar poisson field of interfering nodes assuming slotted aloha and the signal to interference and noise ratio capture condition , with the usual power law path loss model and rayleigh fading , we explicitly evaluate several local and end to end performance characteristics related to the nearest neighbor packet relaying on this line , and study their dependence on the model parameters \( the density of relaying and interfering nodes , aloha tuning and the external noise power \) our model can be applied in two cases the first use is for vehicular ad hoc networks , where vehicles are randomly located on a straight road the second use is to study a typical route traced in a \( general \) planar ad hoc network by some routing mechanism the approach we have chosen allows us to quantify the non efficiency of long distance routing in pure ad hoc networks and evaluate a possible remedy for it in the form of additional fixed relaying nodes , called road side units in a vehicular network it also allows us to consider a more general field of interfering nodes and study the impact of the clustering of its nodes the routing performance as a special case of a field with more clustering than the poison field , we consider a poisson line field of interfering nodes , in which all the nodes are randomly located on random straight lines the comparison to our basic model reveals a paradox clustering of interfering nodes decreases the outage probability of a single \( typical \) transmission on the route , but increases the mean end to end delay
this paper proposes a new framework for citation content analysis \( cca \) , for syntactic and semantic analysis of citation content that can be used to better analyze the rich sociocultural context of research behavior the framework could be considered the next generation of citation analysis this paper briefly reviews the history and features of content analysis in traditional social sciences , and its previous application in library and information science based on critical discussion of the theoretical necessity of a new method as well as the limits of citation analysis , the nature and purposes of cca are discussed , and potential procedures to conduct cca , including principles to identify the reference scope , a two dimensional \( citing and cited \) and two modular \( syntactic and semantic modules \) codebook , are provided and described future works and implications are also suggested
we consider the problem of finding a low discrepancy coloring for sparse set systems where each element lies in at most t sets we give an efficient algorithm that finds a coloring with discrepancy o \( \( t log n \) 1 2 \) , matching the best known non constructive bound for the problem due to banaszczyk the previous algorithms only achieved an o \( t 1 2 log n \) bound the result also extends to the more general koml ' o s setting and gives an algorithmic o \( log 1 2 n \) bound
millimeter wave \( mmwave \) communication is a promising technology for 5g cellular systems to compensate for the severe path loss in mmwave systems , large antenna arrays are generally used to achieve significant beamforming gains however , due to the high hardware and power consumption cost associated with radio frequency \( rf \) chains , it is desirable to achieve the large antenna gains , but with only limited number of rf chains for mmwave communications to this end , we study in this paper a new lens antenna array enabled mmwave mimo communication system we first show that the array response of the proposed lens antenna array at the receiver transmitter follows a sinc function , where the antenna with the peak response is determined by the angle of arrival \( aoa \) departure \( aod \) of the received transmitted signal by exploiting this unique property of lens antenna arrays along with the multi path sparsity of mmwave channels , we propose a novel low cost and capacity achieving mimo transmission scheme , termed emph orthogonal path division multiplexing \( opdm \) for channels with insufficiently separated aoas and or aods , we also propose a simple emph path grouping technique with group based small scale mimo processing to mitigate the inter path interference numerical results are provided to compare the performance of the proposed lens antenna arrays for mmwave mimo system against that of conventional arrays , under different practical setups it is shown that the proposed system achieves significant throughput gain as well as complexity and hardware cost reduction , both making it an appealing new paradigm for mmwave mimo communications
this paper addresses a multi criteria decision method properly designed to effectively evaluate the most performing strategy for multicast content delivery in long term evolution \( lte \) and beyond systems we compared the legacy conservative based approach with other promising strategies in literature , i e , opportunistic multicasting and subgroup based policies tailored to exploit different cost functions , such as maximum throughput , proportional fairness and the multicast dissatisfaction index \( mdi \) we provide a comparison among above schemes in terms of aggregate data rate \( adr \) , fairness and spectral efficiency we further design a multi criteria decision making method , namely topsis , to evaluate through a single mark the overall performance of considered strategies the obtained results show that the mdi subgrouping strategy represents the most suitable approach for multicast content delivery as it provides the most promising trade off between the fairness and the throughput achieved by the multicast members
every organism in an environment , whether biological , robotic or virtual , must be able to predict certain aspects of its environment in order to survive or perform whatever task is intended it needs a model that is capable of estimating the consequences of possible actions , so that planning , control , and decision making become feasible for scientific purposes , such models are usually created in a problem specific manner using differential equations and other techniques from control and system theory in contrast to that , we aim for an unsupervised approach that builds up the desired model in a self organized fashion inspired by slow feature analysis \( sfa \) , our approach is to extract sub signals from the input , that behave as predictable as possible these predictable features are highly relevant for modeling , because predictability is a desired property of the needed consequence estimating model by definition in our approach , we measure predictability with respect to a certain prediction model we focus here on the solution of the arising optimization problem and present a tractable algorithm based on algebraic methods which we call predictable feature analysis \( pfa \) we prove that the algorithm finds the globally optimal signal , if this signal can be predicted with low error to deal with cases where the optimal signal has a significant prediction error , we provide a robust , heuristically motivated variant of the algorithm and verify it empirically additionally , we give formal criteria a prediction model must meet to be suitable for measuring predictability in the pfa setting and also provide a suitable default model along with a formal proof that it meets these criteria
this paper studies the coordinated beamforming \( cobf \) design for the multiple input single output interference channel , provided that only channel distribution information is known to the transmitters the problem under consideration is a probabilistically constrained optimization problem which maximizes a predefined system utility subject to constraints on rate outage probability and power budget of each transmitter our recent analysis has shown that the outage constrained cobf problem is intricately difficult , e g , np hard therefore , the focus of this paper is on suboptimal but computationally efficient algorithms specifically , by leveraging on the block successive upper bound minimization \( bsum \) method in optimization , we propose a gauss seidel type algorithm , called distributed bsum algorithm , which can handle differentiable , monotone and concave system utilities by exploiting a weighted minimum mean square error \( wmmse \) reformulation , we further propose a jocobi type algorithm , called distributed wmmse algorithm , which can optimize the weighted sum rate utility in a fully parallel manner to provide a performance benchmark , a relaxed approximation method based on polyblock outer approximation is also proposed simulation results show that the proposed algorithms are significantly superior to the existing successive convex approximation method in both performance and computational efficiency , and can yield promising approximation performance
program specialization is a program transformation methodology which improves program efficiency by exploiting the information about the input data which are available at compile time we show that current techniques for program specialization based on partial evaluation do not perform well on nondeterministic logic programs we then consider a set of transformation rules which extend the ones used for partial evaluation , and we propose a strategy for guiding the application of these extended rules so to derive very efficient specialized programs the efficiency improvements which sometimes are exponential , are due to the reduction of nondeterminism and to the fact that the computations which are performed by the initial programs in different branches of the computation trees , are performed by the specialized programs within single branches in order to reduce nondeterminism we also make use of mode information for guiding the unfolding process to exemplify our technique , we show that we can automatically derive very efficient matching programs and parsers for regular languages the derivations we have performed could not have been done by previously known partial evaluation techniques
the history of phishing traces back in important ways to the mid 1990s when hacking software facilitated the mass targeting of people in password stealing scams on america online \( aol \) the first of these software programs was mine , called aohell , and it was where the word phishing was coined the software provided an automated password and credit card stealing mechanism starting in january 1995 though the practice of tricking users in order to steal passwords or information possibly goes back to the earliest days of computer networking , aohell 's phishing system was the first automated tool made publicly available for this purpose the program influenced the creation of many other automated phishing systems that were made over a number of years these tools were available to amateurs who used them to engage in a countless number of phishing attacks by the later part of the decade , the activity moved from aol to other networks and eventually grew to involve professional criminals on the internet what began as a scheme by rebellious teenagers to steal passwords evolved into one of the top computer security threats affecting people , corporations , and governments
information embedding \( ie \) is the transmission of information within a host signal subject to a distortion constraint there are two types of embedding methods , namely irreversible ie and reversible ie , depending upon whether or not the host , as well as the message , is recovered at the decoder in irreversible ie , only the embedded message is recovered at the decoder , and in reversible ie , both the message and the host are recovered at the decoder this paper considers combinations of irreversible and reversible ie in multiple access channels \( mac \) and physically degraded broadcast channels \( bc \)
in this paper we discuss the use of cooperative game theory for analyzing interference channels we extend our previous work , to games with n players as well as frequency selective channels and joint tdm fdm strategies we show that the nash bargaining solution can be computed using convex optimization techniques we also show that the same results are applicable to interference channels where only statistical knowledge of the channel is available moreover , for the special case of two players 2 times k frequency selective channel \( with k frequency bins \) we provide an o \( k log 2 k \) complexity algorithm for computing the nash bargaining solution under mask constraint and using joint fdm tdm strategies simulation results are also provided
this paper addresses the problem of channel estimation in multi cell interference limited cellular networks we consider systems employing multiple antennas and are interested in both the finite and large scale antenna number regimes \( so called massive mimo \) such systems deal with the multi cell interference by way of per cell beamforming applied at each base station channel estimation in such networks , which is known to be hampered by the pilot contamination effect , constitute a major bottleneck for overall performance we present a novel approach which tackles this problem by enabling a low rate coordination between cells during the channel estimation phase itself the coordination makes use of the additional second order statistical information about the user channels , which are shown to offer a powerful way of discriminating across interfering users with even strongly correlated pilot sequences importantly , we demonstrate analytically that in the large number of antennas regime , the pilot contamination effect is made to vanish completely under certain conditions on the channel covariance gains over the conventional channel estimation framework are confirmed by our simulations for even small antenna array sizes
a path in an edge colored graph g , where adjacent edges may be colored the same , is called a rainbow path if no two edges of g are colored the same for a kappa connected graph g and an integer k with 1 leq k leq kappa , the rainbow k connectivity rc k \( g \) of g is defined as the minimum integer j for which there exists a j edge coloring of g such that every two distinct vertices of g are connected by k internally disjoint rainbow paths let g be a complete \( ell 1 \) partite graph with ell parts of size r and one part of size p where 0 leq p r \( in the case p 0 , g is a complete ell partite graph with each part of size r \) this paper is to investigate the rainbow k connectivity of g we show that for every pair of integers k geq 2 and r geq 1 , there is an integer f \( k , r \) such that if ell geq f \( k , r \) , then rc k \( g \) 2 as a consequence , we improve the upper bound of f \( k \) from \( k 1 \) 2 to ck 3 2 c , where 0 c 1 , c o \( k 3 2 \) , and f \( k \) is the integer such that if n geq f \( k \) then rc k \( k n \) 2
this article introduces the novel concept of spatiotemporal multicast \( stm \) , which is the issue of sending a message to mobile devices that have been residing at a specific area during a certain time span in the past a wide variety of applications can be envisioned for this concept , including crime investigation , disease control , and social applications an important aspect of these applications is the need to protect the privacy of its users in this article , we present an extensive overview of applications and objectives to be fulfilled by an stm service furthermore , we propose a first cluster based spatiotemporal multicast \( cstm \) approach and provide a detailed discussion of its privacy features finally , we evaluate the performance of our scheme in a large scale simulation setup
in the recursive lamination of the disk , one tries to add chords one after another at random a chord is kept and inserted if it does not intersect any of the previously inserted ones curien and le gall ann probab 39 \( 2011 \) 2224 2270 have proved that the set of chords converges to a limit triangulation of the disk encoded by a continuous process mathscr m based on a new approach resembling ideas from the so called contraction method in function spaces , we prove that , when properly rescaled , the planar dual of the discrete lamination converges almost surely in the gromov hausdorff sense to a limit real tree mathscr t , which is encoded by mathscr m this confirms a conjecture of curien and le gall
suppose q is a prime power and f is a univariate polynomial \( over the finite field mathbf f q \) with exactly t monomial terms and degree q 1 to establish a finite field analogue of descartes' rule , bi , cheng , and rojas recently proved an upper bound of 2 \( q 1 \) \( t 2 \) \( t 1 \) on the number of cosets of mathbf f q needed to cover the roots of f in mathbf f q here , we exhibit explicit f with root structure approaching this bound we present f vanishing on q \( t 2 \) t q 1 t 1 distinct cosets of mathbf f q for p prime , we give some computational evidence that the maximal number of roots in mathbf f p for a trinomial a bx e 2 x e 3 , with gcd \( e 2 , e 3 \) 1 , may grow more slowly than p 1 3 furthermore , we present such trinomials , with omega \( frac log log p log log log p \) distinct roots in mathbf f p and , assuming the generalized riemann hypothesis , omega \( frac log p log log p \) distinct roots in mathbf f p
continuous time signals are well known for not being perfectly localized in both time and frequency domains conversely , a signal defined over the vertices of a graph can be perfectly localized in both vertex and frequency domains we derive the conditions ensuring the validity of this property and then , building on this theory , we provide the conditions for perfect reconstruction of a graph signal from its samples next , we provide a finite step algorithm for the reconstruction of a band limited signal from its samples and then we show the effect of sampling a non perfectly band limited signal and show how to select the bandwidth that minimizes the mean square reconstruction error
mining frequent itemsets is at the core of mining association rules , and is by now quite well understood algorithmically however , most algorithms for mining frequent itemsets assume that the main memory is large enough for the data structures used in the mining , and very few efficient algorithms deal with the case when the database is very large or the minimum support is very low mining frequent itemsets from a very large database poses new challenges , as astronomical amounts of raw data is ubiquitously being recorded in commerce , science and government in this paper , we discuss approaches to mining frequent itemsets when data structures are too large to fit in main memory several divide and conquer algorithms are given for mining from disks many novel techniques are introduced experimental results show that the techniques reduce the required disk accesses by orders of magnitude , and enable truly scalable data mining
we consider the problem of exact and inexact matching of weighted undirected graphs , in which a bijective correspondence is sought to minimize a quadratic weight disagreement this computationally challenging problem is often relaxed as a convex quadratic program , in which the space of permutations is replaced by the space of doubly stochastic matrices however , the applicability of such a relaxation is poorly understood we define a broad class of friendly graphs characterized by an easily verifiable spectral property we prove that for friendly graphs , the convex relaxation is guaranteed to find the exact isomorphism or certify its inexistence this result is further extended to approximately isomorphic graphs , for which we develop an explicit bound on the amount of weight disagreement under which the relaxation is guaranteed to find the globally optimal approximate isomorphism we also show that in many cases , the graph matching problem can be further harmlessly relaxed to a convex quadratic program with only n separable linear equality constraints , which is substantially more efficient than the standard relaxation involving 2n equality and n 2 inequality constraints finally , we show that our results are still valid for unfriendly graphs if additional information in the form of seeds or attributes is allowed , with the latter satisfying an easy to verify spectral characteristic
let p be a path graph of n vertices embedded in a metric space we consider the problem of adding a new edge to p such that the diameter of the resulting graph is minimized previously \( in icalp 2015 \) the problem was solved in o \( n log 3 n \) time in this paper , based on new observations and different algorithmic techniques , we present an o \( n log n \) time algorithm
a new dictionary for sparse representation of chirp echo in broadband radar is put forward in this paper different with chirplet decomposition which decomposes echo in time frequency plane , the dictionary transforms the sparsity of target observed by radar in distance range to the sparsity in frequency domain by stretch processing and the sparse representation of echo is realized using strict deduction with mathematics , the sparsity of echo in dictionary is proved and the dictionary is orthogonal in the application property , the construction of dictionary is simple , the parameters that are needed for dictionary can be obtained conveniently and the dictionary is convenient to use furthermore , the object of application can be expanded to the echo of multi component chirps with single freedom degree
in this article , we study the problem of secret key generation in the multiterminal source model , where the terminals have access to correlated gaussian sources we assume that the sources form a markov chain on a tree we give a nested lattice based key generation scheme whose computational complexity is polynomial in the number , n , of independent and identically distributed samples observed by each source we also compute the achievable secret key rate and give a class of examples where our scheme is optimal in the fine quantization limit however , we also give examples that show that our scheme is not always optimal in the limit of fine quantization
a two user discrete memoryless compound multiple access channel with a common message and conferencing decoders is considered the capacity region is characterized in the special cases of physically degraded channels and unidirectional cooperation , and achievable rate regions are provided for the general case the results are then extended to the corresponding gaussian model in the gaussian setup , the provided achievable rates are shown to lie within some constant number of bits from the boundary of the capacity region in several special cases an alternative model , in which the encoders are connected by conferencing links rather than having a common message , is studied as well , and the capacity region for this model is also determined for the cases of physically degraded channels and unidirectional cooperation numerical results are also provided to obtain insights about the potential gains of conferencing at the decoders and encoders
theoretical analyses of the dendritic cell algorithm \( dca \) have yielded several criticisms about its underlying structure and operation as a result , several alterations and fixes have been suggested in the literature to correct for these findings a contribution of this work is to investigate the effects of replacing the classification stage of the dca \( which is known to be flawed \) with a traditional machine learning technique this work goes on to question the merits of those unique properties of the dca that are yet to be thoroughly analysed if none of these properties can be found to have a benefit over traditional approaches , then fixing the dca is arguably less efficient than simply creating a new algorithm this work examines the dynamic filtering property of the dca and questions the utility of this unique feature for the anomaly detection problem it is found that this feature , while advantageous for noisy , time ordered classification , is not as useful as a traditional static filter for processing a synthetic dataset it is concluded that there are still unique features of the dca left to investigate areas that may be of benefit to the artificial immune systems community are suggested
a foundation is investigated for the application of loosely structured data on the web this area is often referred to as linked data , due to the use of uris in data to establish links this work focuses on emerging w3c standards which specify query languages for linked data the approach is to provide an abstract syntax to capture linked data structures and queries , which are then internalised in a process calculus an operational semantics for the calculus specifies how queries , data and processes interact a labelled transition system is shown to be sound with respect to the operational semantics bisimulation over the labelled transition system is used to verify an algebra over queries the derived algebra is a contribution to the application domain for instance , the algebra may be used to rewrite a query to optimise its distribution across a cluster of servers the framework used to provide the operational semantics is powerful enough to model related calculi for the web
a growing community that shares digital 3d designs has created an opportunity to study , encourage and stimulate innovation this remix community allows people not only to prototype at a minimal cost but also to work on projects they are genuinely interested in participants free of the limitations typically imposed by formal organizations develop products driven by their own interest
the design of the channel part of a digital communication system \( e g , error correction , modulation \) is heavily based on the assumption that the data to be transmitted forms a fair bit stream however , simple source encoders such as short huffman codes generate bit streams that poorly match this assumption as a result , the channel input distribution does not match the original design criteria in this work , a simple method called half huffman coding \( halfhc \) is developed halfhc transforms a huffman code into a source code whose output is more similar to a fair bit stream this is achieved by permuting the codewords such that the frequency of 1s at the output is close to 0 5 the permutations are such that the optimality in terms of achieved compression ratio is preserved halfhc is applied in a practical example , and the resulting overall system performs better than when conventional huffman coding is used
we describe the trace representations of two families of binary sequences derived from fermat quotients modulo an odd prime p \( one is the binary threshold sequences , the other is the legendre fermat quotient sequences \) via determining the defining pairs of all binary characteristic sequences of cosets , which coincide with the sets of pre images modulo p 2 of each fixed value of fermat quotients from the defining pairs , we can obtain an earlier result of linear complexity for the binary threshold sequences and a new result of linear complexity for the legendre fermat quotient sequences under the assumption of 2 p 1 not equiv 1 bmod p 2
discrete energy minimization is widely used in computer vision and machine learning for problems such as map inference in graphical models the problem , in general , is notoriously intractable , and finding the global optimal solution is known to be np hard however , is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time \? we show in this paper that the answer is no specifically , we show that general energy minimization , even in the 2 label pairwise case , and planar energy minimization with three or more labels are exp apx complete this finding rules out the existence of any approximation algorithm with a sub exponential approximation ratio in the input size for these two problems , including constant factor approximations moreover , we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes po , apx , and exp apx , corresponding to problems that are solvable , approximable , and inapproximable in polynomial time problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones this paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms
in the big data era , scalability has become a crucial requirement for any useful computational model probabilistic graphical models are very useful for mining and discovering data insights , but they are not scalable enough to be suitable for big data problems bayesian networks particularly demonstrate this limitation when their data is represented using few random variables while each random variable has a massive set of values with hierarchical data data that is arranged in a treelike structure with several levels one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels when modeling this kind of hierarchical data across large data sets , bayesian networks become infeasible for representing the probability distributions for the following reasons i \) each level represents a single random variable with hundreds of thousands of values , ii \) the number of levels is usually small , so there are also few random variables , and iii \) the structure of the network is predefined since the dependency is modeled top down from each parent to each of its child nodes , so the network would contain a single linear path for the random variables from each parent to each child node in this paper we present a scalable probabilistic graphical model to overcome these limitations for massive hierarchical data we believe the proposed model will lead to an easily scalable , more readable , and expressive implementation for problems that require probabilistic based solutions for massive amounts of hierarchical data we successfully applied this model to solve two different challenging probabilistic based problems on massive hierarchical data sets for different domains , namely , bioinformatics and latent semantic discovery over search logs
in directed tree width , j combin theory ser b 82 \( 2001 \) , 138 154 we introduced the notion of tree width of directed graphs and presented a conjecture , formulated during discussions with noga alon and bruce reed , stating that a digraph of huge tree width has a large cylindrical grid minor here we prove the conjecture for planar digraphs , but many steps of the proof work in general this is an unedited and unpolished manuscript from october 2001 since many people asked for copies we are making it available in the hope that it may be useful the conjecture was proved by kawarabayashi and kreutzer in arxiv 1411 5681
we introduce the notion of a network 's conduciveness , a probabilistically interpretable measure of how the network 's structure allows it to be conducive to roaming agents , in certain conditions , from one portion of the network to another we exemplify its use through an application to the two problems in combinatorial optimization that , given an undirected graph , ask that its so called chromatic and independence numbers be found though np hard , when solved on sequences of expanding random graphs there appear marked transitions at which optimal solutions can be obtained substantially more easily than right before them we demonstrate that these phenomena can be understood by resorting to the network that represents the solution space of the problems for each graph and examining its conduciveness between the non optimal solutions and the optimal ones at the said transitions , this network becomes strikingly more conducive in the direction of the optimal solutions than it was just before them , while at the same time becoming less conducive in the opposite direction we believe that , besides becoming useful also in other areas in which network theory has a role to play , network conduciveness may become instrumental in helping clarify further issues related to np hardness that remain poorly understood
we demonstrate that any physical object , as long as its volume is conserved when coupled with suitable operations , provides a sophisticated decision making capability we consider the problem of finding , as accurately and quickly as possible , the most profitable option from a set of options that gives stochastic rewards these decisions are made as dictated by a physical object , which is moved in a manner similar to the fluctuations of a rigid body in a tug of war game our analytical calculations validate statistical reasons why our method exhibits higher efficiency than conventional algorithms
studying nash dynamics is an important approach for analyzing the outcome of games with repeated selfish behavior of self interested agents sink equilibria has been introduced by goemans , mirrokni , and vetta for studying social cost on nash dynamics over pure strategies in games however , they do not address the complexity of sink equilibria in these games recently , fabrikant and papadimitriou initiated the study of the complexity of nash dynamics in two classes of games in order to completely understand the complexity of nash dynamics in a variety of games , we study the following three questions for various games \( i \) given a state in game , can we verify if this state is in a sink equilibrium or not \? \( ii \) given an instance of a game , can we verify if there exists any sink equilibrium other than pure nash equilibria \? and \( iii \) given an instance of a game , can we verify if there exists a pure nash equilibrium \( i e , a sink equilibrium with one state \) \? in this paper , we almost answer all of the above questions for a variety of classes of games with succinct representation , including anonymous games , player specific and weighted congestion games , valid utility games , and two sided market games in particular , for most of these problems , we show that \( i \) it is pspace complete to verify if a given state is in a sink equilibrium , \( ii \) it is np hard to verify if there exists a pure nash equilibrium in the game or not , \( iii \) it is pspace complete to verify if there exists any sink equilibrium other than pure nash equilibria to solve these problems , we illustrate general techniques that could be used to answer similar questions in other classes of games
indexing highly repetitive collections has become a relevant problem with the emergence of large repositories of versioned documents , among other applications these collections may reach huge sizes , but are formed mostly of documents that are near copies of others traditional techniques for indexing these collections fail to properly exploit their regularities in order to reduce space we introduce new techniques for compressing inverted indexes that exploit this near copy regularity they are based on run length , lempel ziv , or grammar compression of the differential inverted lists , instead of the usual practice of gap encoding them we show that , in this highly repetitive setting , our compression methods significantly reduce the space obtained with classical techniques , at the price of moderate slowdowns moreover , our best methods are universal , that is , they do not need to know the versioning structure of the collection , nor that a clear versioning structure even exists we also introduce compressed self indexes in the comparison these are designed for general strings \( not only natural language texts \) and represent the text collection plus the index structure \( not an inverted index \) in integrated form we show that these techniques can compress much further , using a small fraction of the space required by our new inverted indexes yet , they are orders of magnitude slower
agent based modelling and simulation offers a new and exciting way of understanding the world of work in this paper we describe the development of an agent based simulation model , designed to help to understand the relationship between human resource management practices and retail productivity we report on the current development of our simulation model which includes new features concerning the evolution of customers over time to test some of these features we have conducted a series of experiments dealing with customer pool sizes , standard and noise reduction modes , and the spread of the word of mouth our multi disciplinary research team draws upon expertise from work psychologists and computer scientists despite the fact we are working within a relatively novel and complex domain , it is clear that intelligent agents offer potential for fostering sustainable organisational capabilities in the future
graphical models have been widely applied in solving distributed inference problems in sensor networks in this paper , the problem of coordinating a network of sensors to train a unique ensemble estimator under communication constraints is discussed the information structure of graphical models with specific potential functions is employed , and this thus converts the collaborative training task into a problem of local training plus global inference two important classes of algorithms of graphical model inference , message passing algorithm and sampling algorithm , are employed to tackle low dimensional , parametrized and high dimensional , non parametrized problems respectively the efficacy of this approach is demonstrated by concrete examples
the paper studies the routing in the network shared by several users each user seeks to optimize either its own performance or some combination between its own performance and that of other users , by controlling the routing of its given flow demand we parameterize the degree of cooperation which allows to cover the fully non cooperative behavior , the fully cooperative behavior , and even more , the fully altruistic behavior , all these as special cases of the parameter 's choice a large part of the work consists in exploring the impact of the degree of cooperation on the equilibrium our first finding is to identify multiple nash equilibria with cooperative behavior that do not occur in the non cooperative case under the same conditions \( cost , demand and topology \) we then identify braess like paradox \( in which adding capacity or adding a link to a network results in worse performance to all users \) and study the impact of the degree of cooperation on it we identify another type of paradox in cooperation scenario we identify that when we increase the degree of cooperation of a user while other users keep unchanged their degree of cooperation , leads to an improvement in performance of that user we then pursue the exploration and carry it on to the setting of mixed equilibrium \( i e some users are non atomic they have infinitesimally small demand , and other have finite fixed demand \) we finally obtain some theoretical results that show that for low degree of cooperation the equilibrium is unique , confirming the results of our numerical study
traditional relation extraction predicts relations within some fixed and finite target schema machine learning approaches to this task require either manual annotation or , in the case of distant supervision , existing structured sources of the same schema the need for existing datasets can be avoided by using a universal schema the union of all involved schemas \( surface form predicates as in openie , and relations in the schemas of pre existing databases \) this schema has an almost unlimited set of relations \( due to surface forms \) , and supports integration with existing structured data \( through the relation types of existing databases \) to populate a database of such schema we present a family of matrix factorization models that predict affinity between database tuples and relations we show that this achieves substantially higher accuracy than the traditional classification approach more importantly , by operating simultaneously on relations observed in text and in pre existing structured dbs such as freebase , we are able to reason about unstructured and structured data in mutually supporting ways by doing so our approach outperforms state of the art distant supervision systems
individuals change their behavior during an epidemic in response to whether they and or those they interact with are healthy or sick healthy individuals are concerned about contracting a disease from their sick contacts and may utilize protective measures sick individuals may be concerned with spreading the disease to their healthy contacts and adopt preemptive measures yet , in practice both protective and preemptive changes in behavior come with costs this paper proposes a stochastic network disease game model that captures the self interests of individuals during the spread of a susceptible infected susceptible \( sis \) disease where individuals react to current risk of disease spread , and their reactions together with the current state of the disease stochastically determine the next stage of the disease we show that there is a critical level of concern , i e , empathy , by the sick individuals above which disease is eradicated fast furthermore , we find that if the network and disease parameters are above the epidemic threshold , the risk averse behavior by the healthy individuals cannot eradicate the disease without the preemptive measures of the sick individuals this imbalance in the role played by the response of the infected versus the susceptible individuals in disease eradication affords critical policy insights
in this letter , we analyze the ergodic capacity of bidirectional amplify and forward relay selection \( rs \) with imperfect channel state information \( csi \) , i e , outdated csi and imperfect channel estimation practically , the optimal rs scheme in maximizing the ergodic capacity cannot be achieved , due to the imperfect csi therefore , two suboptimal rs schemes are discussed and analyzed , in which the first rs scheme is based on the imperfect channel coefficients , and the second rs scheme is based on the predicted channel coefficients the lower bound of the ergodic capacity with imperfect csi is derived in a closed form , which matches tightly with the simulation results the results reveal that once csi is imperfect , the ergodic capacity of bidirectional rs degrades greatly , whereas the rs scheme based on the predicted channel has better performance , and it approaches infinitely to the optimal performance , when the prediction length is sufficiently large
we consider the problem of performing a random walk in a distributed network given bandwidth constraints , the goal of the problem is to minimize the number of rounds required to obtain a random walk sample das sarma et al podc'10 show that a random walk of length ell on a network of diameter d can be performed in tilde o \( sqrt ell d d \) time a major question left open is whether there exists a faster algorithm , especially whether the multiplication of sqrt ell and sqrt d is necessary in this paper , we show a tight unconditional lower bound on the time complexity of distributed random walk computation specifically , we show that for any n , d , and d leq ell leq \( n \( d 3 log n \) \) 1 4 , performing a random walk of length theta \( ell \) on an n node network of diameter d requires omega \( sqrt ell d d \) time this bound is em unconditional , i e , it holds for any \( possibly randomized \) algorithm to the best of our knowledge , this is the first lower bound that the diameter plays a role of multiplicative factor our bound shows that the algorithm of das sarma et al is time optimal our proof technique introduces a new connection between em bounded round communication complexity and distributed algorithm lower bounds with d as a trade off parameter , strengthening the previous study by das sarma et al stoc'11 in particular , we make use of the bounded round communication complexity of the pointer chasing problem our technique can be of independent interest and may be useful in showing non trivial lower bounds on the complexity of other fundamental distributed computing problems
distance hereditary graphs form an important class of graphs , from the theoretical point of view , due to the fact that they are the totally decomposable graphs for the split decomposition the previous best enumerative result for these graphs is from nakano et al \( j comp sci tech , 2007 \) , who have proven that the number of distance hereditary graphs on n vertices is bounded by 2 lceil 3 59n rceil in this paper , using classical tools of enumerative combinatorics , we improve on this result by providing an exact enumeration of distance hereditary graphs , which allows to show that the number of distance hereditary graphs on n vertices is tightly bounded by \( 7 24975 ldots \) n opening the perspective such graphs could be encoded on 3n bits we also provide the exact enumeration and asymptotics of an important subclass , the 3 leaf power graphs our work illustrates the power of revisiting graph decomposition results through the framework of analytic combinatorics
an active line of research has considered games played on networks in which payoffs depend on both a player 's individual decision and also the decisions of her neighbors such games have been used to model issues including the formation of opinions and the adoption of technology a basic question that has remained largely open in this area is to consider games where the strategies available to the players come from a fixed , discrete set , and where players may have different intrinsic preferences among the possible strategies it is natural to model the tension among these different preferences by positing a distance function on the strategy set that determines a notion of similarity among strategies a player 's payoff is determined by the distance from her chosen strategy to her preferred strategy and to the strategies chosen by her network neighbors even when there are only two strategies available , this framework already leads to natural open questions about a version of the classical battle of the sexes problem played on a graph we develop a set of techniques for analyzing this class of games , which we refer to as discrete preference games we parametrize the games by the relative extent to which a player takes into account the effect of her preferred strategy and the effect of her neighbors' strategies , allowing us to interpolate between network coordination games and unilateral decision making when these two effects are balanced , we show that the price of stability is equal to 1 for any discrete preference game in which the distance function on the strategies is a tree metric as a special case , this includes the battle of the sexes on a graph we also show that trees form the maximal family of metrics for which the price of stability is 1 , and produce a collection of metrics on which the price of stability converges to a tight bound of 2
the purpose of this paper is to give an overview of the open hardware microrobotic project swarmrobot org and the platform jasmine for building large scale artificial swarms the project targets an open development of cost effective hardware and software for a quick implementation of swarm behavior with real robots detailed instructions for making the robot , open source simulator , software libraries and multiple publications about performed experiments are ready for download and intend to facilitate exploration of collective and emergent phenomena , guided self organization and swarm robotics in experimental way
in this paper , a new class of circulant matrices built from deterministic sequences is proposed for convolution based compressed sensing \( cs \) in contrast to random convolution , the coefficients of the underlying filter are given by the discrete fourier transform of a deterministic sequence with good autocorrelation both uniform recovery and non uniform recovery of sparse signals are investigated , based on the coherence parameter of the proposed sensing matrices many examples of the sequences are investigated , particularly the frank zadoff chu \( fzc \) sequence , the textit m sequence and the golay sequence a salient feature of the proposed sensing matrices is that they can not only handle sparse signals in the time domain , but also those in the frequency and or or discrete cosine transform \( dct \) domain
we derive rigorous bounds for well defined community structure in complex networks for a stochastic block model \( sbm \) benchmark in particular , we analyze the effect of inter community noise \( inter community edges \) on any community detection algorithm 's ability to correctly group nodes assigned to a planted partition , a problem which has been proven to be np complete in a standard rendition our result does not rely on the use of any one particular algorithm nor on the analysis of the limitations of inference rather , we turn the problem on its head and work backwards to examine when , in the first place , well defined structure may exist in sbms the method that we introduce here could potentially be applied to other computational problems the objective of community detection algorithms is to partition a given network into optimally disjoint subgraphs \( or communities \) similar to k sat and other combinatorial optimization problems , community detection exhibits different phases networks that lie in the unsolvable phase lack well defined structure and thus have no partition that is meaningful solvable systems splinter into two disparate phases those in the hard phase and those in the easy phase as befits its name , within the easy phase , a partition is easy to achieve by known algorithms when a network lies in the hard phase , it still has an underlying structure yet finding a meaningful partition which can be checked in polynomial time requires an exhaustive computational effort that rapidly increases with the size of the graph when taken together , \( i \) the rigorous results that we report here on when graphs have an underlying structure and \( ii \) recent results concerning the limits of rather general algorithms , suggest bounds on the hard phase
this paper presents a two dimensional modal logic for reasoning about the changing patterns of knowledge and social relationships in networks organised on the basis of a symmetric 'friendship' relation , providing a precise language for exploring 'logic in the community' 11 agents are placed in the model , allowing us to express such indexical facts as 'i am your friend' and 'you , my friends , are in danger' the technical framework for this work is general dynamic dynamic logic \( gddl \) 4 , which provides a general method for extending modal logics with dynamic operators for reasoning about a wide range of model transformations , starting with those definable in propositional dynamic logic \( pdl \) and extended to allow for the more subtle operators involved in , for example , private communication , as represented in dynamic epistemic logic \( del \) and related systems we provide a hands on introduction to gddl , introducing elements of the formalism as we go , but leave the reader to consult 4 for technical details instead , the purpose of this paper is to investigate a number of conceptual issues that arise when considering communication between agents in such networks , both from one agent to another , and broadcasts to socially defined groups of agents , such as the group of my friends
we present a methodology for the global sampled data stabilization of systems with a compact absorbing set and input measurement delays the methodology is based on the inter sample predictor , observer , predictor , delay free controller \( isp o p dfc \) scheme and the stabilization is robust to perturbations of the sampling schedule the obtained results are novel even for the delay free case
the recent availability of data describing social networks is changing our understanding of the microscopic structure of a social tie a social tie indeed is an aggregated outcome of many social interactions such as face to face conversations or phone calls analysis of data on face to face interactions shows that such events , as many other human activities , are bursty , with very heterogeneous durations in this paper we present a model for social interactions at short time scales , aimed at describing contexts such as conference venues in which individuals interact in small groups we present a detailed anayltical and numerical study of the model 's dynamical properties , and show that it reproduces important features of empirical data the model allows for many generalizations toward an increasingly realistic description of social interactions in particular in this paper we investigate the case where the agents have intrinsic heterogeneities in their social behavior , or where dynamic variations of the local number of individuals are included finally we propose this model as a very flexible framework to investigate how dynamical processes unfold in social networks
nowadays one of the main obstacles the research comes up against is the difficulty in accessing the required computational resources grid is able to offer the user a wide set of resources , even if they are often too hard to exploit for non expert end user use simplification has today become a common practice in the access and utilization of cloud , grid , and data center resources with the launch of l grid gateway , we introduced a new way to deal with grid portals l grid is an extremely light portal developed in order to access the egi grid infrastructure via web , allowing users to submit their jobs from whatever web browser in a few minutes , without any knowledge about the underlying grid infrastructure
in this paper we prove threshold saturation for spatially coupled turbo codes \( sc tcs \) and braided convolutional codes \( bccs \) over the binary erasure channel we introduce a compact graph representation for the ensembles of sc tc and bcc codes which simplifies their description and the analysis of the message passing decoding we demonstrate that by few assumptions in the ensembles of these codes , it is possible to rewrite their vector recursions in a form which places these ensembles under the category of scalar admissible systems this allows us to define potential functions and prove threshold saturation using the proof technique introduced by yedla et al
in this note , we present results for the colouring problem on small world graphs created by rewiring square , triangular , and two kinds of cubic \( with coordination numbers 5 and 6 \) lattices as the rewiring parameter p tends to 1 , we find the expected crossover to the behaviour of random graphs with corresponding connectivity however , for the cubic lattices there is a region near p 0 for which the graphs are colourable this could in principle be used as an additional heuristic for solving real world colouring or scheduling problems small worlds with connectivity 5 and p 0 1 provide an interesting ensemble of graphs whose colourability is hard to determine for square lattices , we get good data collapse plotting the fraction of colourable graphs against the rescaled parameter parameter p n nu with nu 1 35 no such collapse can be obtained for the data from lattices with coordination number 5 or 6
in this paper , we investigate the cumulative distribution function \( cdf \) of the aggregate interference in carrier sensing multiple access collision avoidance \( csma ca \) networks measured at an arbitrary time and position we assume that nodes are deployed in an infinite two dimensional plane by poisson point process \( ppp \) and the channel model follows the singular path loss function and rayleigh fading to find the effective active node density we analyze the distributed coordinate function \( dcf \) dynamics in a common sensing area and obtain the steady state power distribution within a spatial disk of radius r 2 , where r is the effective carrier sensing distance the results of massive simulation using network simulator 2 \( ns 2 \) show a high correlation with the derived cdf
the squashed entanglement of a quantum channel is an additive function of quantum channels , which finds application as an upper bound on the rate at which secret key and entanglement can be generated when using a quantum channel a large number of times in addition to unlimited classical communication this quantity has led to an upper bound of log \( \( 1 eta \) \( 1 eta \) \) on the capacity of an optical communication channel for such a task , where eta is the average fraction of photons that make it from the input to the output of the channel the purpose of the present paper is to extend these results beyond the single sender single receiver setting to the more general case of a single sender and multiple receivers \( a quantum broadcast channel \) we employ multipartite generalizations of the squashed entanglement to constrain the rates at which secret key and entanglement can be generated between any subset of the users of such a channel , along the way developing several new properties of these measures we apply our results to the case of an optical broadcast channel with one sender and two receivers
integer forcing \( if \) is a new framework , based on compute and forward , for decoding multiple integer linear combinations from the output of a gaussian multiple input multiple output channel this work applies the if approach to arrive at a new low complexity scheme , if source coding , for distributed lossy compression of correlated gaussian sources under a minimum mean squared error distortion measure all encoders use the same nested lattice codebook each encoder quantizes its observation using the fine lattice as a quantizer and reduces the result modulo the coarse lattice , which plays the role of binning rather than directly recovering the individual quantized signals , the decoder first recovers a full rank set of judiciously chosen integer linear combinations of the quantized signals , and then inverts it in general , the linear combinations have smaller average powers than the original signals this allows to increase the density of the coarse lattice , which in turn translates to smaller compression rates we also propose and analyze a one shot version of if source coding , that is simple enough to potentially lead to a new design principle for analog to digital converters that can exploit spatial correlations between the sampled signals
the problem of securing a network coding communication system against a wiretapper adversary is considered the network implements linear network coding to deliver n packets from source to each receiver , and the wiretapper can eavesdrop on mu arbitrarily chosen links a coding scheme is proposed that can achieve the maximum possible rate of k n mu packets that are information theoretically secure from the adversary a distinctive feature of our scheme is that it is universal it can be applied on top of any communication network without requiring knowledge of or any modifications on the underlying network code in fact , even a randomized network code can be used our approach is based on rouayheb soljanin 's formulation of a wiretap network as a generalization of the ozarow wyner wiretap channel of type ii essentially , the linear mds code in ozarow wyner 's coset coding scheme is replaced by a maximum rank distance code over an extension of the field in which linear network coding operations are performed
in this paper , the concept of the it broken diagonal pair in the chess like square board is used to define some well structured block designs whose incidence matrices can be considered as the parity check matrices of some high rate cycle codes with girth 12 the structure of the proposed parity check matrices significantly reduces the complexity of encoding and decoding interestingly , the constructed regular cycle codes with row weights t , 3 leq t leq 20 , t neq 7 , 15 , 16 , have the best lengths among the known regular girth 12 cycle codes in addition , the proposed cycle codes can be easily extended to some high rate column weight 3 ldpc codes with girth 6 simulation results show that the constructed codes achieve excellent performances , specially the constructed column weight 3 ldpc codes outperform ldpc codes based on steiner triple systems \( sts \)
this paper presents 28 ghz and 73 ghz empirically derived large scale and small scale channel model parameters that characterize average temporal and angular properties of multipaths omnidirectional azimuth scans at both the transmitter and receiver used high gain directional antennas , from which the mean global azimuth and zenith spreads of arrival were found to be 33 6 degrees and 5 9 degrees at 28 ghz , and 33 4 degrees and 3 6 degrees at 73 ghz , respectively , in non line of sight \( nlos \) small scale measurements at 28 ghz reveal a mean cross polar ratio for individual multipath components of 29 7 db and 16 7 db in line of sight and nlos , respectively small scale parameters extracted using the kpowermeans algorithm yielded on average 5 3 and 4 6 clusters at 28 ghz and 73 ghz , respectively , in nlos an alternative physically based binning procedure uses temporal clusters and spatial lobes that faithfully reproduces first and second order statistics of measured millimeter wave channels
we consider power allocations in downlink cellular wireless systems where the basestations are equipped with multiple transmit antennas and the mobile users are equipped with single receive antennas such systems can be modeled as multiuser miso systems we assume that the multi antenna transmitters employ some fixed beamformers to transmit data , and the objective is to optimize the power allocation for different users to satisfy certain qos constraints , with imperfect transmitter side channel state information \( csi \) specifically , for miso interference channels , we consider the transmit power minimization problem and the max min sinr problem for miso broadcast channels , we consider the mse constrained transmit power minimization problem all these problems are formulated as probability constrained optimization problems we make use of the bernstein approximation to conservatively transform the probabilistic constraints into deterministic ones , and consequently convert the original stochastic optimization problems into convex optimization problems however , the transformed problems cannot be straightforwardly solved using standard solver , since one of the constraints is itself an optimization problem we employ the long step logarithmic barrier cutting plane \( llbcp \) algorithm to overcome difficulty extensive simulation results are provided to demonstrate the effectiveness of the proposed method , and the performance advantage over some existing methods
let s be a set of n points in the unit square 0 , 1 2 , one of which is the origin we construct n pairwise interior disjoint axis aligned empty rectangles such that the lower left corner of each rectangle is a point in s , and the rectangles jointly cover at least a positive constant area \( about 0 09 \) this is a first step towards the solution of a longstanding conjecture that the rectangles in such a packing can jointly cover an area of at least 1 2
this paper proves the existence of fundamental relations between information theory and estimation theory for network coded flows when the network is represented by a directed graph g \( v , e \) and under the assumption of uncorrelated noise over information flows between the directed links connecting transmitters , switches \( relays \) , and receivers we unveil that there yet exist closed form relations for the gradient of the mutual information with respect to different components of the system matrix m on the one hand , this result casts further insights into effects of the network topology , topological changes when nodes are mobile , and the impact of errors and delays in certain links into the network capacity which can be further studied in scenarios where one source multi sinks multicasts and multi source multicast where the inevertability and the rank of matrix m plays a significant role in the decoding process and therefore , on the network capacity on the other hand , it opens further research questions of finding precoding solutions adapted to the network level
the order of letters is not always relevant in a communication task this paper discusses the implications of order irrelevance on source coding , presenting results in several major branches of source coding theory lossless coding , universal lossless coding , rate distortion , high rate quantization , and universal lossy coding the main conclusions demonstrate that there is a significant rate savings when order is irrelevant in particular , lossless coding of n letters from a finite alphabet requires theta \( log n \) bits and universal lossless coding requires n o \( n \) bits for many countable alphabet sources however , there are no universal schemes that can drive a strong redundancy measure to zero results for lossy coding include distribution free expressions for the rate savings from order irrelevance in various high rate quantization schemes rate distortion bounds are given , and it is shown that the analogue of the shannon lower bound is loose at all finite rates
this paper considers the optimization of multi edge type low density parity check \( metldpc \) codes to maximize the decoding threshold we propose an algorithm to jointly optimize the node degree distribution and the multi edge structure of met ldpc codes for given values of the maximum number of edge types and maximum node degrees this joint optimization is particularly important for met ldpc codes as it is not clear a priori which structures will be good using several examples , we demonstrate that the met ldpc codes designed by the proposed joint optimization algorithm exhibit improved decoding thresholds compared to previously reported met ldpc codes
in this paper we propose a heuristic technique for distributing points on the surface of a unit n dimensional euclidean sphere , generated as the orbit of a finite cyclic subgroup of orthogonal matrices , the so called cyclic group codes massive numerical experiments were done and many new cyclic group codes have been obtained in several dimensions at various rate the obtained results assure that the heuristic approach have performance comparable to a brute force search technique with the advantage of having low complexity , allowing for designing codes with a large number of points in higher dimensions
allowing the input auxiliary random variables to be correlated and using the binning scheme , the han kobayashi \( hk \) rate region for general interference channel is partially improved the obtained partially new achievable rate region \( i \) is compared to the hk region and its simplified description , i e , chong motani garg \( cmg \) region , in a detailed and favorable manner , by considering different versions of the regions , and \( ii \) has an interesting and easy interpretation as expected , any rate in our region has generally two additional terms in comparison with the hk region \( one due to the input correlation and the other as a result of the binning scheme \) keywords interference channel , input correlation , binning scheme
a random construction of bipolar sensing matrices based on binary linear codes is introduced and its rip \( restricted isometry property \) is analyzed based on an argument on the ensemble average of the weight distribution of binary linear codes
information security is one of the most important aspects of technology , we cannot protect the best interests of our organizations' assets \( be that personnel , data , or other resources \) , without ensuring that these assetsare protected to the best of their ability within the defense department , this is vital to the security of not just those assets but also the national security of the united states compromise insecurity could lead severe consequences however , technology changes so rapidly that change has to be made to reflect these changes with security in mind this article outlines a growing technological change \( virtualization and cloud computing \) , and how to properly address it security concerns within an operating environment by leveraging a series of encrypted physical and virtual systems , andnetwork isolation measures , this paper delivered a secured high performance computing environment that efficiently utilized computing resources , reduced overall computer processing costs , and ensures confidentiality , integrity , and availability of systems within the operating environment
spectrum refarming \( sr \) refers to a radio resource management technique which allows different generations of cellular networks to operate in the same radio spectrum in this paper , an underlay sr model is proposed , in which an orthogonal frequency division multiple access \( ofdma \) system refarms the spectrum of a code division multiple access \( cdma \) system through intelligently exploiting the interference margin provided by the cdma system we investigate the mutual effect of the two systems by evaluating the asymptotic signal to interference plus noise ratio \( sinr \) of the users , based on which the interference margin tolerable by the cdma system is determined by using the interference margin together with the transmit power constraints , the uplink resource allocation problem of ofdma system is formulated and solved through dual decomposition method simulation results have verified our theoretical analysis , and validated the effectiveness of the proposed resource allocation algorithm and its capability to protect the legacy cdma users the proposed sr system requires the least information flow from the cdma system to the ofdma system , and importantly , no upgrading of legacy cdma system is needed thus it can be deployed by telecom operators to maximize the spectral efficiency of their cellular networks
buffer aided relaying has recently attracted a lot of attention due to the improvement in the system throughput however , a side effect usually deemed is that buffering at relay nodes results in the increase in packet delays in this paper , we study the effect of buffering relays on the end to end delay of users' data , from the time they arrive at source until delivery to the destination we use simple discussions to provide an insight on the overall waiting time of the packets in the system by studying the bernoulli distributed channel conditions , and using intuitive generalizations , we conclude that the use of buffers at relays improves not only throughput , but ironically the end to end delay as well computer simulations in the settings of practical systems confirm the above results
this paper presents first experimental results for an ip telephony based steganographic method called lack \( lost audio packets steganography \) this method utilizes the fact that in typical multimedia communication protocols like rtp \( real time transport protocol \) , excessively delayed packets are not used for the reconstruction of transmitted data at the receiver , i e these packets are considered useless and discarded the results presented in this paper were obtained basing on a functional lack prototype and show the method 's impact on the quality of voice transmission achievable steganographic bandwidth for the different ip telephony codecs is also calculated
a pair of independent and crossing edges in a drawing of a graph is planarly connected if there is a crossing free edge that connects endpoints of the crossed edges a graph is a planarly connected crossing \( pcc \) graph , if it admits a drawing in which every pair of independent and crossing edges is planarly connected we prove that a pcc graph with n vertices has o \( n \) edges
a graph h is t apex if h x is planar for some subset x of v \( h \) of size t for any integer t 0 and a fixed t apex graph h , we give a polynomial time algorithm to decide whether a \( t 3 \) connected h minor free graph is colorable from a given assignment of lists of size t 4 the connectivity requirement is the best possible in the sense that for every t 1 , there exists a t apex graph h such that testing \( t 4 \) colorability of \( t 2 \) connected h minor free graphs is np complete similarly , the size of the lists cannot be decreased \( unless p np \) , since for every t 1 , testing \( t 3 \) list colorability of \( t 3 \) connected k t 4 minor free graphs is np complete
in this manuscript we review and discuss some theoretical aspects of amari 's natural gradient method , provide a unifying picture of the many different versions of it which have appeared over the years , and offer some new insights and perspectives regarding the method and its relationship to other optimization methods among our various contributions is the identification of a general condition under which the fisher information matrix and schraudolph 's generalized gauss newton matrix are equivalent this equivalence implies that optimization methods which use the latter matrix , such as the hessian free optimization approach of martens , are actually natural gradient methods in disguise it also lets us view natural gradient methods as approximate newton methods , justifying the application of various update damping techniques to them , which are designed to compensate for break downs in local quadratic approximations additionally , we analyze the parameterization invariance possessed by the natural gradient method in the idealized setting of infinitesimally small update steps , and consider the extent to which it holds for practical versions of the method which take large discrete steps we go on to show that parameterization invariance is not possessed by the classical newton raphson method \( even in the idealized setting \) , and then give a general characterization of gradient based methods which do possess it
we propose a quantized decoding algorithm for low density parity check codes where the variable node update rule of the standard min sum algorithm is replaced with a look up table \( lut \) that is designed using an information theoretic criterion we show that even with message resolutions as low as 3 bits , the proposed algorithm can achieve better error rates than a floating point min sum decoder moreover , we study in detail the effect of different decoder design parameters , like the design snr and the lut tree structure on the performance of our decoder , and we propose some complexity reduction techniques , such as lut re use and message alphabet downsizing
smart grid is an emerging technology which is able to control the power load via price signaling the communication between the power supplier and power customers is a key issue in smart grid performance degradation like delay or outage may cause significant impact on the stability of the pricing based control and thus the reward of smart grid therefore , a qos mechanism is proposed for the communication system in smart grid , which incorporates the derivation of qos requirement and applies qos routing in the communication network for deriving the qos requirement , the dynamics of power load and the load price mapping are studied the corresponding impacts of different qos metrics like delay are analyzed then , the qos is derived via an optimization problem that maximizes the total revenue based on the derived qos requirement , a simple greedy qos routing algorithm is proposed for the requirement of high speed routing in smart grid it is also proven that the proposed greedy algorithm is a k approximation numerical simulation shows that the proposed mechanism and algorithm can effectively derive and secure the communication qos in smart grid
current generation flash devices experience significant read channel degradation from damage to the oxide layer during program and erase operations information about the read channel degradation drives advanced signal processing methods in flash to mitigate its effect in this context , channel estimation must be ongoing since channel degradation evolves over time and as a function of the number of program erase \( p e \) cycles this paper proposes a framework for ongoing model based channel estimation using limited channel measurements \( reads \) this paper uses a channel model characterizing degradation resulting from retention time and the amount of charge programmed and erased for channel histogram measurements , bin selection to achieve approximately equal probability bins yields a good approximation to the original distribution using only ten bins \( i e nine reads \) with the channel model and binning strategy in place , this paper explores candidate numerical least squares algorithms and ultimately demonstrates the effectiveness of the levenberg marquardt algorithm which provides both speed and accuracy
in this paper , we present a novel approach that uses deep learning techniques for colorizing grayscale images by utilizing a pre trained convolutional neural network , which is originally designed for image classification , we are able to separate content and style of different images and recombine them into a single image we then propose a method that can add colors to a grayscale image by combining its content with style of a color image having semantic similarity with the grayscale one as an application , to our knowledge the first of its kind , we use the proposed method to colorize images of ukiyo e a genre of japanese painting \? and obtain interesting results , showing the potential of this method in the growing field of computer assisted art
security policies are naturally dynamic reflecting this , there has been a growing interest in studying information flow properties which change during program execution , including concepts such as declassification , revocation , and role change a static verification of a dynamic information flow policy , from a semantic perspective , should only need to concern itself with two things 1 \) the dependencies between data in a program , and 2 \) whether those dependencies are consistent with the intended flow policies as they change over time in this paper we provide a formal ground for this intuition we present a straightforward extension to the principal flow sensitive type system introduced by hunt and sands \( popl '06 , esop '11 \) to infer both end to end dependencies and dependencies at intermediate points in a program this allows typings to be applied to verification of both static and dynamic policies our extension preserves the principal type system 's distinguishing feature , that type inference is independent of the policy to be enforced a single , generic dependency analysis \( typing \) can be used to verify many different dynamic policies of a given program , thus achieving a clean separation between \( 1 \) and \( 2 \) we also make contributions to the foundations of dynamic information flow arguably , the most compelling semantic definitions for dynamic security conditions in the literature are phrased in the so called knowledge based style we contribute a new definition of knowledge based termination insensitive security for dynamic policies we show that the new definition avoids anomalies of previous definitions and enjoys a simple and useful characterisation as a two run style property
a graph g has maximal local edge connectivity k if the maximum number of edge disjoint paths between every pair of distinct vertices x and y is at most k we prove brooks type theorems for k connected graphs with maximal local edge connectivity k , and for any graph with maximal local edge connectivity 3 we also consider several related graph classes defined by constraints on connectivity in particular , we show that there is a polynomial time algorithm that , given a 3 connected graph g with maximal local connectivity 3 , outputs an optimal colouring for g on the other hand , we prove , for k ge 3 , that k colourability is np complete when restricted to minimally k connected graphs , and 3 colourability is np complete when restricted to \( k 1 \) connected graphs with maximal local connectivity k finally , we consider a parameterization of k colourability based on the number of vertices of degree at least k 1 , and prove that , even when k is part of the input , the corresponding parameterized problem is fpt
the rapid development of autonomous vehicles spurred a careful investigation of the potential benefits of all autonomous transportation networks most studies conclude that autonomous systems can enable drastic improvements in performance a widely studied concept is all autonomous , collision free intersections , where vehicles arriving in a traffic intersection with no traffic light adjust their speeds to cross safely through the intersection as quickly as possible in this paper , we propose a coordination control algorithm for this problem , assuming stochastic models for the arrival times of the vehicles the proposed algorithm provides provable guarantees on safety and performance more precisely , it is shown that no collisions occur surely , and moreover a rigorous upper bound is provided for the expected wait time the algorithm is also demonstrated in simulations the proposed algorithms are inspired by polling systems in fact , the problem studied in this paper leads to a new polling system where customers are subject to differential constraints , which may be interesting in its own right
we analyze the problem of regression when both input covariates and output responses are functions from a nonparametric function class function to function regression \( ffr \) covers a large range of interesting applications including time series prediction problems , and also more general tasks like studying a mapping between two separate types of distributions however , previous nonparametric estimators for ffr type problems scale badly computationally with the number of input output pairs in a data set given the complexity of a mapping between general functions it may be necessary to consider large data sets in order to achieve a low estimation risk to address this issue , we develop a novel scalable nonparametric estimator , the triple basis estimator \( 3be \) , which is capable of operating over datasets with many instances to the best of our knowledge , the 3be is the first nonparametric ffr estimator that can scale to massive datasets we analyze the 3be 's risk and derive an upperbound rate furthermore , we show an improvement of several orders of magnitude in terms of prediction speed and a reduction in error over previous estimators in various real world data sets
loopy and generalized belief propagation are popular algorithms for approximate inference in markov random fields and bayesian networks fixed points of these algorithms correspond to extrema of the bethe and kikuchi free energy however , belief propagation does not always converge , which explains the need for approaches that explicitly minimize the kikuchi bethe free energy , such as cccp and ups here we describe a class of algorithms that solves this typically nonconvex constrained minimization of the kikuchi free energy through a sequence of convex constrained minimizations of upper bounds on the kikuchi free energy intuitively one would expect tighter bounds to lead to faster algorithms , which is indeed convincingly demonstrated in our simulations several ideas are applied to obtain tight convex bounds that yield dramatic speed ups over cccp
large unweighted directed graphs are commonly used to capture relations between entities a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices despite the significance of this problem , statistical characterization of the proposed metrics has been limited we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time \( ltht \) the metric serves as a natural , provably well behaved alternative to the expected hitting time we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph we show that the ltht is consistent with respect to the underlying metric of a geometric graph , preserves clustering tendency , and remains robust against random addition of non geometric edges tests on simulated and real world data show that the ltht matches theoretical predictions and outperforms alternatives
in this paper , we show that all fat hoffman graphs with smallest eigenvalue at least 1 tau , where tau is the golden ratio , can be described by a finite set of fat \( 1 tau \) irreducible hoffman graphs in the terminology of woo and neumaier , we mean that every fat hoffman graph with smallest eigenvalue at least 1 tau is an h line graph , where h is the set of isomorphism classes of maximal fat \( 1 tau \) irreducible hoffman graphs it turns out that there are 37 fat \( 1 tau \) irreducible hoffman graphs , up to isomorphism
joseph miller 16 and independently andre nies , frank stephan and sebastiaan terwijn 18 gave a complexity characterization of 2 random sequences in terms of plain kolmogorov complexity c they are sequences that have infinitely many initial segments with o \( 1 \) maximal plain complexity \( among the strings of the same length \) later miller 17 showed that prefix complexity k can also be used in a similar way a sequence is 2 random if and only if it has infinitely many initial segments with o \( 1 \) maximal prefix complexity \( which is n k \( n \) for strings of length n \) the known proofs of these results are quite involved in this paper we provide simple direct proofs for both of them in 16 miller also gave a quantitative version of the first result the 0' randomness deficiency of a sequence omega equals lim inf n c \( omega 1 omega n \) o \( 1 \) \( our simplified proof can also be used to prove this \) we show \( and this seems to be a new result \) that a similar quantitative result is also true for prefix complexity 0' randomness deficiency equals lim inf n k \( n \) k \( omega 1 omega n \) o \( 1 \)
csma \( carrier sense multiple access \) , which resolves contentions over wireless networks in a fully distributed fashion , has recently gained a lot of attentions since it has been proved that appropriate control of csma parameters guarantees optimality in terms of stability \( i e , scheduling \) and system wide utility \( i e , scheduling and congestion control \) most csma based algorithms rely on the popular mcmc \( markov chain monte carlo \) technique , which enables one to find optimal csma parameters through iterative loops of `simulation and update' however , such a simulation based approach often becomes a major cause of exponentially slow convergence , being poorly adaptive to flow topology changes in this paper , we develop distributed iterative algorithms which produce approximate solutions with convergence in polynomial time for both stability and utility maximization problems in particular , for the stability problem , the proposed distributed algorithm requires , somewhat surprisingly , only one iteration among links our approach is motivated by the bethe approximation \( introduced by yedidia , freeman and weiss in 2005 \) allowing us to express approximate solutions via a certain non linear system with polynomial size our polynomial convergence guarantee comes from directly solving the non linear system in a distributed manner , rather than multiple simulation and update loops in existing algorithms we provide numerical results to show that the algorithm produces highly accurate solutions and converges much faster than the prior ones
for the k user , single input single output \( siso \) , frequency selective interference channel , a new low complexity transmit beamforming design that improves the achievable sum rate is presented jointly employing the interference alignment \( ia \) scheme presented by cadambe and jafar in 1 and linear minimum mean square error \( mmse \) decoding at the transmitters and receivers , respectively , the new ia precoding design improves the average sum rate while preserving the achievable degrees of freedom of the cadambe and jafar scheme , k 2
the aim of this paper is to study the fusion at feature extraction level for face and fingerprint biometrics the proposed approach is based on the fusion of the two traits by extracting independent feature pointsets from the two modalities , and making the two pointsets compatible for concatenation moreover , to handle the problem of curse of dimensionality , the feature pointsets are properly reduced in dimension different feature reduction techniques are implemented , prior and after the feature pointsets fusion , and the results are duly recorded the fused feature pointset for the database and the query face and fingerprint images are matched using techniques based on either the point pattern matching , or the delaunay triangulation comparative experiments are conducted on chimeric and real databases , to assess the actual advantage of the fusion performed at the feature extraction level , in comparison to the matching score level
a novel inter frame coding approach to the problem of varying channel state conditions in broadcast wireless communication is developed in this paper this problem causes the appropriate code rate to vary across different transmitted frames and different receivers as well the main aspect of the proposed approach is that it incorporates an iterative rate matching process into the decoding of the received set of frames , such that throughout inter frame decoding , the code rate of each frame is progressively lowered to or below the appropriate value , prior to applying or re applying conventional physical layer channel decoding on it this iterative rate matching process is asymptotically analyzed in this paper it is shown to be optimal , in the sense defined in the paper consequently , the data rates achievable by the proposed scheme are derived overall , it is concluded that , compared to the existing solutions , inter frame coding presents a better complexity versus data rate tradeoff in terms of complexity , the overhead of inter frame decoding includes operations that are similar in type and scheduling to those employed in the relatively simple iterative erasure decoding in terms of data rates , compared to the state of the art two stage scheme involving both error correcting and erasure coding , inter frame coding increases the data rate by a factor that reaches up to 1 55x
we introduce novel mathematical models and algorithms to generate \( shortest or k different \) explanations for biomedical queries , using answer set programming we implement these algorithms and integrate them in bioquery asp we illustrate the usefulness of these methods with some complex biomedical queries related to drug discovery , over the biomedical knowledge resources pharmgkb , drugbank , biogrid , ctd , sider , disease ontology and orphadata to appear in theory and practice of logic programming \( tplp \)
the american physical society \( aps \) march meeting of condensed matter physics has grown to nearly 10 , 000 participants , comprises 23 individual aps groups , and even warrants its own hashtag \( apsmarch \) here we analyze the text and data from march meeting abstracts of the past nine years and discuss trends in condensed matter physics over this time period we find that in comparison to atomic , molecular , and optical physics , condensed matter changes rapidly , and that condensed matter appears to be moving increasingly toward subject matter that is traditionally in materials science and engineering
we consider a network of agents that aim to learn some unknown state of the world using private observations and exchange of beliefs at each time , agents observe private signals generated based on the true unknown state each agent might not be able to distinguish the true state based only on her private observations this occurs when some other states are observationally equivalent to the true state from the agent 's perspective to overcome this shortcoming , agents must communicate with each other to benefit from local observations we propose a model where each agent selects one of her neighbors randomly at each time then , she refines her opinion using her private signal and the prior of that particular neighbor the proposed rule can be thought of as a bayesian agent who cannot recall the priors based on which other agents make inferences this learning without recall approach preserves some aspects of the bayesian inference while being computationally tractable by establishing a correspondence with a random walk on the network graph , we prove that under the described protocol , agents learn the truth exponentially fast in the almost sure sense the asymptotic rate is expressed as the sum of the relative entropies between the signal structures of every agent weighted by the stationary distribution of the random walk
exploiting the information about members of a social network \( sn \) represents one of the most attractive and dwelling subjects for both academic and applied scientists the community of complexity science and especially those researchers working on multiplex social systems are devoting increasing efforts to outline general laws , models , and theories , to the purpose of predicting emergent phenomena in sn 's \( e g success of a product \) on the other side the semantic web community aims at engineering a new generation of advanced services tailored to specific people needs this implies defining constructs , models and methods for handling the semantic layer of sns we combined models and techniques from both the former fields to provide a hybrid approach to understand a basic \( yet complex \) phenomenon the propagation of individual interests along the social networks since information may move along different social networks , one should take into account a multiplex structure therefore we introduced the notion of semantic multiplex in this paper we analyse two different semantic social networks represented by authors publishing in the computer science and those in the american physical society journals the comparison allows to outline common and specific features
many problems in machine learning and statistics can be formulated as \( generalized \) eigenproblems in terms of the associated optimization problem , computing linear eigenvectors amounts to finding critical points of a quadratic function subject to quadratic constraints in this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems we derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector we apply the inverse power method to 1 spectral clustering and sparse pca which can naturally be formulated as nonlinear eigenproblems in both applications we achieve state of the art results in terms of solution quality and runtime moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems
although researchers often comment on the rising popularity of nature inspired meta heuristics \( nim \) , there has been a paucity of data to directly support the claim that nim are growing in prominence compared to other optimization techniques this study presents evidence that the use of nim is not only growing , but indeed appears to have surpassed mathematical optimization techniques \( mot \) in several important metrics related to academic research activity \( publication frequency \) and commercial activity \( patenting frequency \) motivated by these findings , this article discusses some of the possible origins of this growing popularity i review different explanations for nim popularity and discuss why some of these arguments remain unsatisfying i argue that a compelling and comprehensive explanation should directly account for the manner in which most nim success has actually been achieved , e g through hybridization and customization to different problem environments by taking a problem lifecycle perspective , this paper offers a fresh look at the hypothesis that nature inspired meta heuristics derive much of their utility from being flexible i discuss global trends within the business environments where optimization algorithms are applied and i speculate that highly flexible algorithm frameworks could become increasingly popular within our diverse and rapidly changing world
recurrent neural networks are powerful models for sequential data , able to represent complex dependencies in the sequence that simpler models such as hidden markov models cannot handle yet they are notoriously hard to train here we introduce a training procedure using a gradient ascent in a riemannian metric this produces an algorithm independent from design choices such as the encoding of parameters and unit activities this metric gradient ascent is designed to have an algorithmic cost close to backpropagation through time for sparsely connected networks we use this procedure on gated leaky neural networks \( glnns \) , a variant of recurrent neural networks with an architecture inspired by finite automata and an evolution equation inspired by continuous time networks glnns trained with a riemannian gradient are demonstrated to effectively capture a variety of structures in synthetic problems basic block nesting as in context free grammars \( an important feature of natural languages , but difficult to learn \) , intersections of multiple independent markov type relations , or long distance relationships such as the distant xor problem this method does not require adjusting the network structure or initial parameters the network used is a sparse random graph and the initialization is identical for all problems considered
recent experimental results have shown that full duplex communication is possible for short range communications however , extending full duplex to long range communication remains a challenge , primarily due to residual self interference even with a combination of passive suppression and active cancellation methods in this paper , we investigate the root cause of performance bottlenecks in current full duplex systems we first classify all known full duplex architectures based on how they compute their cancelling signal and where the cancelling signal is injected to cancel self interference based on the classification , we analytically explain several published experimental results the key bottleneck in current systems turns out to be the phase noise in the local oscillators in the transmit and receive chain of the full duplex node as a key by product of our analysis , we propose signal models for wideband and mimo full duplex systems , capturing all the salient design parameters , and thus allowing future analytical development of advanced coding and signal design for full duplex systems
the model of cognition developed in \( smolensky and legendre , 2006 \) seeks to unify two levels of description of the cognitive process the connectionist and the symbolic the theory developed brings together these two levels into the integrated connectionist symbolic cognitive architecture \( ics \) clark and pulman \( 2007 \) draw a parallel with semantics where meaning may be modelled on both distributional and symbolic levels , developed by coecke et al , 2010 into the distributional compositional \( disco \) model of meaning in the current work , we revisit smolensky and legendre \( s l \) 's model we describe the disco framework , summarise the key ideas in s l 's architecture , and describe how their description of harmony as a graded measure of grammaticality may be applied in the disco model
the minimum mean square error of the estimation of a signal where observed from the additive white gaussian noise \( wgn \) channel 's output , is analyzed it is assumed that the channel input 's signal is composed of a \( normalized \) sum of n narrowband , mutually independent waves it is shown that if n goes to infinity , then for any fixed signal energy to noise energy ratio \( no mater how big \) both the causal minimum mean square error cmmse and the non causal minimum mean square error mmse converge to the signal energy at a rate which is proportional to 1 n
as instance of an overarching principle of exclusion an algorithm is presented that compactly \( thus not one by one \) generates all models of a horn formula the principle of exclusion can be adapted to generate only the models of weight k we compare and contrast it with constraint programming , 0 , 1 integer programming , and binary decision diagrams
places 2016 \( full title programming language approaches to concurrency and communication centric software \) is the ninth edition of the places workshop series after the first places , which was affiliated to discotec in 2008 , the workshop has been part of etaps every year since 2009 and is now an established part of the etaps satellite events places 2016 was held on 8th april in eindhoven , the netherlands the workshop series was started in order to promote the application of novel programming language ideas to the increasingly important problem of developing software for systems in which concurrency and communication are intrinsic aspects this includes software for both multi core systems and large scale distributed and or service oriented systems the scope of places includes new programming language features , whole new programming language designs , new type systems , new semantic approaches , new program analysis techniques , and new implementation mechanisms this volume consists of the papers accepted for presentation at the workshop
principal component analysis \( pca \) is a classical method for dimensionality reduction based on extracting the dominant eigenvectors of the sample covariance matrix however , pca is well known to behave poorly in the ``large p , small n '' setting , in which the problem dimension p is comparable to or larger than the sample size n this paper studies pca in this high dimensional regime , but under the additional assumption that the maximal eigenvector is sparse , say , with at most k nonzero components we consider a spiked covariance model in which a base matrix is perturbed by adding a k sparse maximal eigenvector , and we analyze two computationally tractable methods for recovering the support set of this maximal eigenvector , as follows \( a \) a simple diagonal thresholding method , which transitions from success to failure as a function of the rescaled sample size theta mathrm dia \( n , p , k \) n k 2 log \( p k \) and \( b \) a more sophisticated semidefinite programming \( sdp \) relaxation , which succeeds once the rescaled sample size theta mathrm sdp \( n , p , k \) n k log \( p k \) is larger than a critical threshold in addition , we prove that no method , including the best method which has exponential time complexity , can succeed in recovering the support if the order parameter theta mathrm sdp \( n , p , k \) is below a threshold our results thus highlight an interesting trade off between computational and statistical efficiency in high dimensional inference
concurrent ml 's events and event combinators facilitate modular concurrent programming with first class synchronization abstractions a standard implementation of these abstractions relies on fairly complex manipulations of first class continuations in the underlying language in this paper , we present a lightweight implementation of these abstractions in concurrent haskell , a language that already provides first order message passing at the heart of our implementation is a new distributed synchronization protocol in contrast with several previous translations of event abstractions in concurrent languages , we remain faithful to the standard semantics for events and event combinators for example , we retain the symmetry of mathtt choose for expressing selective communication
the paper describes a novel social network based open educational resource for learning foreign languages in real time from native speakers , based on the predefined teaching materials this virtual learning platform , named i2istudy , eliminates misunderstanding by providing prepared and predefined scenarios , enabling the participants to understand each other and , as a consequence , to communicate freely the system allows communication through the real time video and audio feed in addition to establishing the communication , it tracks the student progress and allows rating the instructor , based on the learner 's experience the system went live in april 2014 , and had over six thousand active daily users , with over 40 , 000 total registered users currently monetization is being added to the system , and time will show how popular the system will become in the future
this paper presents an analytical characterization of the ergodic capacity of amplify and forward \( af \) mimo dual hop relay channels , assuming that the channel state information is available at the destination terminal only in contrast to prior results , our expressions apply for arbitrary numbers of antennas and arbitrary relay configurations we derive an expression for the exact ergodic capacity , simplified closed form expressions for the high snr regime , and tight closed form upper and lower bounds these results are made possible to employing recent tools from finite dimensional random matrix theory to derive new closed form expressions for various statistical properties of the equivalent af mimo dual hop relay channel , such as the distribution of an unordered eigenvalue and certain random determinant properties based on the analytical capacity expressions , we investigate the impact of the system and channel characteristics , such as the antenna configuration and the relay power gain we also demonstrate a number of interesting relationships between the dual hop af mimo relay channel and conventional point to point mimo channels in various asymptotic regimes
a path from s to t on a polyhedral terrain is descending if the height of a point p never increases while we move p along the path from s to t no efficient algorithm is known to find a shortest descending path \( sdp \) from s to t in a polyhedral terrain we give a simple approximation algorithm that solves the sdp problem on general terrains our algorithm discretizes the terrain with o \( n 2 x e \) steiner points so that after an o \( n 2 x e log \( n x e \) \) time preprocessing phase for a given vertex s , we can determine a \( 1 e \) approximate sdp from s to any point v in o \( n \) time if v is either a vertex of the terrain or a steiner point , and in o \( n x e \) time otherwise here n is the size of the terrain , and x is a parameter of the geometry of the terrain
we propose a method to increase the capacity achieved by uniform prior in discrete memoryless channels \( dmc \) with high input cardinality it consists in appropriately reducing the input set different design criteria of the input subset are discussed we develop an efficient algorithm to solve this problem based on the maximization of the cut off rate the method is applied to a mono bit transceiver mimo system , and it is shown that the capacity can be approached within tenths of a db by employing standard binary codes while avoiding the use of distribution shapers
multilinear and tensor decompositions are a popular tool in linear and multilinear algebra and have a wide range of important applications to modern computing our paper of 1972 presented the first nontrivial application of such decompositions to fundamental matrix computations and was also a landmark in the history of the acceleration of matrix multiplication published in 1972 in russian , it has never been translated into english it has been very rarely cited in the western literature on matrix multiplication and never in the works on multilinear and tensor decompositions this motivates us to present its translation into english , together with our brief comments on its impact on the two fields
we describe random generation algorithms for a large class of random combinatorial objects called schur processes , which are sequences of random \( integer \) partitions subject to certain interlacing conditions this class contains several fundamental combinatorial objects as special cases , such as plane partitions , tilings of aztec diamonds , pyramid partitions and more generally steep domino tilings of the plane our algorithm , which is of polynomial complexity , is both exact \( i e the output follows exactly the target probability law , which is either boltzmann or uniform in our case \) , and entropy optimal \( i e it reads a minimal number of random bits as an input \) the algorithm encompasses previous growth procedures for special schur processes related to the primal and dual rsk algorithm , as well as the famous domino shuffling algorithm for domino tilings of the aztec diamond it can be easily adapted to deal with symmetric schur processes and general schur processes involving infinitely many parameters it is more concrete and easier to implement than borodin 's algorithm , and it is entropy optimal at a technical level , it relies on unified bijective proofs of the different types of cauchy and littlewood identities for schur functions , and on an adaptation of fomin 's growth diagram description of the rsk algorithm to that setting simulations performed with this algorithm suggest interesting limit shape phenomena for the corresponding tiling models , some of which are new
we consider the cryptographic problem of constructing an invertible random permutation from a public random function \( i e , which can be accessed by the adversary \) this goal is formalized by the notion of indifferentiability of maurer et al \( tcc 2004 \) this is the natural extension to the public setting of the well studied problem of building random permutations from random functions , which was first solved by luby and rackoff \( siam j comput , '88 \) using the so called feistel construction the most important implication of such a construction is the equivalence of the random oracle model \( bellare and rogaway , ccs '93 \) and the ideal cipher model , which is typically used in the analysis of several constructions in symmetric cryptography coron et al \( crypto 2008 \) gave a rather involved proof that the six round feistel construction with independent random round functions is indifferentiable from an invertible random permutation also , it is known that fewer than six rounds do not suffice for indifferentiability the first contribution \( and starting point \) of our paper is a concrete distinguishing attack which shows that the indifferentiability proof of coron et al is not correct in addition , we provide supporting evidence that an indifferentiability proof for the six round feistel construction may be very hard to find to overcome this gap , our main contribution is a proof that the feistel construction with eigthteen rounds is indifferentiable from an invertible random permutation the approach of our proof relies on assigning to each of the rounds in the construction a unique and specific role needed in the proof this avoids many of the problems that appear in the six round case
we establish new upper bounds about symmetric bilinear complexity in any extension of finite fields note that these bounds are not asymptotical but uniform moreover , we discuss the validity of certain published bounds
this paper investigates the enumeration , rate region computation , and hierarchy of general multi source multi sink hyperedge networks under network coding , which includes multiple network models , such as independent distributed storage systems and index coding problems , as special cases a notion of minimal networks and a notion of network equivalence under group action are defined an efficient algorithm capable of directly listing single minimal canonical representatives from each network equivalence class is presented and utilized to list all minimal canonical networks with up to 5 sources and hyperedges computational tools are then applied to obtain the rate regions of all of these canonical networks , providing exact expressions for 744 , 119 newly solved network coding rate regions corresponding to more than 2 trillion isomorphic network coding problems in order to better understand and analyze the huge repository of rate regions through hierarchy , several embedding and combination operations are defined so that the rate region of the network after operation can be derived from the rate regions of networks involved in the operation the embedding operations enable the definition and determination of a list of forbidden network minors for the sufficiency of classes of linear codes the combination operations enable the rate regions of some larger networks to be obtained as the combination of the rate regions of smaller networks the integration of both the combinations and embedding operators is then shown to enable the calculation of rate regions for many networks not reachable via combination operations alone
bootstrap techniques \( also called resampling computation techniques \) have introduced new advances in modeling and model evaluation using resampling methods to construct a series of new samples which are based on the original data set , allows to estimate the stability of the parameters properties such as convergence and asymptotic normality can be checked for any particular observed data set in most cases , the statistics computed on the generated data sets give a good idea of the confidence regions of the estimates in this paper , we debate on the contribution of such methods for model selection , in the case of feedforward neural networks the method is described and compared with the leave one out resampling method the effectiveness of the bootstrap method , versus the leave one out methode , is checked through a number of examples
in this paper , we introduce a new framework for robust multiple signal classification \( music \) the proposed framework , called robust measure transformed \( mt \) music , is based on applying a transform to the probability distribution of the received signals , i e , transformation of the probability measure defined on the observation space in robust mt music , the sample covariance is replaced by the empirical mt covariance by judicious choice of the transform we show that 1 \) the resulting empirical mt covariance is b robust , with bounded influence function that takes negligible values for large norm outliers , and 2 \) under the assumption of spherically contoured noise distribution , the noise subspace can be determined from the eigendecomposition of the mt covariance furthermore , we derive a new robust measure transformed minimum description length \( mdl \) criterion for estimating the number of signals , and extend the mt music framework to the case of coherent signals the proposed approach is illustrated in simulation examples that show its advantages as compared to other robust music and mdl generalizations
this paper considers a cooperative ofdma based cognitive radio network where the primary system leases some of its subchannels to the secondary system for a fraction of time in exchange for the secondary users \( sus \) assisting the transmission of primary users \( pus \) as relays our aim is to determine the cooperation strategies among the primary and secondary systems so as to maximize the sum rate of sus while maintaining quality of service \( qos \) requirements of pus we formulate a joint optimization problem of pu transmission mode selection , su \( or relay \) selection , subcarrier assignment , power control , and time allocation by applying dual method , this mixed integer programming problem is decomposed into parallel per subcarrier subproblems , with each determining the cooperation strategy between one pu and one su we show that , on each leased subcarrier , the optimal strategy is to let a su exclusively act as a relay or transmit for itself this result is fundamentally different from the conventional spectrum leasing in single channel systems where a su must transmit a fraction of time for itself if it helps the pu 's transmission we then propose a subgradient based algorithm to find the asymptotically optimal solution to the primal problem in polynomial time simulation results demonstrate that the proposed algorithm can significantly enhance the network performance
we draw relationships between the generalized data processing theorems of zakai and ziv \( 1973 and 1975 \) and the dynamical version of the second law of thermodynamics , a k a the boltzmann h theorem , which asserts that the shannon entropy , h \( x t \) , pertaining to a finite state markov process x t , is monotonically non decreasing as a function of time t , provided that the steady state distribution of this process is uniform across the state space \( which is the case when the process designates an isolated system \) it turns out that both the generalized data processing theorems and the boltzmann h theorem can be viewed as special cases of a more general principle concerning the monotonicity \( in time \) of a certain generalized information measure applied to a markov process this gives rise to a new look at the generalized data processing theorem , which suggests to exploit certain degrees of freedom that may lead to better bounds , for a given choice of the convex function that defines the generalized mutual information
the sand pile model \( spm \) and its generalization , the ice pile model \( ipm \) , originate from physics and have various applications in the description of the evolution of granular systems in this article , we deal with the enumeration and the exhaustive generation of the accessible configuration of the system our work is based on a new recursive decomposition theorem for spm configurations using the notion of staircase bases based on this theorem , we provide a recursive formula for the enumeration of spm \( n \) and a constant amortized time \( cat \) algorithm for the generation of all spm \( n \) configurations the extension of the same approach to the ice pile model is also discussed
we develop a new analysis for the length of controlled bad sequences in well quasi orderings based on higman 's lemma this leads to tight multiply recursive upper bounds that readily apply to several verification algorithms for well structured systems
this paper considers the problem of perfectly secure communication on a modified version of wyner 's wiretap channel ii where both the main and wiretapper 's channels have some erasures a secret message is to be encoded into n channel symbols and transmitted the main channel is such that the legitimate receiver receives the transmitted codeword with exactly n nu erasures , where the positions of the erasures are random additionally , an eavesdropper \( wire tapper \) is able to observe the transmitted codeword with n mu erasures in a similar fashion this paper studies the maximum achievable information rate with perfect secrecy on this channel and gives a coding scheme using nested codes that achieves the secrecy capacity
recently , a new structure called butterfly introduced by perrin et at has very good cryptographic properties the differential uniformity is at most equal to 4 and algebraic degree is also very good when exponent e 3 however , the nonlinearity has not been proved in general it is conjecture that the nonlinearity is also optimal in this paper , we further study the butterfly structure and show that these structure with exponent e 2 i 1 have also very good cryptographic properties more importantly , we prove in theory the nonlinearity is optimal for every odd k , which completely solve the open problem finally , we study the butter structure with trivial coefficient and show these butterflies have also optimal nonlinearity furthermore , we show that the closed butterfly with trivial coefficient is bijective as well , which also can be used to serve as a cryptographic primitive
testing whether there is an induced path in a graph spanning k given vertices is already np complete in general graphs when k 3 we show how to solve this problem in polynomial time on claw free graphs , when k is not part of the input but an arbitrarily fixed integer
in supervised learning , an inductive learning algorithm extracts general rules from observed training instances , then the rules are applied to test instances we show that this splitting of training and application arises naturally , in the classical setting , from a simple independence requirement with a physical interpretation of being non signalling thus , two seemingly different definitions of inductive learning happen to coincide this follows from very specific properties of classical information , which break down in the quantum setup we prove a quantum de finetti theorem for quantum channels , which shows that in the quantum case , the equivalence holds in the asymptotic setting \( for large number of test instances \) this reveals a natural analogy between classical learning protocols and their quantum counterparts , thus allowing to naturally enquire about standard elements in computational learning theory , such as structural risk minimization , model and sample complexity
how common is self citation in scholarly publication and does the practice vary by gender \? using novel methods and a dataset of 1 5 million research papers in the scholarly database jstor published between 1779 2011 , we find that nearly 10 of references are self citations by a paper 's authors we further find that over the years between 1779 2011 , men cite their own papers 56 more than women do in the last two decades of our data , men self cite 70 more than women women are also more than ten percentage points more likely than men to not cite their own previous work at all despite increased representation of women in academia , this gender gap in self citation rates has remained stable over the last 50 years we break down self citation patterns by academic field and number of authors , and comment on potential mechanisms behind these observations these findings have important implications for scholarly visibility and likely consequences for academic careers
just as the hamming weight spectrum of a linear block code sheds light on the performance of a maximum likelihood decoder , the pseudo weight spectrum provides insight into the performance of a linear programming decoder using properties of polyhedral cones , we find the pseudo weight spectrum of some short codes we also present two general lower bounds on the minimum pseudo weight the first bound is based on the column weight of the parity check matrix the second bound is computed by solving an optimization problem in some cases , this bound is more tractable to compute than previously known bounds and thus can be applied to longer codes
faults and viruses often spread in networked environments by propagating from site to neighboring site we model this process of em network contamination by graphs consider a graph g \( v , e \) , whose vertex set is contaminated and our goal is to decontaminate the set v \( g \) using mobile decontamination agents that traverse along the edge set of g temporal immunity tau \( g \) ge 0 is defined as the time that a decontaminated vertex of g can remain continuously exposed to some contaminated neighbor without getting infected itself the emph immunity number of g , iota k \( g \) , is the least tau that is required to decontaminate g using k agents we study immunity number for some classes of graphs corresponding to network topologies and present upper bounds on iota 1 \( g \) , in some cases with matching lower bounds variations of this problem have been extensively studied in literature , but proposed algorithms have been restricted to em monotone strategies , where a vertex , once decontaminated , may not be recontaminated we exploit nonmonotonicity to give bounds which are strictly better than those derived using monotone strategies
we suggest a new approach to hypothesis testing for ergodic and stationary processes in contrast to standard methods , the suggested approach gives a possibility to make tests , based on any lossless data compression method even if the distribution law of the codeword lengths is not known we apply this approach to the following four problems goodness of fit testing \( or identity testing \) , testing for independence , testing of serial independence and homogeneity testing and suggest nonparametric statistical tests for these problems it is important to note that practically used so called archivers can be used for suggested testing
in this paper , we analyze the convergence of the alternating direction method of multipliers \( admm \) for minimizing a nonconvex and possibly nonsmooth objective function , f \( x \) h \( y \) , subject to linear equality constraints that couple x and y both the functions f and h can be nonconvex , but h needs to be smooth the developed convergence guarantee covers a variety of nonconvex functions such as piecewise linear functions , ell q quasi norm , schatten q quasi norm \( 0 q 1 \) and scad , as well as the indicator functions of compact smooth manifolds \( e g , spherical , stiefel , and grassman manifolds \) by applying our analysis , we show , for the first time , that several admm algorithms applied to solve nonconvex models in statistical learning , optimization on manifold , and matrix decomposition are guaranteed to converge admm has been regarded as a variant to the augmented lagrangian method \( alm \) we present a simple example to illustrate how admm converges but alm diverges by this example and other analysis in this paper , admm is a much better choice than alm for nonconvex nonsmooth problems not only is admm easier to implement , it is also more likely to converge
i propose a novel method for practical joint processing dl comp implementation in lte lte a systems using a supervised machine learning technique dl comp has not been thoroughly studied in previous work although cluster formation and interference mitigation have been studied extensively in this paper , i attempt to improve the cell edge user data rate served by a heterogeneous network cluster by means of dynamically changing the dl sinr threshold at which dl comp is triggered i do so by allowing the base stations to derive a threshold on the basis of machine learning inference the simulation results show an improved user throughput at the cell edge of 40 and a 6 4 improvement to the average cell throughput compared to the baseline of static triggering
the aim of the paper to answer a long standing open problem on the relationship between np and bqp the paper shows that bqp contains np by proposing a bqp quantum algorithm for the max e3 sat problem which is a fundamental np hard problem given an e3 cnf boolean formula , the aim of the max e3 sat problem is to find the variable assignment that maximizes the number of satisfied clauses the proposed algorithm runs in o \( m 2 \) for an e3 cnf boolean formula with m clauses and in the worst case runs in o \( n 6 \) for an e3 cnf boolean formula with n inputs the proposed algorithm maximizes the set of satisfied clauses using a novel iterative partial negation and partial measurement technique the algorithm is shown to achieve an arbitrary high probability of success of 1 epsilon for small epsilon 0 using a polynomial resources in addition to solving the max e3 sat problem , the proposed algorithm can also be used to decide if an e3 cnf boolean formula is satisfiable or not , which is an np complete problem , based on the maximum number of satisfied clauses
on the multi antenna broadcast channel , the spatial degrees of freedom support simultaneous transmission to multiple users the optimal multiuser transmission , known as dirty paper coding , is not directly realizable moreover , close to optimal solutions such as tomlinson harashima precoding are sensitive to csi inaccuracy this paper considers a more practical design called per user unitary and rate control \( pu2rc \) , which has been proposed for emerging cellular standards pu2rc supports multiuser simultaneous transmission , enables limited feedback , and is capable of exploiting multiuser diversity its key feature is an orthogonal beamforming \( or precoding \) constraint , where each user selects a beamformer \( or precoder \) from a codebook of multiple orthonormal bases in this paper , the asymptotic throughput scaling laws for pu2rc with a large user pool are derived for different regimes of the signal to noise ratio \( snr \) in the multiuser interference limited regime , the throughput of pu2rc is shown to scale logarithmically with the number of users in the normal snr and noise limited regimes , the throughput is found to scale double logarithmically with the number of users and also linearly with the number of antennas at the base station in addition , numerical results show that pu2rc achieves higher throughput and is more robust against csi quantization errors than the popular alternative of zero forcing beamforming if the number of users is sufficiently large
text alignment is crucial to the accuracy of machine translation \( mt \) systems , some nlp tools or any other text processing tasks requiring bilingual data this research proposes a language independent sentence alignment approach based on polish \( not position sensitive language \) to english experiments this alignment approach was developed on the ted talks corpus , but can be used for any text domain or language pair the proposed approach implements various heuristics for sentence recognition some of them value synonyms and semantic text structure analysis as a part of additional information minimization of data loss was ensured the solution is compared to other sentence alignment implementations also an improvement in mt system score with text processed with described tool is shown
the evolution of cellular networks is driven by the dream of ubiquitous wireless connectivity any data service is instantly accessible everywhere with each generation of cellular networks , we have moved closer to this wireless dream first by delivering wireless access to voice communications , then by providing wireless data services , and recently by delivering a wifi like experience with wide area coverage and user mobility management the support for high data rates has been the main objective in recent years , as seen from the academic focus on sum rate optimization and the efforts from standardization bodies to meet the peak rate requirements specified in imt advanced in contrast , a variety of metrics objectives are put forward in the technological preparations for 5g networks higher peak rates , improved coverage with uniform user experience , higher reliability and lower latency , better energy efficiency , lower cost user devices and services , better scalability with number of devices , etc these multiple objectives are coupled , often in a conflicting manner such that improvements in one objective lead to degradation in the other objectives hence , the design of future networks calls for new optimization tools that properly handle the existence and tradeoffs between multiple objectives in this article , we provide a review of multi objective optimization \( moo \) , which is a mathematical framework to solve design problems with multiple conflicting objectives \( \) we provide a survey of the basic definitions , properties , and algorithmic tools in moo this reveals how signal processing algorithms are used to visualize the inherent conflicts between 5g performance objectives , thereby allowing the network designer to understand the possible operating points and how to balance the objectives in an efficient and satisfactory way for clarity , we provide a case study on massive mimo
in probabilistic approaches to classification and information extraction , one typically builds a statistical model of words under the assumption that future data will exhibit the same regularities as the training data in many data sets , however , there are scope limited features whose predictive power is only applicable to a certain subset of the data for example , in information extraction from web pages , word formatting may be indicative of extraction category in different ways on different web pages the difficulty with using such features is capturing and exploiting the new regularities encountered in previously unseen data in this paper , we propose a hierarchical probabilistic model that uses both local scope limited features , such as word formatting , and global features , such as word content the local regularities are modeled as an unobserved random parameter which is drawn once for each local data set this random parameter is estimated during the inference process and then used to perform classification with both the local and global features a procedure which is akin to automatically retuning the classifier to the local regularities on each newly encountered web page exact inference is intractable and we present approximations via point estimates and variational methods empirical results on large collections of web data demonstrate that this method significantly improves performance from traditional models of global features alone
evolutionary algorithms \( eas \) are population based general purpose optimization algorithms , and have been successfully applied in various real world optimization tasks however , previous theoretical studies often employ eas with only a parent or offspring population and focus on specific problems furthermore , they often only show upper bounds on the running time , while lower bounds are also necessary to get a complete understanding of an algorithm in this paper , we analyze the running time of the \( mu lambda \) ea \( a general population based ea with mutation only \) on the class of pseudo boolean functions with a unique global optimum by applying the recently proposed switch analysis approach , we prove the lower bound omega \( n ln n mu lambda n ln ln n ln n \) for the first time particularly on the two widely studied problems , onemax and leadingones , the derived lower bound discloses that the \( mu lambda \) ea will be strictly slower than the \( 1 1 \) ea when the population size mu or lambda is above a moderate order our results imply that the increase of population size , while usually desired in practice , bears the risk of increasing the lower bound of the running time and thus should be carefully considered
a distributed inference scheme which uses bounded transmission functions over a gaussian multiple access channel is considered when the sensor measurements are decreasingly reliable as a function of the sensor index , the conditions on the transmission functions under which consistent estimation and reliable detection are possible is characterized for the distributed estimation problem , an estimation scheme that uses bounded transmission functions is proved to be strongly consistent provided that the variance of the noise samples are bounded and that the transmission function is one to one the proposed estimation scheme is compared with the amplify and forward technique and its robustness to impulsive sensing noise distributions is highlighted in contrast to amplify and forward schemes , it is also shown that bounded transmissions suffer from inconsistent estimates if the sensing noise variance goes to infinity for the distributed detection problem , similar results are obtained by studying the deflection coefficient simulations corroborate our analytical results
the problem of three user multiple access channel \( mac \) with correlated sources is investigated an extension to the cover el gamal salehi \( ces \) scheme is introduced we use a combination of this scheme with linear codes and propose a new coding strategy we derive new sufficient conditions to transmit correlated sources reliably we consider an example of three user mac with binary inputs using this example , we show strict improvements over the ces scheme
autonomous intelligent agent research is a domain situated at the forefront of artificial intelligence interest based negotiation \( ibn \) is a form of negotiation in which agents exchange information about their underlying goals , with a view to improve the likelihood and quality of a offer in this paper we model and verify a multi agent argumentation scenario of resource sharing mechanism to enable resource sharing in a distributed system we use ibn in our model wherein agents express their interests to the others in the society to gain certain resources
most of the conventional models for opinion dynamics mainly account for a fully local influence , where myopic agents decide their actions after they interact with other agents that are adjacent to them for example , in the case of social interactions , this includes family , friends , and other strong social ties the model proposed in this contribution , embodies a global influence as well where , by global , we mean that each node also observes a sample of the average behavior of the entire population \( in the social example , people observe other people on the streets , subway , and other social venues \) we consider a case where nodes have dichotomous states \( examples include elections with two major parties , whether or not to adopt a new technology or product , and any yes no opinion such as in voting on a referendum \) the dynamics of states on a network with arbitrary degree distribution are studied for a given initial condition , we find the probability to reach consensus on each state and the expected time reach to consensus the effect of an exogenous bias on the average orientation of the system is investigated , to model mass media to do so , we add an external field to the model that favors one of the states over the other this field interferes with the regular decision process of each node and creates a constant probability to lean towards one of the states we solve for the average state of the system as a function of time for given initial conditions then anti conformists \( stubborn nodes who never revise their states \) are added to the network , in an effort to circumvent the external bias we find necessary conditions on the number of these defiant nodes required to cancel the effect of the external bias our analysis is based on a mean field approximation of the agent opinions
the problem of rare and unknown words is an important issue that can potentially effect the performance of many nlp systems , including both traditional count based models and deep learning models we propose a novel way to deal with the rare and unseen words for the neural network models with attention our model uses two softmax layers in order to predict the next word in conditional language models one of the softmax layers predicts the location of a word in the source sentence , and the other softmax layer predicts a word in the shortlist vocabulary the decision of which softmax layer to use at each timestep is adaptively made by an mlp which is conditioned on the context we motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known using our proposed model , we observe improvements in two tasks , neural machine translation on the europarl english to french parallel corpora and text summarization on the gigaword dataset
the open language archives community \( olac \) is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources the dublin core \( dc \) element set and the oai protocol have provided a solid foundation for the olac framework however , we need more precision in community specific aspects of resource description than is offered by dc furthermore , many of the institutions and individuals who might participate in olac do not have the technical resources to support the oai protocol this paper presents our solutions to these two problems to address the first , we have developed an extensible application profile for language resource metadata to address the second , we have implemented vida \( the virtual data provider \) and viser \( the virtual service provider \) , which permit community members to provide data and services without having to implement the oai protocol these solutions are generic and could be adopted by other specialized subcommunities
a new interpolation based decoding principle for interleaved gabidulin codes is presented the approach consists of two steps first , a multi variate linearized polynomial is constructed which interpolates the coefficients of the received word and second , the roots of this polynomial have to be found due to the specific structure of the interpolation polynomial , both steps \( interpolation and root finding \) can be accomplished by solving a linear system of equations this decoding principle can be applied as a list decoding algorithm \( where the list size is not necessarily bounded polynomially \) as well as an efficient probabilistic unique decoding algorithm for the unique decoder , we show a connection to known unique decoding approaches and give an upper bound on the failure probability finally , we generalize our approach to incorporate not only errors , but also row and column erasures
in the paradigm of multi task learning , mul tiple related prediction tasks are learned jointly , sharing information across the tasks we propose a framework for multi task learn ing that enables one to selectively share the information across the tasks we assume that each task parameter vector is a linear combi nation of a finite number of underlying basis tasks the coefficients of the linear combina tion are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these our model is based on on the assumption that task pa rameters within a group lie in a low dimen sional subspace but allows the tasks in differ ent groups to overlap with each other in one or more bases experimental results on four datasets show that our approach outperforms competing methods
the problem of obtaining network coding maps for the physical layer network coded two way relay channel is considered , using the denoise and forward forward protocol it is known that network coding maps used at the relay node which ensure unique decodability at the end nodes form a latin square also , it is known that minimum distance of the effective constellation at the relay node becomes zero , when the ratio of the fade coefficients from the end node to the relay node , belongs to a finite set of complex numbers determined by the signal set used , called the singular fade states furthermore , it has been shown recently that the problem of obtaining network coding maps which remove the harmful effects of singular fade states , reduces to the one of obtaining latin squares , which satisfy certain constraints called textit singularity removal constraints in this paper , it is shown that the singularity removal constraints along with the row and column exclusion conditions of a latin square , can be compactly represented by a graph called the textit singularity removal graph determined by the singular fade state and the signal set used it is shown that a latin square which removes a singular fade state can be obtained from a proper vertex coloring of the corresponding singularity removal graph the minimum number of symbols used to fill in a latin square which removes a singular fade state is equal to the chromatic number of the singularity removal graph it is shown that for any square m qam signal set , there exists singularity removal graphs whose chromatic numbers exceed m and hence require more than m colors for vertex coloring also , it is shown that for any 2 lambda psk signal set , lambda geq 3 , all the singularity removal graphs can be colored using 2 lambda colors
in a two user broadcast channel where one user has full csir and the other has none , a recent result showed that tdma is strictly suboptimal and a product superposition requiring non coherent signaling achieves dof gains under many antenna configurations this work introduces product superposition in the domain of coherent signaling with pilots , demonstrates the advantages of product superposition in low snr as well as high snr , and establishes dof gains in a wider set of receiver antenna configurations two classes of decoders , with and without interference cancellation , are studied achievable rates are established by analysis and illustrated by simulations
deterministic constructions of measurement matrices in compressed sensing \( cs \) are considered in this paper the constructions are inspired by the recent discovery of dimakis , smarandache and vontobel which says that parity check matrices of good low density parity check \( ldpc \) codes can be used as provably good measurement matrices for compressed sensing under ell 1 minimization the performance of the proposed binary measurement matrices is mainly theoretically analyzed with the help of the analyzing methods and results from \( finite geometry \) ldpc codes particularly , several lower bounds of the spark \( i e , the smallest number of columns that are linearly dependent , which totally characterizes the recovery performance of ell 0 minimization \) of general binary matrices and finite geometry matrices are obtained and they improve the previously known results in most cases simulation results show that the proposed matrices perform comparably to , sometimes even better than , the corresponding gaussian random matrices moreover , the proposed matrices are sparse , binary , and most of them have cyclic or quasi cyclic structure , which will make the hardware realization convenient and easy
unified performance analysis is carried out for amplify and forward \( af \) multiple input multiple output \( mimo \) beamforming \( bf \) two way relay networks in rayleigh fading with five different relaying protocols including two novel protocols for better performance as a result , a novel closed form sum bit error rate \( ber \) expression is presented in a unified expression for all protocols a new closed form high signal to noise ratio \( snr \) performance is also obtained in a single expression , and an analytical high snr gap expression between the five protocols is provided we compare the performance of the five relaying protocols with respect to sum ber with appropriately normalized rate and power , and show that the proposed protocol with four time slots outperforms other protocols when transmit powers from two sources are sufficiently different , and the one with three time slots dominates other protocols when multiple relay antennas are used , at high snr
the single allocation ordered median hub location problem is a recent hub model introduced in puerto et al \( 2011 \) that provides a unifying analysis of a wide class of hub location mod els in this paper , we deal with the capacitated version of this problem , presenting two formulations as well as some preprocessing phases for fixing variables in addition , a strengthening of one of these formulations is also studied through the use of some fami lies of valid inequalities a battery of test problems with data taken from the ap library are solved where it is shown that the running times have been significantly reduced with the improvements presented in the paper
we introduce two hierarchies of clause sets , slur k and uc k , based on the classes slur \( single lookahead unit refutation \) , introduced in 1995 , and uc \( unit refutation complete \) , introduced in 1994 the class slur , introduced in annexstein et al , 1995 , is the class of clause sets for which unit clause propagation \( denoted by r 1 \) detects unsatisfiability , or where otherwise iterative assignment , avoiding obviously false assignments by look ahead , always yields a satisfying assignment it is natural to consider how to form a hierarchy based on slur such investigations were started in cepek et al , 2012 and balyo et al , 2012 we present what we consider the limit hierarchy slur k , based on generalising r 1 by r k , that is , using generalised unit clause propagation introduced in kullmann , 1999 , 2004 the class uc , studied in del val , 1994 , is the class of unit refutation complete clause sets , that is , those clause sets for which unsatisfiability is decidable by r 1 under any falsifying assignment for unsatisfiable clause sets f , the minimum k such that r k determines unsatisfiability of f is exactly the hardness of f , as introduced in ku 99 , 04 for satisfiable f we use now an extension mentioned in ansotegui et al , 2008 the hardness is the minimum k such that after application of any falsifying partial assignments , r k determines unsatisfiability the class uc k is given by the clause sets which have hardness k we observe that uc 1 is exactly uc uc k has a proof theoretic character , due to the relations between hardness and tree resolution , while slur k has an algorithmic character the correspondence between r k and k times nested input resolution \( or tree resolution using clause space k 1 \) means that r k has a dual nature both algorithmic and proof theoretic this corresponds to a basic result of this paper , namely slur k uc k
a distributed detection problem over fading gaussian multiple access channels is considered sensors observe a phenomenon and transmit their observations to a fusion center using the amplify and forward scheme the fusion center has multiple antennas with different channel models considered between the sensors and the fusion center , and different cases of channel state information are assumed at the sensors the performance is evaluated in terms of the error exponent for each of these cases , where the effect of multiple antennas at the fusion center is studied it is shown that for zero mean channels between the sensors and the fusion center when there is no channel information at the sensors , arbitrarily large gains in the error exponent can be obtained with sufficient increase in the number of antennas at the fusion center in stark contrast , when there is channel information at the sensors , the gain in error exponent due to having multiple antennas at the fusion center is shown to be no more than a factor of \( 8 pi \) for rayleigh fading channels between the sensors and the fusion center , independent of the number of antennas at the fusion center , or correlation among noise samples across sensors scaling laws for such gains are also provided when both sensors and antennas are increased simultaneously simple practical schemes and a numerical method using semidefinite relaxation techniques are presented that utilize the limited possible gains available simulations are used to establish the accuracy of the results
the global movement of people and goods has increased the risk of biosecurity threats and their potential to incur large economic , social , and environmental costs conventional manual biosecurity surveillance methods are limited by their scalability in space and time this article focuses on autonomous surveillance systems , comprising sensor networks , robots , and intelligent algorithms , and their applicability to biosecurity threats we discuss the spatial and temporal attributes of autonomous surveillance technologies and map them to three broad categories of biosecurity threat \( i \) vector borne diseases \( ii \) plant pests and \( iii \) aquatic pests our discussion reveals a broad range of opportunities to serve biosecurity needs through autonomous surveillance
traditionally , 802 11 based networks that relied on wired equivalent protocol \( wep \) were especially vulnerable to packet sniffing today , wireless networks are more prolific , and the monitoring devices used to find them are mobile and easy to access securing wireless networks can be difficult because these networks consist of radio transmitters and receivers , and anybody can listen , capture data and attempt to compromise it in recent years , a range of technologies and mechanisms have helped make networking more secure this paper holistically evaluated various enhanced protocols proposed to solve wep related authentication , confidentiality and integrity problems it discovered that strength of each solution depends on how well the encryption , authentication and integrity techniques work the work suggested using a defence in depth strategy and integration of biometric solution in 802 11i comprehensive in depth comparative analysis of each of the security mechanisms is driven by review of related work in wlan security solutions
openaire , the open access infrastructure for research in europe , comprises a database of all european funded research projects , including metadata of their results \( publications and datasets \) these data are stored in an hbase nosql database , post processed , and exposed as html for human consumption , and as xml through a web service interface as an intermediate format to facilitate statistical computations , csv is generated internally to interlink the openaire data with related data on the web , we aim at exporting them as linked open data \( lod \) the lod export is required to integrate into the overall data processing workflow , where derived data are regenerated from the base data every day we thus faced the challenge of identifying the best performing conversion approach we evaluated the performances of creating lod by a mapreduce job on top of hbase , by mapping the intermediate csv files , and by mapping the xml output
probabilistic transition system specifications \( ptsss \) in the nt mu f theta nt mu x theta format provide structural operational semantics for segala type systems that exhibit both probabilistic and nondeterministic behavior and guarantee that bisimilarity is a congruence for all operator defined in such format starting from the nt mu f theta nt mu x theta format , we obtain restricted formats that guarantee that three coarser bisimulation equivalences are congruences we focus on \( i \) segala 's variant of bisimulation that considers combined transitions , which we call here convex bisimulation \( ii \) the bisimulation equivalence resulting from considering park milner 's bisimulation on the usual stripped probabilistic transition system \( translated into a labelled transition system \) , which we call here probability obliterated bisimulation and \( iii \) a probability abstracted bisimulation , which , like bisimulation , preserves the structure of the distributions but instead , it ignores the probability values in addition , we compare these bisimulation equivalences and provide a logic characterization for each of them
we introduce deep variational bayes filters \( dvbf \) , a new method for unsupervised learning of latent markovian state space models leveraging recent advances in stochastic gradient variational bayes , dvbf can overcome intractable inference distributions by means of variational inference thus , it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding this also enables realistic long term prediction
we study a problem of allocating divisible jobs , arriving online , to workers in a crowdsourcing setting which involves learning two parameters of strategically behaving workers each job is split into a certain number of tasks that are then allocated to workers each arriving job has to be completed within a deadline and each task has to be completed satisfying an upper bound on probability of failure the job population is homogeneous while the workers are heterogeneous in terms of costs , completion times , and times to failure the job completion time and time to failure of each worker are stochastic with fixed but unknown means the requester is faced with the challenge of learning two separate parameters of each \( strategically behaving \) worker simultaneously , namely , the mean job completion time and the mean time to failure the time to failure of a worker depends on the duration of the task handled by the worker assuming non strategic workers to start with , we solve this biparameter learning problem by applying the robust ucb algorithm then , we non trivially extend this algorithm to the setting where the workers are strategic about their costs our proposed mechanism is dominant strategy incentive compatible and ex post individually rational with asymptotically optimal regret performance
complex systems are usually illustrated by networks which captures the topology of the interactions between the entities to better understand the roles played by the entities in the system one needs to uncover the underlying community structure of the system in recent years , systems with interactions that have various types or can change over time between the entities have attracted an increasing research attention however , algorithms aiming to solve the key problem community detection in multilayer networks are still limited in this work , we first introduce the multilayer network model representation with multiple aspects , which is flexible to a variety of networks then based on this model , we naturally derive the multilayer modularity a widely adopted objective function of community detection in networks from a static perspective as an evaluation metric to evaluate the quality of the communities detected in multilayer networks it enables us to better understand the essence of the modularity by pointing out the specific kind of communities that will lead to a high modularity score we also propose a spectral method called mspec for the optimization of the proposed modularity function based on the supra adjacency representation of the multilayer networks experiments on the electroencephalograph network and the comparison results on several empirical multilayer networks demonstrate the feasibility and reliable performance of the proposed method
multi person event recognition is a challenging task , often with many people active in the scene but only a small subset contributing to an actual event in this paper , we propose a model which learns to detect events in such videos while automatically attending to the people responsible for the event our model does not use explicit annotations regarding who or where those people are during training and testing in particular , we track people in videos and use a recurrent neural network \( rnn \) to represent the track features we learn time varying attention weights to combine these features at each time instant the attended features are then processed using another rnn for event detection classification since most video datasets with multiple people are restricted to a small number of videos , we also collected a new basketball dataset comprising 257 basketball games with 14k event annotations corresponding to 11 event classes our model outperforms state of the art methods for both event classification and detection on this new dataset additionally , we show that the attention mechanism is able to consistently localize the relevant players
demixing is the problem of identifying multiple structured signals from a superimposed , undersampled , and noisy observation this work analyzes a general framework , based on convex optimization , for solving demixing problems when the constituent signals follow a generic incoherence model , this analysis leads to precise recovery guarantees these results admit an attractive interpretation each signal possesses an intrinsic degrees of freedom parameter , and demixing can succeed if and only if the dimension of the observation exceeds the total degrees of freedom present in the observation
we derive optimal snr based transmit antenna selection rules at the source and relay for the nonregenerative half duplex mimo relay channel while antenna selection is a suboptimal form of beamforming , it has the advantage that the optimization is tractable and can be implemented with only a few bits of feedback from the destination to the source and relay we compare the bit error rate of optimal antenna selection at both the source and relay to other proposed beamforming techniques and propose methods for performing the necessary limited feedback
this paper has been divided into three papers arxiv 0809 3232 , arxiv 0808 4134 , arxiv cs 0607105
the lambek calculus can be considered as a version of non commutative intuitionistic linear logic one of the interesting features of the lambek calculus is the so called lambek 's restriction , that is , the antecedent of any provable sequent should be non empty in this paper we discuss ways of extending the lambek calculus with the linear logic exponential modality while keeping lambek 's restriction interestingly enough , we show that for any system equipped with a reasonable exponential modality the following holds if the system enjoys cut elimination and substitution to the full extent , then the system necessarily violates lambek 's restriction nevertheless , we show that two of the three conditions can be implemented namely , we design a system with lambek 's restriction and cut elimination and another system with lambek 's restriction and substitution for both calculi we prove that they are undecidable , even if we take only one of the two divisions provided by the lambek calculus the system with cut elimination and substitution and without lambek 's restriction is folklore and known to be undecidable
motion behaviors of a rigid body can be characterized by a 6 dimensional motion trajectory , which contains position vectors of a reference point on the rigid body and rotations of this rigid body over time this paper devises a rotation and relative velocity \( rrv \) descriptor by exploring the local translational and rotational invariants of motion trajectories of rigid bodies , which is insensitive to noise , invariant to rigid transformation and scaling a flexible metric is also introduced to measure the distance between two rrv descriptors the rrv descriptor is then applied to characterize motions of a human body skeleton modeled as articulated interconnections of multiple rigid bodies to illustrate the descriptive ability of the rrv descriptor , we explore it for different rigid body motion recognition tasks the experimental results on benchmark datasets demonstrate that this simple rrv descriptor outperforms the previous ones regarding recognition accuracy without increasing computational cost
the network , the nodes of which are concepts \( people 's names , companies' names , etc \) , extracted from web publications , is considered a working algorithm of extracting such concepts is presented edges of the network under consideration refer to the reference frequency which depends on the fact how many times the concepts , which correspond to the nodes , are mentioned in the same documents web documents being published within a period of time together form an information flow , which defines the dynamics of the network studied the phenomenon of its structure stability , when the number of web publications , constituting its formation bases , increases , is discussed
we study the problem of rewriting an ontology o1 expressed in a dl l1 into an ontology o2 in a horn dl l2 such that o1 and o2 are equisatisfiable when extended with an arbitrary dataset ontologies that admit such rewritings are amenable to reasoning techniques ensuring tractability in data complexity after showing undecidability whenever l1 extends alcf , we focus on devising efficiently checkable conditions that ensure existence of a horn rewriting by lifting existing techniques for rewriting disjunctive datalog programs into plain datalog to the case of arbitrary first order programs with function symbols , we identify a class of ontologies that admit horn rewritings of polynomial size our experiments indicate that many real world ontologies satisfy our sufficient conditions and thus admit polynomial horn rewritings
multidimensional scaling \( mds \) is a class of projective algorithms traditionally used in euclidean space to produce two or three dimensional visualizations of datasets of multidimensional points or point distances more recently however , several authors have pointed out that for certain datasets , hyperbolic target space may provide a better fit than euclidean space in this paper we develop pd mds , a metric mds algorithm designed specifically for the poincare disk \( pd \) model of the hyperbolic plane emphasizing the importance of proceeding from first principles in spite of the availability of various black box optimizers , our construction is based on an elementary hyperbolic line search and reveals numerous particulars that need to be carefully addressed when implementing this as well as more sophisticated iterative optimization methods in a hyperbolic space model
we consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility in these environments , agents must learn communication protocols in order to share information that is needed to solve the tasks by embracing deep neural networks , we are able to demonstrate end to end learning of protocols in complex environments inspired by communication riddles and multi agent computer vision problems with partial observability we propose two approaches for learning in these domains reinforced inter agent learning \( rial \) and differentiable inter agent learning \( dial \) the former uses deep q learning , while the latter exploits the fact that , during learning , agents can propagate error derivatives through \( noisy \) communication channels hence , this approach uses centralised learning but decentralised execution our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains
one of the key open problems in network information theory is to obtain the capacity region for the two user interference channel \( ic \) in this paper , new results are derived for this channel as a first result , a noisy interference regime is given for the general ic where the sum rate capacity is achieved by treating interference as noise at the receivers to obtain this result , a single letter outer bound in terms of some auxiliary random variables is first established for the sum rate capacity of the general ic and then those conditions under which this outer bound is reduced to the achievable sum rate given by the simple treating interference as noise strategy are specified the main benefit of this approach is that it is applicable for any two user ic \( potentially non gaussian \) for the special case of gaussian channel , our result is reduced to the noisy interference regime that was previously obtained next , some results are given on the han kobayashi \( hk \) achievable rate region the evaluation of this rate region is in general difficult in this paper , a simple characterization of the hk rate region is derived for some special cases , specifically , for a novel very weak interference regime as a remarkable characteristic , it is shown that for this very weak interference regime , the achievable sum rate due to the hk region is identical to the one given by the simple treating interference as noise strategy
we present a formal model of human decision making in explore exploit tasks using the context of multi armed bandit problems , where the decision maker must choose among multiple options with uncertain rewards we address the standard multi armed bandit problem , the multi armed bandit problem with transition costs , and the multi armed bandit problem on graphs we focus on the case of gaussian rewards in a setting where the decision maker uses bayesian inference to estimate the reward values we model the decision maker 's prior knowledge with the bayesian prior on the mean reward we develop the upper credible limit \( ucl \) algorithm for the standard multi armed bandit problem and show that this deterministic algorithm achieves logarithmic cumulative expected regret , which is optimal performance for uninformative priors we show how good priors and good assumptions on the correlation structure among arms can greatly enhance decision making performance , even over short time horizons we extend to the stochastic ucl algorithm and draw several connections to human decision making behavior we present empirical data from human experiments and show that human performance is efficiently captured by the stochastic ucl algorithm with appropriate parameters for the multi armed bandit problem with transition costs and the multi armed bandit problem on graphs , we generalize the ucl algorithm to the block ucl algorithm and the graphical block ucl algorithm , respectively we show that these algorithms also achieve logarithmic cumulative expected regret and require a sub logarithmic expected number of transitions among arms we further illustrate the performance of these algorithms with numerical examples
this article proposes a novel iterative algorithm based on low density parity check \( ldpc \) codes for compression of correlated sources at rates approaching the slepian wolf bound the setup considered in the article looks at the problem of compressing one source at a rate determined based on the knowledge of the mean source correlation at the encoder , and employing the other correlated source as side information at the decoder which decompresses the first source based on the estimates of the actual correlation we demonstrate that depending on the extent of the actual source correlation estimated through an iterative paradigm , significant compression can be obtained relative to the case the decoder does not use the implicit knowledge of the existence of correlation
in intrusion detection systems , classifiers still suffer from several drawbacks such as data dimensionality and dominance , different network feature types , and data impact on the classification in this paper two significant enhancements are presented to solve these drawbacks the first enhancement is an improved feature selection using sequential backward search and information gain this , in turn , extracts valuable features that enhance positively the detection rate and reduce the false positive rate the second enhancement is transferring nominal network features to numeric ones by exploiting the discrete random variable and the probability mass function to solve the problem of different feature types , the problem of data dominance , and data impact on the classification the latter is combined to known normalization methods to achieve a significant hybrid normalization approach finally , an intensive and comparative study approves the efficiency of these enhancements and shows better performance comparing to other proposed methods
motivated by streaming applications with stringent delay constraints , we consider the design of online network coding algorithms with timely delivery guarantees assuming that the sender is providing the same data to multiple receivers over independent packet erasure channels , we focus on the case of perfect feedback and heterogeneous erasure probabilities based on a general analytical framework for evaluating the decoding delay , we show that existing arq schemes fail to ensure that receivers with weak channels are able to recover from packet losses within reasonable time to overcome this problem , we re define the encoding rules in order to break the chains of linear combinations that cannot be decoded after one of the packets is lost our results show that sending uncoded packets at key times ensures that all the receivers are able to meet specific delay requirements with very high probability
daligault , rao and thomass 'e proposed in 2010 a fascinating conjecture connecting two seemingly unrelated notions clique width and well quasi ordering they asked if the clique width of graphs in a hereditary class which is well quasi ordered under labelled induced subgraphs is bounded by a constant this is equivalent to asking whether every hereditary class of unbounded clique width has a labelled infinite antichain we believe the answer to this question is positive and propose a stronger conjecture stating that every minimal hereditary class of graphs of unbounded clique width has a canonical labelled infinite antichain to date , only two hereditary classes are known to be minimal with respect to clique width and each of them is known to contain a canonical antichain in the present paper , we discover two more minimal hereditary classes of unbounded clique width and show that both of them contain canonical antichains
mimo processing techniques in fiber optical communications have been proposed as a promising approach to meet increasing demand for information throughput in this context , the multiple channels correspond to the multiple modes and or multiple cores in the fiber in this paper we characterize the distribution of the mutual information with gaussian input in a simple channel model for this system assuming significant cross talk between cores , negligible backscattering and near lossless propagation in the fiber , we model the transmission channel as a random complex unitary matrix the loss in the transmission may be parameterized by a number of unutilized channels in the fiber we analyze the system in a dual fashion first , we evaluate a closed form expression for the outage probability , which is handy for small matrices we also apply the asymptotic approach , in particular the coulomb gas method from statistical mechanics , to obtain closed form results for the ergodic mutual information , its variance as well as the outage probability for gaussian input in the limit of large number of cores modes by comparing our analytic results to simulations , we see that , despite the fact that this method is nominally valid for large number of modes , our method is quite accurate even for small to modest number of channels
wildfires are frequent , devastating events in australia that regularly cause significant loss of life and widespread property damage fire weather indices are a widely adopted method for measuring fire danger and they play a significant role in issuing bushfire warnings and in anticipating demand for bushfire management resources existing systems that calculate fire weather indices are limited due to low spatial and temporal resolution localized wireless sensor networks , on the other hand , gather continuous sensor data measuring variables such as air temperature , relative humidity , rainfall and wind speed at high resolutions however , using wireless sensor networks to estimate fire weather indices is a challenge due to data quality issues , lack of standard data formats and lack of agreement on thresholds and methods for calculating fire weather indices within the scope of this paper , we propose a standardized approach to calculating fire weather indices \( a k a fire danger ratings \) and overcome a number of the challenges by applying semantic web technologies to the processing of data streams from a wireless sensor network deployed in the springbrook region of south east queensland this paper describes the underlying ontologies , the semantic reasoning and the semantic fire weather index \( sfwi \) system that we have developed to enable domain experts to specify and adapt rules for calculating fire weather indices we also describe the web based mapping interface that we have developed , that enables users to improve their understanding of how fire weather indices vary over time within a particular region finally , we discuss our evaluation results that indicate that the proposed system outperforms state of the art techniques in terms of accuracy , precision and query performance
this tutorial provides an overview of and introduction to rissanen 's minimum description length \( mdl \) principle the first chapter provides a conceptual , entirely non technical introduction to the subject it serves as a basis for the technical introduction given in the second chapter , in which all the ideas of the first chapter are made mathematically precise the main ideas are discussed in great conceptual and technical detail this tutorial is an extended version of the first two chapters of the collection advances in minimum description length theory and application \( edited by p grunwald , i j myung and m pitt , to be published by the mit press , spring 2005 \)
this paper addresses the following foundational question what is the maximum theoretical delay performance achievable by an overlay peer to peer streaming system where the streamed content is subdivided into chunks \? as shown in this paper , when posed for chunk based systems , and as a consequence of the store and forward way in which chunks are delivered across the network , this question has a fundamentally different answer with respect to the case of systems where the streamed content is distributed through one or more flows \( sub streams \) to circumvent the complexity emerging when directly dealing with delay , we express performance in term of a convenient metric , called stream diffusion metric we show that it is directly related to the end to end minimum delay achievable in a p2p streaming network in a homogeneous scenario , we derive a performance bound for such metric , and we show how this bound relates to two fundamental parameters the upload bandwidth available at each node , and the number of neighbors a node may deliver chunks to in this bound , k step fibonacci sequences do emerge , and appear to set the fundamental laws that characterize the optimal operation of chunk based systems
in this paper we study the problem of answering cloze style questions over short documents we introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader this enables the reader to build query specific representations of tokens in the document which are further used for answer selection our model , the gated attention reader , outperforms all state of the art models on several large scale benchmark datasets for this task the cnn dailymail news stories and children 's book test we also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features the analysis sheds light on the strengths and weaknesses of several existing models
how should you choose a good set of \( say \) 48 planes in four dimensions \? more generally , how do you find packings in grassmannian spaces \? in this article i give a brief introduction to the work that i have been doing on this problem in collaboration with a r calderbank , j h conway , r h hardin , e m rains and p w shor we have found many nice examples of specific packings \( 70 4 spaces in 8 space , for instance \) , several general constructions , and an embedding theorem which shows that a packing in grassmannian space g \( m , n \) is a subset of a sphere in r d , where d \( m 2 \) \( m 1 \) 2 , and leads to a proof that many of our packings are optimal there are a number of interesting unsolved problems
we introduce utility directed procedures for mediating the flow of potentially distracting alerts and communications to computer users we present models and inference procedures that balance the context sensitive costs of deferring alerts with the cost of interruption we describe the challenge of reasoning about such costs under uncertainty via an analysis of user activity and the content of notifications after introducing principles of attention sensitive alerting , we focus on the problem of guiding alerts about email messages we dwell on the problem of inferring the expected criticality of email and discuss work on the priorities system , centering on prioritizing email by criticality and modulating the communication of notifications to users about the presence and nature of incoming email
this document collects the lecture notes from my course communication complexity \( for algorithm designers \) , '' taught at stanford in the winter quarter of 2015 the two primary goals of the course are 1 learn several canonical problems that have proved the most useful for proving lower bounds \( disjointness , index , gap hamming , etc \) 2 learn how to reduce lower bounds for fundamental algorithmic problems to communication complexity lower bounds along the way , we 'll also 3 get exposure to lots of cool computational models and some famous results about them data streams and linear sketches , compressive sensing , space query time trade offs in data structures , sublinear time algorithms , and the extension complexity of linear programs 4 scratch the surface of techniques for proving communication complexity lower bounds \( fooling sets , corruption bounds , etc \)
we investigate the issues of inductive problem solving and learning by doxastic agents we provide topological characterizations of solvability and learnability , and we use them to prove that agm style belief revision is universal , i e , that every solvable problem is solvable by agm conditioning
in this paper , we discuss our research towards developing special properties that introduce autonomic behavior in pattern recognition systems in our approach we use assl \( autonomic system specification language \) to formally develop such properties for dmarf \( distributed modular audio recognition framework \) these properties enhance dmarf with an autonomic middleware that manages the four stages of the framework 's pattern recognition pipeline dmarf is a biologically inspired system employing pattern recognition , signal processing , and natural language processing helping us process audio , textual , or imagery data needed by a variety of scientific applications , e g , biometric applications in that context , the notion go autonomic dmarf \( admarf \) can be employed by autonomous and robotic systems that theoretically require less to none human intervention other than data collection for pattern analysis and observing the results in this article , we explain the assl specification models for the autonomic properties of dmarf
as two major players in terrestrial wireless communications , wi fi systems and cellular networks have different origins and have largely evolved separately motivated by the exponentially increasing wireless data demand , cellular networks are evolving towards a heterogeneous and small cell network architecture , wherein small cells are expected to provide very high capacity however , due to the limited licensed spectrum for cellular networks , any effort to achieve capacity growth through network densification will face the challenge of severe inter cell interference in view of this , recent standardization developments have started to consider the opportunities for cellular networks to use the unlicensed spectrum bands , including the 2 4 ghz and 5 ghz bands that are currently used by wi fi , zigbee and some other communication systems in this article , we look into the coexistence of wi fi and 4g cellular networks sharing the unlicensed spectrum we introduce a network architecture where small cells use the same unlicensed spectrum that wi fi systems operate in without affecting the performance of wi fi systems we present an almost blank subframe \( abs \) scheme without priority to mitigate the co channel interference from small cells to wi fi systems , and propose an interference avoidance scheme based on small cells estimating the density of nearby wi fi access points to facilitate their coexistence while sharing the same unlicensed spectrum simulation results show that the proposed network architecture and interference avoidance schemes can significantly increase the capacity of 4g heterogeneous cellular networks while maintaining the service quality of wi fi systems
this paper answers open questions about the correctness and the completeness of dart zobel algorithm for testing the inclusion relation between two regular types we show that the algorithm is incorrect for regular types we also prove that the algorithm is complete for regular types as well as correct for tuple distributive regular types also presented is a simplified version of dart zobel algorithm for tuple distributive regular types
data compression has become a necessity not only the in the field of communication but also in various scientific experiments the data that is being received is more and the processing time required has also become more a significant change in the algorithms will help to optimize the processing speed with the invention of technologies like iot and in technologies like machine learning there is a need to compress data for example training an artificial neural network requires a lot of data that should be processed and trained in small interval of time for which compression will be very helpful there is a need to process the data faster and quicker in this paper we present a method that reduces the data size this method is known as optimized huffmans coding in the huffmans coding we encode the messages so as to reduce the data and here in the optimized huffmans coding we compress the data to a great extent which helps in various signal processing algorithms and has advantages in many applications here in this paper we have presented the optimized huffmans coding method for text compression this method but has advantages over the normal huffmans coding this algorithm presented here says that every letter can be grouped together and encoded which not only reduces the size but also the huffmans tree data that is required for decoding , hence reducing the data size this method has huge scientific applications
time series classification has attracted considerable research attention due to the various domains where time series data are observed , ranging from medicine to econometrics traditionally , the focus of time series classification has been on short time series data composed of a unique pattern with intraclass pattern distortions and variations , while recently there have been attempts to focus on longer series composed of various local patterns this study presents a novel method which can detect local patterns in long time series via fitting local polynomial functions of arbitrary degrees the coefficients of the polynomial functions are converted to symbolic words via equivolume discretizations of the coefficients' distributions the symbolic polynomial words enable the detection of similar local patterns by assigning the same words to similar polynomials moreover , a histogram of the frequencies of the words is constructed from each time series' bag of words each row of the histogram enables a new representation for the series and symbolize the existence of local patterns and their frequencies experimental evidence demonstrates outstanding results of our method compared to the state of art baselines , by exhibiting the best classification accuracies in all the datasets and having statistically significant improvements in the absolute majority of experiments
non adaptive group testing involves grouping arbitrary subsets of n items into different pools each pool is then tested and defective items are identified a fundamental question involves minimizing the number of pools required to identify at most d defective items motivated by applications in network tomography , sensor networks and infection propagation , a variation of group testing problems on graphs is formulated unlike conventional group testing problems , each group here must conform to the constraints imposed by a graph for instance , items can be associated with vertices and each pool is any set of nodes that must be path connected in this paper , a test is associated with a random walk in this context , conventional group testing corresponds to the special case of a complete graph on n vertices for interesting classes of graphs a rather surprising result is obtained , namely , that the number of tests required to identify d defective items is substantially similar to what is required in conventional group testing problems , where no such constraints on pooling is imposed specifically , if t \( n \) corresponds to the mixing time of the graph g , it is shown that with m o \( d 2t 2 \( n \) log \( n d \) \) non adaptive tests , one can identify the defective items consequently , for the erdos renyi random graph g \( n , p \) , as well as expander graphs with constant spectral gap , it follows that m o \( d 2 log 3n \) non adaptive tests are sufficient to identify d defective items next , a specific scenario is considered that arises in network tomography , for which it is shown that m o \( d 3 log 3n \) non adaptive tests are sufficient to identify d defective items noisy counterparts of the graph constrained group testing problem are considered , for which parallel results are developed we also briefly discuss extensions to compressive sensing on graphs
in network data aggregation in wireless sensor networks \( wsns \) provides efficient bandwidth utilization and energy efficient computing supporting efficient in network data aggregation while preserving the privacy of the data of individual sensor nodes has emerged as an important requirement in numerous wsn applications for privacy preserving data aggregation in wsns , he et al \( infocom 2007 \) have proposed a cluster based private data aggregation \( cpda \) that uses a clustering protocol and a well known key distribution scheme for computing an additive aggregation function in a privacy preserving manner in spite of the wide popularity of cpda , it has been observed that the protocol is not secure and it is also possible to enhance its efficiency in this paper , we first identify a security vulnerability in the existing cpda scheme , wherein we show how a malicious participant node can launch an attack on the privacy protocol so as to get access to the private data of its neighboring sensor nodes next it is shown how the existing cpda scheme can be made more efficient by suitable modification of the protocol further , suitable modifications in the existing protocol have been proposed so as to plug the vulnerability of the protocol
the capacity region of a multiple input multiple output interference channel \( mimo ic \) where the channel matrices are square and invertible is studied the capacity region for strong interference is established where the definition of strong interference parallels that of scalar channels moreover , the sum rate capacity for z interference , noisy interference , and mixed interference is established these results generalize known results for the scalar gaussian ic
this paper discusses efsm for sdl and transforms efsm into a novel control model of discrete event systems we firstly propose a control model of discrete event systems , where the event set is made up of several conflicting pairs and control is implemented to select one event of the pair then we transform efsm for sdl to the control model to clarify the control mechanism functioning in sdl flow graphs this work views the efsm for sdl in the perspective of supervisory control theory , and this contributes to the field of software cybernetics , which explores the theoretically justified interplay of software and the control
as we add rigid bars between points in the plane , at what point is there a giant \( linear sized \) rigid component , which can be rotated and translated , but which has no internal flexibility \? if the points are generic , this depends only on the combinatorics of the graph formed by the bars we show that if this graph is an erdos renyi random graph g \( n , c n \) , then there exists a sharp threshold for a giant rigid component to emerge for c c 2 , w h p all rigid components span one , two , or three vertices , and when c c 2 , w h p there is a giant rigid component the constant c 2 approx 3 588 is the threshold for 2 orientability , discovered independently by fernholz and ramachandran and cain , sanders , and wormald in soda'07 we also give quantitative bounds on the size of the giant rigid component when it emerges , proving that it spans a \( 1 o \( 1 \) \) fraction of the vertices in the \( 3 2 \) core informally , the \( 3 2 \) core is maximal induced subgraph obtained by starting from the 3 core and then inductively adding vertices with 2 neighbors in the graph obtained so far
we consider scheduled message communication over a discrete memoryless degraded broadcast channel the framework we consider here models both the random message arrivals and the subsequent reliable communication by suitably combining techniques from queueing theory and information theory the channel from the transmitter to each of the receivers is quasi static , flat , and with independent fades across the receivers requests for message transmissions are assumed to arrive according to an i i d arrival process then , \( i \) we derive an outer bound to the region of message arrival vectors achievable by the class of stationary scheduling policies , \( ii \) we show for any message arrival vector that satisfies the outerbound , that there exists a stationary ``state independent'' policy that results in a stable system for the corresponding message arrival process , and \( iii \) under two asymptotic regimes , we show that the stability region of nat arrival rate vectors has information theoretic capacity region interpretation
approximate message passing \( amp \) has been shown to be a superior method for inference problems , such as the recovery of signals from sets of noisy , lower dimensionality measurements , both in terms of reconstruction accuracy and in computational efficiency however , amp suffers from serious convergence issues in contexts that do not exactly match its assumptions we propose a new approach to stabilizing amp in these contexts by applying amp updates to individual coefficients rather than in parallel our results show that this change to the amp iteration can provide theoretically expected , but hitherto unobtainable , performance for problems on which the standard amp iteration diverges additionally , we find that the computational costs of this swept coefficient update scheme is not unduly burdensome , allowing it to be applied efficiently to signals of large dimensionality
this paper studies the structure of downlink sum rate maximizing selective decentralized feedback policies for opportunistic beamforming under finite feedback constraints on the average number of mobile users feeding back firstly , it is shown that any sum rate maximizing selective decentralized feedback policy must be a threshold feedback policy this result holds for all fading channel models with continuous distribution functions secondly , the resulting optimum threshold selection problem is analyzed in detail this is a non convex optimization problem over finite dimensional euclidean spaces by utilizing the theory of majorization , an underlying schur concave structure in the sum rate function is identified , and the sufficient conditions for the optimality of homogenous threshold feedback policies are obtained applications of these results are illustrated for well known fading channel models such as rayleigh , nakagami and rician fading channels , along with various engineering and design insights rather surprisingly , it is shown that using the same threshold value at all mobile users is not always a rate wise optimal feedback strategy , even for a network with identical mobile users experiencing statistically the same channel conditions for the rayleigh fading channel model , on the other hand , homogenous threshold feedback policies are proven to be rate wise optimal if multiple orthonormal data carrying beams are used to communicate with multiple mobile users simultaneously
in this paper we study emph many to one boundary labeling with backbone leaders in this new many to one model , a horizontal backbone reaches out of each label into the feature enclosing rectangle feature points that need to be connected to this label are linked via vertical line segments to the backbone we present dynamic programming algorithms for label number and total leader length minimization of crossing free backbone labelings when crossings are allowed , we aim to obtain solutions with the minimum number of crossings this can be achieved efficiently in the case of fixed label order , however , in the case of flexible label order we show that minimizing the number of leader crossings is np hard
in heterogeneous cellular networks \( hcns \) , it is desirable to offload mobile users to small cells , which are typically significantly less congested than the macrocells to achieve sufficient load balancing , the offloaded users often have much lower sinr than they would on the macrocell this sinr degradation can be partially alleviated through interference avoidance , for example time or frequency resource partitioning , whereby the macrocell turns off in some fraction of such resources naturally , the optimal offloading strategy is tightly coupled with resource partitioning the optimal amount of which in turn depends on how many users have been offloaded in this paper , we propose a general and tractable framework for modeling and analyzing joint resource partitioning and offloading in a two tier cellular network with it , we are able to derive the downlink rate distribution over the entire network , and an optimal strategy for joint resource partitioning and offloading we show that load balancing , by itself , is insufficient , and resource partitioning is required in conjunction with offloading to improve the rate of cell edge users in co channel heterogeneous networks
we introduce a cost sharing framework for ridesharing that explicitly takes into account the inconvenience costs of passengers due to detours we introduce the notion of sequential individual rationality \( sir \) that requires that the disutility of existing passengers is non increasing as additional passengers are picked up , and show that these constraints induce a natural limit on the incremental detours permissible as the ride progresses we provide an exact characterization of all routes for which there exists some cost sharing scheme that is sir on that route , and under these constraints , for realistic scenarios , we also show a theta \( sqrt n \) upper bound and a theta \( log n \) lower bound on the total detour experienced by the passengers as a fraction of the direct distance to their destination next , we observe that under any budget balanced cost sharing scheme that is sir on a route , the total amount by which the passengers' disutilities decrease \( which can be viewed as the total incremental benefit due to ridesharing \) is a constant this observation inspires a dual notion of viewing cost sharing schemes as benefit sharing schemes , under which we introduce a natural definition of sequential fairness the total incremental benefit due to the addition of a new passenger is \( partly \) shared among the existing passengers in proportion to the incremental inconvenience costs they suffer we then provide an exact characterization of sequentially fair cost sharing schemes , which brings out several useful structural properties , including a strong requirement that passengers must compensate each other for the detour inconveniences that they cause finally , we conclude with an extended discussion of new algorithmic problems related to and motivated by sir , and important future work
in this paper , we prove that every planar 4 connected graph has a cz representation a string representation using paths in a rectangular grid that contain at most one vertical segment furthermore , two paths representing vertices u , v intersect precisely once whenever there is an edge between u and v the required size of the grid is n times 2n
an information reconciliation method for continuous variable quantum key distribution with gaussian modulation that is based on non binary low density parity check \( ldpc \) codes is presented sets of regular and irregular ldpc codes with different code rates over the galois fields gf \( 8 \) , gf \( 16 \) , gf \( 32 \) , and gf \( 64 \) have been constructed we have performed simulations to analyze the efficiency and the frame error rate using the sum product algorithm the proposed method achieves an efficiency between 0 94 and 0 98 if the signal to noise ratio is between 4 db and 24 db
the capacity regions of multiple input multiple output gaussian z interference channels are established for the very strong interference and aligned strong interference cases the sum rate capacity of such channels is established under noisy interference these results generalize known results for scalar gaussian z interference channels
we introduce a novel technique for verification and model synthesis of sequential programs our technique is based on learning a regular model of the set of feasible paths in a program , and testing whether this model contains an incorrect behavior exact learning algorithms require checking equivalence between the model and the program , which is a difficult problem , in general undecidable our learning procedure is therefore based on the framework of probably approximately correct \( pac \) learning , which uses sampling instead and provides correctness guarantees expressed using the terms error probability and confidence besides the verification result , our procedure also outputs the model with the said correctness guarantees obtained preliminary experiments show encouraging results , in some cases even outperforming mature software verifiers
structural operational semantics can be studied at the general level of distributive laws of syntax over behaviour this yields specification formats for well behaved algebraic operations on final coalgebras , which are a domain for the behaviour of all systems of a given type functor we introduce a format for specification of algebraic operations that restrict to the rational fixpoint of a functor , which captures the behaviour of finite systems in other words , we show that rational behaviour is closed under operations specified in our format as applications we consider operations on regular languages , regular processes and finite weighted transition systems
distributed source coding is traditionally viewed in the block coding context all the source symbols are known in advance at the encoders this paper instead considers a streaming setting in which iid source symbol pairs are revealed to the separate encoders in real time and need to be reconstructed at the decoder with some tolerable end to end delay using finite rate noiseless channels a sequential random binning argument is used to derive a lower bound on the error exponent with delay and show that both ml decoding and universal decoding achieve the same positive error exponents inside the traditional slepian wolf rate region the error events are different from the block coding error events and give rise to slightly different exponents because the sequential random binning scheme is also universal over delays , the resulting code eventually reconstructs every source symbol correctly with probability 1
electricity theft is a major problem around the world in both developed and developing countries and may range up to 40 of the total electricity distributed more generally , electricity theft belongs to non technical losses \( ntl \) , which are losses that occur during the distribution of electricity in power grids in this paper , we build features from the neighborhood of customers we first split the area in which the customers are located into grids of different sizes for each grid cell we then compute the proportion of inspected customers and the proportion of ntl found among the inspected customers we then analyze the distributions of features generated and show why they are useful to predict ntl in addition , we compute features from the consumption time series of customers we also use master data features of customers , such as their customer class and voltage of their connection we compute these features for a big data base of 31m meter readings , 700k customers and 400k inspection results we then use these features to train four machine learning algorithms that are particularly suitable for big data sets because of their parallelizable structure logistic regression , k nearest neighbors , linear support vector machine and random forest using the neighborhood features instead of only analyzing the time series has resulted in appreciable results for big data sets for varying ntl proportions of 1 90 this work can therefore be deployed to a wide range of different regions around the world
one of the most widely used methods for community detection in networks is the maximization of the quality function known as modularity of the many maximization techniques that have been used in this context , some of the most conceptually attractive are the spectral methods , which are based on the eigenvectors of the modularity matrix spectral algorithms have , however , been limited by and large to the division of networks into only two or three communities , with divisions into more than three being achieved by repeated two way division here we present a spectral algorithm that can directly divide a network into any number of communities the algorithm makes use of a mapping from modularity maximization to a vector partitioning problem , combined with a fast heuristic for vector partitioning we compare the performance of this spectral algorithm with previous approaches and find it to give superior results , particularly in cases where community sizes are unbalanced we also give demonstrative applications of the algorithm to two real world networks and find that it produces results in good agreement with expectations for the networks studied
in this paper we propose an approach to preference elicitation that is suitable to large configuration spaces beyond the reach of existing state of the art approaches our setwise max margin method can be viewed as a generalization of max margin learning to sets , and can produce a set of diverse items that can be used to ask informative queries to the user moreover , the approach can encourage sparsity in the parameter space , in order to favor the assessment of utility towards combinations of weights that concentrate on just few features we present a mixed integer linear programming formulation and show how our approach compares favourably with bayesian preference elicitation alternatives and easily scales to realistic datasets
this note deals with a class of variables that , if conditioned on , tends to amplify confounding bias in the analysis of causal effects this class , independently discovered by bhattacharya and vogt \( 2007 \) and wooldridge \( 2009 \) , includes instrumental variables and variables that have greater influence on treatment selection than on the outcome we offer a simple derivation and an intuitive explanation of this phenomenon and then extend the analysis to non linear models we show that 1 the bias amplifying potential of instrumental variables extends over to non linear models , though not as sweepingly as in linear models 2 in non linear models , conditioning on instrumental variables may introduce new bias where none existed before 3 in both linear and non linear models , instrumental variables have no effect on selection induced bias
the purported black box nature of neural networks is a barrier to adoption in applications where interpretability is essential here we present deeplift \( learning important features \) , an efficient and effective method for computing importance scores in a neural network deeplift compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference we apply deeplift to models trained on natural images and genomic data , and show significant advantages over gradient based methods
recently , jouve et al \( a a , 2008 \) published the paper that presents the numerical benchmark for the solar dynamo models here , i would like to show a way how to get it with help of computer algebra system maxima this way was used in our paper \( pipin seehafer , a a 2008 , in print \) to test some new ideas in the large scale stellar dynamos in the present paper i complement the dynamo benchmark with the standard test that address the problem of the free decay modes in the sphere which is submerged in vacuum
this paper studies the positioning problem based on two way time of arrival \( tw toa \) measurements in asynchronous wireless sensor networks since the optimal estimator for this problem involves difficult nonconvex optimization , we propose two suboptimal estimators based on squared range least squares and least absolute mean of residual errors the former approach is formulated as a general trust region subproblem which can be solved exactly under mild conditions the latter approach is formulated as a difference of convex functions programming \( dcp \) , which can be solved using a concave convex procedure simulation results illustrate the high performance of the proposed techniques , especially for the dcp approach
evaluating a boolean conjunctive query q against a guarded first order theory f is equivalent to checking whether f and not q is unsatisfiable this problem is relevant to the areas of database theory and description logic since q may not be guarded , well known results about the decidability , complexity , and finite model property of the guarded fragment do not obviously carry over to conjunctive query answering over guarded theories , and had been left open in general by investigating finite guarded bisimilar covers of hypergraphs and relational structures , and by substantially generalising rosati 's finite chase , we prove for guarded theories f and \( unions of \) conjunctive queries q that \( i \) q is true in each model of f iff q is true in each finite model of f and \( ii \) determining whether f implies q is 2exptime complete we further show the following results \( iii \) the existence of polynomial size conformal covers of arbitrary hypergraphs \( iv \) a new proof of the finite model property of the clique guarded fragment \( v \) the small model property of the guarded fragment with optimal bounds \( vi \) a polynomial time solution to the canonisation problem modulo guarded bisimulation , which yields \( vii \) a capturing result for guarded bisimulation invariant ptime
in this paper we have focused a variety of techniques , approaches and different areas of the research which are helpful and marked as the important field of data mining technologies as we are aware that many multinational companies and large organizations are operated in different places of the different countries each place of operation may generate large volumes of data corporate decision makers require access from all such sources and take strategic decisions the data warehouse is used in the significant business value by improving the effectiveness of managerial decision making in an uncertain and highly competitive business environment , the value of strategic information systems such as these are easily recognized however in todays business environment , efficiency or speed is not the only key for competitiveness this type of huge amount of data are available in the form of tera topeta bytes which has drastically changed in the areas of science and engineering to analyze , manage and make a decision of such type of huge amount of data we need techniques called the data mining which will transforming in many fields this paper imparts more number of applications of the data mining and also focuses scope of the data mining which will helpful in the further research
ge and stefankovic have recently introduced a novel two variable graph polynomial when specialised to a bipartite graphs g and evaluated at the point \( 1 2 , 1 \) this polynomial gives the number of independent sets in the graph inspired by this polynomial , they also introduced a markov chain which , if rapidly mixing , would provide an efficient sampling procedure for independent sets in g this sampling procedure in turn would imply the existence of efficient approximation algorithms for a number of significant counting problems whose complexity is so far unresolved the proposed markov chain is promising , in the sense that it overcomes the most obvious barrier to mixing however , we show here , by exhibiting a sequence of counterexamples , that the mixing time of their markov chain is exponential in the size of the input when the input is chosen from a particular infinite family of bipartite graphs
the noise model of deletions poses significant challenges in coding theory , with basic questions like the capacity of the binary deletion channel still being open in this paper , we study the harder model of worst case deletions , with a focus on constructing efficiently decodable codes for the two extreme regimes of high noise and high rate specifically , we construct polynomial time decodable codes with the following trade offs \( for any eps 0 \) \( 1 \) codes that can correct a fraction 1 eps of deletions with rate poly \( eps \) over an alphabet of size poly \( 1 eps \) \( 2 \) binary codes of rate 1 o \( sqrt \( eps \) \) that can correct a fraction eps of deletions and \( 3 \) binary codes that can be list decoded from a fraction \( 1 2 eps \) of deletions with rate poly \( eps \) our work is the first to achieve the qualitative goals of correcting a deletion fraction approaching 1 over bounded alphabets , and correcting a constant fraction of bit deletions with rate aproaching 1 the above results bring our understanding of deletion code constructions in these regimes to a similar level as worst case errors
fuzzy logic programming is a growing declarative paradigm aiming to integrate fuzzy logic into logic programming one of the most difficult tasks when specifying a fuzzy logic program is determining the right weights for each rule , as well as the most appropriate fuzzy connectives and operators in this paper , we introduce a symbolic extension of fuzzy logic programs in which some of these parameters can be left unknown , so that the user can easily see the impact of their possible values furthermore , given a number of test cases , the most appropriate values for these parameters can be automatically computed
to meet the growing spectrum demands , future cellular systems are expected to share the spectrum of other services such as radar in this paper , we consider a network multiple input multiple output \( mimo \) with partial cooperation model where radar stations cooperate with cellular base stations \( bs \) s to deliver messages to intended mobile users so the radar stations act as bss in the cellular system however , due to the high power transmitted by radar stations for detection of far targets , the cellular receivers could burnout when receiving these high radar powers therefore , we propose a new projection method called small singular values space projection \( ssvsp \) to mitigate these harmful high power and enable radar stations to collaborate with cellular base stations in addition , we formulate the problem into a mimo interference channel with general constraints \( mimo ifc gc \) finally , we provide a solution to minimize the weighted sum mean square error minimization problem \( wsmmse \) with enforcing power constraints on both radar and cellular stations
departing from traditional communication theory where decoding algorithms are assumed to perform without error , a system where noise perturbs both computational devices and communication channels is considered here this paper studies limits in processing noisy signals with noisy circuits by investigating the effect of noise on standard iterative decoders for low density parity check codes concentration of decoding performance around its average is shown to hold when noise is introduced into message passing and local computation density evolution equations for simple faulty iterative decoders are derived in one model , computing nonlinear estimation thresholds shows that performance degrades smoothly as decoder noise increases , but arbitrarily small probability of error is not achievable probability of error may be driven to zero in another system model the decoding threshold again decreases smoothly with decoder noise as an application of the methods developed , an achievability result for reliable memory systems constructed from unreliable components is provided
biometrics are an important and widely used class of methods for identity verification and access control biometrics are attractive because they are inherent properties of an individual they need not be remembered like passwords , and are not easily lost or forged like identifying documents at the same time , bio metrics are fundamentally noisy and irreplaceable there are always slight variations among the measurements of a given biometric , and , unlike passwords or identification numbers , biometrics are derived from physical characteristics that cannot easily be changed the proliferation of biometric usage raises critical privacy and security concerns that , due to the noisy nature of biometrics , cannot be addressed using standard cryptographic methods in this article we present an overview of secure biometrics , also referred to as biometric template protection , an emerging class of methods that address these concerns
this article deals with the spatio temporal sensors deployment in order to maximize detection probability of an intelligent and randomly moving target in an area under surveillance our work is based on the rare events simulation framework more precisely , we derive a novel stochastic optimization algorithm based on the generalized splitting method this new approach offers promising results without any state space discretization and can handle various types of constraints
in a balloon drawing of a tree , all the children under the same parent are placed on the circumference of the circle centered at their parent , and the radius of the circle centered at each node along any path from the root reflects the number of descendants associated with the node among various styles of tree drawings reported in the literature , the balloon drawing enjoys a desirable feature of displaying tree structures in a rather balanced fashion for each internal node in a balloon drawing , the ray from the node to each of its children divides the wedge accommodating the subtree rooted at the child into two sub wedges depending on whether the two sub wedge angles are required to be identical or not , a balloon drawing can further be divided into two types even sub wedge and uneven sub wedge types in the most general case , for any internal node in the tree there are two dimensions of freedom that affect the quality of a balloon drawing \( 1 \) altering the order in which the children of the node appear in the drawing , and \( 2 \) for the subtree rooted at each child of the node , flipping the two sub wedges of the subtree in this paper , we give a comprehensive complexity analysis for optimizing balloon drawings of rooted trees with respect to angular resolution , aspect ratio and standard deviation of angles under various drawing cases depending on whether the tree is of even or uneven sub wedge type and whether \( 1 \) and \( 2 \) above are allowed it turns out that some are np complete while others can be solved in polynomial time we also derive approximation algorithms for those that are intractable in general
we study the asymptotic outage performance of incremental redundancy automatic repeat request \( inr arq \) transmission over the multiple input multiple output \( mimo \) block fading channels with discrete input constellations we first show that transmission with random codes using a discrete signal constellation across all transmit antennas achieves the optimal outage diversity given by the singleton bound we then analyze the optimal snr exponent and outage diversity of inr arq transmission over the mimo block fading channel we show that a significant gain in outage diversity is obtained by providing more than one bit feedback at each arq round thus , the outage performance of inr arq transmission can be remarkably improved with minimal additional overhead a suboptimal feedback and power adaptation rule , which achieves the optimal outage diversity , is proposed for mimo inr arq , demonstrating the benefits provided by multi bit feedback
in this paper , we discuss a class of distributed detection algorithms which can be viewed as implementations of bayes' law in distributed settings some of the algorithms are proposed in the literature most recently , and others are first developed in this paper the common feature of these algorithms is that they all combine \( i \) certain kinds of consensus protocols with \( ii \) bayesian updates they are different mainly in the aspect of the type of consensus protocol and the order of the two operations after discussing their similarities and differences , we compare these distributed algorithms by numerical examples we focus on the rate at which these algorithms detect the underlying true state of an object we find that \( a \) the algorithms with consensus via geometric average is more efficient than that via arithmetic average \( b \) the order of consensus aggregation and bayesian update does not apparently influence the performance of the algorithms \( c \) the existence of communication delay dramatically slows down the rate of convergence \( d \) more communication between agents with different signal structures improves the rate of convergence
the accuracy of machine learning systems is a widely studied research topic established techniques such as cross validation predict the accuracy on unseen data of the classifier produced by applying a given learning method to a given training data set however , they do not predict whether incurring the cost of obtaining more data and undergoing further training will lead to higher accuracy in this paper we investigate techniques for making such early predictions we note that when a machine learning algorithm is presented with a training set the classifier produced , and hence its error , will depend on the characteristics of the algorithm , on training set 's size , and also on its specific composition in particular we hypothesise that if a number of classifiers are produced , and their observed error is decomposed into bias and variance terms , then although these components may behave differently , their behaviour may be predictable we test our hypothesis by building models that , given a measurement taken from the classifier created from a limited number of samples , predict the values that would be measured from the classifier produced when the full data set is presented we create separate models for bias , variance and total error our models are built from the results of applying ten different machine learning algorithms to a range of data sets , and tested with unseen algorithms and datasets we analyse the results for various numbers of initial training samples , and total dataset sizes results show that our predictions are very highly correlated with the values observed after undertaking the extra training finally we consider the more complex case where an ensemble of heterogeneous classifiers is trained , and show how we can accurately estimate an upper bound on the accuracy achievable after further training
this paper investigates the benefits of amplify and forward \( af \) relaying in the setup of multi antenna wireless networks the concept of random sequential \( rs \) relaying is previously introduced in the literature and showed that it achieves the maximum diversity gain in a general multi antenna network here , we show that random unitary matrix multiplication at the relay nodes empowers the rs scheme to achieve a better diversity multiplexing tradeoff \( dmt \) as compared to the traditional af relaying first , we study the case of a multi antenna full duplex single relay two hop network , for which we show that the rs achieves the optimum dmt applying this result , we derive a new achievable dmt for the case of multi antenna half duplex parallel relay network interestingly , it turns out that the dmt of the rs scheme is optimum for the case of multi antenna two parallel non interfering half duplex relays next , we show that random unitary matrix multiplication also improves the dmt of the non orthogonal af relaying scheme in the case of a multi antenna single relay channel finally , we study the general case of multi antenna full duplex relay networks and derive a new lower bound on its dmt using the rs scheme
coalition formation is a fundamental type of interaction that involves the creation of coherent groupings of distinct , autonomous , agents in order to efficiently achieve their individual or collective goals forming effective coalitions is a major research challenge in the field of multi agent systems central to this endeavour is the problem of determining which of the many possible coalitions to form in order to achieve some goal this usually requires calculating a value for every possible coalition , known as the coalition value , which indicates how beneficial that coalition would be if it was formed once these values are calculated , the agents usually need to find a combination of coalitions , in which every agent belongs to exactly one coalition , and by which the overall outcome of the system is maximized however , this coalition structure generation problem is extremely challenging due to the number of possible solutions that need to be examined , which grows exponentially with the number of agents involved to date , therefore , many algorithms have been proposed to solve this problem using different techniques ranging from dynamic programming , to integer programming , to stochastic search all of which suffer from major limitations relating to execution time , solution quality , and memory requirements with this in mind , we develop an anytime algorithm to solve the coalition structure generation problem specifically , the algorithm uses a novel representation of the search space , which partitions the space of possible solutions into sub spaces such that it is possible to compute upper and lower bounds on the values of the best coalition structures in them these bounds are then used to identify the sub spaces that have no potential of containing the optimal solution so that they can be pruned the algorithm , then , searches through the remaining sub spaces very efficiently using a branch and bound technique to avoid examining all the solutions within the searched subspace \( s \) in this setting , we prove that our algorithm enumerates all coalition structures efficiently by avoiding redundant and invalid solutions automatically moreover , in order to effectively test our algorithm we develop a new type of input distribution which allows us to generate more reliable benchmarks compared to the input distributions previously used in the field given this new distribution , we show that for 27 agents our algorithm is able to find solutions that are optimal in 0 175 of the time required by the fastest available algorithm in the literature the algorithm is anytime , and if interrupted before it would have normally terminated , it can still provide a solution that is guaranteed to be within a bound from the optimal one moreover , the guarantees we provide on the quality of the solution are significantly better than those provided by the previous state of the art algorithms designed for this purpose for example , for the worst case distribution given 25 agents , our algorithm is able to find a 90 efficient solution in around 10 of time it takes to find the optimal solution
real time systems are computing systems in which the meeting of their requirements is vital for their correctness consequently , if the real time requirements of these systems are poorly understood and verified , the results can be disastrous and lead to irremediable project failures at the early phases of development the present work addresses the problem of detecting deadlock situations early in the requirements specification phase of a concurrent real time system , proposing a simple proof of concepts prototype that joins scenario based requirements specifications and techniques based on topology the efforts are concentrated in the integration of the formal representation of message sequence chart scenarios into the deadlock detection algorithm of fajstrup et al , based on geometric and algebraic topology
in this paper , we investigate the design of artificial noise aided secure multi antenna transmission in slow fading channels the primary design concerns include the transmit power allocation and the rate parameters of the wiretap code we consider two scenarios with different complexity levels i \) the design parameters are chosen to be fixed for all transmissions , ii \) they are adaptively adjusted based on the instantaneous channel feedback from the intended receiver in both scenarios , we provide explicit design solutions for achieving the maximal throughput subject to a secrecy constraint , given by a maximum allowable secrecy outage probability we then derive accurate approximations for the maximal throughput in both scenarios in the high signal to noise ratio region , and give new insights into the additional power cost for achieving a higher security level , whilst maintaining a specified target throughput in the end , the throughput gain of adaptive transmission over non adaptive transmission is also quantified and analyzed
with the successful worldwide deployment of 3rd generation mobile communication , security aspects are ensured partly researchers are now looking for 4g mobile for its deployment with high data rate , enhanced security and reliability so that world should look for calm , continuous air interface for long and medium range communication this calm will be a reliable high data rate secured mobile communication to be deployed for car to car communication \( c2c \) for safety application this paper reviewed the wimax , 60 ghz rf carrier for c2c the system is tested at smit laboratory with multimedia transmission and reception with proper deployment of this 60 ghz system on vehicles , the existing commercial products for 802 11p will be required to be replaced or updated soon
we use confirmatory factor analysis to derive a unifying measure of comparison of scientists based on bibliometric measurements , by utilizing the h index , some similar h type indices as well as other common measures of scientific performance we use a real data example from nine well known departments of statistics to demonstrate our approach and argue that our combined measure results in a better overall evaluation of a researchers' scientific work
this study proposes a general , scalable method to learn control oriented thermal models of buildings that could enable wide scale deployment of cost effective predictive controls an unscented kalman filter augmented for parameter and disturbance estimation is shown to accurately learn and predict a building 's thermal response recent studies of heating , ventilating , and air conditioning \( hvac \) systems have shown significant energy savings with advanced model predictive control \( mpc \) a scalable cost effective method to readily acquire accurate , robust models of individual buildings' unique thermal envelopes has historically been elusive and hindered the widespread deployment of prediction based control systems continuous commissioning and lifetime performance of these thermal models requires deployment of on line data driven system identification and parameter estimation routines we propose a novel gray box approach using an unscented kalman filter based on a multi zone thermal network and validate it with energyplus simulation data the filter quickly learns parameters of a thermal network during periods of known or constrained loads and then characterizes unknown loads in order to provide accurate 24 hour energy predictions this study extends our initial investigation by formalizing parameter and disturbance estimation routines and demonstrating results across a year long study
community structure in networks is often a consequence of homophily , or assortative mixing , based on some attribute of the vertices for example , researchers may be grouped into communities corresponding to their research topic this is possible if vertex attributes have discrete values , but many networks exhibit assortative mixing by some continuous valued attribute , such as age or geographical location in such cases , no discrete communities can be identified we consider how the notion of community structure can be generalized to networks that are based on continuous valued attributes in general , a network may contain discrete communities which are ordered according to their attribute values we propose a method of generating synthetic ordered networks and investigate the effect of ordered community structure on the spread of infectious diseases we also show that community detection algorithms fail to recover community structure in ordered networks , and evaluate an alternative method using a layout algorithm to recover the ordering
several exact recovery criteria \( erc \) ensuring that orthogonal matching pursuit \( omp \) identifies the correct support of sparse signals have been developed in the last few years these erc rely on the restricted isometry property \( rip \) , the associated restricted isometry constant \( ric \) and sometimes the restricted orthogonality constant \( roc \) in this paper , three of the most recent erc for omp are examined the contribution is to show that these erc remain valid for a generalization of omp , entitled simultaneous orthogonal matching pursuit \( somp \) , that is capable to process several measurement vectors simultaneously and return a common support estimate for the underlying sparse vectors the sharpness of the bounds is also briefly discussed in light of previous works focusing on omp
mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations the current practice for estimating the parameters of such models relies on local search heuristics \( e g , the em algorithm \) which are prone to failure , and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components this work develops an efficient method of moments approach to parameter estimation for a broad class of high dimensional mixture models with many components , including multi view mixtures of gaussians \( such as mixtures of axis aligned gaussians \) and hidden markov models the new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works and , because of its simplicity , it offers a viable alternative to em for practical deployment
the network traffic matrix is widely used in network operation and management it is therefore of crucial importance to analyze the components and the structure of the network traffic matrix , for which several mathematical approaches such as principal component analysis \( pca \) were proposed in this paper , we first argue that pca performs poorly for analyzing traffic matrix that is polluted by large volume anomalies , and then propose a new decomposition model for the network traffic matrix according to this model , we carry out the structural analysis by decomposing the network traffic matrix into three sub matrices , namely , the deterministic traffic , the anomaly traffic and the noise traffic matrix , which is similar to the robust principal component analysis \( rpca \) problem previously studied in 13 based on the relaxed principal component pursuit \( relaxed pcp \) method and the accelerated proximal gradient \( apg \) algorithm , we present an iterative approach for decomposing a traffic matrix , and demonstrate its efficiency and flexibility by experimental results finally , we further discuss several features of the deterministic and noise traffic our study develops a novel method for the problem of structural analysis of the traffic matrix , which is robust against pollution of large volume anomalies
power series solution method has been traditionally used to solve ordinary and partial linear differential equations however , despite their usefulness the application of this method has been limited to this particular kind of equations in this work we use the method of power series to solve nonlinear partial differential equations the method is applied to solve three versions of nonlinear time dependent burgers type differential equations in order to demonstrate its scope and applicability
one of the key challenges in sensor networks is the extraction of information by fusing data from a multitude of distinct , but possibly unreliable sensors recovering information from the maximum number of dependable sensors while specifying the unreliable ones is critical for robust sensing this sensing task is formulated here as that of finding the maximum number of feasible subsystems of linear equations , and proved to be np hard useful links are established with compressive sampling , which aims at recovering vectors that are sparse in contrast , the signals here are not sparse , but give rise to sparse residuals capitalizing on this form of sparsity , four sensing schemes with complementary strengths are developed the first scheme is a convex relaxation of the original problem expressed as a second order cone program \( socp \) it is shown that when the involved sensing matrices are gaussian and the reliable measurements are sufficiently many , the socp can recover the optimal solution with overwhelming probability the second scheme is obtained by replacing the initial objective function with a concave one the third and fourth schemes are tailored for noisy sensor data the noisy case is cast as a combinatorial problem that is subsequently surrogated by a \( weighted \) socp interestingly , the derived cost functions fall into the framework of robust multivariate linear regression , while an efficient block coordinate descent algorithm is developed for their minimization the robust sensing capabilities of all schemes are verified by simulated tests
we construct classes of permutation polynomials over f q 2 by exhibiting classes of low degree rational functions over f q 2 which induce bijections on the set of \( q 1 \) th roots of unity in f q 2 as a consequence , we prove two conjectures about permutation trinomials from a recent paper by tu , zeng , hu and li
visualizing high dimensional data by projecting them into two or three dimensional space is one of the most effective ways to intuitively understand the data 's underlying characteristics , for example their class neighborhood structure while data visualization in low dimensional space can be efficient for revealing the data 's underlying characteristics , classifying a new sample in the reduced dimensional space is not always beneficial because of the loss of information in expressing the data it is possible to classify the data in the high dimensional space , while visualizing them in the low dimensional space , but in this case , the visualization is often meaningless because it fails to illustrate the underlying characteristics that are crucial for the classification process in this paper , the performance preserving property of the previously proposed restricted radial basis function network in reducing the dimension of labeled data is explained here , it is argued through empirical experiments that the internal representation of the restricted radial basis function network , which during the supervised learning process organizes a visualizable two dimensional map , does not only preserve the topographical structure of high dimensional data but also captures their class neighborhood structures that are important for classifying them hence , unlike many of the existing dimension reduction methods , the restricted radial basis function network offers two dimensional visualization that is strongly correlated with the classification process
in this paper , we propose a bid optimizer for sponsored keyword search auctions which leads to better retention of advertisers by yielding attractive utilities to the advertisers without decreasing the revenue to the search engine the bid optimizer is positioned as a key value added tool the search engine provides to the advertisers the proposed bid optimizer algorithm transforms the reported values of the advertisers for a keyword into a correlated bid profile using many ideas from cooperative game theory the algorithm is based on a characteristic form game involving the search engine and the advertisers ideas from nash bargaining theory are used in formulating the characteristic form game to provide for a fair share of surplus among the players involved the algorithm then computes the nucleolus of the characteristic form game since we find that the nucleolus is an apt way of allocating the gains of cooperation among the search engine and the advertisers the algorithm next transforms the nucleolus into a correlated bid profile using a linear programming formulation this bid profile is input to a standard generalized second price mechanism \( gsp \) for determining the allocation of sponsored slots and the prices to be be paid by the winners the correlated bid profile that we determine is a locally envy free equilibrium and also a correlated equilibrium of the underlying game through detailed simulation experiments , we show that the proposed bid optimizer retains more customers than a plain gsp mechanism and also yields better long run utilities to the search engine and the advertisers
in this paper we present a queueing network approach to the problem of routing and rebalancing a fleet of self driving vehicles providing on demand mobility within a capacitated road network we refer to such systems as autonomous mobility on demand systems , or amod we first cast an amod system into a closed , multi class bcmp queueing network model second , we present analysis tools that allow the characterization of performance metrics for a given routing policy , in terms , e g , of vehicle availabilities , and first and second order moments of vehicle throughput third , we propose a scalable method for the synthesis of routing policies , with performance guarantees in the limit of large fleet sizes finally , we validate our theoretical results on a case study of new york city collectively , this paper provides a unifying framework for the analysis and control of amod systems , which subsumes earlier jackson and network flow models , provides a quite large set of modeling options \( e g , the inclusion of road capacities and general travel time distributions \) , and allows the analysis of second and higher order moments for the performance metrics
the adaboost algorithm was designed to combine many weak hypotheses that perform slightly better than random guessing into a strong hypothesis that has very low error we study the rate at which adaboost iteratively converges to the minimum of the exponential loss unlike previous work , our proofs do not require a weak learning assumption , nor do they require that minimizers of the exponential loss are finite our first result shows that at iteration t , the exponential loss of adaboost 's computed parameter vector will be at most epsilon more than that of any parameter vector of ell 1 norm bounded by b in a number of rounds that is at most a polynomial in b and 1 epsilon we also provide lower bounds showing that a polynomial dependence on these parameters is necessary our second result is that within c epsilon iterations , adaboost achieves a value of the exponential loss that is at most epsilon more than the best possible value , where c depends on the dataset we show that this dependence of the rate on epsilon is optimal up to constant factors , i e , at least omega \( 1 epsilon \) rounds are necessary to achieve within epsilon of the optimal exponential loss
a path in an edge colored graph is said to be a rainbow path if no two edges on the path have the same color an edge colored graph is \( strongly \) rainbow connected if there exists a \( geodesic \) rainbow path between every pair of vertices the \( strong \) rainbow connectivity of a graph g , denoted by \( src \( g \) , respectively \) rc \( g \) is the smallest number of colors required to edge color the graph such that the graph is \( strong \) rainbow connected it is known that for emph even k to decide whether the rainbow connectivity of a graph is at most k or not is np hard it was conjectured that for all k , to decide whether rc \( g \) leq k is np hard in this paper we prove this conjecture we also show that it is np hard to decide whether src \( g \) leq k or not even when g is a bipartite graph
trapezoidal words are finite words having at most n 1 distinct factors of length n , for every n 0 they encompass finite sturmian words we distinguish trapezoidal words into two disjoint subsets open and closed trapezoidal words a trapezoidal word is closed if its longest repeated prefix has exactly two occurrences in the word , the second one being a suffix of the word otherwise it is open we show that open trapezoidal words are all primitive and that closed trapezoidal words are all sturmian we then show that trapezoidal palindromes are closed \( and therefore sturmian \) this allows us to characterize the special factors of sturmian palindromes we end with several open problems
in this article , we discuss the problem of establishing relations between information measures assessed for network structures two types of entropy based measures namely , the shannon entropy and its generalization , the r ' e nyi entropy have been considered for this study our main results involve establishing formal relationship , in the form of implicit inequalities , between these two kinds of measures when defined for graphs further , we also state and prove inequalities connecting the classical partition based graph entropies and the functional based entropy measures in addition , several explicit inequalities are derived for special classes of graphs
a person 's interests exist as an internal state and are difficult to define since only external actions are observable , a proxy must be used that represents someone 's interests techniques like collaborative filtering , behavioral targeting , and hashtag analysis implicitly model an individual 's interests i argue that these models are limited to shallow , temporary interests , which do not reflect people 's deeper interests or passions i propose an alternative model of interests that takes advantage of a user 's social graph the basic principle is that people only follow those that interest them , so the social graph is an effective and robust proxy for people 's interests
folding a sequence s into a multidimensional box is a well known method which is used as a multidimensional coding technique the operation of folding is generalized in a way that the sequence s can be folded into various shapes and not just a box the new definition of folding is based on a lattice tiling for the given shape cs and a direction in the d dimensional integer grid necessary and sufficient conditions that a lattice tiling for cs combined with a direction define a folding of a sequence into cs are derived the immediate and most impressive application is some new lower bounds on the number of dots in two dimensional synchronization patterns this can be also generalized for multidimensional synchronization patterns the technique and its application for two dimensional synchronization patterns , raise some interesting problems in discrete geometry we will also discuss these problems it is also shown how folding can be used to construct multidimensional error correcting codes finally , by using the new definition of folding , multidimensional pseudo random arrays with various shapes are generated
a vertex subset graph problem q defines which subsets of the vertices of an input graph are feasible solutions a reconfiguration variant of a vertex subset problem asks , given two feasible solutions s and t of size k , whether it is possible to transform s into t by a sequence of vertex additions and deletions such that each intermediate set is also a feasible solution of size bounded by k we study reconfiguration variants of two classical vertex subset problems , namely independent set and dominating set we denote the former by isr and the latter by dsr both isr and dsr are pspace complete on graphs of bounded bandwidth and w 1 hard parameterized by k on general graphs we show that isr is fixed parameter tractable parameterized by k when the input graph is of bounded degeneracy or nowhere dense as a corollary , we answer positively an open question concerning the parameterized complexity of the problem on graphs of bounded treewidth moreover , our techniques generalize recent results showing that isr is fixed parameter tractable on planar graphs and graphs of bounded degree for dsr , we show the problem fixed parameter tractable parameterized by k when the input graph does not contain large bicliques , a class of graphs which includes graphs of bounded degeneracy and nowhere dense graphs
we present results of expanding the contents of the china biographical database by text mining historical local gazetteers , difangzhi the goal of the database is to see how people are connected together , through kinship , social connections , and the places and offices in which they served the gazetteers are the single most important collection of names and offices covering the song through qing periods although we begin with local officials we shall eventually include lists of local examination candidates , people from the locality who served in government , and notable local figures with biographies the more data we collect the more connections emerge the value of doing systematic text mining work is that we can identify relevant connections that are either directly informative or can become useful without deep historical research academia sinica is developing a name database for officials in the central governments of the ming and qing dynasties
making sense of incomplete and conflicting narrative knowledge in the presence of abnormalities , unobservable processes , and other real world considerations is a challenge and crucial requirement for cognitive robotics systems an added challenge , even when suitably specialised action languages and reasoning systems exist , is practical integration and application within large scale robot control frameworks in the backdrop of an autonomous wheelchair robot control task , we report on application driven work to realise postdiction triggered abnormality detection and re planning for real time robot control \( a \) narrative based knowledge about the environment is obtained via a larger smart environment framework and \( b \) abnormalities are postdicted from stable models of an answer set program corresponding to the robot 's epistemic model the overall reasoning is performed in the context of an approximate epistemic action theory based planner implemented via a translation to answer set programming
this paper presents a framework for exact discovery of the most interesting sequential patterns it combines \( 1 \) a novel definition of the expected support for a sequential pattern a concept on which most interestingness measures directly rely with \( 2 \) skopus a new branch and bound algorithm for the exact discovery of top k sequential patterns under a given measure of interest we carry out experiments on both synthetic data with known patterns and real world datasets both experiments confirm the consistency and relevance of our approach
in this work , we present a simplified successive cancellation list decoder that uses a chase like decoding process to achieve a six time improvement in speed compared to successive cancellation list decoding while maintaining the same error correction performance advantage over standard successive cancellation polar decoders we discuss the algorithm and detail the data structures and methods used to obtain this speed up we also propose an adaptive decoding algorithm that significantly improves the throughput while retaining the error correction performance simulation results over the additive white gaussian noise channel are provided and show that the proposed system is up to 16 times faster than an ldpc decoder of the same frame size , code rate , and similar error correction performance , making it more suitable for use as a software decoding solution
we present five examples where quantum finite automata \( qfas \) outperform their classical counterparts this may be useful as a relatively simple technique to introduce quantum computation concepts to computer scientists we also describe a modern qfa model involving superoperators that is able to simulate all known qfa and classical finite automaton variants
we present an asymptotically exact analysis of the problem of detecting communities in sparse random networks our results are also applicable to detection of functional modules , partitions , and colorings in noisy planted models using a cavity method analysis , we unveil a phase transition from a region where the original group assignment is undetectable to one where detection is possible in some cases , the detectable region splits into an algorithmically hard region and an easy one our approach naturally translates into a practical algorithm for detecting modules in sparse networks , and learning the parameters of the underlying model
quantum k sat is the problem of deciding whether there is a n qubit state which is perpendicular to a set of vectors , each of which lies in the hilbert space of k qubits equivalently , the problem is to decide whether a particular type of local hamiltonian has a ground state with zero energy we consider random quantum k sat formulas with n variables and m alpha n clauses , and ask at what value of alpha these formulas cease to be satisfiable we show that the threshold for random quantum 3 sat is at most 3 594 for comparison , convincing arguments from statistical physics suggest that the classical 3 sat threshold is alpha approx 4 267 for larger k , we show that the quantum threshold is a constant factor smaller than the classical one our bounds work by determining the generic rank of the satisfying subspace for certain gadgets , and then using the technique of differential equations to analyze various algorithms that partition the hypergraph into a collection of these gadgets our use of differential equation to establish upper bounds on a satisfiability threshold appears to be novel , and our techniques may apply to various classical problems as well
i formalize important theorems about classical propositional logic in the proof assistant coq the main theorems i prove are \( 1 \) the soundness and completeness of natural deduction calculus , \( 2 \) the equivalence between natural deduction calculus , hilbert systems and sequent calculus and \( 3 \) cut elimination for sequent calculus
we consider the task of lossy compression of high dimensional vectors through quantization we propose the approach that learns quantization parameters by minimizing the distortion of scalar products and squared distances between pairs of points this is in contrast to previous works that obtain these parameters through the minimization of the reconstruction error of individual points the proposed approach proceeds by finding a linear transformation of the data that effectively reduces the minimization of the pairwise distortions to the minimization of individual reconstruction errors after such transformation , any of the previously proposed quantization approaches can be used despite the simplicity of this transformation , the experiments demonstrate that it achieves considerable reduction of the pairwise distortions compared to applying quantization directly to the untransformed data
the pi vertex deletion problem has as input an undirected graph g \( v , e \) and an integer k and asks whether there is a set of at most k vertices that can be deleted such that the resulting graph is a member of the graph class pi by a classic result of lewis and yannakakis j comput syst sci '80 , pi vertex deletion is np hard for all hereditary properties pi we adapt the original np hardness construction to show that under the exponential time hypothesis \( eth \) tight complexity results can be obtained we show that pi vertex deletion does not admit a 2 o \( n \) time algorithm where n is the number of vertices in g we also obtain a dichotomy for the number m of edges in the input graph if pi contains all independent sets , then there is no 2 o \( m \) time algorithm for pi vertex deletion if there is a fixed independent set that is not contained in pi , then pi vertex deletion can be solved in 2 o \( m \) time finally , we consider the special case that g is planar and obtain that pi vertex deletion cannot be solved in 2 o \( sqrt n \) time for all hereditary pi containing and excluding infinitely many planar graphs
generalized spatial modulation \( gsm \) uses n antenna elements but fewer radio frequency \( rf \) chains \( r \) at the transmitter spatial modulation and spatial multiplexing are special cases of gsm with r 1 and r n , respectively in gsm , apart from conveying information bits through r modulation symbols , information bits are also conveyed through the indices of the r active transmit antennas in this paper , we derive lower and upper bounds on the the capacity of a \( n , m , r \) gsm mimo system , where m is the number of receive antennas further , we propose a computationally efficient gsm encoding \( i e , bits to signal mapping \) method and a message passing based low complexity detection algorithm suited for large scale gsm mimo systems
bayesian sequence prediction is a simple technique for predicting future symbols sampled from an unknown measure on infinite sequences over a countable alphabet while strong bounds on the expected cumulative error are known , there are only limited results on the distribution of this error we prove tight high probability bounds on the cumulative error , which is measured in terms of the kullback leibler \( kl \) divergence we also consider the problem of constructing upper confidence bounds on the kl and hellinger errors similar to those constructed from hoeffding like bounds in the i i d case the new results are applied to show that bayesian sequence prediction can be used in the knows what it knows \( kwik \) framework with bounds that match the state of the art
we examine the impact of torpid mixing and meta stability issues on the delay performance in wireless random access networks focusing on regular meshes as prototypical scenarios , we show that the mean delays in an l times l toric grid with normalized load rho are of the order \( frac 1 1 rho \) l this superlinear delay scaling is to be contrasted with the usual linear growth of the order frac 1 1 rho in conventional queueing networks the intuitive explanation for the poor delay characteristics is that \( i \) high load requires a high activity factor , \( ii \) a high activity factor implies extremely slow transitions between dominant activity states , and \( iii \) slow transitions cause starvation and hence excessively long queues and delays our proof method combines both renewal and conductance arguments a critical ingredient in quantifying the long transition times is the derivation of the communication height of the uniformized markov chain associated with the activity process we also discuss connections with glauber dynamics , conductance and mixing times our proof framework can be applied to other topologies as well , and is also relevant for the hard core model in statistical physics and the sampling from independent sets using single site update markov chains
in this paper , we analyze the performance of regularized channel inversion \( rci \) precoding in multiple input single output \( miso \) broadcast channels with confidential messages under transmit side channel correlation we derive a deterministic equivalent for the achievable per user secrecy rate which is almost surely exact as the number of transmit antennas and the number of users grow to infinity in a fixed ratio , and we determine the optimal regularization parameter that maximizes the secrecy rate furthermore , we obtain deterministic equivalents for the secrecy rates achievable by \( i \) zero forcing precoding and \( ii \) single user beamforming the accuracy of our analysis is validated by simulations of finite size systems
short circuit evaluation denotes the semantics of propositional connectives in which the second argument is evaluated only if the first argument does not suffice to determine the value of the expression in programming , short circuit evaluation is widely used , with sequential conjunction and disjunction as primitive connectives a short circuit logic is a variant of propositional logic \( pl \) that can be defined with help of hoare 's conditional , a ternary connective comparable to if then else , and that implies all identities that follow from four basic axioms for the conditional and can be expressed in pl \( e g , axioms for associativity of conjunction and double negation shift \) in the absence of side effects , short circuit evaluation characterizes pl however , short circuit evaluation admits the possibility to model side effects and gives rise to various different short circuit logics the first extreme case is fscl \( free short circuit logic \) , which characterizes the setting in which evaluation of each atom \( propositional variable \) can yield a side effect the other extreme case is mscl \( memorizing short circuit logic \) , the most identifying variant we distinguish below pl in mscl , only a very restricted type of side effects can be modelled , while sequential conjunction is non commutative we provide axiomatizations for fscl and mscl extending mscl with one simple axiom yields sscl \( static short circuit logic , or sequential pl \) , for which we also provide a completeness result we briefly discuss two variants in between fscl and mscl , among which a logic that admits contraction of atoms and of their negations
inverse optimal control , also known as inverse reinforcement learning , is the problem of recovering an unknown reward function in a markov decision process from expert demonstrations of the optimal policy we introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality , and is suitable for large , continuous domains where even computing a full policy is impractical by using a local approximation of the reward function , our method can also drop the assumption that the demonstrations are globally optimal , requiring only local optimality this allows it to learn from examples that are unsuitable for prior methods
modern networks are large , highly complex and dynamic add to that the mobility of the agents comprising many of these networks it is difficult or even impossible for such systems to be managed centrally in an efficient manner it is imperative for such systems to attain a degree of self management self healing i e the capability of a system in a good state to recover to another good state in face of an attack , is desirable for such systems in this paper , we discuss the self healing model for dynamic reconfigurable systems in this model , an omniscient adversary inserts or deletes nodes from a network and the algorithm responds by adding a limited number of edges in order to maintain invariants of the network we look at some of the results in this model and argue for their applicability and further extensions of the results and the model we also look at some of the techniques we have used in our earlier work , in particular , we look at the idea of maintaining virtual graphs mapped over the existing network and assert that this may be a useful technique to use in many problem domains
we consider wireless powered amplify and forward and decode and forward relaying in cooperative communications , where an energy constrained relay node first harvests energy through the received radio frequency signal from the source and then uses the harvested energy to forward the source information to the destination node we propose time switching based energy harvesting \( eh \) and information transmission \( it \) protocols with two modes of eh at the relay for continuous time eh , the eh time can be any percentage of the total transmission block time for discrete time eh , the whole transmission block is either used for eh or it the proposed protocols are attractive because they do not require channel state information at the transmitter side and enable relay transmission with preset fixed transmission power we derive analytical expressions of the achievable throughput for the proposed protocols the derived expressions are verified by comparison with computer simulations and allow the system performance to be determined as a function of the system parameters finally , we show that the proposed protocols outperform the existing fixed time duration eh protocols in the literature , since they intelligently track the level of the harvested energy to switch between eh and it in an online fashion , allowing efficient use of resources
this paper presents a novel approach to including non instantaneous discrete control transitions in the linear hybrid automaton approach to simulation and verification of hybrid control systems in this paper we study the control of a continuously evolving analog plant using a controller programmed in a synchronous programming language we provide extensions to the synchronous subset of the systemj programming language for modeling , implementation , and verification of such hybrid systems we provide a sound rewrite semantics that approximate the evolution of the continuous variables in the discrete domain inspired from the classical supervisory control theory the resultant discrete time model can be verified using classical model checking tools finally , we show that systems designed using our approach have a higher fidelity than the ones designed using the hybrid automaton approach
this work establishes the complexity class of several instances of the s packing coloring problem for a graph g , a positive integer k and a non decreasing list of integers s \( s 1 , , s k \) , g is s colorable , if its vertices can be partitioned into sets s i , i 1 , , k , where each s i being a s i packing \( a set of vertices at pairwise distance greater than s i \) for a list of three integers , a dichotomy between np complete problems and polynomial time solvable problems is determined for subcubic graphs moreover , for an unfixed size of list , the complexity of the s packing coloring problem is determined for several instances of the problem these properties are used in order to prove a dichotomy between np complete problems and polynomial time solvable problems for lists of at most four integers
social media channels such as twitter have emerged as popular platforms for crowds to respond to public events such as speeches , sports and debates while this promises tremendous opportunities to understand and make sense of the reception of an event from the social media , the promises come entwined with significant technical challenges in particular , given an event and an associated large scale collection of tweets , we need approaches to effectively align tweets and the parts of the event they refer to this in turn raises questions about how to segment the event into smaller yet meaningful parts , and how to figure out whether a tweet is a general one about the entire event or specific one aimed at a particular segment of the event in this work , we present et lda , an effective method for aligning an event and its tweets through joint statistical modeling of topical influences from the events and their associated tweets the model enables the automatic segmentation of the events and the characterization of tweets into two categories \( 1 \) episodic tweets that respond specifically to the content in the segments of the events , and \( 2 \) steady tweets that respond generally about the events we present an efficient inference method for this model , and a comprehensive evaluation of its effectiveness over existing methods in particular , through a user study , we demonstrate that users find the topics , the segments , the alignment , and the episodic tweets discovered by et lda to be of higher quality and more interesting as compared to the state of the art , with improvements in the range of 18 41
many analytic results for the connectivity , coverage , and capacity of wireless networks have been reported for the case where the number of nodes , n , tends to infinity \( large scale networks \) the majority of these results have not been extended for small or moderate values of n whereas in many practical networks , n is not very large in this paper , we consider finite \( small scale \) wireless sensor networks we first show that previous asymptotic results provide poor approximations for such networks we provide a set of differences between small scale and large scale analysis and propose a methodology for analysis of finite sensor networks furthermore , we consider two models for such networks unreliable sensor grids , and sensor networks with random node deployment we provide easily computable expressions for bounds on the coverage and connectivity of these networks with validation from simulations , we show that the derived analytic expressions give very good estimates of such quantities for finite sensor networks our investigation confirms the fact that small scale networks possesses unique characteristics different from the large scale counterparts , necessitating the development of a new framework for their analysis and design
it has been shown recently by geng et al that in a k user gaussian interference network , if for each user the desired signal strength is no less than the sum of the strengths of the strongest interference from this user and the strongest interference to this user \( all signal strengths measured in db scale \) , then power control and treating interference as noise \( tin \) is sufficient to achieve the entire generalized degrees of freedom \( gdof \) region motivated by the intuition that the deterministic model of avestimehr et al \( adt deterministic model \) is particularly suited for exploring the optimality of tin , the results of geng et al are first re visited under the adt deterministic model , and are shown to directly translate between the gaussian and deterministic settings next , we focus on the extension of these results to parallel interference networks , from a sum capacity sum gdof perspective to this end , we interpret the explicit characterization of the sum capacity sum gdof of a tin optimal network \( without parallel channels \) as a minimum weighted matching problem in combinatorial optimization , and obtain a simple characterization in terms of a partition of the interference network into vertex disjoint cycles aided by insights from the cyclic partition , the sum capacity optimality of tin for k user parallel interference networks is characterized for the adt deterministic model , leading ultimately to corresponding gdof results for the gaussian setting in both cases , subject to a mild invertibility condition the optimality of tin is shown to extend to parallel networks in a separable fashion
in order to formulate mathematical conjectures likely to be true , a number of base cases must be determined however , many combinatorial problems are np hard and the computational complexity makes this research approach difficult using a standard brute force approach on a typical computer one sample problem explored is that of finding a minimum identifying code to work around the computational issues , a variety of methods are explored and consist of a parallel computing approach using matlab , a quantum annealing approach using the d wave computer , and lastly using satisfiability modulo theory \( smt \) and corresponding smt solvers each of these methods requires the problem to be formulated in a unique manner in this paper , we address the challenges of computing solutions to this np hard problem with respect to each of these methods
this paper proposes an efficient technique for partitioning large biometric database during identification in this technique feature vector which comprises of global and local descriptors extracted from offline signature are used by fuzzy clustering technique to partition the database as biometric features posses no natural order of sorting , thus it is difficult to index them alphabetically or numerically hence , some supervised criteria is required to partition the search space at the time of identification the fuzziness criterion is introduced to find the nearest clusters for declaring the identity of query sample the system is tested using bin miss rate and performs better in comparison to traditional k means approach
we consider solving multi objective optimization problems in a distributed manner by a network of cooperating and learning agents the problem is equivalent to optimizing a global cost that is the sum of individual components the optimizers of the individual components do not necessarily coincide and the network therefore needs to seek pareto optimal solutions we develop a distributed solution that relies on a general class of adaptive diffusion strategies we show how the diffusion process can be represented as the cascade composition of three operators two combination operators and a gradient descent operator using the banach fixed point theorem , we establish the existence of a unique fixed point for the composite cascade we then study how close each agent converges towards this fixed point , and also examine how close the pareto solution is to the fixed point we perform a detailed mean square error analysis and establish that all agents are able to converge to the same pareto optimal solution within a sufficiently small mean square error \( mse \) bound even for constant step sizes we illustrate one application of the theory to collaborative decision making in finance by a network of agents
several passive microwave satellites orbit the earth and measure rainfall these measurements have the advantage of almost full global coverage when compared to surface rain gauges however , these satellites have low temporal revisit and missing data over some regions image fusion is a useful technique to fill in the gaps of one image \( one satellite measurement \) using another one the proposed algorithm uses an iterative fusion scheme to integrate information from two satellite measurements the algorithm is implemented on two datasets for 7 years of half hourly data the results show significant improvements in rain detection and rain intensity in the merged measurements
ever since the proposal of first epidemic model , scientists have been attempting to estimate the growth of a disease contagion while in its premature stage despite being the focus of researchers for a long time , understanding epidemiology remains as error prone as a weather forecast , mainly because of the unavailability of large amount of data an epidemic spread is analogous to the diffusion of memes in social networking sites diffusion of memes can be easily studied provided large datasets and computational powers to extract information from online networks so , studying a meme spreading pattern can help us in understanding epidemiology in this paper , we analyse the impact of the topology of a social network , specifically its meso scale properties community structure and core periphery structure , on a meme traversing over it we propose a meme propagation model for synthetic scale free graphs which resemble real world graphs and observe the process of a meme going viral on such a network we also validate our model for real world information propagation
this paper investigates the capacity of a channel in which information is conveyed by the timing of consecutive packets passing through a queue with independent and identically distributed service times such timing channels are commonly studied under the assumption of a work conserving queue in contrast , this paper studies the case of a bufferless queue that drops arriving packets while a packet is in service under this bufferless model , the paper provides upper bounds on the capacity of timing channels and establishes achievable rates for the case of bufferless m m 1 and m g 1 queues in particular , it is shown that a bufferless m m 1 queue at worst suffers less than 10 reduction in capacity when compared to an m m 1 work conserving queue
in this work , the performance analysis of a dual hop relay transmission system composed of asymmetric radio frequency \( rf \) free space optical \( fso \) links with pointing errors is presented more specifically , we build on the system model presented in 1 to derive new exact closed form expressions for the cumulative distribution function , probability density function , moment generating function , and moments of the end to end signal to noise ratio in terms of the meijer 's g function we then capitalize on these results to offer new exact closed form expressions for the higher order amount of fading , average error rate for binary and m ary modulation schemes , and the ergodic capacity , all in terms of meijer 's g functions our new analytical results were also verified via computer based monte carlo simulation results
in this paper we consider the time and the crossing sequence complexities of one tape off line turing machines we show that the running time of each nondeterministic machine accepting a nonregular language must grow at least as n log n , in the case all accepting computations are considered \( accept measure \) we also prove that the maximal length of the crossing sequences used in accepting computations must grow at least as log n on the other hand , it is known that if the time is measured considering , for each accepted string , only the faster accepting computation \( weak measure \) , then there exist nonregular languages accepted in linear time we prove that under this measure , each accepting computation should exhibit a crossing sequence of length at least log log n we also present efficient implementations of algorithms accepting some unary nonregular languages
stretching is a new sparse matrix method that makes matrices sparser by making them larger stretching has implications for computational complexity theory and applications in scientific and parallel computing it changes matrix sparsity patterns to render linear equations more easily solved by parallel and sparse techniques some stretchings increase matrix condition numbers only moderately , and thus solve linear equations stably for example , these stretchings solve arrow equations with accuracy and expense preferable to other solution methods
we consider the analysis and design of space time trellis codes \( sttcs \) for a cooperative relay channel operating in amplify and forward \( af \) mode assuming the source and destination nodes are equipped with multiple antennas but the relay node has single antenna we derive a pairwise error probability \( pep \) expression for the performance of sttcs in this type of channels a simple upper bound on pep is then derived and is maximized to find the optimum sttcs we show that the designed sttcs based on the derived criterion achieve full diversity in the af relay channels especially at high signal to noise ratios \( snrs \) the maximum achievable diversity in relay channels with single antenna relay is bounded by min \( m , n \) where m and n are respectively the number of antennas in source and destination nodes simulation results confirm that the proposed codes achieve the maximum diversity and also provide an appealing coding gain
in recent years the cache oblivious model of external memory computation has provided an attractive theoretical basis for the analysis of algorithms on massive datasets much progress has been made in discovering algorithms that are asymptotically optimal or near optimal however , to date there are still relatively few successful experimental studies in this paper we compare two different cache oblivious priority queues based on the funnel and bucket heap and apply them to the single source shortest path problem on graphs with positive edge weights our results show that when ram is limited and data is swapping to external storage , the cache oblivious priority queues achieve orders of magnitude speedups over standard internal memory techniques however , for the single source shortest path problem both on simulated and real world graph data , these speedups are markedly lower due to the time required to access the graph adjacency list itself
we present a novel approach for semi supervised domain adaptation that is based upon the probabilistic framework of gaussian processes \( gps \) specifically , we introduce domain specific gps as local experts the adaptation is facilitated in a probabilistic fashion by conditioning the available target domain data on multiple source experts the target expert accounts for the target classifier solely a single and confident classifier is then obtained by combining the predictions from multiple experts based on their predictive confidence learning of the model is efficient and requires no retraining reweighting of the source classifiers we evaluate the proposed approach on two publicly available datasets for multi class \( multipie \) and multi label \( disfa \) classification of expressive facial images to this end , we perform adaptation of two contextual factors where \( view \) and who \( subject \) we show in our experiments that the proposed approach consistently outperforms both source and target classifiers , while using as few as 30 target examples it also outperforms the state of the art approaches for semi supervised domain adaptation
the discovery of algebraic geometric codes constructed on curves led to generalizing this construction on higher dimensional varieties in this paper , we use a theorem of b poonen to show that the codes obtained from higher dimensional varieties can be realized as codes on curves one of the important consequences of this result is that the search for good codes on varieties that beat the existing bounds can be restricted to the case of curves
we propose a computationally efficient multilevel coding scheme to achieve the capacity of an isi channel using layers of binary inputs the transmitter employs multilevel coding with linear mapping the receiver uses multistage decoding where each stage performs a separate linear minimum mean square error \( lmmse \) equalization and decoding the optimality of the scheme is due to the fact that the lmmse equalizer is information lossless in an isi channel when signal to noise ratio is sufficiently low the computational complexity is low and scales linearly with the length of the channel impulse response and the number of layers the decoder at each layer sees an equivalent awgn channel , which makes coding straightforward
this paper proposes an adaptive morphological dilation image coding with context weights prediction the new dilation method is not to use fixed models , but to decide whether a coefficient needs to be dilated or not according to the coefficient 's predicted significance degree it includes two key dilation technologies 1 \) controlling dilation process with context weights to reduce the output of insignificant coefficients , and 2 \) using variable length group test coding with context weights to adjust the coding order and cost as few bits as possible to present the events with large probability moreover , we also propose a novel context weight strategy to predict coefficient 's significance degree more accurately , which serves for two dilation technologies experimental results show that our proposed method outperforms the state of the art image coding algorithms available today
in this paper , we revisit the forward , backward and bidirectional bahl cocke jelinek raviv \( bcjr \) soft input soft output \( siso \) maximum a posteriori probability \( map \) decoding process of rate 1 binary convolutional codes from this we establish some interesting explicit relationships between encoding and decoding of rate 1 convolutional codes we observe that the forward and backward bcjr siso map decoders can be simply represented by their dual siso channel encoders using shift registers in the complex number field similarly , the bidirectional map decoding can be implemented by linearly combining the shift register contents of the dual siso encoders of the respective forward and backward decoders the dual encoder structures for various recursive and non recursive rate 1 convolutional codes are derived
we study large scale distributed cooperative systems that use optimistic replication we represent a system as a graph of actions \( operations \) connected by edges that reify semantic constraints between actions constraint types include conflict , execution order , dependence , and atomicity the local state is some schedule that conforms to the constraints because of conflicts , client state is only tentative for consistency , site schedules should converge we designed a decentralised , asynchronous commitment protocol each client makes a proposal , reflecting its tentative and slash or preferred schedules our protocol distributes the proposals , which it decomposes into semantically meaningful units called candidates , and runs an election between comparable candidates a candidate wins when it receives a majority or a plurality the protocol is fully asynchronous each site executes its tentative schedule independently , and determines locally when a candidate has won an election the committed schedule is as close as possible to the preferences expressed by clients
the famous braess paradox describes the following phenomenon it might happen that the improvement of resources , like building a new street within a congested network , may in fact lead to larger costs for the players in an equilibrium in this paper we consider general nonatomic congestion games and give a characterization of the maximal combinatorial property of strategy spaces for which braess paradox does not occur in a nutshell , bases of matroids are exactly this maximal structure we prove our characterization by two novel sensitivity results for convex separable optimization problems over polymatroid base polyhedra which may be of independent interest
here we describe the share system , a web service based framework for distributed querying and reasoning on the semantic web the main innovations of share are \( 1 \) the extension of a sparql query engine to perform on demand data retrieval from web services , and \( 2 \) the extension of an owl reasoner to test property restrictions by means of web service invocations in addition to enabling queries across distributed datasets , the system allows for a target dataset that is significantly larger than is possible under current , centralized approaches although the architecture is equally applicable to all types of data , the share system targets bioinformatics , due to the large number of interoperable web services that are already available in this area share is built entirely on semantic web standards , and is the successor of the biomoby project
in distributed optimization and iterative consensus literature , a standard problem is for n agents to minimize a function f over a subset of euclidean space , where the cost function is expressed as a sum sum f i in this paper , we study the private distributed optimization \( pdop \) problem with the additional requirement that the cost function of the individual agents should remain differentially private the adversary attempts to infer information about the private cost functions from the messages that the agents exchange achieving differential privacy requires that any change of an individual 's cost function only results in unsubstantial changes in the statistics of the messages we propose a class of iterative algorithms for solving pdop , which achieves differential privacy and convergence to the optimal value our analysis reveals the dependence of the achieved accuracy and the privacy levels on the the parameters of the algorithm we observe that to achieve epsilon differential privacy the accuracy of the algorithm has the order of o \( frac 1 epsilon 2 \)
inverse problems involving systems of partial differential equations \( pdes \) with many measurements or experiments can be very expensive to solve numerically in a recent paper we examined dimensionality reduction methods , both stochastic and deterministic , to reduce this computational burden , assuming that all experiments share the same set of receivers in the present article we consider the more general and practically important case where receivers are not shared across experiments we propose a data completion approach to alleviate this problem this is done by means of an approximation using an appropriately restricted gradient or laplacian regularization , extending existing data for each experiment to the union of all receiver locations results using the method of simultaneous sources \( ss \) with the completed data are then compared to those obtained by a more general but slower random subset \( rs \) method which requires no modifications
classification is one of the major issues in data mining research fields the classification problems in medical area often classify medical dataset based on the result of medical diagnosis or description of medical treatment by the medical practitioner this research work discusses the classification process of gene expression data for three different cancers which are breast cancer , lung cancer and leukemia cancer with two classes which are cancerous stage and non cancerous stage we have applied a fuzzy soft set similarity based classifier to enhance the accuracy to predict the stages among cancer genes and the informative genes are selected by using entopy filtering
we present a practically appealing extension of the probabilistic model checker prism rendering it to handle fixed delay continuous time markov chains \( fdctmcs \) with rewards , the equivalent formalism to the deterministic and stochastic petri nets \( dspns \) fdctmcs allow transitions with fixed delays \( or timeouts \) on top of the traditional transitions with exponential rates our extension supports an evaluation of expected reward until reaching a given set of target states the main contribution is that , considering the fixed delays as parameters , we implemented a synthesis algorithm that computes the epsilon optimal values of the fixed delays minimizing the expected reward we provide a performance evaluation of the synthesis on practical examples
we study combinatorial auctions for the secondary spectrum market in this market , short term licenses shall be given to wireless nodes for communication in their local neighborhood in contrast to the primary market , channels can be assigned to multiple bidders , provided that the corresponding devices are well separated such that the interference is sufficiently low interference conflicts are described in terms of a conflict graph in which the nodes represent the bidders and the edges represent conflicts such that the feasible allocations for a channel correspond to the independent sets in the conflict graph in this paper , we suggest a novel lp formulation for combinatorial auctions with conflict graph using a non standard graph parameter , the so called inductive independence number taking into account this parameter enables us to bypass the well known lower bound of omega \( n 1 epsilon \) on the approximability of independent set in general graphs with n nodes \( bidders \) we achieve significantly better approximation results by showing that interference constraints for wireless networks yield conflict graphs with bounded inductive independence number our framework covers various established models of wireless communication , e g , the protocol or the physical model for the protocol model , we achieve an o \( sqrt k \) approximation , where k is the number of available channels for the more realistic physical model , we achieve an o \( sqrt k log 2 n \) approximation based on edge weighted conflict graphs combining our approach with the the lp based framework of lavi and swamy , we obtain incentive compatible mechanisms for general bidders with arbitrary valuations on bundles of channels specified in terms of demand oracles
spontaneous speech in the form of conversations , meetings , voice mail , interviews , oral history , etc is one of the most ubiquitous forms of human communication search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio video archives available to users this project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52 , 000 audiovisual testimonies of survivors and witnesses of the holocaust and other genocides the design incorporates faceted search , along with other ui elements like highlighted search items , tags , snippets , etc , to promote discovery and exploratory search two different designs have been created to support both manual and automated transcripts evaluation was performed using human subjects to measure accuracy in retrieving results , understanding user perspective on the design elements , and ease of parsing information
we investigate channel equalization for rayleigh fading channels under bounded channel uncertainties we analyze three robust methods to estimate an unknown signal transmitted through a rayleigh fading channel , where we avoid directly tuning the equalizer parameters to the available inaccurate channel information these methods are based on minimizing certain mean square error criteria that incorporate the channel uncertainties into the problem formulations we present closed form solutions to the channel equalization problems for each method and for both zero mean and nonzero mean signals we illustrate the performances of the equalization methods through simulations
the world wide web continues to grow at an amazing rate in both the size and complexity of web sites and is well on its way to being the main reservoir of information and data due to this increase in growth and complexity of www , web site publishers are facing increasing difficulty in attracting and retaining users to design popular and attractive websites publishers must understand their users needs therefore analyzing users behaviour is an important part of web page design web usage mining \( wum \) is the application of data mining techniques to web usage log repositories in order to discover the usage patterns that can be used to analyze the users navigational behavior wum contains three main steps preprocessing , knowledge extraction and results analysis the goal of the preprocessing stage in web usage mining is to transform the raw web log data into a set of user profiles each such profile captures a sequence or a set of urls representing a user session
in this part of the two part paper , the claim in part i , that superimposed pilots are superior at mitigating pilot contamination in massive multiple input multiple output \( mimo \) systems , is bolstered through additional performance metrics such as mean squared error \( mse \) and downlink signal to interference plus noise ratio \( sinr \) for a system employing superimposed pilots , it is shown that the mse of the channel estimate diminishes with increasing number of antennas at the base station \( bs \) , with the mse achieving its cramer rao lower bound asymptotically the downlink \( dl \) sinr is also shown to increase without bound at a rate proportional to the square root of the number of antennas at the bs furthermore , we extend the concept of the hybrid system and propose a framework for partitioning users into those that employ superimposed pilots and to those that employ time multiplexed pilots and data the superiority in the dl sinr performance of the channel estimator based on superimposed pilots and the higher throughput of the hybrid system are demonstrated by means of simulations
low rank tensor learning , such as tensor completion and multilinear multitask learning , has received much attention in recent years in this paper , we propose higher order matching pursuit for low rank tensor learning problems with a convex or a nonconvex cost function , which is a generalization of the matching pursuit type methods at each iteration , the main cost of the proposed methods is only to compute a rank one tensor , which can be done efficiently , making the proposed methods scalable to large scale problems moreover , storing the resulting rank one tensors is of low storage requirement , which can help to break the curse of dimensionality the linear convergence rate of the proposed methods is established in various circumstances along with the main methods , we also provide a method of low computational complexity for approximately computing the rank one tensors , with provable approximation ratio , which helps to improve the efficiency of the main methods and to analyze the convergence rate experimental results on synthetic as well as real datasets verify the efficiency and effectiveness of the proposed methods
before establishing a communication link in a cellular network , the user terminal must activate a synchronization procedure called initial cell search in order to acquire specific information about the serving base station to accomplish this task , the primary synchronization signal \( pss \) and secondary synchronization signal \( sss \) are periodically transmitted in the downlink of a long term evolution \( lte \) network since sss detection can be performed only after successful identification of the primary signal , in this work we present a robust scheme for joint pss detection , sector index identification and integer frequency offset \( ifo \) recovery in an lte system the proposed algorithm relies on the maximum likelihood \( ml \) estimation criterion and exploits a suitable reduced rank representation of the channel frequency response to take multipath distortions into account we show that some pss detection methods that were originally introduced through heuristic reasoning can be derived from our ml framework by selecting an appropriate model for the channel gains over the pss subcarriers numerical simulations indicate that the proposed scheme can be effectively applied in the presence of severe multipath propagation , where existing alternatives provide unsatisfactory performance
in this paper , we derive optimal transmission policies for energy harvesting sensors to maximize the utility obtained over a finite horizon first , we consider a single energy harvesting sensor , with discrete energy arrival process , and a discrete energy consumption policy under this model , we show that the optimal finite horizon policy is a threshold policy , and explicitly characterize the thresholds , and the thresholds can be precomputed using a recursion next , we address the case of multiple sensors , with only one of them allowed to transmit at any given time to avoid interference , and derive an explicit optimal policy for this scenario as well
we investigate the approximability of several classes of real valued functions by functions of a small number of variables \( em juntas \) our main results are tight bounds on the number of variables required to approximate a function f 0 , 1 n rightarrow 0 , 1 within ell 2 error epsilon over the uniform distribution 1 if f is submodular , then it is epsilon close to a function of o \( frac 1 epsilon 2 log frac 1 epsilon \) variables this is an exponential improvement over previously known results we note that omega \( frac 1 epsilon 2 \) variables are necessary even for linear functions 2 if f is fractionally subadditive \( xos \) it is epsilon close to a function of 2 o \( 1 epsilon 2 \) variables this result holds for all functions with low total ell 1 influence and is a real valued analogue of friedgut 's theorem for boolean functions we show that 2 omega \( 1 epsilon \) variables are necessary even for xos functions as applications of these results , we provide learning algorithms over the uniform distribution for xos functions , we give a pac learning algorithm that runs in time 2 poly \( 1 epsilon \) poly \( n \) for submodular functions we give an algorithm in the more demanding pmac learning model \( balcan and harvey , 2011 \) which requires a multiplicative 1 gamma factor approximation with probability at least 1 epsilon over the target distribution our uniform distribution algorithm runs in time 2 poly \( 1 \( gamma epsilon \) \) poly \( n \) this is the first algorithm in the pmac model that over the uniform distribution can achieve a constant approximation factor arbitrarily close to 1 for all submodular functions as follows from the lower bounds in \( feldman et al , 2013 \) both of these algorithms are close to optimal we also give applications for proper learning , testing and agnostic learning with value queries of these classes
in contrast to a maximum likelihood decoder , it is often desirable to use an incomplete decoder that can detect its decoding errors with high probability one common choice is the bounded distance decoder bounds are derived for the total word error rate , pw , and the undetected error rate , pu excellent agreement is found with simulation results for a small code , and the bounds are shown to be tractable for a larger code
this paper considers a two hop interference network , where two users transmit independent messages to their respective receivers with the help of two relay nodes the transmitters do not have direct links to the receivers instead , two relay nodes serve as intermediaries between the transmitters and receivers each hop , one from the transmitters to the relays and the other from the relays to the receivers , is modeled as a gaussian interference channel , thus the network is essentially a cascade of two interference channels for this network , achievable symmetric rates for different parameter regimes under decode and forward relaying and amplify and forward relaying are proposed and the corresponding coding schemes are carefully studied numerical results are also provided
randomness extraction is of fundamental importance for information theoretic cryptography it allows to transform a raw key about which an attacker has some limited knowledge into a fully secure random key , on which the attacker has essentially no information up to date , only very few randomness extraction techniques are known to work against an attacker holding quantum information on the raw key this is very much in contrast to the classical \( non quantum \) setting , which is much better understood and for which a vast amount of different techniques are known and proven to work we prove a new randomness extraction technique , which is known to work in the classical setting , to be secure against a quantum attacker as well randomness extraction is done by xor'ing a so called delta biased mask to the raw key our result allows to extend the classical applications of this extractor to the quantum setting we discuss the following two applications we show how to encrypt a long message with a short key , information theoretically secure against a quantum attacker , provided that the attacker has enough quantum uncertainty on the message this generalizes the concept of entropically secure encryption to the case of a quantum attacker as second application , we show how to do error correction without leaking partial information to a quantum attacker such a technique is useful in settings where the raw key may contain errors , since standard error correction techniques may provide the attacker with information on , say , a secret key that was used to obtain the raw key
a new variant of bit interleaved coded modulation \( bicm \) is proposed in the new scheme , called parallel bicm , l identical binary codes are used in parallel using a mapper , a newly proposed finite length interleaver and a binary dither signal as opposed to previous approaches , the scheme does not rely on any assumptions of an ideal , infinite length interleaver over a memoryless channel , the new scheme is proven to be equivalent to a binary memoryless channel therefore the scheme enables one to easily design coded modulation schemes using a simple binary code that was designed for that binary channel the overall performance of the coded modulation scheme is analytically evaluated based on the performance of the binary code over the binary channel the new scheme is analyzed from an information theoretic viewpoint , where the capacity , error exponent and channel dispersion are considered the capacity of the scheme is identical to the bicm capacity the error exponent of the scheme is numerically compared to a recently proposed mismatched decoding exponent analysis of bicm
the bandwidth of the sampling systems , especially for time interleaved analog to digital converters , needs to be extended along with the rapid increase of the sampling rate a digitally assisted technique becomes a feasible approach to extend the analog bandwidth , as it is impractical to implement the extension in analog circuits this paper derives accurate order estimation formulas for the bandwidth extension filter , which is designed in the minimax sense with the ripple constraints as the design criteria the derived filter order estimation is significant in evaluating the computational complexity from the viewpoint of the top level system design moreover , with the proposed order estimates , one can conveniently obtain the minimal order that satisfies the given ripple constraints , which contributes to reducing the design time both the performance of the extension filter and its order estimation are illustrated and demonstrated through simulation examples
we develop a macro model of information retrieval process using game theory as a mathematical theory of conflicts we represent the participants of the information retrieval process as a game of two abstract players the first player is the `intellectual crowd' of users of search engines , the second is a community of information retrieval systems in order to apply game theory , we treat search log data as nash equilibrium strategies and solve the inverse problem of finding appropriate payoff functions for that , we suggest a particular model , which we call alpha model within this model , we suggest a method , called shifting , which makes it possible to partially control the behavior of massive users this note is addressed to researchers in both game theory \( providing a new class of real life problems \) and information retrieval , for whom we present new techniques to control the ir environment
this paper presents a tutorial on stochastic geometry \( sg \) based analysis for cellular networks this tutorial is distinguished by its depth with respect to wireless communication details and its focus on cellular networks the paper starts by modeling and analyzing the baseband interference in a basic cellular network model then , it characterizes signal to interference plus noise ratio \( sinr \) and its related performance metrics in particular , a unified approach to conduct error probability , outage probability , and rate analysis is presented although the main focus of the paper is on cellular networks , the presented unified approach applies for other types of wireless networks that impose interference protection around receivers the paper then extends the baseline unified approach to capture cellular network characteristics \( e g , frequency reuse , multiple antenna , power control , etc \) it also presents numerical examples associated with demonstrations and discussions finally , we point out future research directions
in this paper we present our experience in using visualization in mathematics education the experience with our university courses computer tools in matematics and symbolic algebra provides the basis for mathematics teacher education program http vizuelizacija etf rs the program is intended for elementary and high school teachers the education program deals with modern techniques of visualization by using technologies such as geogegebra , java and html
methods for automated discovery of causal relationships from non interventional data have received much attention recently a widely used and well understood model family is given by linear acyclic causal models \( recursive structural equation models \) for gaussian data both constraint based methods \( spirtes et al , 1993 pearl , 2000 \) \( which output a single equivalence class \) and bayesian score based methods \( geiger and heckerman , 1994 \) \( which assign relative scores to the equivalence classes \) are available on the contrary , all current methods able to utilize non gaussianity in the data \( shimizu et al , 2006 hoyer et al , 2008 \) always return only a single graph or a single equivalence class , and so are fundamentally unable to express the degree of certainty attached to that output in this paper we develop a bayesian score based approach able to take advantage of non gaussianity when estimating linear acyclic causal models , and we empirically demonstrate that , at least on very modest size networks , its accuracy is as good as or better than existing methods we provide a complete code package \( in r \) which implements all algorithms and performs all of the analysis provided in the paper , and hope that this will further the application of these methods to solving causal inference problems
in the last fifteen years , the high performance computing \( hpc \) community has claimed for parallel programming environments that reconciles generality , higher level of abstraction , portability , and efficiency for distributed memory parallel computing platforms the hash component model appears as an alternative for addressing hpc community claims for fitting these requirements this paper presents foundations that will enable a parallel programming environment based on the hash model to address the problems of debugging , performance evaluation and verification of formal properties of parallel program by means of a powerful , simple , and widely adopted formalism petri nets
information on the future state of time varying frequency selective channels can significantly enhance the effectiveness of feedback in adaptive and limited feedback mimo ofdm systems this paper investigates the parametric extrapolation of wideband mimo channels using variations of the double directional mimo model we propose three predictors which estimate parameters of the channel using 4d , 3d and 2d extensions of the esprit algorithm and predict future states of the channel using the models furthermore , using the vector formulation of the cramer rao lower bound for functions of parameters , we derive a bound on the prediction error in wideband mimo channels numerical simulations are used to evaluate the performance of the proposed algorithms under different channel and transmission conditions , and a comparison is made with the derived error bound
this paper focusses on mental state adjectives and offers a unified analysis in the theory of generative lexicon \( pustejovsky , 1991 , 1995 \) we show that , instead of enumerating the various syntactic constructions they enter into , with the different senses which arise , it is possible to give them a rich typed semantic representation which will explain both their semantic and syntactic polymorphism
we propose a new riemannian geometry for fixed rank matrices that is specifically tailored to the low rank matrix completion problem exploiting the degree of freedom of a quotient space , we tune the metric on our search space to the particular least square cost function at one level , it illustrates in a novel way how to exploit the versatile framework of optimization on quotient manifold at another level , our algorithm can be considered as an improved version of lmafit , the state of the art gauss seidel algorithm we develop necessary tools needed to perform both first order and second order optimization in particular , we propose gradient descent schemes \( steepest descent and conjugate gradient \) and trust region algorithms we also show that , thanks to the simplicity of the cost function , it is numerically cheap to perform an exact linesearch given a search direction , which makes our algorithms competitive with the state of the art on standard low rank matrix completion instances
radio frequency \( rf \) impairments in the transceiver hardware of communication systems \( e g , phase noise \( pn \) , high power amplifier \( hpa \) nonlinearities , or in phase quadrature phase \( i q \) imbalance \) can severely degrade the performance of traditional multiple input multiple output \( mimo \) systems although calibration algorithms can partially compensate these impairments , the remaining distortion still has substantial impact despite this , most prior works have not analyzed this type of distortion in this paper , we investigate the impact of residual transceiver hardware impairments on the mimo system performance in particular , we consider a transceiver impairment model , which has been experimentally validated , and derive analytical ergodic capacity expressions for both exact and high signal to noise ratios \( snrs \) we demonstrate that the capacity saturates in the high snr regime , thereby creating a finite capacity ceiling we also present a linear approximation for the ergodic capacity in the low snr regime , and show that impairments have only a second order impact on the capacity furthermore , we analyze the effect of transceiver impairments on large scale mimo systems interestingly , we prove that if one increases the number of antennas at one side only , the capacity behaves similar to the finite dimensional case on the contrary , if the number of antennas on both sides increases with a fixed ratio , the capacity ceiling vanishes thus , impairments cause only a bounded offset in the capacity compared to the ideal transceiver hardware case
both statistical and rule based approaches to part of speech \( pos \) disambiguation have their own advantages and limitations especially for korean , the narrow windows provided by hidden markov model \( hmm \) cannot cover the necessary lexical and long distance dependencies for pos disambiguation on the other hand , the rule based approaches are not accurate and flexible to new tag sets and languages in this regard , the statistical rule based hybrid method that can take advantages of both approaches is called for the robust and flexible pos disambiguation we present one of such method , that is , a two phase learning architecture for the hybrid statistical rule based pos disambiguation , especially for korean in this method , the statistical learning of morphological tagging is error corrected by the rule based learning of brill 1992 style tagger we also design the hierarchical and flexible korean tag set to cope with the multiple tagging applications , each of which requires different tag set our experiments show that the two phase learning method can overcome the undesirable features of solely hmm based or solely rule based tagging , especially for morphologically complex korean
improving performance and delivering value for customers have become a central theme in business the software industry has become an increasingly important sector for the economy growth in tunisia this study aims to show how using value management in the tunisian software industry for project analysis gives new insight about true project value and performance this new approach is considered as an appropriate tool for guiding the process of making decisions it offers tools in order to analyze the service value from the customer and organization perspectives the results showed that the vm allows to have better performance in the software development project by linking customer satisfaction and cost analysis the present case shows to service managers how they can benchmark project function to reduce their costs and improve resource allocation taking into consideration what customers consider important during their overall service experience it can identify best professional practices , orient decisions to improve service value
although employment web sites have recently become the main source for re cruitment and selection process , the relation between those sites and unemploy ment rates is seldom addressed deriving data from 32 countries and 427 web sites , this study explores the correlation between unemployment rates of european countries and the attractiveness of country specific employment web sites it also compares the changes in unemployment rates and traffic on all the aforementioned web sites the results showed that there is a strong correlation between web sites traffic and unemployment rates
searching for and making decisions about information is becoming increasingly difficult as the amount of information and number of choices increases recommendation systems help users find items of interest of a particular type , such as movies or restaurants , but are still somewhat awkward to use our solution is to take advantage of the complementary strengths of personalized recommendation systems and dialogue systems , creating personalized aides we present a system the adaptive place advisor that treats item selection as an interactive , conversational process , with the program inquiring about item attributes and the user responding individual , long term user preferences are unobtrusively obtained in the course of normal recommendation dialogues and used to direct future conversations with the same user we present a novel user model that influences both item search and the questions asked during a conversation we demonstrate the effectiveness of our system in significantly reducing the time and number of interactions required to find a satisfactory item , as compared to a control group of users interacting with a non adaptive version of the system
in this article we investigate the relationships between the classical notions of weakest precondition and weakest liberal precondition , and provide several results , namely that in general , weakest liberal precondition is neither stronger nor weaker than weakest precondition , however , given a deterministic and terminating sequential while program and a postcondition , they are equivalent hence , in such situation , it does not matter which definition is used
this letter is on the performance of the turbo signal recovery \( tsr \) algorithm for partial discrete fourier transform \( dft \) matrices based compressed sensing based on state evolution analysis , we prove that tsr with a partial dft sensing matrix outperforms the well known approximate message passing \( amp \) algorithm with an independent identically distributed \( iid \) sensing matrix
we describe a unified and coherent syntactic framework for supporting a semantically informed syntactic approach to statistical machine translation semantically enriched syntactic tags assigned to the target language training texts improved translation quality the resulting system significantly outperformed a linguistically naive baseline model \( hiero \) , and reached the highest scores yet reported on the nist 2009 urdu english translation task this finding supports the hypothesis \( posed by many researchers in the mt community , e g , in darpa gale \) that both syntactic and semantic information are critical for improving translation quality and further demonstrates that large gains can be achieved for low resource languages with different word order than english
an updated version will be uploaded later
the advent of social media has provided an extraordinary , if imperfect , 'big data' window into the form and evolution of social networks based on nearly 40 million message pairs posted to twitter between september 2008 and february 2009 , we construct and examine the revealed social network structure and dynamics over the time scales of days , weeks , and months at the level of user behavior , we employ our recently developed hedonometric analysis methods to investigate patterns of sentiment expression we find users' average happiness scores to be positively and significantly correlated with those of users one , two , and three links away we strengthen our analysis by proposing and using a null model to test the effect of network topology on the assortativity of happiness we also find evidence that more well connected users write happier status updates , with a transition occurring around dunbar 's number more generally , our work provides evidence of a social sub network structure within twitter and raises several methodological points of interest with regard to social network reconstructions
we consider the problem of partitioning n integers into two subsets of given cardinalities such that the discrepancy , the absolute value of the difference of their sums , is minimized the integers are i i d random variables chosen uniformly from the set 1 , , m we study how the typical behavior of the optimal partition depends on n , m and the bias s , the difference between the cardinalities of the two subsets in the partition in particular , we rigorously establish this typical behavior as a function of the two parameters kappa n 1 log 2m and b s n by proving the existence of three distinct ``phases'' in the kappa b plane , characterized by the value of the discrepancy and the number of optimal solutions a ``perfect phase'' with exponentially many optimal solutions with discrepancy 0 or 1 a ``hard phase'' with minimal discrepancy of order me theta \( n \) and a ``sorted phase'' with an unique optimal partition of order mn , obtained by putting the \( s n \) 2 smallest integers in one subset our phase diagram covers all but a relatively small region in the kappa b plane we also show that the three phases can be alternatively characterized by the number of basis solutions of the associated linear programming problem , and by the fraction of these basis solutions whose pm 1 valued components form optimal integer partitions of the subproblem with the corresponding weights we show in particular that this fraction is one in the sorted phase , and exponentially small in both the perfect and hard phases , and strictly exponentially smaller in the hard phase than in the perfect phase open problems are discussed , and numerical experiments are presented
let b be a finite collection of geometric \( not necessarily convex \) bodies in the plane clearly , this class of geometric objects naturally generalizes the class of disks , lines , ellipsoids , and even convex polygons we consider geometric intersection graphs gb where each body of the collection b is represented by a vertex , and two vertices of gb are adjacent if the intersection of the corresponding bodies is non empty for such graph classes and under natural restrictions on their maximum degree or subgraph exclusion , we prove that the relation between their treewidth and the maximum size of a grid minor is linear these combinatorial results vastly extend the applicability of all the meta algorithmic results of the bidimensionality theory to geometrically defined graph classes
consider a two way communication scenario where two single antenna nodes , operating under full duplex mode , exchange information to one another through the aid of a \( full duplex \) multi antenna relay , and there is another single antenna node who intends to eavesdrop the relay employs artificial noise \( an \) to interfere the eavesdropper 's channel , and amplify forward \( af \) alamouti based rank two beamforming to establish the two way communication links of the legitimate nodes our problem is to optimize the rank two beamformer and an covariance for sum secrecy rate maximization \( ssrm \) this ssrm problem is nonconvex , and we develop an efficient solution approach using semidefinite relaxation \( sdr \) and minorization maximization \( mm \) we prove that sdr is tight for the ssrm problem and thus introduces no loss also , we consider an inexact mm method where an approximately but computationally cheap mm solution update is used in place of the exact update in conventional mm we show that this inexact mm method guarantees convergence to a stationary solution to the ssrm problem the effectiveness of our proposed approach is further demonstrated by an energy harvesting scenario extension , and by extensive simulation results
input buffered switches with virtual output queueing \( voq \) can be unstable when presented with unbalanced loads existing scheduling algorithms , including islip for input queued \( iq \) switches and round robin \( rr \) for combined input and crossbar queued \( cicq \) switches , exhibit instability for some schedulable loads we investigate the use of a queue length threshold and bursting mechanism to achieve stability without requiring internal speed up an analytical model is developed to prove that the burst stabilization protocol achieves stability and to predict the minimum burst value needed as a function of offered load the analytical model is shown to have very good agreement with simulation results these results show the advantage of the rr rr cicq switch as a contender for the next generation of high speed switches
a patient centric approach to healthcare leads to an informal social network among medical professionals this chapter presents a research framework to identify the collaboration structure among physicians that is effective and efficient for patients , discover effective structural attributes of a collaboration network that evolves during the course of providing care , and explore the impact of socio demographic characteristics of healthcare professionals , patients , and hospitals on collaboration structures , from the point of view of measurable outcomes such as cost and quality of care the framework uses illustrative examples drawn from a data set of patients undergoing hip replacement surgery
we study an equivalence of \( i \) deterministic pathwise statements appearing in the online learning literature \( termed emph regret bounds \) , \( ii \) high probability tail bounds for the supremum of a collection of martingales \( of a specific form arising from uniform laws of large numbers for martingales \) , and \( iii \) in expectation bounds for the supremum by virtue of the equivalence , we prove exponential tail bounds for norms of banach space valued martingales via deterministic regret bounds for the online mirror descent algorithm with an adaptive step size we extend these results beyond the linear structure of the banach space we define a notion of emph martingale type for general classes of real valued functions and show its equivalence \( up to a logarithmic factor \) to various sequential complexities of the class \( in particular , the sequential rademacher complexity and its offset version \) for classes with the general martingale type 2 , we exhibit a finer notion of variation that allows partial adaptation to the function indexing the martingale our proof technique rests on sequential symmetrization and on certifying the emph existence of regret minimization strategies for certain online prediction problems
causal models defined in terms of a collection of equations , as defined by pearl , are axiomatized here axiomatizations are provided for three successively more general classes of causal models \( 1 \) the class of recursive theories \( those without feedback \) , \( 2 \) the class of theories where the solutions to the equations are unique , \( 3 \) arbitrary theories \( where the equations may not have solutions and , if they do , they are not necessarily unique \) it is shown that to reason about causality in the most general third class , we must extend the language used by galles and pearl in addition , the complexity of the decision procedures is characterized for all the languages and classes of models considered
to decrease the training overhead and improve the channel estimation accuracy in uplink cloud radio access networks \( c rans \) , a superimposed segment training design is proposed the core idea of the proposal is that each mobile station superimposes a periodic training sequence on the data signal , and each remote radio heads prepends a separate pilot to the received signal before forwarding it to the centralized base band unit pool moreover , a complex exponential basis expansion model based channel estimation algorithm to maximize a posteriori probability is developed , where the basis expansion model coefficients of access links \( als \) and the channel fading of wireless backhaul links are first obtained , after which the time domain channel samples of als are restored in terms of maximizing the average effective signal to noise ratio \( aesnr \) simulation results show that the proposed channel estimation algorithm can effectively decrease the estimation mean square error and increase the aesnr in c rans , thus significantly outperforming the existing solutions
we consider a set up where a file of size m is stored in n distributed storage nodes , using an \( n , k \) minimum storage regenerating \( msr \) code , i e , a maximum distance separable \( mds \) code that also allows efficient exact repair of any failed node the problem of interest in this paper is to minimize the repair bandwidth b for exact regeneration of a single failed node , i e , the minimum data to be downloaded by a new node to replace the failed node by its exact replica previous work has shown that a bandwidth of b m \( n 1 \) k \( n k \) is necessary and sufficient for functional \( not exact \) regeneration it has also been shown that if k max \( n 2 , 3 \) , then there is no extra cost of exact regeneration over functional regeneration the practically relevant setting of low redundancy , i e , k n 1 2 remains open for k 3 and it has been shown that there is an extra bandwidth cost for exact repair over functional repair in this case in this work , we adopt into the distributed storage context an asymptotically optimal interference alignment scheme previously proposed by cadambe and jafar for large wireless interference networks with this scheme we solve the problem of repair bandwidth minimization for \( n , k \) exact msr codes for all \( n , k \) values including the previously open case of k max \( n 2 , 3 \) our main result is that , for any \( n , k \) , and sufficiently large file sizes , there is no extra cost of exact regeneration over functional regeneration in terms of the repair bandwidth per bit of regenerated data more precisely , we show that in the limit as m approaches infinity , the ratio b m \( n 1 \) \( k \( n k \) \)
the binary primitive triple error correcting bch code is a cyclic code of minimum distance 7 with generator polynomial having zeros alpha , alpha 3 and alpha 5 where alpha is a primitive root of unity the zero set of the code is said to be 1 , 3 , 5 in the 1970 's kasami showed that one can construct similar triple error correcting codes using zero sets consisting of different triples than the bch codes furthermore , in 2000 chang et al found new triples leading to triple error correcting codes in this paper a new such triple is presented in addition a new method is presented that may be of interest in finding further such triples
we present kovalenko 's full rank limit as a tight lower bound for decoding error probability of ldpc codes and lt codes over bec from the limit , we derive a full rank overhead as a lower bound for stable overheads for successful maximum likelihood decoding of the codes
this paper describes an abstract machine for linguistic formalisms that are based on typed feature structures , such as hpsg the core design of the abstract machine is given in detail , including the compilation process from a high level language to the abstract machine language and the implementation of the abstract instructions the machine 's engine supports the unification of typed , possibly cyclic , feature structures a separate module deals with control structures and instructions to accommodate parsing for phrase structure grammars we treat the linguistic formalism as a high level declarative programming language , applying methods that were proved useful in computer science to the study of natural languages a grammar specified using the formalism is endowed with an operational semantics
the paging problem is that of deciding which pages to keep in a memory of k pages in order to minimize the number of page faults this paper introduces the marking algorithm , a simple randomized on line algorithm for the paging problem , and gives a proof that its performance guarantee \( competitive ratio \) is o \( log k \) in contrast , no deterministic on line algorithm can have a performance guarantee better than k
the uniform one dimensional fragment of first order logic , u1 , is a recently introduced formalism that extends two variable logic in a natural way to contexts with relations of all arities we survey properties of u1 and investigate its relationship to description logics designed to accommodate higher arity relations , with particular attention given to dlr reg we also define a description logic version of a variant of u1 and prove a range of new results concerning the expressivity of u1 and related logics
we introduce the emph resource control cube , a system consisting of eight intuitionistic lambda calculi with either implicit or explicit control of resources and with either natural deduction or sequent calculus the four calculi of the cube that correspond to natural deduction have been proposed by kesner and renaud and the four calculi that correspond to sequent lambda calculi are introduced in this paper the presentation is parameterized with the set of resources \( weakening or contraction \) , which enables a uniform treatment of the eight calculi of the cube the simply typed resource control cube , on the one hand , expands the curry howard correspondence to intuitionistic natural deduction and intuitionistic sequent logic with implicit or explicit structural rules and , on the other hand , is related to substructural logics we propose a general intersection type system for the resource control cube calculi our main contribution is a characterisation of strong normalisation of reductions in this cube first , we prove that typeability implies strong normalisation in the ''natural deduction base of the cube by adapting the reducibility method we then prove that typeability implies strong normalisation in the ' 'sequent base of the cube by using a combination of well orders and a suitable embedding in the ''natural deduction base finally , we prove that strong normalisation implies typeability in the cube using head subject expansion all proofs are general and can be made specific to each calculus of the cube by instantiating the set of resources
one way to write fast programs is to explore the potential parallelism and take advantage of the high number of cores available in microprocessors this can be achieved by manually specifying which code executes on which thread , by using compiler parallelization hints \( such as openmp or cilk \) , or by using a parallel programming language \( such as x10 , chapel or aeminium regardless of the approach , all of these programs are compiled to an intermediate lower level language that is sequential , thus preventing the backend compiler from optimizing the program and observing its parallel nature this paper presents miso , an intermediate language that expresses the parallel nature of programs and that can be targeted by front end compilers the language defines 'cells' , which are composed by a state and a transition function from one state to the next this language can express both sequential and parallel programs , and provides information for a backend compiler to generate efficient parallel programs moreover , miso can be used to automatically add redundancy to a program , by replicating the state or by taking advantage of different processor cores , in order to provide fault tolerance for programs running on unreliable hardware
this note provides a family of classification problems , indexed by a positive integer k , where all shallow networks with fewer than exponentially \( in k \) many nodes exhibit error at least 1 3 , whereas a deep network with 2 nodes in each of 2k layers achieves zero error , as does a recurrent network with 3 distinct nodes iterated k times the proof is elementary , and the networks are standard feedforward networks with relu \( rectified linear unit \) nonlinearities
the motivation of this work is to illustrate the efficiency of some often overlooked alternatives to deal with optimization problems in systems and control in particular , we will consider a problem for which an iterative linear matrix inequality algorithm \( ilmi \) has been proposed recently as it often happens , this algorithm does not have guaranteed global convergence and therefore many methods may perform better we will put forward how some general purpose optimization solvers are more suited than the ilmi this is illustrated with the considered problem and example , but the general observations remain valid for many similar situations in the literature
we propose a novel general algorithm lhac that efficiently uses second order information to train a class of large scale l1 regularized problems our method executes cheap iterations while achieving fast local convergence rate by exploiting the special structure of a low rank matrix , constructed via quasi newton approximation of the hessian of the smooth loss function a greedy active set strategy , based on the largest violations in the dual constraints , is employed to maintain a working set that iteratively estimates the complement of the optimal active set this allows for smaller size of subproblems and eventually identifies the optimal active set empirical comparisons confirm that lhac is highly competitive with several recently proposed state of the art specialized solvers for sparse logistic regression and sparse inverse covariance matrix selection
this paper is a short description of an information retrieval system enhanced by three model driven retrieval services \( 1 \) co word analysis based query expansion , re ranking via \( 2 \) bradfordizing and \( 3 \) author centrality the different services each favor quite other but still relevant documents than pure term frequency based rankings each service can be interactively combined with each other to allow an iterative retrieval refinement
we study the parameterized complexity of the connected version of the vertex cover problem , where the solution set has to induce a connected subgraph although this problem does not admit a polynomial kernel for general graphs \( unless np is a subset of conp poly \) , for planar graphs guo and niedermeier icalp'08 showed a kernel with at most 14k vertices , subsequently improved by wang et al mfcs'11 to 4k the constant 4 here is so small that a natural question arises could it be already an optimal value for this problem \? in this paper we answer this quesion in negative we show a \( 11 3 \) k vertex kernel for connected vertex cover in planar graphs we believe that this result will motivate further study in search for an optimal kernel
in this letter , we analyze power and rate adaptation in a point to point link with rayleigh fading and impulsive interference we model the impulsive interference as a bernoulli gaussian random process adaptation is used to maximize the average spectral efficiency by changing power and rate of the transmission subject to an average power and instantaneous probability of error constraints without impulsive interference , it is well known that water filling is optimal for block fading we provide two simple schemes that show that the conventional water filling algorithm is not optimal in an impulsive interference channel
following an early work of dwork and stockmeyer on interactive proof systems whose verifiers are two way probabilistic finite automata , the authors initiated in 2004 a study on the computational power of quantum interactive proof systems whose verifiers are particularly limited to quantum finite automata as a follow up to the authors' early journal publication j comput system sci , vol 75 , pp 255 269 , 2009 , we further investigate the quantum nature of interactions between provers and verifiers by studying how various restrictions on quantum interactive proof systems affect the language recognition power of the proof systems in particular , we examine three intriguing restrictions that \( i \) provers always behave in a classical fashion , \( ii \) verifiers always reveal to provers the information on next moves , and \( iii \) the number of interactions between provers and verifiers is bounded
we develop a complete state space solution to h 2 optimal decentralized control of poset causal systems with state feedback our solution is based on the exploitation of a key separability property of the problem , that enables an efficient computation of the optimal controller by solving a small number of uncoupled standard riccati equations our approach gives important insight into the structure of optimal controllers , such as controller degree bounds that depend on the structure of the poset a novel element in our state space characterization of the controller is a remarkable pair of transfer functions , that belong to the incidence algebra of the poset , are inverses of each other , and are intimately related to prediction of the state along the different paths on the poset the results are illustrated by a numerical example
we show that lambda calculus is a computation model which can step by step simulate any sequential deterministic algorithm for any computable function over integers or words or any datatype more formally , given an algorithm above a family of computable functions \( taken as primitive tools , i e , kind of oracle functions for the algorithm \) , for every constant k big enough , each computation step of the algorithm can be simulated by exactly k successive reductions in a natural extension of lambda calculus with constants for functions in the above considered family the proof is based on a fixed point technique in lambda calculus and on gurevich sequential thesis which allows to identify sequential deterministic algorithms with abstract state machines this extends to algorithms for partial computable functions in such a way that finite computations ending with exceptions are associated to finite reductions leading to terms with a particular very simple feature
we propose a novel method for hierarchical topic detection where topics are obtained by clustering documents in multiple ways specifically , we model document collections using a class of graphical models called hierarchical latent tree models \( hltms \) the variables at the bottom level of an hltm are observed binary variables that represent the presence absence of words in a document the variables at other levels are discrete latent variables , with those at the second level representing word co occurrence patterns and those at higher levels representing co occurrence of patterns at the level below each latent variable gives a soft partition of the documents , and document clusters in the partitions are interpreted as topics latent variables at high levels of the hierarchy capture long range word co occurrence patterns and hence give thematically more general topics , while those at low levels of the hierarchy capture short range word co occurrence patterns and give thematically more specific topics compared with lda based topic models , a key advantage of hltms is that they , as graphical models , explicitly model the dependence and independence structure among topics and words , which is conducive to the discovery of meaningful topics and topic hierarchies
a popular approach within the signal processing and machine learning communities consists in modelling signals as sparse linear combinations of atoms selected from a learned dictionary while this paradigm has led to numerous empirical successes in various fields ranging from image to audio processing , there have only been a few theoretical arguments supporting these evidences in particular , sparse coding , or sparse dictionary learning , relies on a non convex procedure whose local minima have not been fully analyzed yet in this paper , we consider a probabilistic model of sparse signals , and show that , with high probability , sparse coding admits a local minimum around the reference dictionary generating the signals our study takes into account the case of over complete dictionaries , noisy signals , and possible outliers , thus extending previous work limited to noiseless settings and or under complete dictionaries the analysis we conduct is non asymptotic and makes it possible to understand how the key quantities of the problem , such as the coherence or the level of noise , can scale with respect to the dimension of the signals , the number of atoms , the sparsity and the number of observations
in this paper , we investigate the spectral efficiency \( se \) of massive multiple input multiple output \( mimo \) systems with a large number of antennas at the base station \( bs \) accounting for physical space constraints in contrast to the vast body of related literature , which considers fixed inter element spacing , we elaborate on a practical topology in which an increase in the number of antennas in a fixed total space induces an inversely proportional decrease in the inter antenna distance for this scenario , we derive exact and approximate expressions , as well as simplified upper lower bounds , for the se of maximum ratio combining \( mrc \) , zero forcing \( zf \) and minimum mean squared error receivers \( mmse \) receivers in particular , our analysis shows that the mrc receiver is non optimal for space constrained massive mimo topologies on the other hand , zf and mmse receivers can still deliver an increasing se as the number of bs antennas grows large numerical results corroborate our analysis and show the effect of the number of antennas , the number of users , and the total antenna array space on the sum se performance
despite large scale availability of social data , our understanding of the basic laws governing human behaviour remains limited , owing to the lack of a proper framework which can capture the interplay of various interdependent factors affecting social interactions in the recent years , multilayer networks has increasingly been realized to provide an efficient framework for understanding the intricacies of complex real world systems the present study encompasses the multilayer network analysis of bollywood , the largest film industry of the world , comprising of a massive time varying social data making around 1500 films annually , bollywood has emerged as a globally recognized and appreciated platform for cultural exchange this film industry acts as a mirror of the society and the rapidly changing nature of the society is reflected in the depictions of films this renders this model system to provide a ripe platform to understand social behaviour by analyzing the patterns of evolution and the success of individuals in the society similarity in the degree distribution across the individual layers validates our basis of multilayering while the degree degree correlations of the whole networks reveal that in general nodes do not exhibit any particular preference for pairing up , multilayer framework indicates the gradual restoration of cooperation in the recent dataset working in more number of genres comes up as an intrinsic property of lead nodes further , versatility in pairs and triads has been shown to demonstrate its impact on the success of lead nodes while repeated cooperation in pairs of dissimilar types of nodes has been shown to yield success to the lead nodes , triads of similar types of nodes turn out to be more successful weak ties analysis emphasize on the importance of every type of node in the society
relating formal grammars is a hard problem that balances between language equivalence \( which is known to be undecidable \) and grammar identity \( which is trivial \) in this paper , we investigate several milestones between those two extremes and propose a methodology for inconsistency management in grammar engineering while conventional grammar convergence is a practical approach relying on human experts to encode differences as transformation steps , guided grammar convergence is a more narrowly applicable technique that infers such transformation steps automatically by normalising the grammars and establishing a structural equivalence relation between them this allows us to perform a case study with automatically inferring bidirectional transformations between 11 grammars \( in a broad sense \) of the same artificial functional language parser specifications with different combinator libraries , definite clause grammars , concrete syntax definitions , algebraic data types , metamodels , xml schemata , object models
topic modeling based on latent dirichlet allocation \( lda \) has been a framework of choice to perform scene recognition and annotation recently , a new type of topic model called the document neural autoregressive distribution estimator \( docnade \) was proposed and demonstrated state of the art performance for document modeling in this work , we show how to successfully apply and extend this model to the context of visual scene modeling specifically , we propose supdocnade , a supervised extension of docnade , that increases the discriminative power of the hidden topic features by incorporating label information into the training objective of the model we also describe how to leverage information about the spatial position of the visual words and how to embed additional image annotations , so as to simultaneously perform image classification and annotation we test our model on the scene15 , labelme and uiuc sports datasets and show that it compares favorably to other topic models such as the supervised variant of lda
research on bias in machine learning algorithms has generally been concerned with the impact of bias on predictive accuracy we believe that there are other factors that should also play a role in the evaluation of bias one such factor is the stability of the algorithm in other words , the repeatability of the results if we obtain two sets of data from the same phenomenon , with the same underlying probability distribution , then we would like our learning algorithm to induce approximately the same concepts from both sets of data this paper introduces a method for quantifying stability , based on a measure of the agreement between concepts we also discuss the relationships among stability , predictive accuracy , and bias
correct inference of genetic regulations inside a cell is one of the greatest challenges in post genomic era for the biologist and researchers several intelligent techniques and models were already proposed to identify the regulatory relations among genes from the biological database like time series microarray data recurrent neural network \( rnn \) is one of the most popular and simple approach to model the dynamics as well as to infer correct dependencies among genes in this paper , bat algorithm \( ba \) was applied to optimize the model parameters of rnn model of gene regulatory network \( grn \) initially the proposed method is tested against small artificial network without any noise and the efficiency was observed in term of number of iteration , number of population and ba optimization parameters the model was also validated in presence of different level of random noise for the small artificial network and that proved its ability to infer the correct inferences in presence of noise like real world dataset in the next phase of this research , ba based rnn is applied to real world benchmark time series microarray dataset of e coli the results shown that it can able to identify the maximum true positive regulation but also include some false positive regulations therefore , ba is very suitable for identifying biological plausible grn with the help rnn model
interacting systems are increasingly common many examples pervade our everyday lives automobiles , aircraft , defense systems , telephone switching systems , financial systems , national governments , and so on closer to computer science , embedded systems and systems of systems are further examples of interacting systems common to all of these is that some whole is made up of constituent parts , and these parts interact with each other by design , these interactions are intentional , but it is the unintended interactions that are problematic the systems of systems literature uses the terms constituent systems and constituents to refer to systems that interact with each other that practice is followed here this paper presents a visual formalism , swim lane event driven petri nets , that is proposed as a basis for model based testing \( mbt \) of interacting systems in the absence of available tools , this model can only support the offline form of model based testing
this paper considers downlink multicast transmit beamforming for secure layered transmission systems with wireless simultaneous information and power transfer we study the power allocation algorithm design for minimizing the total transmit power in the presence of passive eavesdroppers and energy harvesting receivers the algorithm design is formulated as a non convex optimization problem our problem formulation promotes the dual use of energy signals in providing secure communication and facilitating efficient energy transfer besides , we take into account a minimum required power for energy harvesting at the idle receivers and heterogeneous quality of service \( qos \) requirements for the multicast video receivers in light of the intractability of the problem , we reformulate the considered problem by replacing a non convex probabilistic constraint with a convex deterministic constraint then , a semidefinite programming relaxation \( sdr \) approach is adopted to obtain an upper solution for the reformulated problem subsequently , sufficient conditions for the global optimal solution of the reformulated problem are revealed furthermore , we propose two suboptimal power allocation schemes based on the upper bound solution simulation results demonstrate the excellent performance and significant transmit power savings achieved by the proposed schemes compared to isotropic energy signal generation
we consider a simple information theoretic model of communication , in which two species of bacteria have the option of exchanging information about their environment , thereby improving their chances of survival for this purpose , we model a system consisting of two species whose dynamics in the world are modelled by a bet hedging strategy it is well known that such models lend themselves to elegant information theoretical interpretations by relating their respective long term growth rate to the information the individual species has about its environment we are specifically interested in modelling how this dynamics are affected when the species interact cooperatively or in an antagonistic way in a scenario with limited resources for this purpose , we consider the exchange of environmental information between the two species in the framework of a game our results show that a transition from a cooperative to an antagonistic behaviour in a species results as a response to a change in the availability of resources species cooperate in abundance of resources , while they behave antagonistically in scarcity
we observe that certain large clique graph triangulations can be useful to reduce both computational and space requirements when making queries on mixed stochastic deterministic graphical models we demonstrate that many of these large clique triangulations are non minimal and are thus unattainable via the variable elimination algorithm we introduce ancestral pairs as the basis for novel triangulation heuristics and prove that no more than the addition of edges between ancestral pairs need be considered when searching for state space optimal triangulations in such graphs empirical results on random and real world graphs show that the resulting triangulations that yield significant speedups are almost always non minimal we also give an algorithm and correctness proof for determining if a triangulation can be obtained via elimination , and we show that the decision problem associated with finding optimal state space triangulations in this mixed stochastic deterministic setting is np complete
we report the results of the first experiments with learning proof dependencies from the formalizations done with the coq system we explain the process of obtaining the dependencies from the coq proofs , the characterization of formulas that is used for the learning , and the evaluation method various machine learning methods are compared on a dataset of 5021 toplevel coq proofs coming from the corn repository the best resulting method covers on average 75 of the needed proof dependencies among the first 100 predictions , which is a comparable performance of such initial experiments on other large theory corpora
we show that if any number of variables are allowed to be simultaneously and independently randomized in any one experiment , log2 \( n \) 1 experiments are sufficient and in the worst case necessary to determine the causal relations among n 2 variables when no latent variables , no sample selection bias and no feedback cycles are present for all k , 0 k 1 \( 2n \) we provide an upper bound on the number experiments required to determine causal structure when each experiment simultaneously randomizes k variables for large n , these bounds are significantly lower than the n 1 bound required when each experiment randomizes at most one variable for kmax n 2 , we show that \( n kmax 1 \) n \( 2kmax \) log2 \( kmax \) experiments aresufficient and in the worst case necessary we over a conjecture as to the minimal number of experiments that are in the worst case sufficient to identify all causal relations among n observed variables that are a subset of the vertices of a dag
in social network analysis , there is a common perception that influence is relevant to determine the global behavior of the society and thus it can be used to enforce cooperation by targeting an adequate initial set of individuals or to analyze global choice processes here we propose centrality measures that can be used to analyze the relevance of the actors in process related to spread of influence in 39 it was considered a multiagent system in which the agents are eager to perform a collective task depending on the perception of the willingness to perform the task of other individuals the setting is modeled using a notion of simple games called influence games those games are defined on graphs were the nodes are labeled by their influence threshold and the spread of influence between its nodes is used to determine whether a coalition is winning or not influence games provide tools to measure the importance of the actors of a social network by means of classic power indices and provide a framework to consider new centrality criteria in this paper we consider two of the most classical power indices , i e , banzhaf and shapley shubik indices , as centrality measures for social networks in influence games although there is some work related to specific scenarios of game theoretic networks , here we use such indices as centrality measures in any social network where the spread of influence phenomenon can be applied further , we define new centrality measures such as satisfaction and effort that , as far as we know , have not been considered so far we also perform a comparison of the proposed measures with other three classic centrality measures , degree , closeness and betweenness , considering three social networks we show that in some cases our measurements provide centrality hierarchies similar to those of other measures , while in other cases provide different hierarchies
we examine carefully the rationale underlying the approaches to belief change taken in the literature , and highlight what we view as methodological problems we argue that to study belief change carefully , we must be quite explicit about the ``ontology'' or scenario underlying the belief change process this is something that has been missing in previous work , with its focus on postulates our analysis shows that we must pay particular attention to two issues that have often been taken for granted the first is how we model the agent 's epistemic state \( do we use a set of beliefs , or a richer structure , such as an ordering on worlds \? and if we use a set of beliefs , in what language are these beliefs are expressed \? \) we show that even postulates that have been called ``beyond controversy'' are unreasonable when the agent 's beliefs include beliefs about her own epistemic state as well as the external world the second is the status of observations \( are observations known to be true , or just believed \? in the latter case , how firm is the belief \? \) issues regarding the status of observations arise particularly when we consider iterated belief revision , and we must confront the possibility of revising by p and then by not p
in this article , i present a conjecture on the number of independent sets on graph covers i also show that the conjecture implies that the partition function of a binary pairwise attractive model is greater than that of the bethe approximation
motif discovery in dna sequences is a challenging task in molecular biology in computational motif discovery , planted \( l , d \) motif finding is a widely studied problem and numerous algorithms are available to solve it both hardware and software accelerators have been introduced to accelerate the motif finding algorithms however , the use of hardware accelerators such as fpgas needs hardware specialists to design such systems software based acceleration methods on the other hand are easier to implement than hardware acceleration techniques grid computing is one such software based acceleration technique which has been used in acceleration of motif finding however , drawbacks such as network communication delays and the need of fast interconnection between nodes in the grid can limit its usage and scalability as using multicore cpus to accelerate cpu intensive tasks are becoming increasingly popular and common nowadays , we can employ it to accelerate motif finding and it can be a faster method than grid based acceleration in this paper , we have explored the use of multicore cpus to accelerate motif finding we have accelerated the skip brute force algorithm on multicore cpus parallelizing it using the posix thread library our method yielded an average speed up of 34x on a 32 core processor compared to a speed up of 21x on a grid based implementation of 32 nodes
a new random linear network coding scheme for reliable communications for time division duplexing channels is proposed the setup assumes a packet erasure channel and that nodes cannot transmit and receive information simultaneously the sender transmits coded data packets back to back before stopping to wait for the receiver to acknowledge \( ack \) the number of degrees of freedom , if any , that are required to decode correctly the information we provide an analysis of this problem to show that there is an optimal number of coded data packets , in terms of mean completion time , to be sent before stopping to listen this number depends on the latency , probabilities of packet erasure and ack erasure , and the number of degrees of freedom that the receiver requires to decode the data this scheme is optimal in terms of the mean time to complete the transmission of a fixed number of data packets we show that its performance is very close to that of a full duplex system , while transmitting a different number of coded packets can cause large degradation in performance , especially if latency is high also , we study the throughput performance of our scheme and compare it to existing half duplex go back n and selective repeat arq schemes numerical results , obtained for different latencies , show that our scheme has similar performance to the selective repeat in most cases and considerable performance gain when latency and packet error probability is high
as a significant factor in urban planning , traffic forecasting and prediction of epidemics , modeling patterns of human mobility draws intensive attention from researchers for decades power law distribution and its variations are observed from quite a few real world human mobility datasets such as the movements of banking notes , trackings of cell phone users' locations and trajectories of vehicles in this paper , we build models for 20 million trajectories with fine granularity collected from more than 10 thousand taxis in beijing in contrast to most models observed in human mobility data , the taxis' traveling displacements in urban areas tend to follow an exponential distribution instead of a power law similarly , the elapsed time can also be well approximated by an exponential distribution worth mentioning , analysis of the interevent time indicates the bursty nature of human mobility , similar to many other human activities
this paper addresses robust communication on a fading relay channel in which the relay is connected to the decoder via an out of band digital link of limited capacity both the source to relay and the source to destination links are subject to fading gains , which are generally unknown to the encoder prior to transmission to overcome this impairment , a hybrid automatic retransmission request \( harq \) protocol is combined with multi layer broadcast transmission , thus allowing for variable rate decoding moreover , motivated by cloud radio access network applications , the relay operation is limited to compress and forward the aim is maximizing the throughput performance as measured by the average number of successfully received bits per channel use , under either long term static channel \( ltsc \) or short term static channel \( stsc \) models in order to opportunistically leverage better channel states based on the harq feedback from the decoder , an adaptive compression strategy at the relay is also proposed numerical results confirm the effectiveness of the proposed strategies
achieving long battery lives or even self sustainability has been a long standing challenge for designing mobile devices this paper presents a novel solution that seamlessly integrates two technologies , mobile cloud computing and microwave power transfer \( mpt \) , to enable computation in passive low complexity devices such as sensors and wearable computing devices specifically , considering a single user system , a base station \( bs \) either transfers power to or offloads computation from a mobile to the cloud the mobile uses harvested energy to compute given data either locally or by offloading a framework for energy efficient computing is proposed that comprises a set of policies for controlling cpu cycles for the mode of local computing , time division between mpt and offloading for the other mode of offloading , and mode selection given the cpu cycle statistics information and channel state information \( csi \) , the policies aim at maximizing the probability of successfully computing given data , called computing probability , under the energy harvesting and deadline constraints the policy optimization is translated into the equivalent problems of minimizing the mobile energy consumption for local computing and maximizing the mobile energy savings for offloading which are solved using convex optimization theory the structures of the resultant policies are characterized in closed form furthermore , given non causal csi , the said analytical framework is further developed to support computation load allocation over multiple channel realizations , which further increases computing probability last , simulation demonstrates the feasibility of wirelessly powered mobile cloud computing and the gain of its optimal control
we study \( constrained \) least squares regression as well as multiple response least squares regression and ask the question of whether a subset of the data , a coreset , suffices to compute a good approximate solution to the regression we give deterministic , low order polynomial time algorithms to construct such coresets with approximation guarantees , together with lower bounds indicating that there is not much room for improvement upon our results
the approximate message passing \( amp \) algorithm shows advantage over conventional convex optimization methods in recovering under sampled sparse signals amp is analytically tractable and has a much lower complexity however , it requires that the true parameters of the input and output channels are known in this paper , we propose an amp algorithm with built in parameter estimation that jointly estimates the sparse signals along with the parameters by treating them as unknown random variables with simple priors specifically , the maximum a posterior \( map \) parameter estimation is presented and shown to produce estimations that converge to the true parameter values experiments on sparse signal recovery show that the performance of the proposed approach matches that of the oracle amp algorithm where the true parameter values are known
using gaussian inputs and treating interference as noise at the receivers has recently been shown to be sum capacity achieving for the two user single input single output \( siso \) gaussian interference channel in a low interference regime , where the interference levels are below certain thresholds in this paper , such a low interference regime is characterized for multiple input multiple output \( mimo \) gaussian interference channels conditions are provided on the direct and cross channel gain matrices under which using gaussian inputs and treating interference as noise at the receivers is sum capacity achieving for the special cases of the symmetric multiple input single output \( miso \) and single input multiple output \( simo \) gaussian interference channels , more explicit expressions for the low interference regime are derived in particular , the threshold on the interference levels that characterize low interference regime is related to the input snr and the angle between the direct and cross channel gain vectors it is shown that the low interference regime can be quite significant for mimo interference channels , with the low interference threshold being at least as large as the sine of the angle between the direct and cross channel gain vectors for the miso and simo cases
fractal structures emerge from statistical and hierarchical processes in urban development or network evolution in a class of efficient and robust geographical networks , we derive the size distribution of layered areas , and estimate the fractal dimension by using the distribution without huge computations this method can be applied to self similar tilings based on a stochastic process
in this paper we introduce a definition for nonanticipative rate distortion function \( rdf \) on abstract alphabets , and we invoke weak convergence of probability measures to show various of its properties , such as , existence of the optimal reproduction conditional distribution , compactness of the fidelity set , lower semicontinuity of the rdf functional , etc further , we derive the closed form expression of the optimal nonstationary reproduction distribution this expression is computed recursively backward in time throughout the paper we point out an operational meaning of the nonanticipative rdf by recalling the coding theorem derive in cite tatikonda2000 , and we state relations to gorbunov pinsker 's nonanticipatory epsilon entropy cite gorbunov pinsker
optimal zero delay coding \( quantization \) of a vector valued markov source driven by a noise process is considered using a stochastic control problem formulation , the existence and structure of optimal quantization policies are studied for a finite horizon problem with bounded per stage distortion measure , the existence of an optimal zero delay quantization policy is shown provided that the quantizers allowed are ones with convex codecells the bounded distortion assumption is relaxed to cover cases that include the linear quadratic gaussian problem for the infinite horizon problem and a stationary markov source the optimality of deterministic markov coding policies is shown the existence of optimal stationary markov quantization policies is also shown provided randomization that is shared by the encoder and the decoder is allowed
the human brain processes information showing learning and prediction abilities but the underlying neuronal mechanisms still remain unknown recently , many studies prove that neuronal networks are able of both generalizations and associations of sensory inputs in this paper , following a set of neurophysiological evidences , we propose a learning framework with a strong biological plausibility that mimics prominent functions of cortical circuitries we developed the inductive conceptual network \( icn \) , that is a hierarchical bio inspired network , able to learn invariant patterns by variable order markov models implemented in its nodes the outputs of the top most node of icn hierarchy , representing the highest input generalization , allow for automatic classification of inputs we found that the icn clusterized mnist images with an error of 5 73 and usps images with an error of 12 56
for graphs g and h , a homomorphism from g to h is a function varphi colon v \( g \) to v \( h \) , which maps vertices adjacent in g to adjacent vertices of h a homomorphism is locally injective if no two vertices with a common neighbor are mapped to a single vertex in h many cases of graph homomorphism and locally injective graph homomorphism are np complete , so there is little hope to design polynomial time algorithms for them in this paper we present an algorithm for graph homomorphism and locally injective homomorphism working in time mathcal o \( \( b 2 \) v \( g \) \) , where b is the bandwidth of the complement of h
in this paper , we introduce a new public quantum interactive proof system and the first quantum alternating turing machine qam proof system and qatm , respectively both are obtained from their classical counterparts \( arthur merlin proof system and alternating turing machine , respectively , \) by augmenting them with a fixed size quantum register we focus on space bounded computation , and obtain the following surprising results both of them with constant space are turing equivalent more specifically , we show that for any turing recognizable language , there exists a constant space weak qam system , \( the nonmembers do not need to be rejected with high probability \) , and we show that any turing recognizable language can be recognized by a constant space qatm even with one way input head for strong proof systems , where the nonmembers must be rejected with high probability , we show that the known space bounded classical private protocols can also be simulated by our public qam system with the same space bound besides , we introduce a strong version of qatm the qatm that must halt in every computation path then , we show that strong qatms \( similar to private atms \) can simulate deterministic space with exponentially less space this leads to shifting the deterministic space hierarchy exactly by one level the method behind the main results is a new public protocol cleverly using its fixed size quantum register interestingly , the quantum part of this public protocol cannot be simulated by any space bounded classical protocol in some cases
design of it online algorithms for assigning mobile users to basestations is considered with the objective of maximizing the sum rate , when all users associated to any one basestation equally share each basestation 's resources each user on its arrival reveals the rates it can obtain if connected to each of the basestations , and the problem is to assign each user to any one basestation irrevocably so that the sum rate is maximized at the end of all user arrivals , without knowing the future user arrival or rate information or its statistics at each user arrival online algorithms with constant factor loss in comparison to offline algorithms \( that know both the user arrival and user rates profile in advance \) are derived the proposed online algorithms are motivated from the famous online k secretary problem and online maximum weight matching problem
distributed real time \( drt \) systems are among the most complex software systems to design , test , maintain and evolve the existence of components distributed over a network often conflicts with real time requirements , leading to design strategies that depend on domain and even application specific knowledge distributed virtual environment \( dve \) systems are drt systems that connect multiple users instantly with each other and with a shared virtual space over a network dve systems deviate from traditional drt systems in the importance of the quality of the end user experience we present an analysis of important , but challenging , issues in the design , testing and evaluation of dve systems through the lens of experiments with a concrete dve , opensimulator we frame our observations within six dimensions of well known design concerns correctness , fault tolerance prevention , scalability , time sensitivity , consistency , and overhead of distribution furthermore , we place our experimental work in a broader historical context , showing that these challenges are intrinsic to dves and suggesting lines of future research
we present ideas about creating a next generation intrusion detection system based on the latest immunological theories the central challenge with computer security is determining the difference between normal and potentially harmful activity for half a century , developers have protected their systems by coding rules that identify and block specific events however , the nature of current and future threats in conjunction with ever larger it systems urgently requires the development of automated and adaptive defensive tools a promising solution is emerging in the form of artificial immune systems the human immune system can detect and defend against harmful and previously unseen invaders , so can we not build a similar intrusion detection system for our computers
the recent wave of mobilizations in the arab world and across western countries has generated much discussion on how digital media is connected to the diffusion of protests we examine that connection using data from the surge of mobilizations that took place in spain in may 2011 we study recruitment patterns in the twitter network and find evidence of social influence and complex contagion we identify the network position of early participants \( i e the leaders of the recruitment process \) and of the users who acted as seeds of message cascades \( i e the spreaders of information \) we find that early participants cannot be characterized by a typical topological position but spreaders tend to me more central to the network these findings shed light on the connection between online networks , social contagion , and collective dynamics , and offer an empirical test to the recruitment mechanisms theorized in formal models of collective action
the mceliece cryptosystem is a public key cryptosystem based on coding theory that has successfully resisted cryptanalysis for thirty years the original version , based on goppa codes , is able to guarantee a high level of security , and is faster than competing solutions , like rsa despite this , it has been rarely considered in practical applications , due to two major drawbacks i \) large size of the public key and ii \) low transmission rate low density parity check \( ldpc \) codes are state of art forward error correcting codes that permit to approach the shannon limit while ensuring limited complexity quasi cyclic \( qc \) ldpc codes are a particular class of ldpc codes , able to join low complexity encoding of qc codes with high performing and low complexity decoding of ldpc codes in a previous work it has been proposed to adopt a particular family of qc ldpc codes in the mceliece cryptosystem to reduce the key size and increase the transmission rate recently , however , new attacks have been found that are able to exploit a flaw in the transformation from the private key to the public one such attacks can be effectively countered by changing the form of some constituent matrices , without altering the system parameters this work gives an overview of the qc ldpc codes based mceliece cryptosystem and its cryptanalysis two recent versions are considered , and their ability to counter all the currently known attacks is discussed a third version able to reach a higher security level is also proposed finally , it is shown that the new qc ldpc codes based cryptosystem scales favorably with the key length
we investigate properties of a channel coding scheme leading to the minimum possible frame error ratio when transmitting over a memoryless channel with rate r c the results are compared to the well known properties of a channel coding scheme leading to minimum bit error ratio it is concluded that these two optimization requests are contradicting a valuable application of the derived results is presented
we derive generalization error bounds for stationary univariate autoregressive \( ar \) models we show that imposing stationarity is enough to control the gaussian complexity without further regularization this lets us use structural risk minimization for model selection we demonstrate our methods by predicting interest rate movements
we prove that every distributional problem solvable in polynomial time on the average with respect to the uniform distribution has a frequently self knowingly correct polynomial time algorithm we also study some features of probability weight of correctness with respect to generalizations of procaccia and rosenschein 's junta distributions pr07b
this paper describes an automated , formal and rigorous analysis of the ad hoc on demand distance vector \( aodv \) routing protocol , a popular protocol used in wireless mesh networks we give a brief overview of a model of aodv implemented in the uppaal model checker it is derived from a process algebraic model which reflects precisely the intention of aodv and accurately captures the protocol specification furthermore , we describe experiments carried out to explore aodv 's behaviour in all network topologies up to 5 nodes we were able to automatically locate problematic and undesirable behaviours this is in particular useful to discover protocol limitations and to develop improved variants this use of model checking as a diagnostic tool complements other formal methods based protocol modelling and verification techniques , such as process algebra
recurrent neural networks \( rnns \) , including long short term memory \( lstm \) rnns , have produced state of the art results on a variety of speech recognition tasks however , these models are often too large in size for deployment on mobile devices with memory and latency constraints in this work , we study mechanisms for learning compact rnns and lstms via low rank factorizations and parameter sharing schemes our goal is to investigate redundancies in recurrent architectures where compression can be admitted without losing performance a hybrid strategy of using structured matrices in the bottom layers and shared low rank factors on the top layers is found to be particularly effective , reducing the parameters of a standard lstm by 75 , at a small cost of 0 3 increase in wer , on a 2 , 000 hr english voice search task
broadband wireless channel is a time dispersive and becomes strongly frequency selective in most cases , the channel is composed of a few dominant coefficients and a large part of coefficients is approximately zero or zero to exploit the sparsity of multi path channel \( mpc \) , there are various methods have been proposed they are , namely , greedy algorithms , iterative algorithms , and convex program the former two algorithms are easy to be implemented but not stable on the other hand , the last method is stable but difficult to be implemented as practical channel estimation problems because of computational complexity in this paper , we proposed a novel channel estimation strategy by using modified smoothed \( msl0 \) algorithm which combines stable and low complexity computer simulations confirm the effectiveness of the introduced algorithm comparisons with the existing methods we also give
we address in this paper decoding aspects of the compute and forward \( cf \) physical layer network coding strategy it is known that the original decoder for the cf is asymptotically optimal however , its performance gap to optimal decoders in practical settings are still not known in this work , we develop and assess the performance of novel decoding algorithms for the cf operating in the multiple access channel for the fading channel , we analyze the ml decoder and develop a novel diophantine approximation based decoding algorithm showed numerically to outperform the original cf decoder for the gaussian channel , we investigate the maximum a posteriori \( map \) decoder we derive a novel map decoding metric and develop practical decoding algorithms proved numerically to outperform the original one
power talk is a novel concept for communication among control units in microgrids \( mgs \) , carried out without a dedicated modem , but by using power electronics that interface the common bus the information is transmitted by modulating the parameters of the primary control , incurring subtle power deviations that can be detected by other units in this paper , we develop power talk communication strategies for dc mg systems with arbitrary number of control units that carry out all to all communication we investigate two multiple access strategies 1 \) tdma , where only one unit transmits at a time , and 2 \) full duplex , where all units transmit and receive simultaneously we introduce the notions of signaling space , where the power talk symbol constellations are constructed , and detection space , where the demodulation of the symbols is performed the proposed communication technique is challenged by the random changes of the bus parameters due to load variations in the system to this end , we employ a solution based on training sequences , which re establishes the signaling and detection spaces and thus enables reliable information exchange the presented results show that power talk is an effective solution for reliable communication among units in dc mg systems
we consider the problem of throughput optimal broadcast ing in time varying wireless networks , whose underlying topology is restricted to directed acyclic graphs \( dag \) previous broadcast algorithms route packets along spanning trees in large networks with time varying connectivities , these trees are difficult to compute and maintain in this paper we propose a new online throughput optimal broadcast algorithm which makes packet by packet scheduling and routing decisions , obviating the need for maintaining any global topological structures , such as spanning trees our algorithm relies on system state information for making transmission decisions and hence , may be thought of as a generalization of the well known back pressure algorithm which makes point to point unicast transmission decisions based on queue length information , without requiring knowledge of end to end paths technically , the back pressure algorithm is derived by stochastically stabilizing the network queues however , because of packet duplications associated with broadcast , the work conservation principle is violated and queuing processes are difficult to define in the broadcast problem to address this fundamental issue , we identify certain state variables which behave like virtual queues in the broadcast setting by stochastically stabilizing these virtual queues , we devise a throughput optimal broadcast policy we also derive new characterizations of the broadcast capacity of time varying wireless dags and derive an efficient algorithm to compute the capacity exactly under certain assumptions of interference model , and a poly time approximation algorithm for computing the capacity under less restrictive assumptions
approximate dynamic programming has been investigated and used as a method to approximately solve optimal regulation problems however , the extension of this technique to optimal tracking problems for continuous time nonlinear systems has remained a non trivial open problem the control development in this paper guarantees ultimately bounded tracking of a desired trajectory , while also ensuring that the controller converges to an approximate optimal policy
in the monograph kounchev , o i , multivariate polysplines applications to numerical and wavelet analysis , academic press , san diego london , 2001 , and in the paper kounchev o , render , h , cardinal interpolation with polysplines on annuli , journal of approximation theory 137 \( 2005 \) 89 107 , we have introduced and studied a new paradigm for cardinal interpolation which is related to the theory of multivariate polysplines in the present paper we show that this is related to a new sampling paradigm in the multivariate case , whereas we obtain a shannon type function s \( x \) and the following shannon type formula f \( r theta \) sum j infty infty int qtr bbb s n 1 s \( e j r theta \) f \( e j theta \) d theta this formula relies upon infinitely many shannon type formulas for the exponential splines arising from the radial part of the polyharmonic operator delta p for fixed p geq 1 acknowledgement the first and the second author have been partially supported by the institutes partnership project with the alexander von humboldt foundation the first has been partially sponsored by the greek bulgarian bilateral project bgr 17 , and the second author by grant mtm2006 13000 c03 03 of the d g i of spain
we study two layer belief networks of binary random variables in which the conditional probabilities pr childlparents depend monotonically on weighted sums of the parents in large networks where exact probabilistic inference is intractable , we show how to compute upper and lower bounds on many probabilities of interest in particular , using methods from large deviation theory , we derive rigorous bounds on marginal probabilities such as pr children and prove rates of convergence for the accuracy of our bounds as a function of network size our results apply to networks with generic transfer function parameterizations of the conditional probability tables , such as sigmoid and noisy or they also explicitly illustrate the types of averaging behavior that can simplify the problem of inference in large networks
in this paper , we revise and further investigate the coordination control approach proposed for supervisory control of distributed discrete event systems with synchronous communication based on the ramadge wonham automata framework the notions of conditional decomposability , conditional controllability , and conditional closedness ensuring the existence of a solution are carefully revised and simplified the paper is generalized to non prefix closed languages , that is , supremal conditionally controllable sublanguages of not necessary prefix closed languages are discussed non prefix closed languages introduce the blocking issue into coordination control , hence a procedure to compute a coordinator for nonblockingness is included the optimization problem concerning the size of a coordinator is under investigation we prove that to find the minimal extension of the coordinator event set for which a given specification language is conditionally decomposable is np hard in other words , unless p np , it is not possible to find a polynomial algorithm to compute the minimal coordinator with respect to the number of events
the calculus of wrapped compartments \( cwc \) is a variant of the calculus of looping sequences \( cls \) while keeping the same expressiveness , cwc strongly simplifies the development of automatic tools for the analysis of biological systems the main simplification consists in the removal of the sequencing operator , thus lightening the formal treatment of the patterns to be matched in a term \( whose complexity in cls is strongly affected by the variables matching in the sequences \) we define a stochastic semantics for this new calculus as an application we model the interaction between macrophages and apoptotic neutrophils and a mechanism of gene regulation in e coli
we formulate and study the algorithmic mechanism design problem for a general class of resource allocation settings , where the center redistributes the private resources brought by individuals money transfer is forbidden distinct from the standard literature , which assumes the amount of resources brought by an individual to be public information , we consider this amount as an agent 's private , possibly multi dimensional type our goal is to design truthful mechanisms that achieve two objectives max min and pareto efficiency for each objective , we provide a reduction that converts any optimal algorithm into a strategy proof mechanism that achieves the same objective our reductions do not inspect the input algorithms but only query these algorithms as oracles applying the reductions , we produce strategy proof mechanisms in a non trivial application network route allocation our models and result in the application are valuable on their own rights
we study a distributed antenna system where l antenna terminals \( ats \) are connected to a central processor \( cp \) via digital error free links of finite capacity r 0 , and serve k user terminals \( uts \) we contribute to the subject in the following ways 1 \) for the uplink , we apply the compute and forward \( cof \) approach and examine the corresponding system optimization at finite snr 2 \) for the downlink , we propose a novel precoding scheme nicknamed reverse compute and forward \( rcof \) 3 \) in both cases , we present low complexity versions of cof and rcof based on standard scalar quantization at the receivers , that lead to discrete input discrete output symmetric memoryless channel models for which near optimal performance can be achieved by standard single user linear coding 4 \) for the case of large r 0 , we propose a novel integer forcing beamforming \( ifb \) scheme that generalizes the popular zero forcing beamforming and achieves sum rate performance close to the optimal gaussian dirty paper coding the proposed uplink and downlink system optimization focuses specifically on the ats and uts selection problem we present low complexity ats and uts selection schemes and demonstrate , through monte carlo simulation in a realistic environment with fading and shadowing , that the proposed schemes essentially eliminate the problem of rank deficiency of the system matrix and greatly mitigate the non integer penalty affecting cof rcof at high snr comparison with other state of the art information theoretic schemes , such as quantize remap and forward for the uplink and compressed dirty paper coding for the downlink , show competitive performance of the proposed approaches with significantly lower complexity
the deluge of date rate in today 's networks poses a cost burden on the backhaul network design designing cost efficient backhaul solutions becomes an interesting , yet challenging , problem traditional technologies for backhaul networks include either radio frequency backhauls \( rf \) or optical fibers \( of \) while rf is a cost effective solution as compared to of , it supports lower data rate requirements another promising backhaul solution that may combine both a high data rate and a relatively low cost is the free space optics \( fso \) fso , however , is sensitive to nature conditions \( e g rain , fog , line of sight , etc \) a more reliable alternative is therefore to combine rf and fso solutions through a hybrid structure called hybrid rf fso consider a backhaul network , where the base stations \( bs \) can be connected to each other either via of or via hybrid rf fso backhaul links the paper addresses the problem of minimizing the cost of backhaul planning under connectivity and data rates constraints , so as to choose the appropriate cost effective backhaul type between bss \( i e , either of or hybrid rf fso \) the paper solves the problem using graph theory techniques by introducing the corresponding planning graph it shows that under a specified realistic assumption about the cost of of and hybrid rf fso links , the problem is equivalent to a maximum weight clique problem , which can can be solved with moderate complexity simulation results show that our proposed solution shows a close to optimal performance , especially for practical prices of the hybrid rf fso
the amount of useful information available on the web has been growing at a dramatic pace in recent years and people rely more and more on the web to fulfill their information needs in this paper , we study truthfulness of deep web data in two domains where we believed data are fairly clean and data quality is important to people 's lives em stock and em flight to our surprise , we observed a large amount of inconsistency on data from different sources and also some sources with quite low accuracy we further applied on these two data sets state of the art em data fusion methods that aim at resolving conflicts and finding the truth , analyzed their strengths and limitations , and suggested promising research directions we wish our study can increase awareness of the seriousness of conflicting data on the web and in turn inspire more research in our community to tackle this problem
we describe a case study in the application of em symbolic machine learning techniques for the discovery of linguistic rules and categories a supervised rule induction algorithm is used to learn to predict the correct diminutive suffix given the phonological representation of dutch nouns the system produces rules which are comparable to rules proposed by linguists furthermore , in the process of learning this morphological task , the phonemes used are grouped into phonologically relevant categories we discuss the relevance of our method for linguistics and language technology
understanding how epidemics spread in a system is a crucial step to prevent and control outbreaks , with broad implications on the system 's functioning , health , and associated costs this can be achieved by identifying the elements at higher risk of infection and implementing targeted surveillance and control measures one important ingredient to consider is the pattern of disease transmission contacts among the elements , however lack of data or delays in providing updated records may hinder its use , especially for time varying patterns here we explore to what extent it is possible to use past temporal data of a system 's pattern of contacts to predict the risk of infection of its elements during an emerging outbreak , in absence of updated data we focus on two real world temporal systems a livestock displacements trade network among animal holdings , and a network of sexual encounters in high end prostitution we define the node 's loyalty as a local measure of its tendency to maintain contacts with the same elements over time , and uncover important non trivial correlations with the node 's epidemic risk we show that a risk assessment analysis incorporating this knowledge and based on past structural and temporal pattern properties provides accurate predictions for both systems its generalizability is tested by introducing a theoretical model for generating synthetic temporal networks high accuracy of our predictions is recovered across different settings , while the amount of possible predictions is system specific the proposed method can provide crucial information for the setup of targeted intervention strategies
a method of representation of a solution as segments of the series in powers of the step of the independent variable is expanded for solving complex systems of ordinary differential equations \( ode \) the lorenz system and other systems a new procedure of reduction of the representation of the solution to a sum of two parts \( regular and random \) is performed a shifting procedure is applied in each level of the independent variable to the random part and it acts as the filter that extracts the values to the regular part in certain cases it is possible to omit the random part and construct the approximation which does not converge but still provides the qualitative information about the full solution \( a linear approximation provides a simple exact solution \) evaluation of the error for this case is performed constructing the analytical representation of the solutions for these systems by the developed method is presented
the crushing demand for wireless data services will soon exceed the capability of the current homogeneous cellular architecture an emerging solution is to overlay small cell networks with the macro cell networks in this paper , we propose an amplitude space sharing \( ass \) method among the macro cell user and small cell users by transmit layer design and data rate optimization , the signals and interferences are promised to be separable at each receiver and the network sum rate is maximized the han koboyashi coding is employed and optimal power allocation is derived for the one small cell scenario , and a simple ass transmission scheme is developed for the multiple small cells scenarios simulation results show great superiority over other interference management schemes
educators have developed an effective technique to get feedback after in person lectures , called muddy card students are given time to reflect and write the muddiest \( least clear \) point on an index card , to hand in as they leave class this practice of assigning end of lecture reflection tasks to generate explicit student feedback is well suited for adaptation to the challenge of supporting feedback in online video lectures we describe the design and evaluation of mudslide , a prototype system that translates the practice of muddy cards into the realm of online lecture videos based on an in lab study of students and teachers , we find that spatially contextualizing students' muddy point feedback with respect to particular lecture slides is advantageous to both students and teachers we also reflect on further opportunities for enhancing this feedback method based on teachers' and students' experiences with our prototype
network coding can significantly improve the transmission rate of communication networks with packet loss compared with routing however , using network coding usually incurs high computational and storage costs in the network devices and terminals for example , some network coding schemes require the computational and or storage capacities of an intermediate network node to increase linearly with the number of packets for transmission , making such schemes difficult to be implemented in a router like device that has only constant computational and storage capacities in this paper , we introduce batched sparse code \( bats code \) , which enables a digital fountain approach to resolve the above issue bats code is a coding scheme that consists of an outer code and an inner code the outer code is a matrix generation of a fountain code it works with the inner code that comprises random linear coding at the intermediate network nodes bats codes preserve such desirable properties of fountain codes as ratelessness and low encoding decoding complexity the computational and storage capacities of the intermediate network nodes required for applying bats codes are independent of the number of packets for transmission almost capacity achieving bats code schemes are devised for unicast networks , two way relay networks , tree networks , a class of three layer networks , and the butterfly network for general networks , under different optimization criteria , guaranteed decoding rates for the receiving nodes can be obtained
we consider the problem of computing reachability probabilities given a markov chain , an initial state of the markov chain , and a set of goal states of the markov chain , what is the probability of reaching any of the goal states from the initial state \? this problem can be reduced to solving a linear equation ax b for x , where a is a matrix and b is a vector we consider two iterative methods to solve the linear equation the jacobi method and the biconjugate gradient stabilized \( bicgstab \) method for both methods , a sequential and a parallel version have been implemented the parallel versions have been implemented on the compute unified device architecture \( cuda \) so that they can be run on a nvidia graphics processing unit \( gpu \) from our experiments we conclude that as the size of the matrix increases , the cuda implementations outperform the sequential implementations furthermore , the bicgstab method performs better than the jacobi method for dense matrices , whereas the jacobi method does better for sparse ones since the reachability probabilities problem plays a key role in probabilistic model checking , we also compared the implementations for matrices obtained from a probabilistic model checker our experiments support the conjecture by bosnacki et al that the jacobi method is superior to krylov subspace methods , a class to which the bicgstab method belongs , for probabilistic model checking
research on querying the web of data is still in its infancy in this paper , we provide an initial set of general features that we envision should be considered in order to define a query language for the web of data furthermore , for each of these features , we pose questions that have not been addressed before in the context of querying the web of data we believe that addressing these questions and studying these features may guide the next 10 years of research on the web of data
kalman filters are one of the most influential models of time varying phenomena they admit an intuitive probabilistic interpretation , have a simple functional form , and enjoy widespread adoption in a variety of disciplines motivated by recent variational methods for learning deep generative models , we introduce a unified algorithm to efficiently learn a broad spectrum of kalman filters of particular interest is the use of temporal generative models for counterfactual inference we investigate the efficacy of such models for counterfactual inference , and to that end we introduce the healing mnist dataset where long term structure , noise and actions are applied to sequences of digits we show the efficacy of our method for modeling this dataset we further show how our model can be used for counterfactual inference for patients , based on electronic health record data of 8 , 000 patients over 4 5 years
this paper presents the systems developed by lium and cvc for the wmt16 multimodal machine translation challenge we explored various comparative methods , namely phrase based systems and attentional recurrent neural networks models trained using monomodal or multimodal data we also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation our systems obtained the best results for both tasks according to the automatic evaluation metrics bleu and meteor
the increasing number of applications requiring the solution of large scale singular value problems have rekindled interest in iterative methods for the svd some promising recent ad vances in large scale iterative methods are still plagued by slow convergence and accuracy limitations for computing smallest singular triplets furthermore , their current implementations in matlab cannot address the required large problems recently , we presented a preconditioned , two stage method to effectively and accurately compute a small number of extreme singular triplets in this research , we present a high performance software , primme svds , that implements our hybrid method based on the state of the art eigensolver package primme for both largest and smallest singular values primme svds fills a gap in production level software for computing the partial svd , especially with preconditioning the numerical experiments demonstrate its superior perfor mance compared to other state of the art software and its good parallel performance under strong and weak scaling
we consider the discrete time intersymbol interference \( isi \) channel model , with additive gaussian noise and fixed i i d inputs in this setting , we investigate the expression put forth by shamai and laroia as a conjectured lower bound for the input output mutual information after application of a mmse dfe receiver a low snr expansion is used to prove that the conjectured bound does not hold under general conditions , and to characterize inputs for which it is particularly ill suited one such input is used to construct a counterexample , indicating that the shamai laroia expression does not always bound even the achievable rate of the channel , thus excluding a natural relaxation of the original conjectured bound however , this relaxed bound is then shown to hold for any finite entropy input and isi channel , when the snr is sufficiently high finally , new simple bounds for the achievable rate are proven , and compared to other known bounds information estimation relations and estimation theoretic bounds play a key role in establishing our results
motivated by applications in telecommunications , computer science and physics , we consider a discrete time markov process with restart in the borel state space at each step the process either with a positive probability restarts from a given distribution , or with the complementary probability continues according to a markov transition kernel the main focus of the present work is the expectation of the hitting time \( to a given target set \) of the process with restart , for which we obtain the explicit formula observing that the process with restart is positive harris recurrent , we obtain the expression of its unique invariant probability then we show the equivalence of the following statements \( a \) the boundedness \( with respect to the initial state \) of the expectation of the hitting time \( b \) the finiteness of the expectation of the hitting time for almost all the initial states with respect to the unique invariant probability and \( c \) the target set is of positive measure with respect to the invariant probability we illustrate our results with two examples in uncountable and countable state spaces
internet of things \( iot \) will create a cyberphysical world where all the things around us are connected to the inter net , sense and produce big data that has to be stored , processed and communicated with minimum human intervention with the ever increasing emergence of new sensors , interfaces and mobile devices , the grand challenge is to keep up with this race in developing software drivers and wrappers for iot things in this paper , we examine the approaches that automate the process of developing middleware drivers wrappers for the iot things we propose ascm4gsn architecture to address this challenge efficiently and effectively we demonstrate the proposed approach using global sensor network \( gsn \) middleware which exemplifies a cluster of data streaming engines the ascm4gsn architecture significantly speeds up the wrapper development and sensor configuration process as demonstrated for android mobile phone based sensors as well as for sun spot sensors
in this paper , we argue for the need to distinguish between task and dialogue initiatives , and present a model for tracking shifts in both types of initiatives in dialogue interactions our model predicts the initiative holders in the next dialogue turn based on the current initiative holders and the effect that observed cues have on changing them our evaluation across various corpora shows that the use of cues consistently improves the accuracy in the system 's prediction of task and dialogue initiative holders by 2 4 and 8 13 percentage points , respectively , thus illustrating the generality of our model
the problem of detecting communities in a graph is maybe one the most studied inference problems , given its simplicity and widespread diffusion among several disciplines a very common benchmark for this problem is the stochastic block model or planted partition problem , where a phase transition takes place in the detection of the planted partition by changing the signal to noise ratio optimal algorithms for the detection exist which are based on spectral methods , but we show these are extremely sensible to slight modification in the generative model recently javanmard , montanari and ricci tersenghi \( arxiv 1511 08769 \) have used statistical physics arguments , and numerical simulations to show that finding communities in the stochastic block model via semidefinite programming is quasi optimal further , the resulting semidefinite relaxation can be solved efficiently , and is very robust with respect to changes in the generative model in this paper we study in detail several practical aspects of this new algorithm based on semidefinite programming for the detection of the planted partition the algorithm turns out to be very fast , allowing the solution of problems with o \( 10 5 \) variables in few second on a laptop computer
this work proposes a fully decentralized strategy for maintaining the formation rigidity of a multi robot system using only range measurements , while still allowing the graph topology to change freely over time in this direction , a first contribution of this work is an extension of rigidity theory to weighted frameworks and the rigidity eigenvalue , which when positive ensures the infinitesimal rigidity of the framework we then propose a distributed algorithm for estimating a common relative position reference frame amongst a team of robots with only range measurements in addition to one agent endowed with the capability of measuring the bearing to two other agents this first estimation step is embedded into a subsequent distributed algorithm for estimating the rigidity eigenvalue associated with the weighted framework the estimate of the rigidity eigenvalue is finally used to generate a local control action for each agent that both maintains the rigidity property and enforces additional con straints such as collision avoidance and sensing communication range limits and occlusions as an additional feature of our approach , the communication and sensing links among the robots are also left free to change over time while preserving rigidity of the whole framework the proposed scheme is then experimentally validated with a robotic testbed consisting of 6 quadrotor uavs operating in a cluttered environment
in this paper , we study the graphical structure of elementary trapping sets \( ets \) of variable regular low density parity check \( ldpc \) codes etss are known to be the main cause of error floor in ldpc coding schemes for the set of ldpc codes with a given variable node degree d l and girth g , we identify all the non isomorphic structures of an arbitrary class of \( a , b \) etss , where a is the number of variable nodes and b is the number of odd degree check nodes in the induced subgraph of the ets our study leads to a simple characterization of dominant classes of etss \( those with relatively small values of a and b \) based on short cycles in the tanner graph of the code for such classes of etss , we prove that any set cal s in the class is a layered superset \( lss \) of a short cycle , where the term layered is used to indicate that there is a nested sequence of etss that starts from the cycle and grows , one variable node at a time , to generate cal s this characterization corresponds to a simple search algorithm that starts from the short cycles of the graph and finds all the etss with lss property in a guaranteed fashion specific results on the structure of etss are presented for d l 3 , 4 , 5 , 6 , g 6 , 8 and a , b leq 10 in this paper the results of this paper can be used for the error floor analysis and for the design of ldpc codes with low error floors
we analyze by means of granger causality the effect of synergy and redundancy in the inference \( from time series data \) of the information flow between subsystems of a complex network whilst we show that fully conditioned granger causality is not affected by synergy , the pairwise analysis fails to put in evidence synergetic effects in cases when the number of samples is low , thus making the fully conditioned approach unfeasible , we show that partially conditioned granger causality is an effective approach if the set of conditioning variables is properly chosen we consider here two different strategies \( based either on informational content for the candidate driver or on selecting the variables with highest pairwise influences \) for partially conditioned granger causality and show that depending on the data structure either one or the other might be valid on the other hand , we observe that fully conditioned approaches do not work well in presence of redundancy , thus suggesting the strategy of separating the pairwise links in two subsets those corresponding to indirect connections of the fully conditioned granger causality \( which should thus be excluded \) and links that can be ascribed to redundancy effects and , together with the results from the fully connected approach , provide a better description of the causality pattern in presence of redundancy we finally apply these methods to two different real datasets first , analyzing electrophysiological data from an epileptic brain , we show that synergetic effects are dominant just before seizure occurrences second , our analysis applied to gene expression time series from hela culture shows that the underlying regulatory networks are characterized by both redundancy and synergy
this work considers distributed sensing and transmission of sporadic random samples lower bounds are derived for the reconstruction error of a single normally or uniformly distributed finite dimensional vector imperfectly measured by a network of sensors and transmitted with finite energy to a common receiver via an additive white gaussian noise asynchronous multiple access channel transmission makes use of a perfect causal feedback link to the encoder connected to each sensor a retransmission protocol inspired by the classical scheme in 1 applied to the transmission of single and bi variate analog samples analyzed in 2 and 3 is extended to the more general network scenario , for which asymptotic upper bounds on the reconstruction error are provided both the upper and lower bounds show that collaboration can be achieved through energy accumulation under certain circumstances in order to investigate the practical performance of the proposed retransmission protocol we provide a numerical evaluation of the upper bounds in the non asymptotic energy regime using low order quantization in the sensors the latter includes a minor modification of the protocol to improve reconstruction fidelity numerical results show that an increase in the size of the network brings benefit in terms of performance , but that the gain in terms of energy efficiency diminishes quickly at finite energies due to a non coherent combining loss
the importance of unsupervised clustering and topic modeling is well recognized with ever increasing volumes of text data in this paper , we propose a fast method for hierarchical clustering and topic modeling called hiernmf2 our method is based on fast rank 2 nonnegative matrix factorization \( nmf \) that performs binary clustering and an efficient node splitting rule further utilizing the final leaf nodes generated in hiernmf2 and the idea of nonnegative least squares fitting , we propose a new clustering topic modeling method called flatnmf2 that recovers a flat clustering topic modeling result in a very simple yet significantly more effective way than any other existing methods we describe highly optimized open source software in c for both hiernmf2 and flatnmf2 for hierarchical and partitional clustering topic modeling of document data sets substantial experimental tests are presented that illustrate significant improvements both in computational time as well as quality of solutions we compare our methods to other clustering methods including k means , standard nmf , and cluto , and also topic modeling methods including latent dirichlet allocation \( lda \) and recently proposed algorithms for nmf with separability constraints overall , we present efficient tools for analyzing large scale data sets , and techniques that can be generalized to many other data analytics problem domains
we consider the problem of sequential sampling from a finite number of independent statistical populations to maximize the expected infinite horizon average outcome per period , under a constraint that the expected average sampling cost does not exceed an upper bound the outcome distributions are not known we construct a class of consistent adaptive policies , under which the average outcome converges with probability 1 to the true value under complete information for all distributions with finite means we also compare the rate of convergence for various policies in this class using simulation
we introduce the idea of distortion side information , which does not directly depend on the source but instead affects the distortion measure we show that such distortion side information is not only useful at the encoder , but that under certain conditions , knowing it at only the encoder is as good as knowing it at both encoder and decoder , and knowing it at only the decoder is useless thus distortion side information is a natural complement to the signal side information studied by wyner and ziv , which depends on the source but does not involve the distortion measure furthermore , when both types of side information are present , we characterize the penalty for deviating from the configuration of encoder only distortion side information and decoder only signal side information , which in many cases is as good as full side information knowledge
this work considers an estimation task in compressive sensing , where the goal is to estimate an unknown signal from compressive measurements that are corrupted by additive pre measurement noise \( interference , or clutter \) as well as post measurement noise , in the specific setting where some \( perhaps limited \) prior knowledge on the signal , interference , and noise is available the specific aim here is to devise a strategy for incorporating this prior information into the design of an appropriate compressive measurement strategy here , the prior information is interpreted as statistics of a prior distribution on the relevant quantities , and an approach based on bayesian experimental design is proposed experimental results on synthetic data demonstrate that the proposed approach outperforms traditional random compressive measurement designs , which are agnostic to the prior information , as well as several other knowledge enhanced sensing matrix designs based on more heuristic notions
in this paper , we study the impact of aliased adjacent band blocker signal \( aabs \) on the information sum rate of massive multiple input multiple output \( mimo \) uplink , with both perfect and imperfect channel estimates , in order to determine the required adjacent band attenuation in rf bandpass filters \( bpfs \) with imperfect channel estimates , our study reveals that as the number of base station \( bs \) antennas \( m \) increases , with m to infty , the required attenuation at the bpfs increases as mathcal o \( sqrt m \) , provided the desired information sum rate \( both in the presence and absence of aabs \) remains fixed this implies a practical limit on the number of bs antennas due to the increase in bpf design complexity and power consumption with increasing m
given a finite simple graph g \( v , e \) with chromatic number c and chromatic polynomial c \( x \) every vertex graph coloring f of g defines an index i f \( x \) satisfying the poincare hopf theorem sum x i f \( x \) chi \( g \) as a variant to the index expectation result we prove that e i f \( x \) is equal to curvature k \( x \) satisfying gauss bonnet sum x k \( x \) chi \( g \) , where the expectation is the average over the finite probability space containing the c \( c \) possible colorings with c colors , for which each coloring has the same probability
the ability of a robot to detect and respond to changes in its environment is potentially very useful , as it draws attention to new and potentially important features we describe an algorithm for learning to filter out previously experienced stimuli to allow further concentration on novel features the algorithm uses a model of habituation , a biological process which causes a decrement in response with repeated presentation experiments with a mobile robot are presented in which the robot detects the most novel stimulus and turns towards it \( `neotaxis' \)
structured prediction applications often involve complex inference problems that require the use of approximate methods approximations based on linear programming \( lp \) relaxations have proved particularly successful in this setting , with both theoretical and empirical support despite the general intractability of inference , it has been observed that in many real world applications the lp relaxation is often tight in this work we propose a theoretical explanation to this striking observation in particular , we show that learning with lp relaxed inference encourages tightness of training instances we complement this result with a generalization bound showing that tightness generalizes from train to test data
artifact centric business processes have recently emerged as an approach in which processes are centred around the evolution of business entities , called artifacts , giving equal importance to control flow and data the recent guard state milestone \( gsm \) approach provides means for specifying business artifacts lifecycles in a declarative manner , using constructs that match how executive level stakeholders think about their business however , it turns out that formal verification of gsm is undecidable even for very simple propositional temporal properties we attack this challenging problem by translating gsm into a well studied formal framework we exploit this translation to isolate an interesting class of state bounded gsm models for which verification of sophisticated temporal properties is decidable we then introduce some guidelines to turn an arbitrary gsm model into a state bounded , verifiable model
it is shown that in star free graphs the maximum independent set problem , the minimum dominating set problem and the minimum independent dominating set problem are approximable up to constant factor by any maximal independent set
in this paper , results of an experimental study of a deep convolution neural network architecture which can classify different handwritten digits using eblearn library are reported the purpose of this neural network is to classify input images into 10 different classes or digits \( 0 9 \) and to explore new findings the input dataset used consists of digits images of size 32x32 in grayscale \( mnist dataset \)
in medical imaging , there is a growing interest to provide real time images with good quality for large anatomical structures to cope with this issue , we developed a library that allows to replace , for some specific clinical applications , more robust systems such as computer tomography \( ct \) and magnetic resonance imaging \( mri \) our python library py3dfreehandus is a package for processing data acquired simultaneously by ultra sonographic systems \( us \) and marker based optoelectronic systems in particular , us data enables to visualize subcutaneous body structures , whereas the optoelectronic system is able to collect the 3d position in space for reflective objects , that are called markers by combining these two measurement devices , it is possible to reconstruct the real 3d morphology of body structures such as muscles , for relevant clinical implications in the present research work , the different steps which allow to obtain a relevant 3d data set as well as the procedures for calibrating the systems and for determining the quality of the reconstruction
we show how gabidulin codes can be decoded by using a parametrization approach our decoding algorithm computes a list of all closest codewords to a given received word we consider a certain module , called the interpolation module , over the ring of linearized polynomials with respect to composition of polynomials we consider minimal bases of this interpolation module and show that the predictable leading monomial \( plm \) property holds in this setting we show that any such minimal basis gives rise to a parametrization , using the plm property as a key ingredient in our proof two subalgorithms are presented to compute such a basis , one iterative , the other an extended euclidean algorithm both of these subalgorithms have polynomial time complexity the complexity of the overall algorithm , using the parametrization , becomes exponential as soon as the closest codewords are beyond the unique decoding radius however , for high rate codes , this complexity beats the complexity of exhaustive search
this paper develops upper bounds on the end to end transmission capacity of multi hop wireless networks potential source destination paths are dynamically selected from a pool of randomly located relays , from which a closed form lower bound on the outage probability is derived in terms of the expected number of potential paths this is in turn used to provide an upper bound on the number of successful transmissions that can occur per unit area , which is known as the transmission capacity the upper bound results from assuming independence among the potential paths , and can be viewed as the maximum diversity case a useful aspect of the upper bound is its simple form for an arbitrary sized network , which allows insights into how the number of hops and other network parameters affect spatial throughput in the non asymptotic regime the outage probability analysis is then extended to account for retransmissions with a maximum number of allowed attempts in contrast to prevailing wisdom , we show that predetermined routing \( such as nearest neighbor \) is suboptimal , since more hops are not useful once the network is interference limited our results also make clear that randomness in the location of relay sets and dynamically varying channel states is helpful in obtaining higher aggregate throughput , and that dynamic route selection should be used to exploit path diversity
density evolution \( de \) is one of the most powerful analytical tools for low density parity check \( ldpc \) codes on memoryless binary input symmetric output channels the case of non symmetric channels is tackled either by the ldpc coset code ensemble \( a channel symmetrizing argument \) or by the generalized de for linear codes on non symmetric channels existing simulations show that the bit error rate performances of these two different approaches are nearly identical this paper explains this phenomenon by proving that as the minimum check node degree d c becomes sufficiently large , the performance discrepancy of the linear and the coset ldpc codes is theoretically indistinguishable this typicality of linear codes among the ldpc coset code ensemble provides insight into the concentration theorem of ldpc coset codes
in this paper , we study the problem of team member replacement given a team of people embedded in a social network working on the same task , find a good candidate who can fit in the team after one team member becomes unavailable we conjecture that a good team member replacement should have good skill matching as well as good structure matching we formulate this problem using the concept of graph kernel to tackle the computational challenges , we propose a family of fast algorithms by \( a \) designing effective pruning strategies , and \( b \) exploring the smoothness between the existing and the new team structures we conduct extensive experimental evaluations on real world datasets to demonstrate the effectiveness and efficiency our algorithms \( a \) perform significantly better than the alternative choices in terms of both precision and recall and \( b \) scale sub linearly
the stochastic block model \( sbm \) is a popular framework for studying community detection in networks this model is limited by the assumption that all nodes in the same community are statistically equivalent and have equal expected degrees the degree corrected stochastic block model \( dcsbm \) is a natural extension of sbm that allows for degree heterogeneity within communities this paper proposes a convexified modularity maximization approach for estimating the hidden communities under dcsbm this approach is based on a convex programming relaxation of the classical \( generalized \) modularity maximization formulation , followed by a novel doubly weighted ell 1 norm k median procedure we establish non asymptotic theoretical guarantees for both approximate clustering and perfect clustering in the special case of sbm , these theoretical results match the best known performance guarantees of computationally feasible algorithms numerically , we provide an efficient implementation of our algorithm , which is applied to both synthetic and real world networks experiment results show that our method enjoys competitive empirical performance compared to the state of the art tractable methods in the literature
we consider a system where an agent \( alice \) aims at transmitting a message to a second agent \( bob \) over a set of parallel channels , while keeping it secret from a third agent \( eve \) by using physical layer security techniques we assume that alice perfectly knows the set of channels with respect to bob , but she has only a statistical knowledge of the channels with respect to eve we derive bounds on the achievable outage secrecy rates , by considering coding either within each channel or across all parallel channels transmit power is adapted to the channel conditions , with a constraint on the average power over the whole transmission we also focus on the maximum cumulative outage secrecy rate that can be achieved moreover , in order to assess the performance in a real life scenario , we consider the use of practical error correcting codes we extend the definitions of security gap and equivocation rate , previously applied to the single additive white gaussian noise channel , to rayleigh distributed parallel channels , on the basis of the error rate targets and the outage probability bounds on these metrics are also derived , taking into account the statistics of the parallel channels numerical results are provided , that confirm the feasibility of the considered physical layer security techniques
ranked document retrieval is a fundamental task in search engines such queries are solved with inverted indexes that require additional 45 80 of the compressed text space , and take tens to hundreds of microseconds per query in this paper we show how ranked document retrieval queries can be solved within tens of milliseconds using essentially no extra space over an in memory compressed representation of the document collection more precisely , we enhance wavelet trees on bytecodes \( wtbcs \) , a data structure that rearranges the bytes of the compressed collection , so that they support ranked conjunctive and disjunctive queries , using just 6 18 of the compressed text space
so far , the scope of computer algebra has been needlessly restricted to exact algebraic methods its possible extension to approximate analytical methods is discussed the entangled roles of functional analysis and symbolic programming , especially the functional and transformational paradigms , are put forward in the future , algebraic algorithms could constitute the core of extended symbolic manipulation systems including primitives for symbolic approximations
spectrum management has been identified as a crucial step towards enabling the technology of a cognitive radio network \( crn \) most of the current works dealing with spectrum management in the crn focus on a single task of the problem , e g , spectrum sensing , spectrum decision , spectrum sharing or spectrum mobility in this two part paper , we argue that for certain network configurations , jointly performing several tasks of the spectrum management improves the spectrum efficiency specifically , our aim is to study the uplink resource management problem in a crn where there exist multiple cognitive users \( cus \) and access points \( aps \) the cus , in order to maximize their uplink transmission rates , have to associate to a suitable ap \( spectrum decision \) , and to share the channels used by this ap with other cus \( spectrum sharing \) these tasks are clearly interdependent , and the problem of how they should be carried out efficiently and in a distributed manner is still open in the literature
the primate visual system achieves remarkable visual object recognition performance even in brief presentations and under changes to object exemplar , geometric transformations , and background variation \( a k a core visual object recognition \) this remarkable performance is mediated by the representation formed in inferior temporal \( it \) cortex in parallel , recent advances in machine learning have led to ever higher performing models of object recognition using artificial deep neural networks \( dnns \) it remains unclear , however , whether the representational performance of dnns rivals that of the brain to accurately produce such a comparison , a major difficulty has been a unifying metric that accounts for experimental limitations such as the amount of noise , the number of neural recording sites , and the number trials , and computational limitations such as the complexity of the decoding classifier and the number of classifier training examples in this work we perform a direct comparison that corrects for these experimental limitations and computational considerations as part of our methodology , we propose an extension of kernel analysis that measures the generalization accuracy as a function of representational complexity our evaluations show that , unlike previous bio inspired models , the latest dnns rival the representational performance of it cortex on this visual object recognition task furthermore , we show that models that perform well on measures of representational performance also perform well on measures of representational similarity to it and on measures of predicting individual it multi unit responses whether these dnns rely on computational mechanisms similar to the primate visual system is yet to be determined , but , unlike all previous bio inspired models , that possibility cannot be ruled out merely on representational performance grounds
in this paper , we propose a new class of quantized message passing decoders for ldpc codes over the bsc the messages take values \( or levels \) from a finite set the update rules do not mimic belief propagation but instead are derived using the knowledge of trapping sets we show that the update rules can be derived to correct certain error patterns that are uncorrectable by algorithms such as bp and min sum in some cases even with a small message set , these decoders can guarantee correction of a higher number of errors than bp and min sum we provide particularly good 3 bit decoders for 3 left regular ldpc codes they significantly outperform the bp and min sum decoders , but more importantly , they achieve this at only a fraction of the complexity of the bp and min sum decoders
we consider the uplink of massive multipleinput multiple output systems in a multicell environment since the base station \( bs \) estimates the channel state information \( csi \) using the pilot signals transmitted from the users , each bs will have imperfect csi in practice assuming zero forcing method to eliminate the multi user interference , we derive the exact analytical expressions for the probability density function \( pdf \) of the signal to interference plus noise ratio \( sinr \) , the corresponding achievable rate , the outage probability , and the symbol error rate \( ser \) when the bs has imperfect csi an upper bound of the ser is also derived for an arbitrary number of antennas at the bs moreover , we derive the upper bound of the achievable rate for the case where the number of antennas at the bs goes to infinity , and the analysis is verified by presenting numerical results
social dynamics on a network may be accelerated or decelerated depending on which pairs of individuals in the network communicate early and which pairs do later the order with which the links in a given network are sequentially used , which we call the link order , may be a strong determinant of dynamical behaviour on networks , potentially adding a new dimension to effects of temporal networks relative to static networks here we study the effect of the link order on linear synchronisation dynamics we show that the synchronisation speed considerably depends on specific orders of links in addition , applying each single link for a long time to ensure strong pairwise synchronisation before moving to a next pair of individuals does not often enhance synchronisation of the entire network we also implement a simple greedy algorithm to optimise the link order in favour of fast synchronisation
audio watermarking has played an important role in multimedia security in many applications using audio watermarking , d a and a d conversions \( denoted by da ad in this paper \) are often involved in previous works , however , the robustness issue of audio watermarking against the da ad conversions has not drawn sufficient attention yet in our extensive investigation , it has been found that the degradation of a watermarked audio signal caused by the da ad conversions manifests itself mainly in terms of wave magnitude distortion and linear temporal scaling , making the watermark extraction failed accordingly , a dwt based audio watermarking algorithm robust against the da ad conversions is proposed in this paper to resist the magnitude distortion , the relative energy relationships among different groups of the dwt coefficients in the low frequency sub band are utilized in watermark embedding by adaptively controlling the embedding strength furthermore , the resynchronization is designed to cope with the linear temporal scaling the time frequency localization characteristics of dwt are exploited to save the computational load in the resynchronization consequently , the proposed audio watermarking algorithm is robust against the da ad conversions , other common audio processing manipulations , and the attacks in stirmark benchmark for audio , which has been verified by experiments
recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem , which is one of the most commonly used method for learning sparse representations to address today 's ever increasing large dataset , effective screening relies on a tight region bound on the solution to the dual lasso typical region bounds are in the form of an intersection of a sphere and multiple half spaces one way to tighten the region bound is using more half spaces , which however , adds to the overhead of solving the high dimensional optimization problem in lasso screening this paper reveals the interesting property that the optimization problem only depends on the projection of features onto the subspace spanned by the normals of the half spaces this property converts an optimization problem in high dimension to much lower dimension , and thus sheds light on reducing the computation overhead of lasso screening based on tighter region bounds
we consider three important and well studied algorithmic problems in group theory the word , geodesic , and conjugacy problem we show transfer results from individual groups to graph products we concentrate on logspace complexity because the challenge is actually in small complexity classes , only the most difficult transfer result is for the conjugacy problem we have a general result for graph products , but even in the special case of a graph group the result is new graph groups are closely linked to the theory of mazurkiewicz traces which form an algebraic model for concurrent processes our proofs are combinatorial and based on well known concepts in trace theory we also use rewriting techniques over traces for the group theoretical part we apply bass serre theory but as we need explicit formulae and as we design concrete algorithms all our group theoretical calculations are completely explicit and accessible to non specialists
an algorithmic framework to compute sparse or minimal tv solutions of linear systems is proposed the framework includes both the kaczmarz method and the linearized bregman method as special cases and also several new methods such as a sparse kaczmarz solver the algorithmic framework has a variety of applications and is especially useful for problems in which the linear measurements are slow and expensive to obtain we present examples for online compressed sensing , tv tomographic reconstruction and radio interferometry
in order to classify the nonlinear feature with linear classifier and improve the classification accuracy , a deep learning network named kernel principal component analysis network \( kpcanet \) is proposed first , mapping the data into higher space with kernel principal component analysis to make the data linearly separable then building a two layer kpcanet to obtain the principal components of image finally , classifying the principal components with linearly classifier experimental results show that the proposed kpcanet is effective in face recognition , object recognition and hand writing digits recognition , it also outperforms principal component analysis network \( pcanet \) generally as well besides , kpcanet is invariant to illumination and stable to occlusion and slight deformation
we introduce game networks \( g nets \) , a novel representation for multi agent decision problems compared to other game theoretic representations , such as strategic or extensive forms , g nets are more structured and more compact more fundamentally , g nets constitute a computationally advantageous framework for strategic inference , as both probability and utility independencies are captured in the structure of the network and can be exploited in order to simplify the inference process an important aspect of multi agent reasoning is the identification of some or all of the strategic equilibria in a game we present original convergence methods for strategic equilibrium which can take advantage of strategic separabilities in the g net structure in order to simplify the computations specifically , we describe a method which identifies a unique equilibrium as a function of the game payoffs , and one which identifies all equilibria
applied impulse controls cutoff markov multidimensional diffusion process during transformation to brownian diffusion and back to markov process , concurrently produce feller kernel and generate quantum information dynamics that initiate schr odinger bridge and entanglement transition jumps describe mixture of alternating markov brownian processes which model interactive process cutting correlation of random process with interacting virtual events of real world reveals hidden classical and quantum information in each cutoff entropy integral functional measures hidden information covering the process correlations for kernel and bridge under this transformation the interactive information processes implies transformation of entropy portion from cutting internal boundary to the information dynamics with feedback to attractive external cutoff boundary of the interaction transforming and selecting the entropy portion through interactive impulse creates information observer which kills uncertainty \( entropy \) to get information in certain information dynamics under minimax law of optimal extraction and consumption of information for complex interactions the minimax law variation equations determine structure of the information dynamics arising at this transformation information path functional integrates multiple hidden information contributions of the cutting process correlations in information units , binds their information in doublets and triplets structures , and enfolds this sequence in the information network \( in \) that successively decreases the entropy and maximizes information of the micro macrodynamics the enclosed triplets , sequentially attaching to in , free the bound information , rise information forces , attract , order , structure information units hierarchy , encode doublet triplet logic , compose quantum , classical computation , and integrate in memory and coding
we provide the first algorithm for online bandit linear optimization whose regret after t rounds is of order sqrt td ln n on any finite class x of n actions in d dimensions , and of order d sqrt t \( up to log factors \) when x is infinite these bounds are not improvable in general the basic idea utilizes tools from convex geometry to construct what is essentially an optimal exploration basis we also present an application to a model of linear bandits with expert advice interestingly , these results show that bandit linear optimization with expert advice in d dimensions is no more difficult \( in terms of the achievable regret \) than the online d armed bandit problem with expert advice \( where exp4 is optimal \)
the linked data paradigm is one of the most promising technologies for publishing , sharing , and connecting data on the web , and offers a new way for data integration and interoperability however , the proliferation of distributed , inter connected sources of information and services on the web poses significant new challenges for managing consistently a huge number of large datasets and their interdependencies in this paper we focus on the key problem of preserving evolving structured interlinked data we argue that a number of issues that hinder applications and users are related to the temporal aspect that is intrinsic in linked data we present a number of real use cases to motivate our approach , we discuss the problems that occur , and propose a direction for a solution
bound propagation is an important artificial intelligence technique used in constraint programming tools to deal with numerical constraints it is typically embedded within a search procedure \( branch and prune \) and used at every node of the search tree to narrow down the search space , so it is critical that it be fast the procedure invokes constraint propagators until a common fixpoint is reached , but the known algorithms for this have a pseudo polynomial worst case time complexity they are fast indeed when the variables have a small numerical range , but they have the well known problem of being prohibitively slow when these ranges are large an important question is therefore whether strongly polynomial algorithms exist that compute the common bound consistent fixpoint of a set of constraints this paper answers this question in particular we show that this fixpoint computation is in fact np complete , even when restricted to binary linear constraints
we propose a transition based dependency parser using recurrent neural networks with long short term memory \( lstm \) units this extends the feedforward neural network parser of chen and manning \( 2014 \) and enables modelling of entire sequences of shift reduce transition decisions on the google web treebank , our lstm parser is competitive with the best feedforward parser on overall accuracy and notably achieves more than 3 improvement for long range dependencies , which has proved difficult for previous transition based parsers due to error propagation and limited context information our findings additionally suggest that dropout regularisation on the embedding layer is crucial to improve the lstm 's generalisation
let g be any connected graph on n vertices , n ge 2 let k be any positive integer suppose that a fire breaks out on some vertex of g then in each turn k firefighters can protect vertices of g each can protect one vertex not yet on fire next a fire spreads to all unprotected neighbours the emph k surviving rate of g , denoted by rho k \( g \) , is the expected fraction of vertices that can be saved from the fire by k firefighters , provided that the starting vertex is chosen uniformly at random in this paper , it is shown that for any planar graph g we have rho 3 \( g \) ge frac 2 21 moreover , 3 firefighters are needed for the first step only after that it is enough to have 2 firefighters per each round this result significantly improves known solutions to a problem of cai and wang \( there was no positive bound known for surviving rate of general planar graph with only 3 firefighters \) the proof is done using the separator theorem for planar graphs
we present several novel identities and inequalities relating the mutual information and the directed information in systems with feedback the internal blocks within such systems are restricted only to be causal mappings , but are allowed to be non linear , stochastic and time varying moreover , the involved signals can be arbitrarily distributed we bound the directed information between signals inside the feedback loop by the mutual information between signals inside and outside the feedback loop this fundamental result has an interesting interpretation as a law of conservation of information flow building upon it , we derive several novel identities and inequalities , which allow us to prove some existing information inequalities under less restrictive assumptions finally , we establish new relationships between nested directed informations inside a feedback loop this yields a new and general data processing inequality for systems with feedback
nonnegative matrix factorization \( nmf \) under the separability assumption can provably be solved efficiently , even in the presence of noise , and has been shown to be a powerful technique in document classification and hyperspectral unmixing this problem is referred to as near separable nmf and requires that there exists a cone spanned by a small subset of the columns of the input nonnegative matrix approximately containing all columns in this paper , we propose a preconditioning based on semidefinite programming making the input matrix well conditioned this in turn can improve significantly the performance of near separable nmf algorithms which is illustrated on the popular successive projection algorithm \( spa \) the new preconditioned spa is provably more robust to noise , and outperforms spa on several synthetic data sets we also show how an active set method allow us to apply the preconditioning on large scale real world hyperspectral images
this paper presents polar coding schemes for the 2 user discrete memoryless broadcast channel \( dm bc \) which achieve marton 's region with both common and private messages this is the best achievable rate region known to date , and it is tight for all classes of 2 user dm bcs whose capacity regions are known to accomplish this task , we first construct polar codes for both the superposition as well as the binning strategy by combining these two schemes , we obtain marton 's region with private messages only finally , we show how to handle the case of common information the proposed coding schemes possess the usual advantages of polar codes , i e , they have low encoding and decoding complexity and a super polynomial decay rate of the error probability we follow the lead of goela , abbe , and gastpar , who recently introduced polar codes emulating the superposition and binning schemes in order to align the polar indices , for both schemes , their solution involves some degradedness constraints that are assumed to hold between the auxiliary random variables and the channel outputs to remove these constraints , we consider the transmission of k blocks and employ a chaining construction that guarantees the proper alignment of the polarized indices the techniques described in this work are quite general , and they can be adopted to many other multi terminal scenarios whenever there polar indices need to be aligned
in the past few years powerful generalizations to the euclidean k means problem have been made , such as bregman clustering 7 , co clustering \( i e , simultaneous clustering of rows and columns of an input matrix \) 9 , 18 , and tensor clustering 8 , 34 like k means , these more general problems also suffer from the np hardness of the associated optimization researchers have developed approximation algorithms of varying degrees of sophistication for k means , k medians , and more recently also for bregman clustering 2 however , there seem to be no approximation algorithms for bregman co and tensor clustering in this paper we derive the first \( to our knowledge \) guaranteed methods for these increasingly important clustering settings going beyond bregman divergences , we also prove an approximation factor for tensor clustering with arbitrary separable metrics through extensive experiments we evaluate the characteristics of our method , and show that it also has practical impact
this paper describes a simple image noise removal method which combines a preprocessing step with the yaroslavsky filter for strong numerical , visual , and theoretical performance on a broad class of images the framework developed is a two stage approach in the first stage the image is filtered with a classical denoising method \( e g , wavelet or curvelet thresholding \) in the second stage a modification of the yaroslavsky filter is performed on the original noisy image , where the weights of the filters are governed by pixel similarities in the denoised image from the first stage similar prefiltering ideas have proved effective previously in the literature , and this paper provides theoretical guarantees and important insight into why prefiltering can be effective empirically , this simple approach achieves very good performance for cartoon images , and can be computed much more quickly than current patch based denoising algorithms
we use a semisupervised learning algorithm based on a topological data analysis approach to assign functional categories to yeast proteins using similarity graphs this new approach to analyzing biological networks yields results that are as good as or better than state of the art existing approaches
in this paper we consider the generalization of binary spatially coupled low density parity check \( sc ldpc \) codes to finite fields gf \( q \) , q geq 2 , and develop design rules for q ary sc ldpc code ensembles based on their iterative belief propagation \( bp \) decoding thresholds , with particular emphasis on low latency windowed decoding \( wd \) we consider transmission over both the binary erasure channel \( bec \) and the binary input additive white gaussian noise channel \( biawgnc \) and present results for a variety of \( j , k \) regular sc ldpc code ensembles constructed over gf \( q \) using protographs thresholds are calculated using protograph versions of q ary density evolution \( for the bec \) and q ary extrinsic information transfer analysis \( for the biawgnc \) we show that wd of q ary sc ldpc codes provides significant threshold gains compared to corresponding \( uncoupled \) q ary ldpc block code \( ldpc bc \) ensembles when the window size w is large enough and that these gains increase as the finite field size q 2 m increases moreover , we demonstrate that the new design rules provide wd thresholds that are close to capacity , even when both m and w are relatively small \( thereby reducing decoding complexity and latency \) the analysis further shows that , compared to standard flooding schedule decoding , wd of q ary sc ldpc code ensembles results in significant reductions in both decoding complexity and decoding latency , and that these reductions increase as m increases for applications with a near threshold performance requirement and a constraint on decoding latency , we show that using q ary sc ldpc code ensembles , with moderate q 2 , instead of their binary counterparts results in reduced decoding complexity
we present a comparative study of the performance of various polar code constructions in an additive white gaussian noise \( awgn \) channel a polar code construction is any algorithm that selects k best among n possible polar bit channels at the design signal to noise ratio \( design snr \) in terms of bit error rate \( ber \) optimal polar code construction is hard and therefore many suboptimal polar code constructions have been proposed at different computational complexities polar codes are also non universal meaning the code changes significantly with the design snr however , it is not known which construction algorithm at what design snr constructs the best polar codes we first present a comprehensive survey of all the well known polar code constructions along with their full implementations we then propose a heuristic algorithm to find the best design snr for constructing best possible polar codes from a given construction algorithm the proposed algorithm involves a search among several possible design snrs we finally use our algorithm to perform a comparison of different construction algorithms using extensive simulations we find that all polar code construction algorithms generate equally good polar codes in an awgn channel , if the design snr is optimized
in the recent research of data mining , frequent structures in a sequence of graphs have been studied intensively , and one of the main concern is changing structures along a sequence of graphs that can capture dynamic properties of data on the contrary , we newly focus on preserving structures in a graph sequence that satisfy a given property for a certain period , and mining such structures is studied as for an onset , we bring up two structures , a connected vertex subset and a clique that exist for a certain period we consider the problem of enumerating these structures and present polynomial delay algorithms for the problems their running time may depend on the size of the representation , however , if each edge has at most one time interval in the representation , the running time is o \( v e 3 \) for connected vertex subsets and o \( min delta 5 , e 2 delta \) for cliques , where the input graph is g \( v , e \) with maximum degree delta to the best of our knowledge , this is the first approach to the treatment of this notion , namely , preserving structures
recently we extended approximate message passing \( amp \) algorithm to be able to handle general invariant matrix ensembles in this contribution we extend our s amp approach to non linear observation models we obtain generalized amp \( gamp \) algorithm as the special case when the measurement matrix has zero mean iid gaussian entries our derivation is based upon 1 \) deriving expectation propagation \( ep \) like algorithms from the stationary points equations of the gibbs free energy under first and second moment constraints and 2 \) applying additive free convolution in free probability theory to get low complexity updates for the second moment quantities
in the modern era , cellular communication consumers are exponentially increasing as they find the system more user friendly due to enormous users and their numerous demands , it has become a mandate to make the best use of the limited radio resources that assures the highest standard of quality of service \( qos \) to reach the guaranteed level of qos for the maximum number of users , maximum utilization of bandwidth is not only the key issue to be considered , rather some other factors like interference , call blocking probability etc are also needed to keep under deliberation the lower performances of these factors may retrograde the overall cellular networks performances keeping these difficulties under consideration , we propose an effective dynamic channel borrowing model that safeguards better qos , other factors as well the proposed scheme reduces the excessive overall call blocking probability and does interference mitigation without sacrificing bandwidth utilization the proposed scheme is modeled in such a way that the cells are bifurcated after the channel borrowing process if the borrowed channels have the same type of frequency band \( i e reused frequency \) we also propose that the unoccupied interfering channels of adjacent cells can also be inactivated , instead of cell bifurcation for interference mitigation the simulation endings show satisfactory performances in terms of overall call blocking probability and bandwidth utilization that are compared to the conventional scheme without channel borrowing furthermore , signal to interference plus noise ratio \( sinr \) level , capacity , and outage probability are compared to the conventional scheme without interference mitigation after channel borrowing that may attract the considerable concentration to the operators
we extend our previous algebraic formalisation of the notion of component based framework in order to formally define two forms , strong and weak , of the notion of full expressiveness our earlier result shows that the bip \( behaviour interaction priority \) framework does not possess the strong full expressiveness in this paper , we show that bip has the weak form of this notion and provide results detailing weak and strong full expressiveness for classical bip and several modifications , obtained by relaxing the constraints imposed on priority models
in this work we study the reliability and secrecy performance achievable by practical ldpc codes over the gaussian wiretap channel while several works have already addressed this problem in asymptotic conditions , i e , under the hypothesis of codewords of infinite length , only a few approaches exist for the finite length regime we propose an approach to measure the performance of practical codes and compare it with that achievable in asymptotic conditions moreover , based on the secrecy metrics we adopt to achieve this target , we propose a code optimization algorithm which allows to design irregular ldpc codes able to approach the ultimate performance limits even at moderately small codeword lengths \( in the order of 10000 bits \)
an attempt is made to define the concept of execution of an instruction sequence it is found to be a special case of directly putting into effect of an instruction sequence directly putting into effect of an instruction sequences comprises interpretation as well as execution directly putting into effect is a special case of putting into effect with other special cases classified as indirectly putting into effect
this paper presents the spare c library , an open source software tool conceived to build generic pattern recognition and soft computing systems the library follows the requirement of the generality most of the implemented algorithms are able to process user defined input data types transparently , such as labeled graphs and generalized sequences , as well as standard feature vectors we give just a high level picture of the spare library characteristics , focusing instead on the specific practical possibility of constructing pattern recognition systems for different input data types in particular , as a proof of concept , we discuss two application instances involving clustering of real valued multidimensional sequences and classification of labeled graphs data
this paper presents a significant modification to the random demodulator \( rd \) of tropp et al for sub nyquist sampling of frequency sparse signals the modification , termed constrained random demodulator , involves replacing the random waveform , essential to the operation of the rd , with a constrained random waveform that has limits on its switching rate because fast switching waveforms may be hard to generate cleanly the result is a relaxation on the hardware requirements with a slight , but manageable , decrease in the recovery guarantees the paper also establishes the importance of properly choosing the statistics of the constrained random waveform if the power spectrum of the random waveform matches the distribution on the tones of the input signal \( i e , the distribution is proportional to the power spectrum \) , then recovery of the input signal tones is improved the theoretical guarantees provided in the paper are validated through extensive numerical simulations and phase transition plots
it is well known as an existence result that every 3 connected graph g \( v , e \) on more than 4 vertices admits a sequence of contractions and a sequence of removal operations to k 4 such that every intermediate graph is 3 connected we show that both sequences can be computed in optimal time , improving the previously best known running times of o \( v 2 \) to o \( v e \) this settles also the open question of finding a linear time 3 connectivity test that is certifying and extends to a certifying 3 edge connectivity test in the same time the certificates used are easy to verify in time o \( e \)
we study the algebraic theory of computable functions , which can be viewed as arising from possibly non halting computer programs or algorithms , acting on some state space , equipped with operations of composition , em if then else and em while do defined in terms of a boolean algebra of conditions it has previously been shown that there is no finite axiomatisation of algebras of partial functions under these operations alone , and this holds even if one restricts attention to transformations \( representing halting programs \) rather than partial functions , and omits em while do from the signature in the halting case , there is a natural fix , which is to allow composition of halting programs with conditions , and then the resulting algebras admit a finite axiomatisation in the current setting such compositions are not possible , but by extending the notion of em if then else , we are able to give finite axiomatisations of the resulting algebras of \( partial \) functions , with em while do in the signature if the state space is assumed finite the axiomatisations are extended to consider the partial predicate of equality all algebras considered turn out to be enrichments of the notion of a \( one sided \) restriction semigroup
in the last decades , the study of models for large real world networks has been a very popular and active area of research a reasonable model should not only replicate all the structural properties that are observed in real world networks \( for example , heavy tailed degree distributions , high clustering and small diameter \) , but it should also be amenable to mathematical analysis there are plenty of models that succeed in the first task but are hard to analyze rigorously on the other hand , a multitude of proposed models , like classical random graphs , can be studied mathematically , but fail in creating certain aspects that are observed in real world networks recently , papadopoulos , krioukov , boguna and vahdat infocom'10 introduced a random geometric graph model that is based on hyperbolic geometry the authors argued empirically and by some preliminary mathematical analysis that the resulting graphs have many of the desired properties moreover , by computing explicitly a maximum likelihood fit of the internet graph , they demonstrated impressively that this model is adequate for reproducing the structure of real graphs with high accuracy in this work we initiate the rigorous study of random hyperbolic graphs we compute exact asymptotic expressions for the expected number of vertices of degree k for all k up to the maximum degree and provide small probabilities for large deviations we also prove a constant lower bound for the clustering coefficient in particular , our findings confirm rigorously that the degree sequence follows a power law distribution with controllable exponent and that the clustering is nonvanishing
the adaptation to situations of sequential choice under uncertainty of decision criteria which deviate from \( subjective \) expected utility raises the problem of ensuring the selection of a nondominated strategy in particular , when following the suggestion of machina and mcclennen of giving up separability \( also known as consequentialism \) , which requires the choice of a substrategy in a subtree to depend only on data relevant to that subtree , one must renounce to the use of dynamic programming , since bellman 's principle is no longer valid an interpretation of mcclennen 's resolute choice , based on cooperation between the successive selves of the decision maker , is proposed implementations of resolute choice which prevent money pumps negative prices of information or , more generally , choices of dominated strategies , while remaining computationally tractable , are proposed
in this paper , we present a construction of linear codes over f 2 t from boolean functions , which is a generalization of ding 's method cite theorem 9 ding15 based on this construction , we give two classes of linear codes tilde c f and c f \( see theorem ref thm maincode1 and theorem ref thm maincodenew \) over f 2 t from a boolean function f f q rightarrow f 2 , where q 2 n and f 2 t is some subfield of f q the complete weight enumerator of tilde c f can be easily determined from the walsh spectrum of f , while the weight distribution of the code c f can also be easily settled particularly , the number of nonzero weights of tilde c f and c f is the same as the number of distinct walsh values of f as applications of this construction , we show several series of linear codes over f 2 t with two or three weights by using bent , semibent , monomial and quadratic boolean function f
we prove in this paper that there exists some infinitary rational relations which are analytic but non borel sets , giving an answer to a question of simonnet automates et th 'eorie descriptive , ph d thesis , universit 'e paris 7 , march 1992
we give a simple order theoretic construction of a cartesian closed category of sequential functions it is based on bistable biorders , which are sets with a partial order the extensional order and a bistable coherence , which captures equivalence of program behaviour , up to permutation of top \( error \) and bottom \( divergence \) we show that monotone and bistable functions \( which are required to preserve bistably bounded meets and joins \) are strongly sequential , and use this fact to prove universality results for the bistable biorder semantics of the simply typed lambda calculus \( with atomic constants \) , and an extension with arithmetic and recursion we also construct a bistable model of spcf , a higher order functional programming language with non local control we use our universality result for the lambda calculus to show that the semantics of spcf is fully abstract we then establish a direct correspondence between bistable functions and sequential algorithms by showing that sequential data structures give rise to bistable biorders , and that each bistable function between such biorders is computed by a sequential algorithm
the social force model is one of the most prominent models of pedestrian dynamics as such naturally much discussion and criticism has spawned around it , some of which concerns the existence of oscillations in the movement of pedestrians this contribution is investigating under which circumstances , parameter choices , and model variants oscillations do occur and how this can be prevented it is shown that oscillations can be excluded if the model parameters fulfill certain relations the fact that with some parameter choices oscillations occur and with some not is exploited to verify a specific computer implementation of the model
in this work , we propose a generalized product of experts \( gpoe \) framework for combining the predictions of multiple probabilistic models we identify four desirable properties that are important for scalability , expressiveness and robustness , when learning and inferring with a combination of multiple models through analysis and experiments , we show that gpoe of gaussian processes \( gp \) have these qualities , while no other existing combination schemes satisfy all of them at the same time the resulting gp gpoe is highly scalable as individual gp experts can be independently learned in parallel very expressive as the way experts are combined depends on the input rather than fixed the combined prediction is still a valid probabilistic model with natural interpretation and finally robust to unreliable predictions from individual experts
data is the central asset of today 's dynamically operating organization and their business this data is usually stored in database a major consideration is applied on the security of that data from the unauthorized access and intruders data encryption is a strong option for security of data in database and especially in those organizations where security risks are high but there is a potential disadvantage of performance degradation when we apply encryption on database then we should compromise between the security and efficient query processing the work of this paper tries to fill this gap it allows the users to query over the encrypted column directly without decrypting all the records it 's improves the performance of the system the proposed algorithm works well in the case of range and fuzzy match queries
we propose a blind interference alignment scheme for the vector broadcast channel where the transmitter is equipped with m antennas and there are k receivers , each equipped with a reconfigurable antenna capable of switching among m preset modes without any knowledge of the channel coefficient values at the transmitters and with only mild assumptions on the channel coherence structure we show that mk m k 1 degrees of freedom are achievable the key to the blind interference alignment scheme is the ability of the receivers to switch between reconfigurable antenna modes to create short term channel fluctuation patterns that are exploited by the transmitter the achievable scheme does not require cooperation between transmit antennas and is therefore applicable to the mxk x network as well only finite symbol extensions are used , and no channel knowledge at the receivers is required to null the interference
in this paper we present an autonomous system for acquiring close range high resolution images that maximize the quality of a later on 3d reconstruction with respect to coverage , ground resolution and 3d uncertainty in contrast to previous work , our system uses the already acquired images to predict the confidence in the output of a dense multi view stereo approach without executing it this confidence encodes the likelihood of a successful reconstruction with respect to the observed scene and potential camera constellations our prediction module runs in real time and can be trained without any externally recorded ground truth we use the confidence prediction for on site quality assurance and for planning further views that are tailored for a specific multi view stereo approach with respect to the given scene we demonstrate the capabilities of our approach with an autonomous unmanned aerial vehicle \( uav \) in a challenging outdoor scenario
in this paper , locally repairable codes with all symbol locality are studied methods to modify already existing codes are presented also , it is shown that with high probability , a random matrix with a few extra columns guaranteeing the locality property , is a generator matrix for a locally repairable code with a good minimum distance the proof of this also gives a constructive method to find locally repairable codes constructions are given of three infinite classes of optimal vector linear locally repairable codes over an alphabet of small size , not depending on the size of the code length
editing on digital images is ubiquitous identification of deliberately modified facial images is a new challenge for face identification system in this paper , we address the problem of identification of a face or person from heavily altered facial images in this face identification problem , the input to the system is a manipulated or transformed face image and the system reports back the determined identity from a database of known individuals such a system can be useful in mugshot identification in which mugshot database contains two views \( frontal and profile \) of each criminal we considered only frontal view from the available database for face identification and the query image is a manipulated face generated by face transformation software tool available online we propose sift features for efficient face identification in this scenario further comparative analysis has been given with well known eigenface approach experiments have been conducted with real case images to evaluate the performance of both methods
a bi directional korean english dialog translation system is designed and implemented using the memory based translation technique the system kemdt \( korean english memory based dialog translation system \) can perform korean to english , and english to korean translation using unified memory network and extended marker passing algorithm we resolve the word order variation and frequent word omission problems in korean by classifying the concept sequence element in four different types and extending the marker passing based translation algorithm unlike the previous memory based translation systems , the kemdt system develops the bilingual memory network and the unified bi directional marker passing translation algorithm for efficient language specific processing , we separate the morphological processors from the memory based translator the kemdt technology provides a hierarchical memory network and an efficient marker based control for the recent example based mt paradigm
we study the quadratic residue weight enumerators of the dual projective reed solomon codes of dimensions 5 and q 4 over the finite field mathbb f q our main results are formulas for the coefficients of the the quadratic residue weight enumerators for such codes if q p v and we fix v and vary p then our formulas for the coefficients of the dimension q 4 code involve only polynomials in p and the trace of the q th and \( q p 2 \) th hecke operators acting on spaces of cusp forms for the congruence groups operatorname sl 2 \( mathbb z \) , gamma 0 \( 2 \) , and gamma 0 \( 4 \) the main tool we use is the eichler selberg trace formula , which gives along the way a variation of a theorem of birch on the distribution of rational point counts for elliptic curves with prescribed 2 torsion over a fixed finite field
this paper considers a 2 player strategic game for network routing under link disruptions player 1 \( defender \) routes flow through a network to maximize her value of effective flow while facing transportation costs player 2 \( attacker \) simultaneously disrupts one or more links to maximize her value of lost flow but also faces cost of disrupting links this game is strategically equivalent to a zero sum game linear programming duality and the max flow min cut theorem are applied to obtain properties that are satisfied in any mixed nash equilibrium in any equilibrium , both players achieve identical payoffs while the defender 's expected transportation cost decreases in attacker 's marginal value of lost flow , the attacker 's expected cost of attack increases in defender 's marginal value of effective flow interestingly , the expected amount of effective flow decreases in both these parameters these results can be viewed as a generalization of the classical max flow with minimum transportation cost problem to adversarial environments
it is demonstrated that appropriately chosen computable metrics based on self correlation properties provide a degree of determinism sufficient to segregate binary strings by level of information content
heisenberg like and fisher information based uncertainty relations which extend and generalize previous similar expressions are obtained for n fermion d dimensional systems the contributions of both spatial and spin degrees of freedom are taken into account the accuracy of some of these generalized spinned uncertainty like relations is numerically examined for a large number of atomic and molecular systems
we study the approximation hardness of the shortest superstring , the maximal compression and the maximum asymmetric traveling salesperson \( max atsp \) problem we introduce a new reduction method that produces strongly restricted instances of the shortest superstring problem , in which the maximal orbit size is eight \( with no character appearing more than eight times \) and all given strings having length four based on this reduction method , we are able to improve the best up to now known approximation lower bound for the shortest superstring problem and the maximal compression problem by an order of magnitude the results imply also an improved approximation lower bound for the max atsp problem
in this position paper we present a new approach for discovering some special classes of assertional knowledge in the text by using large rdf repositories , resulting in the extraction of new non taxonomic ontological relations also we use inductive reasoning beside our approach to make it outperform then , we prepare a case study by applying our approach on sample data and illustrate the soundness of our proposed approach moreover in our point of view current lod cloud is not a suitable base for our proposal in all informational domains therefore we figure out some directions based on prior works to enrich datasets of linked data by using web mining the result of such enrichment can be reused for further relation extraction and ontology enrichment from unstructured free text documents
latent variable models are one popular approach to modeling sequences one problem with sequence models , including latent variable models , is that their exact learning algorithms are usually intractable in t , the length of the sequence , necessitating the use of approximation algorithms though these algorithms are faster than their exact counterparts , they are iterative and computationally expensive however , models with fast algorithms can be designed for commonly occurring subsets of the set of sequences we propose a new statistical model for a subset the set of sequences with inertia our learning algorithms , at time complexity o \( t log t \) , are significantly faster than those of general purpose latent variable sequence models
discussion of latent variable graphical model selection via convex optimization by venkat chandrasekaran , pablo a parrilo and alan s willsky arxiv 1008 1290
structured sparse coding and the related structured dictionary learning problems are novel research areas in machine learning in this paper we present a new application of structured dictionary learning for collaborative filtering based recommender systems our extensive numerical experiments demonstrate that the presented technique outperforms its state of the art competitors and has several advantages over approaches that do not put structured constraints on the dictionary elements
online optimization problems arise in many resource allocation tasks , where the future demands for each resource and the associated utility functions change over time and are not known apriori , yet resources need to be allocated at every point in time despite the future uncertainty in this paper , we consider online optimization problems with general concave utilities we modify and extend an online optimization algorithm proposed by devanur et al for linear programming to this general setting the model we use for the arrival of the utilities and demands is known as the random permutation model , where a fixed collection of utilities and demands are presented to the algorithm in random order we prove that under this model the algorithm achieves a competitive ratio of 1 o \( epsilon \) under a near optimal assumption that the bid to budget ratio is o left \( frac epsilon 2 log left \( m epsilon right \) right \) , where m is the number of resources , while enjoying a significantly lower computational cost than the only algorithm known to achieve optimal competitive ratio proposed by kesselheim et al we draw a connection between the proposed algorithm and subgradient methods used in convex optimization in addition , we present numerical experiments that demonstrate the performance and speed of this algorithm in comparison to existing algorithms
in this paper with the aid of genetic algorithm and fuzzy theory , we present a hybrid job scheduling approach , which considers the load balancing of the system and reduces total execution time and execution cost we try to modify the standard genetic algorithm and to reduce the iteration of creating population with the aid of fuzzy theory the main goal of this research is to assign the jobs to the resources with considering the vm mips and length of jobs the new algorithm assigns the jobs to the resources with considering the job length and resources capacities we evaluate the performance of our approach with some famous cloud scheduling models the results of the experiments show the efficiency of the proposed approach in term of execution time , execution cost and average degree of imbalance \( di \)
modern web browsers are incredibly complex , with millions of lines of code and over one thousand javascript functions and properties available to website authors this work investigates how these browser features are used on the modern , open web we find that javascript features differ wildly in popularity , with over 50 of provided features never used in the alexa 10k we also look at how popular ad and tracking blockers change the distribution of features used by sites , and identify a set of approximately 10 of features that are disproportionately blocked \( prevented from executing by these extensions at least 90 of the time they are used \) we additionally find that in the presence of these blockers , over 83 of available features are executed on less than 1 of the most popular 10 , 000 websites we additionally measure a variety of aspects of browser feature usage on the web , including how complex sites have become in terms of feature usage , how the length of time a browser feature has been in the browser relates to its usage on the web , and how many security vulnerabilities have been associated with related browser features
we consider weighted counting of independent sets using a rational weight x given a graph with n vertices , count its independent sets such that each set of size k contributes x k this is equivalent to computation of the partition function of the lattice gas with hard core self repulsion and hard core pair interaction we show the following conditional lower bounds if counting the satisfying assignments of a 3 cnf formula in n variables \( 3sat \) needs time 2 omega \( n \) \( i e there is a c 0 such that no algorithm can solve 3sat in time 2 cn \) , counting the independent sets of size n 3 of an n vertex graph needs time 2 omega \( n \) and weighted counting of independent sets needs time 2 omega \( n log 3 n \) for all rational weights x neq 0 we have two technical ingredients the first is a reduction from 3sat to independent sets that preserves the number of solutions and increases the instance size only by a constant factor second , we devise a combination of vertex cloning and path addition this graph transformation allows us to adapt a recent technique by dell , husfeldt , and wahlen which enables interpolation by a family of reductions , each of which increases the instance size only polylogarithmically
in this paper we present an fpga based implementation of linear programming \( lp \) decoding lp decoding frames error correction as an optimization problem this is in contrast to variants of belief propagation \( bp \) decoding that view error correction as a problem of graphical inference there are many advantages to taking the optimization perspective convergence guarantees , improved performance in certain regimes , and a methodology for incorporating the latest developments in optimization techniques however , lp decoding , when implemented with standard lp solvers , does not easily scale to the blocklengths of modern error correction codes in earlier work , we showed that by drawing on decomposition methods from optimization theory , specifically the alternating direction method of multipliers \( admm \) , we could build an lp decoding solver that was competitive with bp , both in terms of performance and speed we also observed empirically that lp decoders have much better high snr performance in the error floor regime , a trait of particular relevance to optical transport and storage applications while our previous implementation was in floating point , in this paper we report initial results of a fixed point , hardware based realization of our admm lp decoder
in this paper , a novel approach for optimizing the joint deployment of small cell base stations and wireless backhaul links is proposed this joint deployment scenario is cast as a multi objective optimization problem under the constraints of limited backhaul capacity and outage probability to address the problem , a novel adaptive algorithm that integrates epsilon method , lagrangian relaxation and tabu search is proposed to obtain the pareto optimal solution set simulation results show that the proposed algorithm is quite effective in finding the optimal solutions the proposed joint deployment model can be used for planning small cell networks
a polynomial algorithm is obtained for the np complete linear ordering problem
in 1992 , wilf and zeilberger conjectured that a hypergeometric term in several discrete and continuous variables is holonomic if and only if it is proper strictly speaking the conjecture does not hold , but it is true when reformulated properly payne proved a piecewise interpretation in 1997 , and independently , abramov and petkovsek in 2002 proved a conjugate interpretation both results address the pure discrete case of the conjecture in this paper we extend their work to hypergeometric terms in several discrete and continuous variables and prove the conjugate interpretation of the wilf zeilberger conjecture in this mixed setting
it is a fundamental property of non letter lyndon words that they can be expressed as a concatenation of two shorter lyndon words this leads to a naive lower bound log 2 \( n \) 1 for the number of distinct lyndon factors that a lyndon word of length n must have , but this bound is not optimal in this paper we show that a much more accurate lower bound is log phi \( n \) 1 , where phi denotes the golden ratio \( 1 sqrt 5 \) 2 we show that this bound is optimal in that it is attained by the fibonacci lyndon words we then introduce a mapping l x that counts the number of lyndon factors of length at most n in an infinite word x we show that a recurrent infinite word x is aperiodic if and only if l x l f , where f is the fibonacci infinite word , with equality if and only if f is in the shift orbit closure of f
we investigate the problem of construction of small size universal petri nets with inhibitor arcs we consider four descriptional complexity parameters the number of places , transitions , inhibitor arcs , and the maximal degree of a transition , each of which we try to minimize we give six constructions having the following values of parameters \( listed in the above order \) \( 30 , 34 , 13 , 3 \) , \( 14 , 31 , 51 , 8 \) , \( 11 , 31 , 79 , 11 \) , \( 21 , 25 , 13 , 5 \) , \( 67 , 64 , 8 , 3 \) , \( 58 , 55 , 8 , 5 \) that improve the few known results on this topic our investigation also highlights several interesting trade offs
detection of community structures in social networks has attracted lots of attention in the domain of sociology and behavioral sciences social networks also exhibit dynamic nature as these networks change continuously with the passage of time social networks might also present a hierarchical structure led by individuals that play important roles in a society such as managers and decision makers detection and visualization of these networks changing over time is a challenging problem where communities change as a function of events taking place in the society and the role people play in it in this paper we address these issues by presenting a system to analyze dynamic social networks the proposed system is based on dynamic graph discretization and graph clustering the system allows detection of major structural changes taking place in social communities over time and reveals hierarchies by identifying influential people in a social networks we use two different data sets for the empirical evaluation and observe that our system helps to discover interesting facts about the social and hierarchical structures present in these social networks
polar codes are constructed for m user multiple access channels \( mac \) whose input alphabet size is a prime number the block error probability under successive cancelation decoding decays exponentially with the square root of the block length although the sum capacity is achieved by this coding scheme , some points in the symmetric capacity region may not be achieved in the case where the channel is a combination of linear channels , we provide a necessary and sufficient condition characterizing the channels whose symmetric capacity region is preserved upon the polarization process we also provide a sufficient condition for having a total loss in the dominant face
this research analyzes a who cites whom matrix in terms of aggregated , journal journal citations to determine the location of communication studies on the academic spectrum using the journal of communication as the seed journal , the 2006 data in the journal citation reports are used to map communication studies the results show that social and experimental psychology journals are the most frequently used sources of information in this field in addition , several journals devoted to the use and effects of media and advertising are weakly integrated into the larger communication research community , whereas communication studies are dominated by american journals
we introduce two methods for speeding up adiabatic quantum computations by increasing the energy between the ground and first excited states our methods are even more general they can be used to shift a hamiltonian 's density of states away from the ground state , so that fewer states occupy the low lying energies near the minimum , hence allowing for faster adiabatic passages to find the ground state with less risk of getting caught in an undesired low lying excited state during the passage even more generally , our methods can be used to transform a discrete optimization problem into a new one whose unique minimum still encodes the desired answer , but with the objective function 's values forming a different landscape aspects of the landscape such as the objective function 's range , or the values of certain coefficients , or how many different inputs lead to a given output value , can be decreased or increased one of the many examples for which these methods are useful is in finding the ground state of a hamiltonian using nmr if it is difficult to find a molecule such that the distances between the spins match the interactions in the hamiltonian , the interactions in the hamiltonian can be changed without at all changing the ground state we apply our methods to an aqc algorithm for integer factorization , and the first method reduces the maximum runtime in our example by up to 754 , and the second method reduces the maximum runtime of another example by up to 250 these two methods may also be combined
multitrack detection with array head reading is a promising technique proposed for next generation magnetic storage systems the multihead multitrack \( mhmt \) system is characterized by intersymbol interference \( isi \) in the downtrack direction and intertrack interference \( iti \) in the crosstrack direction constructing the trellis of a mhmt maximum likelihood \( ml \) detector requires knowledge of the iti , which is generally unknown at the receiver in addition , to retain efficiency , the ml detector requires a static estimate of the iti , whose true value may in reality vary in this paper we propose a modified ml detector on the n head , n track \( n h n t \) channel which could efficiently track the change of iti , and adapt to new estimates the trellis used in the proposed detector is shown to be independent of the iti level a gain loop structure is used to estimate the iti simulation results show that the proposed detector offers a performance advantage in settings where complexity constraints limit the traditional ml detector to use a static iti estimate
because of the emerging field of internet of things \( iot \) , future backscatter rfid is required to be more reliable and data intensive motivated by this , orthogonal space time block code \( ostbc \) , which is very successful in mobile communications for its low complexity and high performance , has already been investigated for backscatter rfid on the other hand , a recently proposed scheme called unitary query was shown to be able to considerably improve the reliability of backscatter radio by exploiting query diversity therefore incorporating the classical ostbc \( at the tag end \) with the recently proposed unitary query \( at the query end \) seems to be promising however , in this paper , we show that simple , direct employment of ostbc together with unitary query incurs a linear decoding problem and eventually leads to a severe performance degradation as a re design of the recently proposed unitary query and the classical ostbc specifically for mimo backscatter rfid , we present a butq mostbc design pair idea by proposing the block level unitary query \( butq \) at the query end and the corresponding modified ostbc \( mostbc \) at the tag end the proposed butq mostbc can resolve the linear decoding problem , keep the simplicity and high performance properties of the classical ostbc , and achieve the query diversity for the m times l times n mimo backscatter rfid channel
online discussion forums are complex webs of overlapping subcommunities \( macrolevel structure , across threads \) in which users enact different roles depending on which subcommunity they are participating in within a particular time point \( microlevel structure , within threads \) this sub network structure is implicit in massive collections of threads to uncover this structure , we develop a scalable algorithm based on stochastic variational inference and leverage topic models \( lda \) along with mixed membership stochastic block \( mmsb \) models we evaluate our model on three large scale datasets , cancer threadstarter \( 22k users and 14 4k threads \) , cancer namemention \( 15 1k users and 12 4k threads \) and stackoverflow \( 1 19 million users and 4 55 million threads \) qualitatively , we demonstrate that our model can provide useful explanations of microlevel and macrolevel user presentation characteristics in different communities using the topics discovered from posts quantitatively , we show that our model does better than mmsb and lda in predicting user reply structure within threads in addition , we demonstrate via synthetic data experiments that the proposed active sub network discovery model is stable and recovers the original parameters of the experimental setup with high probability
the main application of name searching has been name matching in a database of names this paper discusses a different application improving information retrieval through name recognition it investigates name recognition accuracy , and the effect on retrieval performance of indexing and searching personal names differently from non name terms in the context of ranked retrieval the main conclusions are that name recognition in text can be effective that names occur frequently enough in a variety of domains , including those of legal documents and news databases , to make recognition worthwhile and that retrieval performance can be improved using name searching
random linear network coding \( rlnc \) has emerged as a powerful tool for robust high throughput multicast projection analysis a recently introduced technique shows that the distributed packetized rlnc protocol achieves \( order \) optimal and perfectly pipelined information dissemination in many settings in the original approach to rnlc intermediate nodes code together all available information this requires intermediate nodes to keep considerable data available for coding moreover , it results in a coding complexity that grows linearly with the size of this data while this has been identified as a problem , approaches that combine queuing theory and network coding have heretofore not provided a succinct representation of the memory needs of network coding at intermediates nodes this paper shows the surprising result that , in all settings with a continuous stream of data , network coding continues to perform optimally even if only one packet per node is kept in active memory and used for computations this leads to an extremely simple rlnc protocol variant with drastically reduced requirements on computational and memory resources by extending the projection analysis , we show that in all settings in which the rlnc protocol was proven to be optimal its finite memory variant performs equally well in the same way as the original projection analysis , our technique applies in a wide variety of network models , including highly dynamic topologies that can change completely at any time in an adversarial fashion
we consider the problem of secret key extraction when n honest parties and an eavesdropper share correlated information we present a family of probability distributions and give the full characterization of its distillation properties this formalism allows us to design a rich variety of cryptographic scenarios in particular , we provide examples of multipartite probability distributions containing non distillable secret correlations , also known as bound information
it has previously been shown that ensembles of terminated protograph based low density parity check \( ldpc \) convolutional codes have a typical minimum distance that grows linearly with block length and that they are capable of achieving capacity approaching iterative decoding thresholds on the binary erasure channel \( bec \) in this paper , we review a recent result that the dramatic threshold improvement obtained by terminating ldpc convolutional codes extends to the additive white gaussian noise \( awgn \) channel also , using a \( 3 , 6 \) regular protograph based ldpc convolutional code ensemble as an example , we perform an asymptotic trapping set analysis of terminated ldpc convolutional code ensembles in addition to capacity approaching iterative decoding thresholds and linearly growing minimum distance , we find that the smallest non empty trapping set of a terminated ensemble grows linearly with block length
security in computer networks is one of the most interesting aspects of computer systems it is typically represented by the initials cia confidentiality , integrity , and authentication or availability although , many access levels for data protection have been identified in computer networks , the intruders would still find lots of ways to harm sites and systems the accommodation proceedings and the security supervision in the network systems , especially wireless sensor networks have been changed into a challenging point one of the newest security algorithms for wireless sensor networks is artificial immune system \( ais \) algorithm human lymphocytes play the main role in recognizing and destroying the unknown elements in this article , we focus on the inspiration of these defective systems to guarantee the complications security using two algorithms the first algorithms proposed to distinguish self nodes from non self ones by the related factors and the second one is to eliminate the enemy node danger the results showed a high rate success and good rate of detecting for unknown object it could present the best nodes with high affinity and fitness to be selected to confront the unknown agents
we study an adaptive anisotropic huber functional based image restoration scheme by using a combination of l2 l1 regularization functions , an adaptive huber functional based energy minimization model provides denoising with edge preservation in noisy digital images we study a convergent finite difference scheme based on continuous piecewise linear functions and use a variable splitting scheme , namely the split bregman , to obtain the discrete minimizer experimental results are given in image denoising and comparison with additive operator splitting , dual fixed point , and projected gradient schemes illustrate that the best convergence rates are obtained for our algorithm
a number of researchers have noted the similarities between ltags and ccgs observing this resemblance , we felt that we could make use of the wide coverage grammar developed in the xtag project to build a wide coverage ccg to our knowledge there have been no attempts to construct a large scale ccg parser with the lexicon to support it in this paper , we describe such a system , built by adapting various xtag components to ccg we find that , despite the similarities between the formalisms , certain parts of the grammatical workload are distributed differently in addition , the flexibility of ccg derivations allows the translated grammar to handle a number of ``non constituent'' constructions which the xtag grammar cannot
we consider an energy minimization problem for cooperative lte networks to reduce energy consumption , we investigate how to jointly optimize the transmit power and the association between cells and user equipments \( ues \) , by taking into consideration joint transmission \( jt \) , one of the coordinated multipoint \( comp \) techniques we formulate the optimization problem mathematically for solving the problem , a dynamic power allocation algorithm that adjusts the transmit power of all cells , and an algorithm for optimizing the cell ue association , are proposed the two algorithms are iteratively used in an algorithmic framework to enhance the energy performance numerically , the proposed algorithms can lead to lower energy consumption than the optimal energy setting in the non jt case in comparison to fixed power allocation in jt , the proposed dynamic power allocation algorithm is able to significantly reduce the energy consumption
we give a new algorithm to find local maximum and minimum of a holonomic function and apply it for the fisher bingham integral on the sphere s n , which is used in the directional statistics the method utilizes the theory and algorithms of holonomic systems
we consider the problem of recovering items matching a partially specified pattern in multidimensional trees \( quadtrees and k d trees \) we assume the traditional model where the data consist of independent and uniform points in the unit square for this model , in a structure on n points , it is known that the number of nodes c n \( xi \) to visit in order to report the items matching a random query xi , independent and uniformly distributed on 0 , 1 , satisfies mathbf e c n \( xi \) sim kappa n beta , where kappa and beta are explicit constants we develop an approach based on the analysis of the cost c n \( s \) of any fixed query s in 0 , 1 , and give precise estimates for the variance and limit distribution of the cost c n \( x \) our results permit us to describe a limit process for the costs c n \( x \) as x varies in 0 , 1 one of the consequences is that mathbf e max x in 0 , 1 c n \( x \) sim gamma n beta this settles a question of devroye pers comm , 2000
schein and gallager introduced the gaussian parallel relay channel in 2000 they proposed the amplify and forward \( af \) and the decode and forward \( df \) strategies for this channel for a long time , the best known achievable rate for this channel was based on the af and df with time sharing \( af df \) recently , a rematch and forward \( rf \) scheme for the scenario in which different amounts of bandwidth can be assigned to the first and second hops were proposed in this paper , we propose a emph combined amplify and decode forward \( cadf \) scheme for the gaussian parallel relay channel we prove that the cadf scheme always gives a better achievable rate compared to the rf scheme , when there is a bandwidth mismatch between the first hop and the second hop furthermore , for the equal bandwidth case \( schein 's setup \) , we show that the time sharing between the cadf and the df schemes \( cadf df \) leads to a better achievable rate compared to the time sharing between the rf and the df schemes \( rf df \) as well as the af df
there is a growing interest in energy efficient or so called green wireless communication to reduce the energy consumption in cellular networks since today 's wireless terminals are typically equipped with multiple network access interfaces such as bluetooth , wi fi , and cellular networks , this paper investigates user terminals cooperating with each other in transmitting their data packets to a base station \( bs \) by exploiting the multiple network access interfaces , referred to as inter network cooperation , to improve the energy efficiency in cellular uplink transmission given target outage probability and data rate requirements , we develop a closed form expression of energy efficiency in bits per joule for the inter network cooperation by taking into account the path loss , fading , and thermal noise effects numerical results show that when the cooperating users move towards to each other , the proposed inter network cooperation significantly improves the energy efficiency as compared with the traditional non cooperation and intra network cooperation this implies that given a certain amount of bits to be transmitted , the inter network cooperation requires less energy than the traditional non cooperation and intra network cooperation , showing the energy saving benefit of inter network cooperation
the maximum genus gamma m \( g \) of a graph g is the largest genus of an orientable surface into which g has a cellular embedding combinatorially , it coincides with the maximum number of disjoint pairs of adjacent edges of g whose removal results in a connected spanning subgraph of g in this paper we prove that removing pairs of adjacent edges from g arbitrarily while retaining connectedness leads to at least gamma m \( g \) 2 pairs of edges removed this allows us to describe a greedy algorithm for the maximum genus of a graph our algorithm returns an integer k such that gamma m \( g \) 2 le k le gamma m \( g \) , providing a simple method to efficiently approximate maximum genus as a consequence of our approach we obtain a 2 approximate counterpart of xuong 's combinatorial characterisation of maximum genus
many applied time dependent problems are characterized by an additive representation of the problem operator additive schemes are constructed using such a splitting and associated with the transition to a new time level on the basis of the solution of more simple problems for the individual operators in the additive decomposition we consider a new class of additive schemes for problems with additive representation of the operator at the time derivative in this paper we construct and study the vector operator difference schemes , which are characterized by a transition from one initial the evolution equation to a system of such equations
we define the geometric thickness of a graph to be the smallest number of layers such that we can draw the graph in the plane with straight line edges and assign each edge to a layer so that no two edges on the same layer cross the geometric thickness lies between two previously studied quantities , the \( graph theoretical \) thickness and the book thickness we investigate the geometric thickness of the family of complete graphs , k n we show that the geometric thickness of k n lies between ceiling \( \( n 5 646 \) 0 342 \) and ceiling \( n 4 \) , and we give exact values of the geometric thickness of k n for n 12 and n in 15 , 16 we also consider the geometric thickness of the family of complete bipartite graphs in particular , we show that , unlike the case of complete graphs , there are complete bipartite graphs with arbitrarily large numbers of vertices for which the geometric thickness coincides with the standard graph theoretical thickness
in this paper , we consider the joint design of the transceivers for a multiple access multiple input and multiple output \( mimo \) system having inter symbol interference \( isi \) channels the system we consider is equipped with the minimum mean square error \( mmse \) decision feedback \( df \) detector traditionally , transmitter designs for this system have been based on constraints of either the transmission power or the signal to interference and noise ratio \( sinr \) for each user here , we explore a novel perspective and examine a transceiver design which is based on a fixed sum mutual information constraint and minimizes the arithmetic average of mean square error of mmse decision feedback detection for this optimization problem , a closed form solution is obtained and is achieved if and only if the averaged sum mutual information is uniformly distributed over each active subchannel meanwhile , the mutual information of the currently detected user is uniformly distributed over each individual symbol within the block signal of the user , assuming all the previous user signals have been perfectly detected
large software projects are among most sophisticated human made systems consisting of a network of interdependent parts past studies of software systems from the perspective of complex networks have already led to notable discoveries with different applications nevertheless , our comprehension of the structure of software networks remains to be only partial we here investigate correlations or mixing between linked nodes and show that software networks reveal dichotomous node degree mixing similar to that recently observed in biological networks we further show that software networks also reveal characteristic clustering profiles and mixing hence , node mixing in software networks significantly differs from that in , e g , the internet or social networks we explain the observed mixing through the presence of groups of nodes with common linking pattern more precisely , besides densely linked groups known as communities , software networks also consist of disconnected groups denoted modules , core periphery structures and other moreover , groups coincide with the intrinsic properties of the underlying software projects , which promotes practical applications in software engineering
a method for selecting solution constructors in narrowing is presented the method is based on a sort discipline that describes regular sets of ground constructor terms as sorts it is extended to cope with regular sets of ground substitutions , thus allowing different sorts to be computed for terms with different variable bindings an algorithm for computing signatures of equationally defined functions is given that allows potentially infinite overloading applications to formal program development are sketched
random network coding recently attracts attention as a technique to disseminate information in a network this paper considers a non coherent multi shot network , where the unknown and time variant network is used several times in order to create dependencies between the different shots , particular convolutional codes in rank metric are used these codes are so called \( partial \) unit memory \( \( p \) um \) codes , i e , convolutional codes with memory one first , distance measures for convolutional codes in rank metric are shown and two constructions of \( p \) um codes in rank metric based on the generator matrices of maximum rank distance codes are presented second , an efficient error erasure decoding algorithm for these codes is presented its guaranteed decoding radius is derived and its complexity is bounded finally , it is shown how to apply these codes for error correction in random linear and affine network coding
in this paper we first propose a new statistical parsing model , which is a generative model of lexicalised context free grammar we then extend the model to include a probabilistic treatment of both subcategorisation and wh movement results on wall street journal text show that the parser performs at 88 1 87 5 constituent precision recall , an average improvement of 2 3 over \( collins 96 \)
we study the performance of stochastic local search algorithms for random instances of the k satisfiability \( k sat \) problem we introduce a new stochastic local search algorithm , chainsat , which moves in the energy landscape of a problem instance by em never going upwards in energy chainsat is a emph focused algorithm in the sense that it considers only variables occurring in unsatisfied clauses we show by extensive numerical investigations that chainsat and other focused algorithms solve large k sat instances almost surely in linear time , up to high clause to variable ratios alpha for example , for k 4 we observe linear time performance well beyond the recently postulated clustering and condensation transitions in the solution space the performance of chainsat is a surprise given that by design the algorithm gets trapped into the first local energy minimum it encounters , yet no such minima are encountered we also study the geometry of the solution space as accessed by stochastic local search algorithms
conventional face to face classrooms are still the main learning system applied in indonesia in assisting such conventional learning towards an optimal learning , formative evaluations are needed to monitor the progress of the class this task can be very hard when the size of the class is large hence , this research attempted to create a classroom monitoring system based on student data of department of electrical engineering and information technology in order to achieve the goal , a student modeling using case based reasoning was proposed a generic student model based on a framework was developed the model represented student knowledge of a subject the result showed that the system was able to store and retrieve student data for suggestion of the current situation and formative evaluation for one of the subject in the department
in this paper we propose a new soft input soft output equalization algorithm , offering very good performance complexity tradeoffs it follows the structure of the bcjr algorithm , but dynamically constructs a simplified trellis during the forward recursion in each trellis section , only the m states with the strongest forward metric are preserved , similar to the m bcjr algorithm unlike the m bcjr , however , the remaining states are not deleted , but rather merged into the surviving states the new algorithm compares favorably with the reduced state bcjr algorithm , offering better performance and more flexibility , particularly for systems with higher order modulations
in ranking problems , the goal is to learn a ranking function from labeled pairs of input points in this paper , we consider the related comparison problem , where the label indicates which element of the pair is better , or if there is no significant difference we cast the learning problem as a margin maximization , and show that it can be solved by converting it to a standard svm we use simulated nonlinear patterns and a real learning to rank sushi data set to show that our proposed svmcompare algorithm outperforms svmrank when there are equality pairs
we study multidimensional configurations \( infinite words \) and subshifts of low pattern complexity using tools of algebraic geometry we express the configuration as a multivariate formal power series over integers and investigate the setup when there is a non trivial annihilating polynomial a non zero polynomial whose formal product with the power series is zero such annihilator exists , for example , if the number of distinct patterns of some finite shape d in the configuration is at most the size d of the shape this is our low pattern complexity assumption we prove that the configuration must be a sum of periodic configurations over integers , possibly with unbounded values as a specific application of the method we obtain an asymptotic version of the well known nivat 's conjecture we prove that any two dimensional , non periodic configuration can satisfy the low pattern complexity assumption with respect to only finitely many distinct rectangular shapes d
learning a good distance metric in feature space potentially improves the performance of the knn classifier and is useful in many real world applications many metric learning algorithms are however based on the point estimation of a quadratic optimization problem , which is time consuming , susceptible to overfitting , and lack a natural mechanism to reason with parameter uncertainty , an important property useful especially when the training set is small and or noisy to deal with these issues , we present a novel bayesian metric learning method , called bayesian nca , based on the well known neighbourhood component analysis method , in which the metric posterior is characterized by the local label consistency constraints of observations , encoded with a similarity graph instead of independent pairwise constraints for efficient bayesian optimization , we explore the variational lower bound over the log likelihood of the original nca objective experiments on several publicly available datasets demonstrate that the proposed method is able to learn robust metric measures from small size dataset and or from challenging training set with labels contaminated by errors the proposed method is also shown to outperform a previous pairwise constrained bayesian metric learning method
puncturing is a well known coding technique widely used for constructing rate compatible codes in this paper , we consider the problem of puncturing low density parity check codes and propose a new algorithm for intentional puncturing the algorithm is based on the puncturing of untainted symbols , i e nodes with no punctured symbols within their neighboring set it is shown that the algorithm proposed here performs better than previous proposals for a range of coding rates and short proportions of punctured symbols
a circle graph is the intersection graph of a set of chords in a circle keil discrete applied mathematics , 42 \( 1 \) 51 63 , 1993 proved that dominating set , connected dominating set , and total dominating set are np complete in circle graphs to the best of our knowledge , nothing was known about the parameterized complexity of these problems in circle graphs in this paper we prove the following results , which contribute in this direction dominating set , independent dominating set , connected dominating set , total dominating set , and acyclic dominating set are w 1 hard in circle graphs , parameterized by the size of the solution whereas both connected dominating set and acyclic dominating set are w 1 hard in circle graphs , it turns out that connected acyclic dominating set is polynomial time solvable in circle graphs if t is a given tree , deciding whether a circle graph has a dominating set isomorphic to t is np complete when t is in the input , and fpt when parameterized by v \( t \) we prove that the fpt algorithm is subexponential
in this paper , we explore the interplay between network topology and time criticality in a military logistics system a general goal of this work \( and previous work \) is to evaluate land transportation requirements or , more specifically , how to design appropriate fleets of military general service vehicles that are tasked with the supply and re supply of military units dispersed in an area of operation the particular focus of this paper is to gain a better understanding of how the logistics environment changes when current army vehicles with fixed transport characteristics are replaced by a new generation of modularised vehicles that can be configured task specifically the experimental work is conducted within a well developed strategic planning simulation environment which includes a scenario generation engine for automatically sampling supply and re supply missions and a multi objective meta heuristic search algorithm \( i e evolutionary algorithm \) for solving the particular scheduling and routing problems the results presented in this paper allow for a better understanding of how \( and under what conditions \) a modularised vehicle fleet can provide advantages over the currently implemented system
a practical approach to protecting networks against epidemic processes such as spreading of infectious diseases , malware , and harmful viral information is to remove some influential nodes beforehand to fragment the network into small components because determining the optimal order to remove nodes is a computationally hard problem , various approximate algorithms have been proposed to efficiently fragment networks by sequential node removal morone and makse proposed an algorithm employing the non backtracking matrix of given networks , which outperforms various existing algorithms with a reasonable computational cost in fact , many empirical networks have community structure , compromising the assumption of local tree like structure on which the original algorithm is based we develop an immunization algorithm by synergistically combining the morone makse algorithm and coarse graining of the network in which we regard a community as a supernode the proposed algorithm works more efficiently than the morone makse and other algorithms on networks with community structure
performing random walks in networks is a fundamental primitive that has found numerous applications in communication networks such as token management , load balancing , network topology discovery and construction , search , and peer to peer membership management while several such algorithms are ubiquitous , and use numerous random walk samples , the walks themselves have always been performed naively in this paper , we focus on the problem of performing random walk sampling efficiently in a distributed network given bandwidth constraints , the goal is to minimize the number of rounds and messages required to obtain several random walk samples in a continuous online fashion we present the first round and message optimal distributed algorithms that present a significant improvement on all previous approaches the theoretical analysis and comprehensive experimental evaluation of our algorithms show that they perform very well in different types of networks of differing topologies in particular , our results show how several random walks can be performed continuously \( when source nodes are provided only at runtime , i e , online \) , such that each walk of length ell can be performed exactly in just tilde o \( sqrt ell d \) rounds , \( where d is the diameter of the network \) , and o \( ell \) messages this significantly improves upon both , the naive technique that requires o \( ell \) rounds and o \( ell \) messages , and the sophisticated algorithm of dassarma et al podc 2010 that has the same round complexity as this paper but requires omega \( m sqrt ell \) messages \( where m is the number of edges in the network \) our theoretical results are corroborated through extensive experiments on various topological data sets our algorithms are fully decentralized , lightweight , and easily implementable , and can serve as building blocks in the design of topologically aware networks
in 2012 b 'ona showed the rather surprising fact that the cumulative number of occurrences of the classical patterns 231 and 213 are the same on the set of permutations avoiding 132 , beside the pattern based statistics 231 and 213 do not have the same distribution on this set here we show that if it is required for the symbols playing the role of 1 and 3 in the occurrences of 231 and 213 to be adjacent , then the obtained statistics are equidistributed on the set of 132 avoiding permutations actually , expressed in terms of vincular patterns , we prove the following more general results the statistics based on the patterns b ca , b ac and ba c , together with other statistics , have the same joint distribution on s n \( 132 \) , and so do the patterns bc a and c ab and up to trivial transformations , these statistics are the only based on length three proper \( not classical nor adjacent \) vincular patterns which are equidistributed on a set of permutations avoiding a classical length three pattern
this paper discusses the topic of the minimum width of a regular resolution refutation of a set of clauses the main result shows that there are examples having small regular resolution refutations , for which any regular refutation must contain a large clause this forms a contrast with corresponding results for general resolution refutations
general purpose intelligent learning agents cycle through \( complex , non mdp \) sequences of observations , actions , and rewards on the other hand , reinforcement learning is well developed for small finite state markov decision processes \( mdps \) so far it is an art performed by human designers to extract the right state representation out of the bare observations , i e to reduce the agent setup to the mdp framework before we can think of mechanizing this search for suitable mdps , we need a formal objective criterion the main contribution of this article is to develop such a criterion i also integrate the various parts into one learning algorithm extensions to more realistic dynamic bayesian networks are developed in a companion article
let p p \( i \) be a measure of strictly positive probabilities on the set of nonnegative integers although the countable number of inputs prevents usage of the huffman algorithm , there are nontrivial p for which known methods find a source code that is optimal in the sense of minimizing expected codeword length for some applications , however , a source code should instead minimize one of a family of nonlinear objective functions , beta exponential means , those of the form log a sum i p \( i \) a n \( i \) , where n \( i \) is the length of the i th codeword and a is a positive constant applications of such minimizations include a novel problem of maximizing the chance of message receipt in single shot communications \( a 1 \) and a previously known problem of minimizing the chance of buffer overflow in a queueing system \( a 1 \) this paper introduces methods for finding codes optimal for such exponential means one method applies to geometric distributions , while another applies to distributions with lighter tails the latter algorithm is applied to poisson distributions and both are extended to alphabetic codes , as well as to minimizing maximum pointwise redundancy the aforementioned application of minimizing the chance of buffer overflow is also considered
auc \( area under roc curve \) is an important evaluation criterion , which has been popularly used in many learning tasks such as class imbalance learning , cost sensitive learning , learning to rank , etc many learning approaches try to optimize auc , while owing to the non convexity and discontinuousness of auc , almost all approaches work with surrogate loss functions thus , the consistency of auc is crucial however , it has been almost untouched before in this paper , we provide a sufficient condition for the asymptotic consistency of learning approaches based on surrogate loss functions based on this result , we prove that exponential loss and logistic loss are consistent with auc , but hinge loss is inconsistent then , we derive the q norm hinge loss and general hinge loss that are consistent with auc we also derive the consistent bounds for exponential loss and logistic loss , and obtain the consistent bounds for many surrogate loss functions under the non noise setting further , we disclose an equivalence between the exponential surrogate loss of auc and exponential surrogate loss of accuracy , and one straightforward consequence of such finding is that adaboost and rankboost are equivalent
low complexity joint estimation of synchronization impairments and channel in a single user mimo ofdm system is presented in this letter based on a system model that takes into account the effects of synchronization impairments such as carrier frequency offset , sampling frequency offset , and symbol timing error , and channel , a maximum likelihood \( ml \) algorithm for the joint estimation is proposed to reduce the complexity of ml grid search , the number of received signal samples used for estimation need to be reduced the conventional channel estimation methods using least squares \( ls \) fail for the reduced sample under determined system , which results in poor performance of the joint estimator the proposed ml algorithm uses compressed sensing \( cs \) based channel estimation method in a sparse fading scenario , where the received samples used for estimation are less than that required for an ls based estimation the performance of the estimation method is studied through numerical simulations , and it is observed that cs based joint estimator performs better than ls based joint estimator
number decision diagrams \( ndd \) provide a natural finite symbolic representation for regular set of integer vectors encoded as strings of digit vectors \( least or most significant digit first \) the convex hull of the set of vectors represented by a ndd is proved to be an effectively computable convex polyhedron
description logic programs \( dl programs \) proposed by eiter et al constitute an elegant yet powerful formalism for the integration of answer set programming with description logics , for the semantic web in this paper , we generalize the notions of completion and loop formulas of logic programs to description logic programs and show that the answer sets of a dl program can be precisely captured by the models of its completion and loop formulas furthermore , we propose a new , alternative semantics for dl programs , called the em canonical answer set semantics , which is defined by the models of completion that satisfy what are called canonical loop formulas a desirable property of canonical answer sets is that they are free of circular justifications some properties of canonical answer sets are also explored
rules complement and extend ontologies on the semantic web we refer to these rules as onto relational since they combine dl based ontology languages and knowledge representation formalisms supporting the relational data model within the tradition of logic programming and deductive databases rule authoring is a very demanding knowledge engineering task which can be automated though partially by applying machine learning algorithms in this chapter we show how inductive logic programming \( ilp \) , born at the intersection of machine learning and logic programming and considered as a major approach to relational learning , can be adapted to onto relational learning for the sake of illustration , we provide details of a specific onto relational learning solution to the problem of learning rule based definitions of dl concepts and roles with ilp
we give an exact algorithm for the 0 1 integer linear programming problem with a linear number of constraints that improves over exhaustive search by an exponential factor specifically , our algorithm runs in time 2 \( 1 text poly \( 1 c \) \) n where n is the number of variables and cn is the number of constraints the key idea for the algorithm is a reduction to the vector domination problem and a new algorithm for that subproblem
parity games are combinatorial representations of closed boolean mu terms by adding to them draw positions , they have been organized by arnold and one of the authors into a mu calculus as done by berwanger et al for the propositional modal mu calculus , it is possible to classify parity games into levels of a hierarchy according to the number of fixed point variables we ask whether this hierarchy collapses w r t the standard interpretation of the games mu calculus into the class of all complete lattices we answer this question negatively by providing , for each n 1 , a parity game gn with these properties it unravels to a mu term built up with n fixed point variables , it is semantically equivalent to no game with strictly less than n 2 fixed point variables
improving the throughput of molecular docking , a computationally intensive phase of the virtual screening process , is a highly sought area of research since it has a significant weight in the drug designing process with such improvements , the world might find cures for incurable diseases like hiv disease and cancer sooner our approach presented in this paper is to utilize a multi core environment to introduce data level parallelism \( dlp \) to the autodock vina software , which is a widely used for molecular docking software autodock vina already exploits instruction level parallelism \( ilp \) in multi core environments and therefore optimized for such environments however , with the results we have obtained , it can be clearly seen that our approach has enhanced the throughput of the already optimized software by more than six times this will dramatically reduce the time consumed for the lead identification phase in drug designing along with the shift in the processor technology from multi core to many core of the current era therefore , we believe that the contribution of this project will effectively make it possible to expand the number of small molecules docked against a drug target and improving the chances to design drugs for incurable diseases
we consider transmission over a binary input additive white gaussian noise channel using low density parity check codes one of the most popular techniques for decoding low density parity check codes is the linear programming decoder in general , the linear programming decoder is suboptimal i e , the word error rate is higher than the optimal , maximum a posteriori decoder in this paper we present a systematic approach to enhance the linear program decoder more precisely , in the cases where the linear program outputs a fractional solution , we give a simple algorithm to identify frustrated cycles which cause the output of the linear program to be fractional then adding these cycles , adaptively to the basic linear program , we show improved word error rate performance
in this paper , we consider the online version of the machine minimization problem \( introduced by chuzhoy et al , focs 2004 \) , where the goal is to schedule a set of jobs with release times , deadlines , and processing lengths on a minimum number of identical machines since the online problem has strong lower bounds if all the job parameters are arbitrary , we focus on jobs with uniform length our main result is a complete resolution of the deterministic complexity of this problem by showing that a competitive ratio of e is achievable and optimal , thereby improving upon existing lower and upper bounds of 2 09 and 5 2 respectively we also give a constant competitive online algorithm for the case of uniform deadlines \( but arbitrary job lengths \) to the best of our knowledge , no such algorithm was known previously finally , we consider the complimentary problem of throughput maximization where the goal is to maximize the sum of weights of scheduled jobs on a fixed set of identical machines \( introduced by bar noy et al stoc 1999 \) we give a randomized online algorithm for this problem with a competitive ratio of e e 1 previous results achieved this bound only for the case of a single machine or in the limit of an infinite number of machines
for most problems in science and engineering we can obtain data that describe the system from various perspectives and record the behaviour of its individual components heterogeneous data sources can be collectively mined by data fusion fusion can focus on a specific target relation and exploit directly associated data together with data on the context or additional constraints in the paper we describe a data fusion approach with penalized matrix tri factorization that simultaneously factorizes data matrices to reveal hidden associations the approach can directly consider any data sets that can be expressed in a matrix , including those from attribute based representations , ontologies , associations and networks we demonstrate its utility on a gene function prediction problem in a case study with eleven different data sources our fusion algorithm compares favourably to state of the art multiple kernel learning and achieves higher accuracy than can be obtained from any single data source alone
we consider the problem of communicating over a multiple input multiple output \( mimo \) real valued channel for which no mathematical model is specified , and achievable rates are given as a function of the channel input and output sequences known a posteriori this paper extends previous results regarding individual channels by presenting a rate function for the mimo individual channel , and showing its achievability in a fixed transmission rate communication scenario
the class of algorithmically computable simple games \( i \) includes the class of games that have finite carriers and \( ii \) is included in the class of games that have finite winning coalitions this paper characterizes computable games , strengthens the earlier result that computable games violate anonymity , and gives examples showing that the above inclusions are strict it also extends nakamura 's theorem about the nonemptyness of the core and shows that computable games have a finite nakamura number , implying that the number of alternatives that the players can deal with rationally is restricted
the layered interference network is investigated with delayed channel state information \( csi \) at all nodes it is demonstrated how multi hopping can be utilized to increase the achievable degrees of freedom \( dof \) in particular , a multi phase transmission scheme is proposed for the k user 2k hop interference network in order to systematically exploit the layered structure of the network and delayed csi to achieve dof values that scale with k this result provides the first example of a network with distributed transmitters and delayed csi whose dof scales with the number of users
peptide sequencing from mass spectrometry data is a key step in proteome research especially de novo sequencing , the identification of a peptide from its spectrum alone , is still a challenge even for state of the art algorithmic approaches in this paper we present antilope , a new fast and flexible approach based on mathematical programming it builds on the spectrum graph model and works with a variety of scoring schemes antilope combines lagrangian relaxation for solving an integer linear programming formulation with an adaptation of yen 's k shortest paths algorithm it shows a significant improvement in running time compared to mixed integer optimization and performs at the same speed like other state of the art tools we also implemented a generic probabilistic scoring scheme that can be trained automatically for a dataset of annotated spectra and is independent of the mass spectrometer type evaluations on benchmark data show that antilope is competitive to the popular state of the art programs pepnovo and novohmm both in terms of run time and accuracy furthermore , it offers increased flexibility in the number of considered ion types antilope will be freely available as part of the open source proteomics library openms
a unified approach to energy efficient power control , applicable to a large family of receivers including the matched filter , the decorrelator , the \( linear \) minimum mean square error detector \( mmse \) , and the individually and jointly optimal multiuser detectors , has recently been proposed for code division multiple access \( cdma \) networks this unified power control \( upc \) algorithm exploits the linear relationship that has been shown to exist between the transmit power and the output signal to interference plus noise ratio \( sir \) in large systems based on this principle and by computing the multiuser efficiency , the upc algorithm updates the users' transmit powers in an iterative way to achieve the desired target sir in this paper , the convergence of the upc algorithm is proved for the matched filter , the decorrelator , and the mmse detector in addition , the performance of the algorithm in finite size systems is studied and compared with that of existing power control schemes the upc algorithm is particularly suitable for systems with randomly generated long spreading sequences \( i e , sequences whose period is longer than one symbol duration \)
