the alignment of a set of objects by means of transformations plays an important role in computer vision whilst the case for only two objects can be solved globally , when multiple objects are considered usually iterative methods are used in practice the iterative methods perform well if the relative transformations between any pair of objects are free of noise however , if only noisy relative transformations are available \( e g due to missing data or wrong correspondences \) the iterative methods may fail based on the observation that the underlying noise free transformations can be retrieved from the null space of a matrix that can directly be obtained from pairwise alignments , this paper presents a novel method for the synchronisation of pairwise transformations such that they are transitively consistent simulations demonstrate that for noisy transformations , a large proportion of missing data and even for wrong correspondence assignments the method delivers encouraging results
this paper studies a shannon theoretic version of the generalized distribution preserving quantization problem where a stationary and memoryless source is encoded subject to a distortion constraint and the additional requirement that the reproduction also be stationary and memoryless with a given distribution the encoder and decoder are stochastic and assumed to have access to independent common randomness recent work has characterized the minimum achievable coding rate at a given distortion level when unlimited common randomness is available here we consider the general case where the available common randomness may be rate limited our main result completely characterizes the set of achievable coding and common randomness rate pairs at any distortion level , thereby providing the optimal tradeoff between these two rate quantities we also consider two variations of this problem where we investigate the effect of relaxing the strict output distribution constraint and the role of `private randomness' used by the decoder on the rate region our results have strong connections with cuff 's recent work on distributed channel synthesis in particular , our achievability proof combines a coupling argument with the approach developed by cuff , where instead of explicitly constructing the encoder decoder pair , a joint distribution is constructed from which a desired encoder decoder pair is established we show however that for our problem , the separated solution of first finding an optimal channel and then synthesizing this channel results in a suboptimal rate region
we consider the crowdsourcing task of learning the answer to simple multiple choice microtasks in order to provide statistically significant results , one often needs to ask multiple workers to answer the same microtask a stopping rule is an algorithm that for a given microtask decides for any given set of worker answers if the system should stop and output an answer or iterate and ask one more worker a quality score for a worker is a score that reflects the historic performance of that worker in this paper we investigate how to devise better stopping rules given such quality scores we conduct a data analysis on a large scale industrial crowdsourcing platform , and use the observations from this analysis to design new stopping rules that use the workers' quality scores in a non trivial manner we then conduct a simulation based on a real world workload , showing that our algorithm performs better than the more naive approaches
since the early 2000s physicists have developed an ingenious but non rigorous formalism called the cavity method to put forward precise conjectures on phase transitions in random problems mezard , parisi , zecchina science 2002 the cavity method predicts that the satisfiability threshold in the random k sat problem is 2 k ln2 frac12 \( 1 ln 2 \) epsilon k , with lim k rightarrow infty epsilon k 0 mertens , mezard , zecchina random structures and algorithms 2006 this paper contains a proof of that conjecture
interference is a main limiting factor of the performance of a wireless ad hoc network the temporal and the spatial correlation of the interference makes the outages correlated temporally \( important for retransmissions \) and spatially correlated \( important for routing \) in this letter we quantify the temporal and spatial correlation of the interference in a wireless ad hoc network whose nodes are distributed as a poisson point process on the plane when aloha is used as the multiple access scheme
we propose a new method , it binary fused compressive sensing \( bfcs \) , to recover sparse piece wise smooth signals from 1 bit compressive measurements the proposed algorithm is a modification of the previous it binary iterative hard thresholding \( biht \) algorithm , where , in addition to the sparsity constraint , the total variation of the recovered signal is upper constrained as in biht , the data term of the objective function is an one sided ell 1 \( or ell 2 \) norm experiments on the recovery of sparse piece wise smooth signals show that the proposed algorithm is able to take advantage of the piece wise smoothness of the original signal , achieving more accurate recovery than biht
in this paper , we study a general additive state dependent gaussian interference channel \( asd gic \) where we consider two user interference channel with two independent states known non causally at both transmitters , but unknown to either of the receivers an special case , where the additive states over the two links are the same is studied in 1 , 2 , in which it is shown that the gap between the achievable symmetric rate and the upper bound is less than 1 4 bit for the strong interference case here , we also consider the case where each channel state has unbounded variance 3 , which is referred to as the strong interferences we first obtain an outer bound on the capacity region by utilizing lattice based coding schemes , we obtain four achievable rate regions depend on noise variance and channel power constraint , achievable rate regions can coincide with the channel capacity region for the symmetric model , the achievable sum rate reaches to within 0 661 bit of the channel capacity for signal to noise ratio \( snr \) greater than one
please see the content of this report
the paper presents a software tool for analysis and interactive engagement in various logical reasoning tasks a first feature of the program consists in providing an interface for working with logic specific repositories of formal knowledge a second feature provides the means to intuitively visualize and interactively generate the underlying logical structure that propels customary logical reasoning tasks starting from this we argue that both aspects have didactic potential and can be integrated in teaching activities to provide an engaging learning experience
the stochastic block model \( sbm \) is a popular tool for community detection in networks , but fitting it by maximum likelihood \( mle \) involves an infeasible optimization problem we propose a new semi definite programming \( sdp \) solution to the problem of fitting the sbm , derived as a relaxation of the mle our relaxation , which we call sdp 1 , is tighter than other recently proposed sdp relaxations , namely what we call sdp 2 and sdp 3 , and thus previously established theoretical guarantees carry over however , we show that sdp 1 is , in fact , strongly consistent \( i e , exactly recovers true communities \) over a wider class of sbms than what current results suggest in particular , one can relax the assumption of strong assortativity , implicit in consistency conditions of current sdps , to that of \( weak \) assortativity for sdp 1 , thus , significantly broadening the class of applicable models our approach in deriving strong consistency results is based on a primal dual witness construction , and as a by product we recover current results for sdp 2 our approach also suggests that strong assortativity is necessary for the success of sdp 2 and sdp 3 and is not an artifact of the current proofs we provide empirical evidence of this conjecture , in addition to other numerical results comparing these sdps , and adjacency based spectral clustering , on real and synthetic data another feature of our relaxation is the tendency to produce more balanced \( i e , equal sized \) communities which , as we show with a real data example , makes it the ideal tool for fitting network histograms , a concept gaining popularity in the graphon estimation literature a general theme throughout will be to view all these sdps within a unified framework , specifically , as relaxations of the mle over various sub classes of the sbm this also leads to a connection with the well known problem of sparse pca
we consider the problem of finding edges of a hidden weighted graph using a certain type of queries let g be a weighted graph with n vertices in the most general setting , the n vertices are known and no other information about g is given the problem is finding all edges of g and their weights using additive queries , where , for an additive query , one chooses a set of vertices and asks the sum of the weights of edges with both ends in the set this model has been extensively used in bioinformatics including genom sequencing extending recent results of bshouty and mazzawi , and choi and kim , we present a polynomial time randomized algorithm to find the hidden weighted graph g when the number of edges in g is known to be at most m geq 2 and the weight w \( e \) of each edge e satisfies ga leq w \( e \) leq gb for fixed constants ga , gb 0 the query complexity of the algorithm is o \( frac m log n log m \) , which is optimal up to a constant factor
petabyte scale distributed storage systems are currently transitioning to erasure codes to achieve higher storage efficiency classical codes like reed solomon are highly sub optimal for distributed environments due to their high overhead in single failure events locally repairable codes \( lrcs \) form a new family of codes that are repair efficient in particular , lrcs minimize the number of nodes participating in single node repairs during which they generate small network traffic two large scale distributed storage systems have already implemented different types of lrcs windows azure storage and the hadoop distributed file system raid used by facebook the fundamental bounds for lrcs , namely the best possible distance for a given code locality , were recently discovered , but few explicit constructions exist in this work , we present an explicit and optimal lrcs that are simple to construct our construction is based on grouping reed solomon \( rs \) coded symbols to obtain rs coded symbols over a larger finite field we then partition these rs symbols in small groups , and re encode them using a simple local code that offers low repair locality for the analysis of the optimality of the code , we derive a new result on the matroid represented by the code generator matrix
drawing large graphs appropriately is an important step for the visual analysis of data from real world networks here we present a novel multilevel algorithm to compute a graph layout with respect to a recently proposed metric that combines layout stress and entropy as opposed to previous work , we do not solve the linear systems of the maxent stress metric with a typical numerical solver instead we use a simple local iterative scheme within a multilevel approach to accelerate local optimization , we approximate long range forces and use shared memory parallelism our experiments validate the high potential of our approach , which is particularly appealing for dynamic graphs in comparison to the previously best maxent stress optimizer , which is sequential , our parallel implementation is on average 30 times faster already for static graphs \( and still faster if executed on one thread \) while producing a comparable solution quality
this paper proposes the use of ``pattern based'' context free grammars as a basis for building machine translation \( mt \) systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society we discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability \( incremental improvement in translation quality through user interaction \) , and describe how our approach meets these requirements
we extend the chow liu algorithm for general random variables while the previous versions only considered finite cases in particular , this paper applies the generalization to suzuki 's learning algorithm that generates from data forests rather than trees based on the minimum description length by balancing the fitness of the data to the forest and the simplicity of the forest as a result , we successfully obtain an algorithm when both of the gaussian and finite random variables are present
a true lie is a lie that becomes true when announced in a logic of announcements , where the announcing agent is not modelled , a true lie is a formula \( that is false and \) that becomes true when announced we investigate true lies and other types of interaction between announced formulas , their preconditions and their postconditions , in the setting gerbrandy 's logic of believed announcements , wherein agents may have or obtain incorrect beliefs our results are on the satisfiability and validity of instantiations of these semantically defined categories , on iterated announcements , including arbitrarily often iterated announcements , and on syntactic characterization we close with results for iterated announcements in the logic of knowledge \( instead of belief \) , and for lying as private announcements \( instead of public announcements \) to different agents detailed examples illustrate our lying concepts
the smart grid is not just about the digitalization of the power grid in its more visionary acceptation , it is a model of energy management in which the users are engaged in producing energy as well as consuming it , while having information systems fully aware of the energy demand response of the network and of dynamically varying prices a natural question is then to make the smart grid a reality will the distribution grid have to be updated \? we assume a positive answer to the question and we consider the lower layers of medium and low voltage to be the most affected by the change in our previous work , we have analyzed samples of the dutch distribution grid in our previous work and we have considered possible evolutions of these using synthetic topologies modeled after studies of complex systems in other technological domains in another previous work in this paper , we take an extra important further step by defining a methodology for evolving any existing physical power grid to a good smart grid model thus laying the foundations for a decision support system for utilities and governmental organizations in doing so , we consider several possible evolution strategies and apply then to the dutch distribution grid we show how more connectivity is beneficial in realizing more efficient and reliable networks our proposal is topological in nature , and enhanced with economic considerations of the costs of such evolutions in terms of cabling expenses and economic benefits of evolving the grid
the diversity multiplexing tradeoff of a general two hop asynchronous cooperative network is examined for various relaying protocols such as non orthogonal selection decode and forward \( nsdf \) , orthogonal selection decode and forward \( osdf \) , non orthogonal amplify and forward \( naf \) , and orthogonal amplify and forward \( oaf \) the transmitter nodes are assumed to send pulse amplitude modulation \( pam \) signals asynchronously , in which information symbols are linearly modulated by a shaping waveform to be sent to the destination we consider two different cases with respect to the length of the shaping waveforms in the time domain in the theoretical case where the shaping waveforms with infinite time support are used , it is shown that asynchronism does not affect the dmt performance of the system and the same dmt as that of the corresponding synchronous network is obtained for all the aforementioned protocols in the practical case where finite length shaping waveforms are used , it is shown that better diversity gains can be achieved at the expense of bandwidth expansion in the decode and forward \( df \) type protocols , the asynchronous network provides better diversity gains than those of the corresponding synchronous network throughout the range of the multiplexing gain in the amplify and forward \( af \) type protocols , the asynchronous network provides the same dmt as that of the corresponding synchronous counterpart under the oaf protocol however , a better diversity gain is achieved under the naf protocol throughout the range of the multiplexing gain in particular , in the single relay asynchronous network , the naf protocol provides the same dmt as that of the 2 times 1 multiple input single output \( miso \) channel
ultra network densification and massive mimo are considered major 5g enablers since they promise huge capacity gains by exploiting proximity , spectral and spatial reuse benefits both approaches rely on increasing the number of access elements per user , either through deploying more access nodes over an area or increasing the number of antenna elements per access node at the network level , optimal user association for a densely and randomly deployed network of massive mimo empowered access nodes must account for both channel and load conditions in this paper we formulate this complex problem , report its computationally intractability and reformulate it to a plausible form , amenable to acquire a global optimal solution with reasonable complexity we apply the proposed optimization model to typical ultra dense outdoor small cell setups and demonstrate \( i \) the significant impact of optimal user association to the achieved rate levels compared to a baseline strategy , and \( ii \) the optimality of alternative network access element deployment strategies
in this paper we investigate the descriptional complexity of knot theoretic problems and show upper bounds for planarity problem of signed and unsigned knot diagrams represented by gauss words since a topological equivalence of knots can involve knot diagrams with arbitrarily many crossings then gauss words will be considered as strings over an infinite \( unbounded \) alphabet for establishing the upper bounds on recognition of knot properties , we study these problems in a context of automata models over an infinite alphabet
traditionally paired with impulsive communications , time hopping cdma \( th cdma \) is a multiple access technique that separates users in time by coding their transmissions into pulses occupying a subset of n mathsf s chips out of the total n included in a symbol period , in contrast with traditional direct sequence cdma \( ds cdma \) where n mathsf s n this work analyzes th cdma with random spreading , by determining whether peculiar theoretical limits are identifiable , with both optimal and sub optimal receiver structures , in particular in the archetypal case of sparse spreading , that is , n mathsf s 1 results indicate that th cdma has a fundamentally different behavior than ds cdma , where the crucial role played by energy concentration , typical of time hopping , directly relates with its intrinsic uneven use of degrees of freedom
immanants are polynomial functions of n by n matrices attached to irreducible characters of the symmetric group s n , or equivalently to young diagrams of size n immanants include determinants and permanents as extreme cases valiant proved that computation of permanents is a complete problem in his algebraic model of np theory , i e , it is vnp complete we prove that computation of immanants is vnp complete if the immanants are attached to a family of diagrams whose separation is omega \( n delta \) for some delta 0 we define the separation of a diagram to be the largest number of overhanging boxes contained in a single row our theorem proves a conjecture of buergisser for a large variety of families , and in particular we recover with new proofs his vnp completeness results for hooks and rectangles
we propose a novel technique for joint estimation of angle and delay of radio wave arrival in a multipath mobile communication channel using knowledge of the transmitted pulse shape function employing an array of sensors to sample the radio received signal , and subsequent array signal processing can provide the characterization of a high rank channel in terms of the multipath angles of arrival and time delays although several works have been reported in the literature for estimation of the high rank channel parameters , we are not aware of any work that deals with the problem of estimation in a fading channel , which essentially leads to a multiplicative noise environment
additive codes over gf \( 9 \) that are self dual with respect to the hermitian trace inner product have a natural application in quantum information theory , where they correspond to ternary quantum error correcting codes however , these codes have so far received far less interest from coding theorists than self dual additive codes over gf \( 4 \) , which correspond to binary quantum codes self dual additive codes over gf \( 9 \) have been classified up to length 8 , and in this paper we extend the complete classification to codes of length 9 and 10 the classification is obtained by using a new algorithm that combines two graph representations of self dual additive codes the search space is first reduced by the fact that every code can be mapped to a weighted graph , and a different graph is then introduced that transforms the problem of code equivalence into a problem of graph isomorphism by an extension technique , we are able to classify all optimal codes of length 11 and 12 there are 56 , 005 , 876 \( 11 , 3 11 , 5 \) codes and 6493 \( 12 , 3 12 , 6 \) codes we also find the smallest codes with trivial automorphism group
in this work we study a modified version of the two dimensional sznajd sociophysics model in particular , we consider the effects of agents' reputations in the persuasion rules in other words , a high reputation group with a common opinion may convince their neighbors with probability p , which induces an increase of the group 's reputation on the other hand , there is always a probability q 1 p of the neighbors to keep their opinions , which induces a decrease of the group 's reputation these rules describe a competition between groups with high reputation and hesitant agents , which makes the full consensus states \( with all spins pointing in one direction \) more difficult to be reached as consequences , the usual phase transition does not occur for p p c sim 0 69 and the system presents realistic democracy like situations , where the majority of spins are aligned in a certain direction , for a wide range of parameters
over the last decade , there have been several data structures that , given a planar subdivision and a probability distribution over the plane , provide a way for answering point location queries that is fine tuned for the distribution all these methods suffer from the requirement that the query distribution must be known in advance we present a new data structure for point location queries in planar triangulations our structure is asymptotically as fast as the optimal structures , but it requires no prior information about the queries this is a 2d analogue of the jump from knuth 's optimum binary search trees \( discovered in 1971 \) to the splay trees of sleator and tarjan in 1985 while the former need to know the query distribution , the latter are statically optimal this means that we can adapt to the query sequence and achieve the same asymptotic performance as an optimum static structure , without needing any additional information
spectral properties and performance of multi pulse impulse radio ultra wideband systems with pulse based polarity randomization are analyzed instead of a single type of pulse transmitted in each frame , multiple types of pulses are considered , which is shown to reduce the effects of multiple access interference first , the spectral properties of a multi pulse impulse radio system is investigated it is shown that the power spectral density is the average of spectral contents of different pulse shapes then , approximate closed form expressions for bit error probability of a multi pulse impulse radio system are derived for rake receivers in asynchronous multiuser environments the theoretical and simulation results indicate that impulse radio systems that are more robust against multiple access interference than a classical impulse radio system can be designed with multiple types of ultra wideband pulses
in this paper , we study the resource allocation and user scheduling problem for a downlink nonorthogonal multiple access network where the base station allocates spectrum and power resources to a set of users we aim to jointly optimize the sub channel assignment and power allocation to maximize the weighted total sum rate while taking into account user fairness we formulate the sub channel allocation problem as equivalent to a many to many two sided user subchannel matching game in which the set of users and sub channels are considered as two sets of players pursuing their own interests we then propose a matching algorithm which converges to a two side exchange stable matching after a limited number of iterations a joint solution is thus provided to solve the sub channel assignment and power allocation problems iteratively simulation results show that the proposed algorithm greatly outperforms the orthogonal multiple access scheme and a previous non orthogonal multiple access scheme
full duplex \( fd \) transceivers may be expensive in terms of complexity , power consumption , and price to be implemented in all user terminals therefore , techniques to exploit in band full duplex communication with fd base stations \( bss \) and half duplex \( hd \) users' equipment \( ues \) are required in this context , 3 node topology \( 3nt \) has been recently proposed for fd bss to reuse the uplink \( ul \) and downlink \( dl \) channels with hd terminals within the same cell in this paper , we present a tractable mathematical framework , based on stochastic geometry , for 3nt in cellular networks to this end , we propose a design paradigm via pulse shaping and partial overlap between ul and dl channels to maximize the harvested rate gains in 3nt the results show that 3nt achieves a close performance to networks with fd bss and fd ues , denoted by 2 node topology \( 2nt \) networks a maximum of 5 rate loss is reported when 3nt is compared to 2nt with efficient self interference cancellation \( sic \) if the sic in 2nt is not efficient , 3nt highly outperforms 2nt consequently , we conclude that , irrespective to the ue duplexing scheme , it is sufficient to have fd bss to harvest fd rate gains
massive multiple input multiple output \( mimo \) relaying is a promising technological paradigm which avails of high spectral efficiency and substantially improved coverage yet , these configurations face some formidable challenges in terms of digital signal processing \( dsp \) power consumption and circuitry complexity , since the number of radio frequency \( rf \) chains may scale with the number of antennas at the relay station in this paper , we envision that performing a portion of the power intensive dsp in the analog domain , using simple phase shifters and with a reduced number of rf paths , can address these challenges in particular , we consider a multipair amplify and forward \( af \) relay system with maximum ratio combining transmission \( mrc mrt \) and we determine the asymptotic spectral efficiency for this hybrid analog digital architecture after that , we extend our analytical results to account for heavily quantized analog phase shifters and show that the performance loss with 2 quantization bits is only 10
recently , social phenomena have received a lot of attention not only from social scientists , but also from physicists , mathematicians and computer scientists , in the emerging interdisciplinary field of complex system science opinion dynamics is one of the processes studied , since opinions are the drivers of human behaviour , and play a crucial role in many global challenges that our complex world and societies are facing global financial crises , global pandemics , growth of cities , urbanisation and migration patterns , and last but not least important , climate change and environmental sustainability and protection opinion formation is a complex process affected by the interplay of different elements , including the individual predisposition , the influence of positive and negative peer interaction \( social networks playing a crucial role in this respect \) , the information each individual is exposed to , and many others several models inspired from those in use in physics have been developed to encompass many of these elements , and to allow for the identification of the mechanisms involved in the opinion formation process and the understanding of their role , with the practical aim of simulating opinion formation and spreading under various conditions these modelling schemes range from binary simple models such as the voter model , to multi dimensional continuous approaches here , we provide a review of recent methods , focusing on models employing both peer interaction and external information , and emphasising the role that less studied mechanisms , such as disagreement , has in driving the opinion dynamics
we study group testing algorithms for resolving broadcast conflicts on a multiple access channel \( mac \) and for identifying the dead sensors in a mobile ad hoc wireless network in group testing algorithms , we are asked to identify all the defective items in a set of items when we can test arbitrary subsets of items in the standard group testing problem , the result of a test is binary the tested subset either contains defective items or not in the more generalized versions we study in this paper , the result of each test is non binary for example , it may indicate whether the number of defective items contained in the tested subset is zero , one , or at least two we give adaptive algorithms that are provably more efficient than previous group testing algorithms we also show how our algorithms can be applied to solve conflict resolution on a mac and dead sensor diagnosis dead sensor diagnosis poses an interesting challenge compared to mac resolution , because dead sensors are not locally detectable , nor are they themselves active participants
we develop the multilingual topic model for unaligned text \( muto \) , a probabilistic model of text that is designed to analyze corpora composed of documents in two languages from these documents , muto uses stochastic em to simultaneously discover both a matching between the languages and multilingual latent topics we demonstrate that muto is able to find shared topics on real world multilingual corpora , successfully pairing related documents across languages muto provides a new framework for creating multilingual topic models without needing carefully curated parallel corpora and allows applications built using the topic model formalism to be applied to a much wider class of corpora
we examine a version of the cops and robber \( cr \) game in which the robber is invisible , i e , the cops do not know his location until they capture him apparently this game \( cir \) has received little attention in the cr literature we examine two variants in the first the robber is adversarial \( he actively tries to avoid capture \) in the second he is drunk \( he performs a random walk \) our goal in this paper is to study the invisible cost of drunkenness \( icod \) , which is defined as the ratio ct i \( g \) dct i \( g \) , with ct i \( g \) and dct i \( g \) being the expected capture times in the adversarial and drunk cir variants , respectively we show that these capture times are well defined , using game theory for the adversarial case and partially observable markov decision processes \( pomdp \) for the drunk case we give exact asymptotic values of icod for several special graph families such as d regular trees , give some bounds for grids , and provide general upper and lower bounds for general classes of graphs we also give an infinite family of graphs showing that icod can be arbitrarily close to any value in 2 , infinty \) finally , we briefly examine one more cir variant , in which the robber is invisible and infinitely fast we argue that this variant is significantly different from the graph search game , despite several similarities between the two games
in this work , we study spectrum auction problem where each request from secondary users has spatial , temporal , and spectral features with the requests of secondary users and the reserve price of the primary user , our goal is to design truthful mechanisms that will either maximize the social efficiency or maximize the revenue of the primary user as the optimal conflict free spectrum allocation problem is np hard , in this work , we design near optimal spectrum allocation mechanisms separately based on the following techniques derandomized allocation from integer programming formulation , its linear programming \( lp \) relaxation , and the dual of the lp we theoretically prove that 1 \) our near optimal allocation methods are bid monotone , which implys truthful auction mechanisms and 2 \) our near optimal allocation methods can achieve a social efficiency or a revenue that is at least 1 frac 1 e times of the optimal respectively at last , we conduct extensive simulations to study the performances \( social efficiency , revenue \) of the proposed methods , and the simulation results corroborate our theoretical analysis
in distributed database \( ddb \) management systems , fragment allocation is one of the most important components that can directly affect the performance of ddb in this research work , we will show that declarative programming languages , e g logic programming languages , can be used to represent different data fragment allocation techniques results indicate that , using declarative programming language significantly simplifies the representation of fragment allocation algorithm , thus opens door for any further developments and optimizations the under consideration case study also show that our approach can be extended to be used in different areas of distributed systems
in this paper , we present a generic framework for constructing systematic minimum storage regenerating codes with two parity nodes based on the invariant subspace technique codes constructed in our framework not only contain some best known codes as special cases , but also include some new codes with key properties such as the optimal access property and the optimal update property in particular , for a given storage capacity of an individual node , one of the new codes has the largest number of systematic nodes and two of the new codes have the largest number of systematic nodes with the optimal update property
the entangled graph states have emerged as an elegant and powerful quantum resource , indeed almost all multiparty protocols can be written in terms of graph states including measurement based quantum computation \( mbqc \) , error correction and secret sharing amongst others in addition they are at the forefront in terms of implementations as such they represent an excellent opportunity to move towards integrated protocols involving many of these elements in this paper we look at expressing and extending graph state secret sharing and mbqc in a common framework and graphical language related to flow we do so with two main contributions first we express in entirely graphical terms which set of players can access which information in graph state secret sharing protocols these succinct graphical descriptions of access allow us to take known results from graph theory to make statements on the generalisation of the previous schemes to present new secret sharing protocols second , we give a set of necessary conditions as to when a graph with flow , i e capable of performing a class of unitary operations , can be extended to include vertices which can be ignored , pointless measurements , and hence considered as unauthorised players in terms of secret sharing , or error qubits in terms of fault tolerance this offers a way to extend existing mbqc patterns to secret sharing protocols our characterisation of pointless measurements is believed also to be a useful tool for further integrated measurement based schemes , for example in constructing fault tolerant mbqc schemes
we consider the g odel bi modal logic determined by fuzzy kripke models where both the propositions and the accessibility relation are infinitely valued over the standard g odel algebra 0 , 1 and prove strong completeness of fischer servi intuitionistic modal logic ik plus the prelinearity axiom with respect to this semantics we axiomatize also the bi modal analogues of t , s4 , and s5 obtained by restricting to models over frames satisfying the 0 , 1 valued versions of the structural properties which characterize these logics as application of the completeness theorems we obtain a representation theorem for bi modal g odel algebras
one of the most powerful techniques to study protein structures is to look for recurrent fragments \( also called substructures or spatial motifs \) , then use them as patterns to characterize the proteins under study an emergent trend consists in parsing proteins three dimensional \( 3d \) structures into graphs of amino acids hence , the search of recurrent spatial motifs is formulated as a process of frequent subgraph discovery where each subgraph represents a spatial motif in this scope , several efficient approaches for frequent subgraph discovery have been proposed in the literature however , the set of discovered frequent subgraphs is too large to be efficiently analyzed and explored in any further process in this paper , we propose a novel pattern selection approach that shrinks the large number of discovered frequent subgraphs by selecting the representative ones existing pattern selection approaches do not exploit the domain knowledge yet , in our approach we incorporate the evolutionary information of amino acids defined in the substitution matrices in order to select the representative subgraphs we show the effectiveness of our approach on a number of real datasets the results issued from our experiments show that our approach is able to considerably decrease the number of motifs while enhancing their interestingness
the increasingly sophisticated sensors supported by modern smartphones open up novel research opportunities , such as mobile phone sensing one of the most challenging of these research areas is context aware and activity recognition the smartrescue project takes advantage of smartphone sensing , processing and communication capabilities to monitor hazards and track people in a disaster the goal is to help crisis managers and members of the public in early hazard detection , prediction , and in devising risk minimizing evacuation plans when disaster strikes in this paper we suggest a novel smartphone based communication framework it uses specific machine learning techniques that intelligently process sensor readings into useful information for the crisis responders core to the framework is a content based publish subscribe mechanism that allows flexible sharing of sensor data and computation results we also evaluate a preliminary implementation of the platform , involving a smartphone app that reads and shares mobile phone sensor data for activity recognition
extremes of information combining inequalities play an important role in the analysis of sparse graph codes under message passing decoding we introduce new tools for the derivation of such inequalities , and show by means of a concrete examples how they can be applied to solve some optimization problems in the analysis of low density parity check codes
dense sub graphs of sparse graphs \( communities \) , which appear in most real world complex networks , play an important role in many contexts most existing community detection algorithms produce a hierarchical structure of community and seek a partition into communities that optimizes a given quality function we propose new methods to improve the results of any of these algorithms first we show how to optimize a general class of additive quality functions \( containing the modularity , the performance , and a new similarity based quality function we propose \) over a larger set of partitions than the classical methods moreover , we define new multi scale quality functions which make it possible to detect the different scales at which meaningful community structures appear , while classical approaches find only one partition
this article presents a new search algorithm for the np hard problem of optimizing functions of binary variables that decompose according to a graphical model it can be applied to models of any order and structure the main novelty is a technique to constrain the search space based on the topology of the model when pursued to the full search depth , the algorithm is guaranteed to converge to a global optimum , passing through a series of monotonously improving local optima that are guaranteed to be optimal within a given and increasing hamming distance for a search depth of 1 , it specializes to iterated conditional modes between these extremes , a useful tradeoff between approximation quality and runtime is established experiments on models derived from both illustrative and real problems show that approximations found with limited search depth match or improve those obtained by state of the art methods based on message passing and linear programming
the split bregman \( sb \) method t goldstein and s osher , siam j imaging sci , 2 \( 2009 \) , pp 323 43 is a fast splitting based algorithm that solves image reconstruction problems with general l1 , e g , total variation \( tv \) and compressed sensing \( cs \) , regularizations by introducing a single variable split to decouple the data fitting term and the regularization term , yielding simple subproblems that are separable \( or partially separable \) and easy to minimize several convergence proofs have been proposed , and these proofs either impose a full column rank assumption to the split or assume exact updates in all subproblems however , these assumptions are impractical in many applications such as the x ray computed tomography \( ct \) image reconstructions , where the inner least squares problem usually cannot be solved efficiently due to the highly shift variant hessian in this paper , we show that when the data fitting term is quadratic , the sb method is a convergent alternating direction method of multipliers \( admm \) , and a straightforward convergence proof with inexact updates is given using j eckstein and d p bertsekas , mathematical programming , 55 \( 1992 \) , pp 293 318 , theorem 8 furthermore , since the sb method is just a special case of an admm algorithm , it seems likely that the admm algorithm will be faster than the sb method if the augmented largangian \( al \) penalty parameters are selected appropriately to have a concrete example , we conduct a convergence rate analysis of the admm algorithm using two splits for image restoration problems with quadratic data fitting term and regularization term according to our analysis , we can show that the two split admm algorithm can be faster than the sb method if the al penalty parameter of the sb method is suboptimal numerical experiments were conducted to verify our analysis
the goal of consensus clustering is to improve the quality of clustering by combining a sample of partitions of a dataset to a single consensus partition this contribution extends condorcet 's jury theorem to the mean partition approach of consensus clustering as a consequence of the proposed result , we challenge and reappraise the role of diversity in consensus clustering
we present an approach to solve the nonconvex optimization problem that arises when designing the transmit covariance matrices in multiuser multiple input multiple output \( mimo \) broadcast networks implementing simultaneous wireless information and power transfer \( swipt \) the mimo swipt problem is formulated as a general multi objective optimization problem , in which data rates and harvested powers are optimized simultaneously two different approaches are applied to reformulate the \( nonconvex \) multi objective problem in the first approach , the transmitter can control the specific amount of power to be harvested by power transfer whereas in the second approach the transmitter can only control the proportion of power to be harvested among the different harvesting users the computational complexity will also be different , with higher computational resources required in the first approach in order to solve the resulting formulations , we propose to use the majorization minimization \( mm \) approach the idea behind this approach is to obtain a convex function that approximates the nonconvex objective and , then , solve a series of convex subproblems that will converge to a locally optimal solution of the general nonconvex multi objective problem the solution obtained from the mm approach is compared to the classical block diagonalization \( bd \) strategy , typically used to solve the nonconvex multiuser mimo network by forcing no interference among users simulation results show that the proposed approach improves over the bd approach both the system sum rate and the power harvested by users additionally , the computational times needed for convergence of the proposed methods are much lower than the ones required for classical gradient based approaches
generating descriptions for videos has many applications including assisting blind people and human robot interaction the recent advances in image captioning as well as the release of large scale movie description datasets such as mpii movie description allow to study this task in more depth many of the proposed methods for image captioning rely on pre trained object classifier cnns and long short term memory recurrent networks \( lstms \) for generating descriptions while image description focuses on objects , we argue that it is important to distinguish verbs , objects , and places in the challenging setting of movie description in this work we show how to learn robust visual classifiers from the weak annotations of the sentence descriptions based on these visual classifiers we learn how to generate a description using an lstm we explore different design choices to build and train the lstm and achieve the best performance to date on the challenging mpii md dataset we compare and analyze our approach and prior work along various dimensions to better understand the key challenges of the movie description task
recently in online social networks \( osns \) , the least cost influence \( lci \) problem has become one of the central research topics it aims at identifying a minimum number of seed users who can trigger a wide cascade of information propagation most of existing literature investigated the lci problem only based on an individual network however , nowadays users often join several osns such that information could be spread across different networks simultaneously therefore , in order to obtain the best set of seed users , it is crucial to consider the role of overlapping users under this circumstances in this article , we propose a unified framework to represent and analyze the influence diffusion in multiplex networks more specifically , we tackle the lci problem by mapping a set of networks into a single one via lossless and lossy coupling schemes the lossless coupling scheme preserves all properties of original networks to achieve high quality solutions , while the lossy coupling scheme offers an attractive alternative when the running time and memory consumption are of primary concern various experiments conducted on both real and synthesized datasets have validated the effectiveness of the coupling schemes , which also provide some interesting insights into the process of influence propagation in multiplex networks
we introduce a distributed opportunistic scheduling \( dos \) strategy , based on two pre determined thresholds , for uplink k cell networks with time invariant channel coefficients each base station \( bs \) opportunistically selects a mobile station \( ms \) who has a large signal strength of the desired channel link among a set of mss generating a sufficiently small interference to other bss then , performance on the achievable throughput scaling law is analyzed as our main result , it is shown that the achievable sum rate scales as k log \( text snr log n \) in a high signal to noise ratio \( snr \) regime , if the total number of users in a cell , n , scales faster than text snr frac k 1 1 epsilon for a constant epsilon in \( 0 , 1 \) this result indicates that the proposed scheme achieves the multiuser diversity gain as well as the degrees of freedom gain even under multi cell environments simulation results show that the dos provides a better sum rate throughput over conventional schemes
we present a new protocol and two lower bounds for quantum coin flipping in our protocol , no dishonest party can achieve one outcome with probability more than 0 75 then , we show that our protocol is optimal for a certain type of quantum protocols for arbitrary quantum protocols , we show that if a protocol achieves a bias of at most epsilon , it must use at least omega \( log log frac 1 epsilon \) rounds of communication this implies that the parallel repetition fails for quantum coin flipping \( the bias of a protocol cannot be arbitrarily decreased by running several copies of it in parallel \)
we investigate hunters rabbit game , where a set of hunters tries to catch an invisible rabbit that slides along the edges of a graph we show that the minimum number of hunters required to win on an \( n times m \) grid is lfloor min n , m 2 rfloor 1 we also show that the extremal value of this number on n vertex trees this number is between omega \( log n log log n \) and o \( log n \)
several anonymization techniques , such as generalization and bucketization , have been designed for privacy preserving microdata publishing recent work has shown that generalization loses considerable amount of information , especially for high dimensional data bucketization , on the other hand , does not prevent membership disclosure and does not apply for data that do not have a clear separation between quasi identifying attributes and sensitive attributes in this paper , we present a novel technique called slicing , which partitions the data both horizontally and vertically we show that slicing preserves better data utility than generalization and can be used for membership disclosure protection another important advantage of slicing is that it can handle high dimensional data we show how slicing can be used for attribute disclosure protection and develop an efficient algorithm for computing the sliced data that obey the l diversity requirement our workload experiments confirm that slicing preserves better utility than generalization and is more effective than bucketization in workloads involving the sensitive attribute our experiments also demonstrate that slicing can be used to prevent membership disclosure
this article evaluates the performance of two techniques for query reformulation in a system for information retrieval , namely , the concept based and the pseudo relevance feedback reformulation the experiments performed on a corpus of arabic text have allowed us to compare the contribution of these two reformulation techniques in improving the performance of an information retrieval system for arabic texts
as social networking takes to the mobile world , smartphone apps provide users with ever changing ways to interact with each other over the past few years , an increasing number of apps have entered the market offering end to end encryption , self destructing messages , and or some degree of anonymity however , little work has carefully examined the features they offer to this end , this paper presents a taxonomy of 18 of such apps we first look at the features they promise in their appeal to broaden their reach and focus on 8 of the more popular ones we present a technical evaluation , based on static and dynamic analysis , and identify a number of gaps between the perception and reality of their promises
various hand crafted features and metric learning methods prevail in the field of person re identification compared to these methods , this paper proposes a more general way that can learn a similarity metric from image pixels directly by using a siamese deep neural network , the proposed method can jointly learn the color feature , texture feature and metric in a unified framework the network has a symmetry structure with two sub networks which are connected by cosine function to deal with the big variations of person images , binomial deviance is used to evaluate the cost between similarities and labels , which is proved to be robust to outliers compared to existing researches , a more practical setting is studied in the experiments that is training and test on different datasets \( cross dataset person re identification \) both in intra dataset and cross dataset settings , the superiorities of the proposed method are illustrated on viper and prid
reactive synthesis deals with the automated construction of implementations of reactive systems from their specifications to make the approach feasible in practice , systems engineers need effective and efficient means of debugging these specifications in this paper , we provide techniques for report based specification debugging , wherein salient properties of a specification are analyzed , and the result presented to the user in the form of a report this provides a low effort way to debug specifications , complementing high effort techniques including the simulation of synthesized implementations we demonstrate the usefulness of our report based specification debugging toolkit by providing examples in the context of generalized reactivity \( 1 \) synthesis
formal languages let us define the textual representation of data with precision formal grammars , typically in the form of bnf like productions , describe the language syntax , which is then annotated for syntax directed translation and completed with semantic actions when , apart from the textual representation of data , an explicit representation of the corresponding data structure is required , the language designer has to devise the mapping between the suitable data model and its proper language specification , and then develop the conversion procedure from the parse tree to the data model instance unfortunately , whenever the format of the textual representation has to be modified , changes have to propagated throughout the entire language processor tool chain these updates are time consuming , tedious , and error prone besides , in case different applications use the same language , several copies of the same language specification have to be maintained in this paper , we introduce a model based parser generator that decouples language specification from language processing , hence avoiding many of the problems caused by grammar driven parsers and parser generators
the region of rates \( straight line \) , where the bsc reliability function is known exactly , is expanded
an information theoretic lower bound is developed for the caching system studied by maddah ali and niesen by comparing the proposed lower bound with the decentralized coded caching scheme of maddah ali and niesen , the optimal memory rate tradeoff is characterized to within a multiplicative gap of 4 7 for the worst case , improving the previous analytical gap of 12 furthermore , for the case when users' requests follow the uniform distribution , the multiplicative gap is tightened to 4 7 , improving the previous analytical gap of 72 as an independent result of interest , for the single user average case in which the user requests multiple files , it is proved that caching the most requested files is optimal
although several rdf knowledge bases are available through the lod initiative , often these data sources remain isolated , lacking schemata and links to other datasets while there are numerous works that focus on establishing that two resources are identical and on adding more instances of an already existing relation , the problem of finding new relations between any two given datasets has not been investigated in detail in this paper , given two entity sets , we present an unsupervised approach to enrich the lod cloud with new relations between them by exploiting the web corpus during the first phase we gather prospective relations from the corpus through pattern extraction and paraphrase detection in the second phase , we perform actual enrichment by extracting instances of these relations we have empirically evaluated our approach on several dataset pairs and found that the system can indeed be used for enriching the existing datasets with new relations
we prove that the determinacy of gale stewart games whose winning sets are accepted by real time 1 counter b uchi automata is equivalent to the determinacy of \( effective \) analytic gale stewart games which is known to be a large cardinal assumption we show also that the determinacy of wadge games between two players in charge of omega languages accepted by 1 counter b uchi automata is equivalent to the \( effective \) analytic wadge determinacy using some results of set theory we prove that one can effectively construct a 1 counter b uchi automaton a and a b uchi automaton b such that \( 1 \) there exists a model of zfc in which player 2 has a winning strategy in the wadge game w \( l \( a \) , l \( b \) \) \( 2 \) there exists a model of zfc in which the wadge game w \( l \( a \) , l \( b \) \) is not determined moreover these are the only two possibilities , i e there are no models of zfc in which player 1 has a winning strategy in the wadge game w \( l \( a \) , l \( b \) \)
prime numbers play a very vital role in modern cryptography and especially the difficulties involved in factoring numbers composed of product of two large prime numbers have been put to use in many modern cryptographic designs thus , the problem of distinguishing prime numbers from the rest is vital and therefore there is a need to have efficient primality testing algorithms although there had been many probabilistic algorithms for primality testing , there was n't a deterministic polynomial time algorithm until 2002 when agrawal , kayal and saxena came with an algorithm , popularly known as the aks algorithm , which could test whether a given number is prime or composite in polynomial time this project is an attempt at understanding the ingenious idea behind this algorithm and the underlying principles of mathematics that is required to study it in fact , through out this project , one of the major objectives has been to make it as much self contained as possible finally , the project provides an implementation of the algorithm using software for algebra and geometry experimentation \( sage \) and arrives at conclusions on how practical or otherwise it is
neurons are modeled electrically based on ferroelectric membranes thin enough to permit charge transfer , conjectured to be the tunneling result of thermally energetic ions and random electrons these membranes can be triggered to produce electrical solitons , the main signals for brain associative memory and logical processing dendritic circuits are modeled , and electrical solitons are simulated to demonstrate the nature of soliton propagation , soliton reflection , the collision of solitons , as well as soliton or gates , and gates , xor gates and not gates
we determine the rate region of the quadratic gaussian two encoder source coding problem this rate region is achieved by a simple architecture that separates the analog and digital aspects of the compression furthermore , this architecture requires higher rates to send a gaussian source than it does to send any other source with the same covariance our techniques can also be used to determine the sum rate of some generalizations of this classical problem our approach involves coupling the problem to a quadratic gaussian ``ceo problem ''
this paper considers the chattering problem of sliding mode control while delay in robot manipulator caused chaos in such electromechanical systems fractional calculus as a powerful theorem to produce a novel sliding mode which has a dynamic essence is used for chattering elimination to realize the control of a class of chaotic systems in master slave configuration this novel fractional dynamic sliding mode control scheme is presented and examined on delay based chaotic robot in joint and work space also the stability of the closed loop system is guaranteed by lyapunov stability theory beside these , delayed robot motions are sorted out for qualitative and quantification study finally , numerical simulation example illustrates the feasibility of proposed control method
in this paper we study a model based approach to calculating approximately optimal policies in markovian decision processes in particular , we derive novel bounds on the loss of using a policy derived from a factored linear model , a class of models which generalize virtually all previous models that come with strong computational guarantees for the first time in the literature , we derive performance bounds for model based techniques where the model inaccuracy is measured in weighted norms moreover , our bounds show a decreased sensitivity to the discount factor and , unlike similar bounds derived for other approaches , they are insensitive to measure mismatch similarly to previous works , our proofs are also based on contraction arguments , but with the main differences that we use carefully constructed norms building on banach lattices , and the contraction property is only assumed for operators acting on compressed spaces , thus weakening previous assumptions , while strengthening previous results
in this paper we provide algorithms faster than o \( 2 n \) for several np complete domination type problems more precisely , we provide an algorithm for capacitated dominating set that solves it in o \( 1 89 n \) , a branch and reduce algorithm solving largest irredundant set in o \( 1 9657 n \) time and a simple iterative dfs algorithm for smallest inclusion maximal irredundant set that solves it in o \( 1 999956 n \) time we also provide an exponential approximation scheme for capacitated dominating set all algorithms require polynomial space despite the fact that the discussed problems are quite similar to the dominating set problem , we are not aware of any published algorithms solving these problems faster than the obvious o \( 2 n \) solution prior to this paper
this paper describes a novel numerical model aiming at solving moving boundary problems such as free surface flows or fluid structure interaction this model uses a moving grid technique to solve the navier stokes equations expressed in the arbitrary lagrangian eulerian kinematics the discretization in space is based on the spectral element method the coupling of the fluid equations and the moving grid equations is essentially done through the conditions on the moving boundaries two and three dimensional simulations are presented translation and rotation of a cylinder in a fluid , and large amplitude sloshing in a rectangular tank the accuracy and robustness of the present numerical model is studied and discussed
we formulate and prove an axiomatic characterization of conditional information geometry , for both the normalized and the nonnormalized cases this characterization extends the axiomatic derivation of the fisher geometry by cencov and campbell to the cone of positive conditional models , and as a special case to the manifold of conditional distributions due to the close connection between the conditional i divergence and the product fisher information metric the characterization provides a new axiomatic interpretation of the primal problems underlying logistic regression and adaboost
we completely characterize possible indices of quasi cyclic subcodes in a cyclic code for a very broad class of cyclic codes we present enumeration results for quasi cyclic subcodes of a fixed index and show that the problem of enumeration is equivalent to enumeration of certain vector subspaces in finite fields in particular , we present enumeration results for quasi cyclic subcodes of the simplex code and duals of certain bch codes our results are based on the trace representation of cyclic codes
understanding complex user behaviour under various conditions , scenarios and journeys can be fundamental to the improvement of the user experience for a given system predictive models of user reactions , responses and in particular , emotions can aid in the design of more intuitive and usable systems building on this theme , the preliminary research presented in this paper correlates events and interactions in an online social network against user behaviour , focusing on personality traits emotional context and tone is analysed and modelled based on varying types of sentiments that users express in their language using the ibm watson developer cloud tools the data collected in this study thus provides further evidence towards supporting the hypothesis that analysing and modelling emotions , sentiments and personality traits provides valuable insight into improving the user experience of complex social computer systems
a method to polarize channels universally is introduced the method is based on combining two distinct channels in each polarization step , as opposed to arikan 's original method of combining identical channels this creates an equal number of only two types of channels , one of which becomes progressively better as the other becomes worse the locations of the good polarized channels are independent of the underlying channel , guaranteeing universality polarizing the good channels further with arikan 's method results in universal polar codes of rate 1 2 the method is generalized to construct codes of arbitrary rates it is also shown that the less noisy ordering of channels is preserved under polarization , and thus a good polar code for a given channel will perform well over a less noisy one
one of the main obstacles to the wider use of the modern error correction codes is that , due to the complex behavior of their decoding algorithms , no systematic method which would allow characterization of the bit error rate \( ber \) is known this is especially true at the weak noise where many systems operate and where coding performance is difficult to estimate because of the diminishingly small number of errors we show how the instanton method of physics allows one to solve the problem of ber analysis in the weak noise range by recasting it as a computationally tractable minimization problem
we consider a broadcast scenario where one transmitter communicates with two receivers under quality of service constraints the transmitter initially employs superposition coding strategies with arbitrarily distributed signals and sends data to both receivers regarding the channel state conditions , the receivers perform successive interference cancellation to decode their own data we express the effective capacity region that provides the maximum allowable sustainable data arrival rate region at the transmitter buffer or buffers given an average transmission power limit , we provide a two step approach to obtain the optimal power allocation policies that maximize the effective capacity region then , we characterize the optimal decoding regions at the receivers in the space spanned by the channel fading power values we finally substantiate our results with numerical presentations
we study so called invariant games played with a fixed number d of heaps of matches a game is described by a finite list mathcal m of integer vectors of length d specifying the legal moves a move consists in changing the current game state by adding one of the vectors in mathcal m , provided all elements of the resulting vector are nonnegative for instance , in a two heap game , the vector \( 1 , 2 \) would mean adding one match to the first heap and removing two matches from the second heap if \( 1 , 2 \) in mathcal m , such a move would be permitted provided there are at least two matches in the second heap two players take turns , and a player unable to make a move loses we show that these games embrace computational universality , and that therefore a number of basic questions about them are algorithmically undecidable in particular , we prove that there is no algorithm that takes two games mathcal m and mathcal m ' \( with the same number of heaps \) as input , and determines whether or not they are equivalent in the sense that every starting position which is a first player win in one of the games is a first player win in the other
we discuss inequalities holding between the vocabulary size , i e , the number of distinct nonterminal symbols in a grammar based compression for a string , and the excess length of the respective universal code , i e , the code based analog of algorithmic mutual information the aim is to strengthen inequalities which were discussed in a weaker form in linguistics but shed some light on redundancy of efficiently computable codes the main contribution of the paper is a construction of universal grammar based codes for which the excess lengths can be bounded easily
the mutual information of two random variables i and j with joint probabilities t ij is commonly used in learning bayesian nets as well as in many other fields the chances t ij are usually estimated by the empirical sampling frequency n ij n leading to a point estimate i \( n ij n \) for the mutual information to answer questions like is i \( n ij n \) consistent with zero \? or what is the probability that the true mutual information is much larger than the point estimate \? one has to go beyond the point estimate in the bayesian framework one can answer these questions by utilizing a \( second order \) prior distribution p \( t \) comprising prior information about t from the prior p \( t \) one can compute the posterior p \( t n \) , from which the distribution p \( i n \) of the mutual information can be calculated we derive reliable and quickly computable approximations for p \( i n \) we concentrate on the mean , variance , skewness , and kurtosis , and non informative priors for the mean we also give an exact expression numerical issues and the range of validity are discussed
finding the infection sources in a network when we only know the network topology and infected nodes , but not the rates of infection , is a challenging combinatorial problem , and it is even more difficult in practice where the underlying infection spreading model is usually unknown a priori in this paper , we are interested in finding a source estimator that is applicable to various spreading models , including the susceptible infected \( si \) , susceptible infected recovered \( sir \) , susceptible infected recovered infected \( siri \) , and susceptible infected susceptible \( sis \) models we show that under the si , sir and siri spreading models and with mild technical assumptions , the jordan center is the infection source associated with the most likely infection path in a tree network with a single infection source this conclusion applies for a wide range of spreading parameters , while it holds for regular trees under the sis model with homogeneous infection and recovery rates since the jordan center does not depend on the infection , recovery and reinfection rates , it can be regarded as a universal source estimator we also consider the case where there are k 1 infection sources , generalize the jordan center definition to a k jordan center set , and show that this is an optimal infection source set estimator in a tree network for the si model simulation results on various general synthetic networks and real world networks suggest that jordan center based estimators consistently outperform the distance , closeness , and betweenness centrality based heuristics , even if the network is not a tree
given a controllable discrete time linear system c , a shortest basis for c is a set of linearly independent generators for c with the least possible lengths a basis b is a shortest basis if and only if it has the predictable span property \( i e , has the predictable delay and degree properties , and is non catastrophic \) , or alternatively if and only if it has the subsystem basis property \( for any interval j , the generators in b whose span is in j is a basis for the subsystem c j \) the dimensions of the minimal state spaces and minimal transition spaces of c are simply the numbers of generators in a shortest basis b that are active at any given state or symbol time , respectively a minimal linear realization for c in controller canonical form follows directly from a shortest basis for c , and a minimal linear realization for c in observer canonical form follows directly from a shortest basis for the orthogonal system c perp this approach seems conceptually simpler than that of classical minimal realization theory
rate based transport protocol determines the rate of data transmission between the sender and receiver and then sends the data according to that rate to notify the rate to the sender , the receiver sends ackplusrate packet based on epoch timer expiry in this paper , through detailed arguments and simulation it is shown that the transmission of ackplusrate packet based on epoch timer expiry consumes more energy in network with low mobility to overcome this problem , a new technique called dynamic rate feedback \( drf \) is proposed drf sends ackplusrate whenever there is a change in rate of \( plus or minus \) 25 percent than the previous rate based on ns2 simulation drf is compared with a reliable transport protocol for ad hoc network \( atp \)
we revisit a one step control problem over an adversarial packet dropping link the link is modeled as a set of binary channels controlled by a strategic jammer whose intention is to wage a `denial of service' attack on the plant by choosing a most damaging channel switching strategy the paper introduces a class of zero sum games between the jammer and controller as a scenario for such attack , and derives necessary and sufficient conditions for these games to have a nontrivial saddle point equilibrium at this equilibrium , the jammer 's optimal policy is to randomize in a region of the plant 's state space , thus requiring the controller to undertake a nontrivial response which is different from what one would expect in a standard stochastic control problem over a packet dropping channel
we study the problem of partitioning a small sample of n individuals from a mixture of k product distributions over a boolean cube 0 , 1 k according to their distributions each distribution is described by a vector of allele frequencies in r k given two distributions , we use gamma to denote the average ell 2 2 distance in frequencies across k dimensions , which measures the statistical divergence between them we study the case assuming that bits are independently distributed across k dimensions this work demonstrates that , for a balanced input instance for k 2 , a certain graph based optimization function returns the correct partition with high probability , where a weighted graph g is formed over n individuals , whose pairwise hamming distances between their corresponding bit vectors define the edge weights , so long as k omega \( ln n gamma \) and kn tilde omega \( ln n gamma 2 \) the function computes a maximum weight balanced cut of g , where the weight of a cut is the sum of the weights across all edges in the cut this result demonstrates a nice property in the high dimensional feature space one can trade off the number of features that are required with the size of the sample to accomplish certain tasks like clustering
this paper describes a simple framework for structured sparse recovery based on convex optimization we show that many interesting structured sparsity models can be naturally represented by linear matrix inequalities on the support of the unknown parameters , where the constraint matrix has a totally unimodular \( tu \) structure for such structured models , tight convex relaxations can be obtained in polynomial time via linear programming our modeling framework unifies the prevalent structured sparsity norms in the literature , introduces new interesting ones , and renders their tightness and tractability arguments transparent
one of the key research interests in the area of constraint satisfaction problem \( csp \) is to identify tractable classes of constraints and develop efficient solutions for them in this paper , we introduce generalized staircase \( gs \) constraints which is an important generalization of one such tractable class found in the literature , namely , staircase constraints gs constraints are of two kinds , down staircase \( ds \) and up staircase \( us \) we first examine several properties of gs constraints , and then show that arc consistency is sufficient to determine a solution to a csp over ds constraints further , we propose an optimal o \( cd \) time and space algorithm to compute arc consistency for gs constraints where c is the number of constraints and d is the size of the largest domain next , observing that arc consistency is not necessary for solving a dscsp , we propose a more efficient algorithm for solving it with regard to us constraints , arc consistency is not known to be sufficient to determine a solution , and therefore , methods such as path consistency or variable elimination are required since arc consistency acts as a subroutine for these existing methods , replacing it by our optimal o \( cd \) arc consistency algorithm produces a more efficient method for solving a uscsp
a temporal theoretic formalism for understanding game theory is described where a strict ordering relation on a set of time points t defines a game on t using this formalism , a proof of zermelo 's theorem , which states that every finite 2 player zero sum game is determined , is given and an exhaustive analysis of the game of nim is presented furthermore , a combinatorial analysis of games on a set of arbitrary time points is given in particular , it is proved that the number of distinct games on a set t with cardinality n is the number of partial orders on a set of n elements by generalizing this theorem from temporal modal frames to s5 modal frames , it is proved that the number of isomorphism classes of s5 modal frames mathcal f w , r with w n is equal to the partition function p \( n \) as a corollary of the fact that the partition function is asymptotic to the hardy ramanujan number frac 1 4 sqrt 3 n e pi sqrt 2n 3 the number of isomorphism classes of s5 modal frames mathcal f w , r with w n is asymptotically the hardy ramanujan number lastly , we use these results to prove that an arbitrary modal frame is an s5 modal frame with probability zero
we present a general setting for structure sequence comparison in a large class of rna structures that unifies and generalizes a number of recent works on specific families on structures our approach is based on tree decomposition of structures and gives rises to a general parameterized algorithm , where the exponential part of the complexity depends on the family of structures for each of the previously studied families , our algorithm has the same complexity as the specific algorithm that had been given before
in two way ofdm relay , carrier frequency offsets \( cfos \) between relay and terminal nodes introduce severe intercarrier interference \( ici \) which degrades the performance of traditional physical layer network coding \( plnc \) moreover , traditional algorithm to compute the posteriori probability in the presence of ici would incur prohibitive computational complexity at the relay node in this paper , we proposed a two step asynchronous plnc scheme at the relay to mitigate the effect of cfos in the first step , we intend to reconstruct the ici component , in which space alternating generalized expectationmaximization \( sage \) algorithm is used to jointly estimate the needed parameters in the second step , a channel decoding and network coding scheme is proposed to transform the received signal into the xor of two terminals' transmitted information using the reconstructed ici it is shown that the proposed scheme greatly mitigates the impact of cfos with a relatively lower computational complexity in two way ofdm relay
this study examines the traditional approach to software development within the united kingdom government and the accreditation process initially we look at the waterfall methodology that has been used for several years we discuss the pros and cons of waterfall before moving onto the agile scrum methodology agile has been adopted by the majority of government digital departments including the government digital services agile , despite its ability to achieve high rates of productivity organized in short , flexible , iterations , has faced security professionals disbelief when working within the u k government one of the major issues is that we develop in agile but the accreditation process is conducted using waterfall resulting in delays to go live dates taking a brief look into the accreditation process that is used within government for i t systems and applications , we focus on giving the accreditor the assurance they need when developing new applications and systems a framework has been produced by utilizing the open web application security project \( owasp \) application security verification standard \( asvs \) this framework will allow security and agile to work side by side and produce secure code
this letter presents an improved peak cancellation \( pc \) scheme for peak to average power ratio \( papr \) reduction in orthogonal frequency division multiplexing \( ofdm \) systems the main idea is based on a serial peak cancellation \( spc \) mode for alleviating the peak regrowth of the conventional schemes based on the spc mode , two particular algorithms are developed with different tradeoff between papr and computational complexity simulation shows that the proposed scheme has a better tradeoff among papr , complexity and signal distortion than the conventional schemes
let c be a proper edge colouring of a graph g \( v , e \) with integers 1 , 2 , ldots , k then k geq delta \( g \) , while by vizing 's theorem , no more than k delta \( g \) 1 is necessary for constructing such c on the course of investigating irregularities in graphs , it has been moreover conjectured that only slightly larger k , i e , k delta \( g \) 2 enables enforcing additional strong feature of c , namely that it attributes distinct sums of incident colours to adjacent vertices in g if only this graph has no isolated edges and is not isomorphic to c 5 we prove the conjecture is valid for planar graphs of sufficiently large maximum degree in fact even stronger statement holds , as the necessary number of colours stemming from the result of vizing is proved to be sufficient for this family of graphs specifically , our main result states that every planar graph g of maximum degree at least 28 which contains no isolated edges admits a proper edge colouring c e to 1 , 2 , ldots , delta \( g \) 1 such that sum e ni u c \( e \) neq sum e ni v c \( e \) for every edge uv of g
this paper shows that the maddah ali tse \( mat \) scheme which establishes the symmetric capacity of two example broadcast channels with strictly causal state information at the transmitter is a simple special case of the shayevitz wigger scheme for the broadcast channel with generalized feedback , which involves block markov coding , compression , superposition coding , marton coding , and coded time sharing focusing on the class of symmetric broadcast channels with state , we derive an expression for the maximum achievable symmetric rate using the shayevitz wigger scheme we show that the mat results can be recovered by evaluating this expression for the special case in which superposition coding and marton coding are not used we then introduce a new broadcast channel example that shares many features of the mat examples we show that another special case of our maximum symmetric rate expression in which superposition coding is also used attains a higher symmetric rate than the mat scheme the symmetric capacity of this example is not known , however
linear receivers are an attractive low complexity alternative to optimal processing for multi antenna mimo communications in this paper we characterize the information theoretic performance of mimo linear receivers in two different asymptotic regimes for fixed number of antennas , we investigate the limit of error probability in the high snr regime in terms of the diversity multiplexing tradeoff \( dmt \) following this , we characterize the error probability for fixed snr in the regime of large \( but finite \) number of antennas as far as the dmt is concerned , we report a negative result we show that both linear zero forcing \( zf \) and linear minimum mean square error \( mmse \) receivers achieve the same dmt , which is largely suboptimal even in the case where outer coding and decoding is performed across the antennas we also provide an approximate quantitative analysis of the markedly different behavior of the mmse and zf receivers at finite rate and non asymptotic snr , and show that while the zf receiver achieves poor diversity at any finite rate , the mmse receiver error curve slope flattens out progressively , as the coding rate increases when snr is fixed and the number of antennas becomes large , we show that the mutual information at the output of a mmse or zf linear receiver has fluctuations that converge in distribution to a gaussian random variable , whose mean and variance can be characterized in closed form this analysis extends to the linear receiver case a well known result previously obtained for the optimal receiver simulations reveal that the asymptotic analysis captures accurately the outage behavior of systems even with a moderate number of antennas
could we define i \? throughout this article we give a negative answer to this question more exactly , we show that there is no definition for i in a certain way but this negative answer depends on our definition of definability here , we try to consider sufficient generalized definition of definability in the middle of paper a paradox will arise which makes us to modify the way we use the concept of property and definability
in this work , we explain the underlying interaction mechanisms which govern students' influence on each other in massive open online courses \( moocs \) specifically , we outline different ways in which students can be negatively exposed to their peers on mooc forums and discuss a simple formulation of learning network diffusion , which formalizes the essence of how such an influence spreads and can potentially lead to student attrition over time we also view the limitations of our student modeling in the light of real world mooc behavior and consequently suggest ways of extending the diffusion model to handle more complex assumptions such an understanding is very beneficial for mooc designers and instructors to create a conducive learning environment that supports students' growth and increases their engagement in the course
one of the most frequently used models for understanding human navigation on the web is the markov chain model , where web pages are represented as states and hyperlinks as probabilities of navigating from one page to another predominantly , human navigation on the web has been thought to satisfy the memoryless markov property stating that the next page a user visits only depends on her current page and not on previously visited ones this idea has found its way in numerous applications such as google 's pagerank algorithm and others recently , new studies suggested that human navigation may better be modeled using higher order markov chain models , i e , the next page depends on a longer history of past clicks yet , this finding is preliminary and does not account for the higher complexity of higher order markov chain models which is why the memoryless model is still widely used in this work we thoroughly present a diverse array of advanced inference methods for determining the appropriate markov chain order we highlight strengths and weaknesses of each method and apply them for investigating memory and structure of human navigation on the web our experiments reveal that the complexity of higher order models grows faster than their utility , and thus we confirm that the memoryless model represents a quite practical model for human navigation on a page level however , when we expand our analysis to a topical level , where we abstract away from specific page transitions to transitions between topics , we find that the memoryless assumption is violated and specific regularities can be observed we report results from experiments with two types of navigational datasets \( goal oriented vs free form \) and observe interesting structural differences that make a strong argument for more contextual studies of human navigation in future work
we introduce a generalized framework for sampling and reconstruction in separable hilbert spaces specifically , we establish that it is always possible to stably reconstruct a vector in an arbitrary riesz basis from sufficiently many of its samples in any other riesz basis this framework can be viewed as an extension of that of eldar et al however , whilst the latter imposes stringent assumptions on the reconstruction basis , and may in practice be unstable , our framework allows for recovery in any \( riesz \) basis in a manner that is completely stable whilst the classical shannon sampling theorem is a special case of our theorem , this framework allows us to exploit additional information about the approximated vector \( or , in this case , function \) , for example sparsity or regularity , to design a reconstruction basis that is better suited examples are presented illustrating this procedure
the ability to comprehend wishes or desires and their fulfillment is important to natural language understanding this paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled we propose various unstructured and structured models that capture fulfillment cues such as the subject 's emotional state and actions our experiments with two different datasets demonstrate the importance of understanding the narrative and discourse structure to address this task
we consider the problem of personalization of online services from the viewpoint of ad targeting , where we seek to find the best ad categories to be shown to each user , resulting in improved user experience and increased advertisers' revenue we propose to address this problem as a task of ranking the ad categories depending on a user 's preference , and introduce a novel label ranking approach capable of efficiently learning non linear , highly accurate models in large scale settings experiments on a real world advertising data set with more than 3 2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top k retrieval performance , strongly suggesting the benefit of using the proposed model on large scale ranking problems
in this paper , we study resource allocation and adaptive modulation in sc fdma which is adopted as the multiple access scheme for the uplink in the 3gpp lte standard a sum utility maximization \( sumax \) , and a joint adaptive modulation and sum cost minimization \( jamscmin \) problems are considered unlike ofdma , in addition to the restriction of allocating a sub channel to one user at most , the multiple sub channels allocated to a user in sc fdma should be consecutive as well this renders the resource allocation problem prohibitively difficult and the standard optimization tools \( e g , lagrange dual approach widely used for ofdma , etc \) can not help towards its optimal solution we propose a novel optimization framework for the solution of these problems that is inspired from the recently developed canonical duality theory we first formulate the optimization problems as binary integer programming problems and then transform these binary integer programming problems into continuous space canonical dual problems that are concave maximization problems based on the solution of the continuous space dual problems , we derive resource allocation \( joint with adaptive modulation for jamscmin \) algorithms for both the problems which have polynomial complexities we provide conditions under which the proposed algorithms are optimal we also propose an adaptive modulation scheme for sumax problem we compare the proposed algorithms with the existing algorithms in the literature to assess their performance
formerly i presented a metric navigation method in the webots mobile robot simulator the navigating khepera like robot builds an occupancy grid of the environment and explores the square shaped room around with a value iteration algorithm now i created a topological navigation procedure based on the occupancy grid process the extension by a skeletonization algorithm results a graph of important places and the connecting routes among them i also show the significant time profit gained during the process
interference alignment \( ia \) has recently emerged as a promising interference mitigation technique for interference networks in this letter , we focus on the ia non iterative transceiver design problem in a multiple input multiple output interfering broadcast channel \( mimo ibc \) , and observed that there is previously unexploited flexibility in different permutations of user ordering by choosing a good user ordering for a pre determined ia inter channel interference allocation , an improved transceiver design can be accomplished in order to achieve a more practical performance complexity tradeoff , a suboptimal user ordering algorithm is proposed simulation shows the proposed suboptimal user ordering algorithm can achieve near optimal performance compared to the optimal ordering while exhibiting only moderate computational complexity
mobile phones have developed into complex platforms with large numbers of installed applications and a wide range of sensitive data application security policies limit the permissions of each installed application as applications may interact , restricting single applications may create a false sense of security for the end users while data may still leave the mobile phone through other applications instead , the information flow needs to be policed for the composite system of applications in a transparent and usable manner in this paper , we propose to employ static analysis based on the software architecture and focused data flow analysis to scalably detect information flows between components specifically , we aim to reveal transitivity of trust problems in multi component mobile platforms we demonstrate the feasibility of our approach with android applications , although the generalization of the analysis to similar composition based architectures , such as service oriented architecture , can also be explored in the future
consumers' purchase decisions are increasingly influenced by user generated online reviews accordingly , there has been growing concern about the potential for posting deceptive opinion spam fictitious reviews that have been deliberately written to sound authentic , to deceive the readers existing approaches mainly focus on developing automatic supervised learning based methods to help users identify deceptive opinion spams this work , we used the lsi and sprinkled lsi technique to reduce the dimension for deception detection we make our contribution to demonstrate what lsi is capturing in latent semantic space and reveal how deceptive opinions can be recognized automatically from truthful opinions finally , we proposed a voting scheme which integrates different approaches to further improve the classification performance
recent advances suggest that a wide range of computer vision problems can be addressed more appropriately by considering non euclidean geometry this paper tackles the problem of sparse coding and dictionary learning in the space of symmetric positive definite matrices , which form a riemannian manifold with the aid of the recently introduced stein kernel \( related to a symmetric version of bregman matrix divergence \) , we propose to perform sparse coding by embedding riemannian manifolds into reproducing kernel hilbert spaces this leads to a convex and kernel version of the lasso problem , which can be solved efficiently we furthermore propose an algorithm for learning a riemannian dictionary \( used for sparse coding \) , closely tied to the stein kernel experiments on several classification tasks \( face recognition , texture classification , person re identification \) show that the proposed sparse coding approach achieves notable improvements in discrimination accuracy , in comparison to state of the art methods such as tensor sparse coding , riemannian locality preserving projection , and symmetry driven accumulation of local features
model predictive control \( mpc \) anticipates future events to take appropriate control actions nonlinear mpc \( nmpc \) describes systems with nonlinear models and or constraints continuation mpc , suggested by t ohtsuka in 2004 , uses krylov newton iterations continuation mpc is suitable for nonlinear problems and has been recently adopted for minimum time problems we extend the continuation mpc approach to a case where the state is implicitly constrained to a smooth manifold we propose an algorithm for on line controller implementation and demonstrate its numerical effectiveness for a test problem on a hemisphere
a fundamental operation in many vision tasks , including motion understanding , stereopsis , visual odometry , or invariant recognition , is establishing correspondences between images or between images and data from other modalities we present an analysis of the role that multiplicative interactions play in learning such correspondences , and we show how learning and inferring relationships between images can be viewed as detecting rotations in the eigenspaces shared among a set of orthogonal matrices we review a variety of recent multiplicative sparse coding methods in light of this observation we also review how the squaring operation performed by energy models and by models of complex cells can be thought of as a way to implement multiplicative interactions this suggests that the main utility of including complex cells in computational models of vision may be that they can encode relations not invariances
in recent years , evaluation of the quality of academic research has become an increasingly important and influential business it determines , often to a large extent , the amount of research funding flowing into universities and similar institutes from governmental agencies and it impacts upon academic careers policy makers are becoming increasingly reliant upon , and influenced by , the outcomes of such evaluations in response , university managers are increasingly attracted to simple indicators as guides to the dynamics of the positions of their various institutions in league tables however , these league tables are frequently drawn up by inexpert bodies such as newspapers and magazines , using rather arbitrary measures and criteria terms such as critical mass' and metrics are often bandied about without proper understanding of what they actually mean rather than accepting the rise and fall of universities , departments and individuals on a turbulent sea of arbitrary measures , we suggest it is incumbent upon the scientific community itself to clarify their nature here we report on recent attempts to do that by properly defining critical mass and showing how group size influences research quality we also examine currently predominant metrics and show that these fail as reliable indicators of group research quality
we consider a two user gaussian multiple access channel with two independent additive white gaussian interferences each interference is known to exactly one transmitter non causally transmitters are allowed to cooperate through finite capacity links the capacity region is characterized to within 3 and 1 5 bits for the stronger user and the weaker user respectively , regardless of channel parameters as a by product , we characterize the capacity region of the case without cooperation to within 1 and 0 5 bits for the stronger user and the weaker user respectively these results are based on a layered modulo lattice transmission architecture which realizes distributed interference cancellation
degraded text recognition is a difficult task given a noisy text image , a word recognizer can be applied to generate several candidates for each word image high level knowledge sources can then be used to select a decision from the candidate set for each word image in this paper , we propose that visual inter word constraints can be used to facilitate candidate selection visual inter word constraints provide a way to link word images inside the text page , and to interpret them systematically
we consider the problem of maximizing submodular functions while this problem is known to be np hard , several numerically efficient local search techniques with approximation guarantees are available in this paper , we propose a novel convex relaxation which is based on the relationship between submodular functions , entropies and probabilistic graphical models in a graphical model , the entropy of the joint distribution decomposes as a sum of marginal entropies of subsets of variables moreover , for any distribution , the entropy of the closest distribution factorizing in the graphical model provides an bound on the entropy for directed graphical models , this last property turns out to be a direct consequence of the submodularity of the entropy function , and allows the generalization of graphical model based upper bounds to any submodular functions these upper bounds may then be jointly maximized with respect to a set , while minimized with respect to the graph , leading to a convex variational inference scheme for maximizing submodular functions , based on outer approximations of the marginal polytope and maximum likelihood bounded treewidth structures by considering graphs of increasing treewidths , we may then explore the trade off between computational complexity and tightness of the relaxation we also present extensions to constrained problems and maximizing the difference of submodular functions , which include all possible set functions
the performance of cbir algorithms is usually measured on an isolated workstation in a real world environment the algorithms would only constitute a minor component among the many interacting components the internet dramati cally changes many of the usual assumptions about measuring cbir performance any cbir benchmark should be designed from a networked systems standpoint these benchmarks typically introduce communication overhead because the real systems they model are distributed applications we present our implementation of a client server benchmark called birds i to measure image retrieval performance over the internet it has been designed with the trend toward the use of small personalized wireless systems in mind web based cbir implies the use of heteroge neous image sets , imposing certain constraints on how the images are organized and the type of performance metrics applicable birds i only requires controlled human intervention for the compilation of the image collection and none for the generation of ground truth in the measurement of retrieval accuracy benchmark image collections need to be evolved incrementally toward the storage of millions of images and that scaleup can only be achieved through the use of computer aided compilation finally , our scoring metric introduces a tightly optimized image ranking window
we have studied the problem of the number of permutations that can be sorted by two stacks in series we do this by first counting all such permutations of length less than 20 exactly , then using a numerical technique to obtain eleven further coefficients approximately analysing these coefficients by a variety of methods we conclude that the ogf behaves as s \( z \) sim a \( 1 mu cdot z \) gamma , where mu 12 4 pm 0 1 , gamma 1 5 pm 0 3 , and a approx 0 026
modeling should play a central role in k 12 stem education , where it could make classes much more engaging a model underlies every scientific theory , and models are central to all the stem disciplines \( science , technology , engineering , math \) this paper describes executable concept modeling of stem concepts using immutable objects and pure functions in python i present examples in math , physics , chemistry , and engineering , built using a proof of concept tool called pystemm the approach applies to all stem areas and supports learning with pictures , narrative , animation , and graph plots models can extend each other , simplifying getting started the functional programming style reduces incidental complexity and code debugging
in a emph group testing scheme , a set of tests is designed to identify a small number t of defective items among a large set \( of size n \) of items in the non adaptive scenario the set of tests has to be designed in one shot in this setting , designing a testing scheme is equivalent to the construction of a emph disjunct matrix , an m times n matrix where the union of supports of any t columns does not contain the support of any other column in principle , one wants to have such a matrix with minimum possible number m of rows \( tests \) one of the main ways of constructing disjunct matrices relies on emph constant weight error correcting codes and their emph minimum distance in this paper , we consider a relaxed definition of a disjunct matrix known as emph almost disjunct matrix this concept is also studied under the name of emph weakly separated design in the literature the relaxed definition allows one to come up with group testing schemes where a close to one fraction of all possible sets of defective items are identifiable our main contribution is twofold first , we go beyond the minimum distance analysis and connect the emph average distance of a constant weight code to the parameters of an almost disjunct matrix constructed from it our second contribution is to explicitly construct almost disjunct matrices based on our average distance analysis , that have much smaller number of rows than any previous explicit construction of disjunct matrices the parameters of our construction can be varied to cover a large range of relations for t and n
lstd is a popular algorithm for value function approximation whenever the number of features is larger than the number of samples , it must be paired with some form of regularization in particular , l1 regularization methods tend to perform feature selection by promoting sparsity , and thus , are well suited for high dimensional problems however , since lstd is not a simple regression algorithm , but it solves a fixed point problem , its integration with l1 regularization is not straightforward and might come with some drawbacks \( e g , the p matrix assumption for lasso td \) in this paper , we introduce a novel algorithm obtained by integrating lstd with the dantzig selector we investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches , and show how it addresses some of their drawbacks
a balanced word is one in which any two factors of the same length contain the same number of each letter of the alphabet up to one finite binary balanced words are called sturmian words a sturmian word is bispecial if it can be extended to the left and to the right with both letters remaining a sturmian word there is a deep relation between bispecial sturmian words and christoffel words , that are the digital approximations of euclidean segments in the plane in 1997 , j berstel and a de luca proved that emph palindromic bispecial sturmian words are precisely the maximal internal factors of emph primitive christoffel words we extend this result by showing that bispecial sturmian words are precisely the maximal internal factors of emph all christoffel words our characterization allows us to give an enumerative formula for bispecial sturmian words we also investigate the minimal forbidden words for the language of sturmian words
this paper uses support vector machines \( svm \) to fuse multiple classifiers for an offline signature system from the signature images , global and local features are extracted and the signatures are verified with the help of gaussian empirical rule , euclidean and mahalanobis distance based classifiers svm is used to fuse matching scores of these matchers finally , recognition of query signatures is done by comparing it with all signatures of the database the proposed system is tested on a signature database contains 5400 offline signatures of 600 individuals and the results are found to be promising
this paper provides insight into when , why , and how forecast strategies fail when they are applied to complicated time series we conjecture that the inherent complexity of real world time series data which results from the dimension , nonlinearity , and non stationarity of the generating process , as well as from measurement issues like noise , aggregation , and finite data length is both empirically quantifiable and directly correlated with predictability in particular , we argue that redundancy is an effective way to measure complexity and predictive structure in an experimental time series and that weighted permutation entropy is an effective way to estimate that redundancy to validate these conjectures , we study 120 different time series data sets for each time series , we construct predictions using a wide variety of forecast models , then compare the accuracy of the predictions with the permutation entropy of that time series we use the results to develop a model free heuristic that can help practitioners recognize when a particular prediction method is not well matched to the task at hand that is , when the time series has more predictive structure than that method can capture and exploit
a non deterministic call by need lambda calculus calc with case , constructors , letrec and a \( non deterministic \) erratic choice , based on rewriting rules is investigated a standard reduction is defined as a variant of left most outermost reduction the semantics is defined by contextual equivalence of expressions instead of using alpha beta \( eta \) equivalence it is shown that several program transformations are correct , for example all \( deterministic \) rules of the calculus , and in addition the rules for garbage collection , removing indirections and unique copy this shows that the combination of a context lemma and a meta rewriting on reductions using complete sets of commuting \( forking , resp \) diagrams is a useful and successful method for providing a semantics of a functional programming language and proving correctness of program transformations
a function f defined on all subsets of a finite ground set e is quasi concave if f \( x cup y \) geq min f \( x \) , f \( y \) for all x , y subset e quasi concave functions arise in many fields of mathematics and computer science such as social choice , theory of graph , data mining , clustering and other fields the maximization of quasi concave function takes , in general , exponential time however , if a quasi concave function is defined by associated monotone linkage function then it can be optimized by the greedy type algorithm in a polynomial time quasi concave functions defined as minimum values of monotone linkage functions were considered on antimatroids , where the correspondence between quasi concave and bottleneck functions was shown \( kempner levit , 2003 \) the goal of this paper is to analyze quasi concave functions on different families of sets and to investigate their relationships with monotone linkage functions
a run is a maximal occurrence of a repetition v with a period p such that 2p le v the maximal number of runs in a string of length n was studied by several authors and it is known to be between 0 944 n and 1 029 n we investigate highly periodic runs , in which the shortest period p satisfies 3p le v we show the upper bound 0 5n on the maximal number of such runs in a string of length n and construct a sequence of words for which we obtain the lower bound 0 406 n
multimedia security has been the aim point of considerable research activity because of its wide application area the major technology to achieve copyright protection , content authentication , access control and multimedia security is watermarking which is the process of embedding data into a multimedia element such as image or audio , this embedded data can later be extracted from , or detected in the embedded element for different purposes in this work , a blind watermarking algorithm based on svd and circulant matrices has been presented every circulant matrix is associated with a matrix for which the svd decomposition coincides with the spectral decomposition this leads to improve the chandra algorithm 1 , our presentation will include a discussion on the data hiding capacity , watermark transparency and robustness against a wide range of common image processing attacks
the concept of city or urban resilience has emerged as one of the key challenges for the next decades as a consequence , institutions like the united nations or rockefeller foundation have embraced initiatives that increase or improve it these efforts translate into funded programs both for action on the ground and to develop quantification of resilience , under the for of an index ironically , on the academic side there is no clear consensus regarding how resilience should be quantified , or what it exactly refers to in the urban context here we attempt to link both extremes providing an example of how to exploit large , publicly available , worldwide urban datasets , to produce objective insight into one of the possible dimensions of urban resilience we do so via well established methods in complexity science , such as percolation theory which has a long tradition at providing valuable information on the vulnerability in complex systems our findings uncover large differences among studied cities , both regarding their infrastructural fragility and the imbalances in the distribution of critical services
the statistical mechanical interpretation of algorithmic information theory \( ait , for short \) was introduced and developed by our former works k tadaki , local proceedings of cie 2008 , pp 425 434 , 2008 and k tadaki , proceedings of lfcs'09 , springer 's lncs , vol 5407 , pp 422 440 , 2009 , where we introduced the notion of thermodynamic quantities , such as partition function z \( t \) , free energy f \( t \) , energy e \( t \) , and statistical mechanical entropy s \( t \) , into ait we then discovered that , in the interpretation , the temperature t equals to the partial randomness of the values of all these thermodynamic quantities , where the notion of partial randomness is a stronger representation of the compression rate by means of program size complexity furthermore , we showed that this situation holds for the temperature itself as a thermodynamic quantity , namely , for each of all the thermodynamic quantities above , the computability of its value at temperature t gives a sufficient condition for t in \( 0 , 1 \) to be a fixed point on partial randomness in this paper , we develop the statistical mechanical interpretation of ait further and pursue its formal correspondence to normal statistical mechanics the thermodynamic quantities in ait are defined based on the halting set of an optimal computer , which is a universal decoding algorithm used to define the notion of program size complexity we show that there are infinitely many optimal computers which give completely different sufficient conditions in each of the thermodynamic quantities in ait we do this by introducing the notion of composition of computers into ait , which corresponds to the notion of composition of systems in normal statistical mechanics
we prove that the number of tile types required to build squares of size n x n , in winfree 's abstract tile assembly model , when restricted to using only non cooperative tile bindings , is at least 2n 1 , which is also the best known upper bound non cooperative self assembly , also known as temperature 1 , is where tiles bind to each other if they match on one or more sides , whereas in cooperative binding , some tiles can bind only if they match on multiple sides our proof introduces a new programming technique for temperature 1 , that disproves the very intuitive and commonly held belief that , in the same model , assembling paths between two points a and b cannot be done with less tile types than the manhattan distance between them then , we prove a necessary condition for these efficient paths to be assembled , and show that this necessary condition cannot hold in completely filled squares this result proves the oldest conjecture in algorithmic self assembly , published by rothemund and winfree in stoc 2000 , in the case where growth starts from a corner of the square as a corollary , we establish n as a lower bound on the tile complexity of the general case the problem of determining the minimal number of tile types to self assemble a shape is known to be sigma p 2 complete
isometric feature mapping \( isomap \) is a promising manifold learning method however , isomap fails to work on data which distribute on clusters in a single manifold or manifolds many works have been done on extending isomap to multi manifolds learning in this paper , we first proposed a new multi manifolds learning algorithm \( m isomap \) with help of a general procedure the new algorithm preserves intra manifold geodesics and multiple inter manifolds edges precisely compared with previous methods , this algorithm can isometrically learn data distributed on several manifolds secondly , the original multi cluster manifold learning algorithm first proposed in cite dcisomap and called d c isomap has been revised so that the revised d c isomap can learn multi manifolds data finally , the features and effectiveness of the proposed multi manifolds learning algorithms are demonstrated and compared through experiments
multi object tracking has been studied for decades however , when it comes to tracking pedestrians in extremely crowded scenes , we are limited to only few works this is an important problem which gives rise to several challenges pre trained object detectors fail to localize targets in crowded sequences this consequently limits the use of data association based multi target tracking methods which rely on the outcome of an object detector additionally , the small apparent target size makes it challenging to extract features to discriminate targets from their surroundings finally , the large number of targets greatly increases computational complexity which in turn makes it hard to extend existing multi target tracking approaches to high density crowd scenarios in this paper , we propose a tracker that addresses the aforementioned problems and is capable of tracking hundreds of people efficiently we formulate online crowd tracking as binary quadratic programing our formulation employs target 's individual information in the form of appearance and motion as well as contextual cues in the form of neighborhood motion , spatial proximity and grouping constraints , and solves detection and data association simultaneously in order to solve the proposed quadratic optimization efficiently , where state of art commercial quadratic programing solvers fail to find the answer in a reasonable amount of time , we propose to use the most recent version of the modified frank wolfe algorithm , which takes advantage of swap steps to speed up the optimization we show that the proposed formulation can track hundreds of targets efficiently and improves state of art results by significant margins on eleven challenging high density crowd sequences
this paper is concerned with the form of typed name binding used by the freshml family of languages its characteristic feature is that a name binding is represented by an abstract \( name , value \) pair that may only be deconstructed via the generation of fresh bound names the paper proves a new result about what operations on names can co exist with this construct in freshml the only observation one can make of names is to test whether or not they are equal this restricted amount of observation was thought necessary to ensure that there is no observable difference between alpha equivalent name binders yet from an algorithmic point of view it would be desirable to allow other operations and relations on names , such as a total ordering this paper shows that , contrary to expectations , one may add not just ordering , but almost any relation or numerical function on names without disturbing the fundamental correctness result about this form of typed name binding \( that object level alpha equivalence precisely corresponds to contextual equivalence at the programming meta level \) , so long as one takes the state of dynamically created names into account
in this paper we look at isometry properties of random matrices during the last decade these properties gained a lot attention in a field called compressed sensing in first place due to their initial use in cite crt , ct namely , in cite crt , ct these quantities were used as a critical tool in providing a rigorous analysis of ell 1 optimization 's ability to solve an under determined system of linear equations with sparse solutions in such a framework a particular type of isometry , called restricted isometry , plays a key role one then typically introduces a couple of quantities , called upper and lower restricted isometry constants to characterize the isometry properties of random matrices those constants are then usually viewed as mathematical objects of interest and their a precise characterization is desirable the first estimates of these quantities within compressed sensing were given in cite crt , ct as the need for precisely estimating them grew further a finer improvements of these initial estimates were obtained in e g cite bctsharp09 , bt10 these are typically obtained through a combination of union bounding strategy and powerful tail estimates of extreme eigenvalues of wishart \( gaussian \) matrices \( see , e g cite edelman88 \) in this paper we attempt to circumvent such an approach and provide an alternative way to obtain similar estimates
with the evolution of mobile devices , and smart phones in particular , comes the ability to create new experiences that enhance the way we see , interact , and manipulate objects , within the world that surrounds us it is now possible to blend data from our senses and our devices in numerous ways that simply were not possible before using augmented reality technology in a near future , when all of the office devices as well as your personal electronic gadgets are on a common wireless network , operating them using a universal remote controller would be possible this paper presents an off the shelf , low cost prototype that leverages the augmented reality technology to deliver a novel and interactive way of operating office network devices around using a mobile device we believe this type of system may provide benefits to controlling multiple integrated devices and visualizing interconnectivity or utilizing visual elements to pass information from one device to another , or may be especially beneficial to control devices when interacting with them physically may be difficult or pose danger or harm
we present abstraction techniques that transform a given non linear dynamical system into a linear system or an algebraic system described by polynomials of bounded degree , such that , invariant properties of the resulting abstraction can be used to infer invariants for the original system the abstraction techniques rely on a change of basis transformation that associates each state variable of the abstract system with a function involving the state variables of the original system we present conditions under which a given change of basis transformation for a non linear system can define an abstraction furthermore , the techniques developed here apply to continuous systems defined by ordinary differential equations \( odes \) , discrete systems defined by transition systems and hybrid systems that combine continuous as well as discrete subsystems the techniques presented here allow us to discover , given a non linear system , if a change of bases transformation involving degree bounded polynomials yielding an algebraic abstraction exists if so , our technique yields the resulting abstract system , as well this approach is further extended to search for a change of bases transformation that abstracts a given non linear system into a system of linear differential inclusions our techniques enable the use of analysis techniques for linear systems to infer invariants for non linear systems we present preliminary evidence of the practical feasibility of our ideas using a prototype implementation
in this paper we present experiments in order to show how some pseudo random number generators can improve the effectiveness of a statistical cryptanalysis algorithm we deduce mainly that a better generator enhance the accuracy of the cryptanalysis algorithm
this paper considers the setup of a parallel mimo relay network in which k relays , each equipped with n antennas , assist the transmitter and the receiver , each equipped with m antennas , in the half duplex mode , under the assumption that n geq m this setup has been studied in the literature like in cite nabar , cite nabar2 , and cite qr in this paper , a simple scheme , the so called incremental cooperative beamforming , is introduced and shown to achieve the capacity of the network in the asymptotic case of k to infty with a gap no more than o \( frac 1 log \( k \) \) this result is shown to hold , as long as the power of the relays scales as omega \( frac log 9 \( k \) k \) finally , the asymptotic snr behavior is studied and it is proved that the proposed scheme achieves the full multiplexing gain , regardless of the number of relays
in this paper we explore the relationship between dual decomposition and the consensus based method for distributed optimization the relationship is developed by examining the similarities between the two approaches and their relationship to gradient based constrained optimization by formulating each algorithm in continuous time , it is seen that both approaches use a gradient method for optimization with one using a proportional control term and the other using an integral control term to drive the system to the constraint set therefore , a significant contribution of this paper is to combine these methods to develop a continuous time proportional integral distributed optimization method furthermore , we establish convergence using lyapunov stability techniques and utilizing properties from the network structure of the multi agent system
in this paper , we study the use of gf \( q \) quantized ldgm codes for binary source coding by employing quantization , it is possible to obtain binary codewords with a non uniform distribution the obtained statistics is hence suitable for optimal , direct quantization of non uniform bernoulli sources we employ a message passing algorithm combined with a decimation procedure in order to perform compression the experimental results based on gf \( q \) ldgm codes with regular degree distributions yield performances quite close to the theoretical rate distortion bounds
modern computer systems typically conbine multicore cpus with accelerators like gpus for inproved performance and energy efficiency however , these sys tems suffer from poor performance portability , code tuned for one device must be retuned to achieve high performance on another image processing is increas ing in importance , with applications ranging from seismology and medicine to photoshop based on our experience with medical image processing , we propose imagecl , a high level domain specific language and source to source compiler , targeting heterogeneous hardware imagecl resembles opencl , but abstracts away per formance optimization details , allowing the programmer to focus on algorithm development , rather than performance tuning the latter is left to our source to source compiler and auto tuner from high level imagecl kernels , our source to source compiler can generate multiple opencl implementations with different optimizations applied we rely on auto tuning rather than machine models or ex pert programmer knowledge to determine which optimizations to apply , making our tuning procedure highly robust furthermore , we can generate high perform ing implementations for different devices from a single source code , thereby im proving performance portability we evaluate our approach on three image processing benchmarks , on different gpu and cpu devices , and are able to outperform other state of the art solutions in several cases , achieving speedups of up to 4 57x
sparse channel estimation problem is one of challenge technical issues in stable broadband wireless communications based on square error criterion \( sec \) , adaptive sparse channel estimation \( asce \) methods , e g , zero attracting least mean square error \( za lms \) algorithm and reweighted za lms \( rza lms \) algorithm , have been proposed to mitigate noise interferences as well as to exploit the inherent channel sparsity however , the conventional sec asce methods are vulnerable to 1 \) random scaling of input training signal and 2 \) imbalance between convergence speed and steady state mean square error \( mse \) performance due to fixed step size of gradient descend method in this paper , a mixed square fourth error criterion \( sfec \) based improved asce methods are proposed to avoid aforementioned shortcomings specifically , the improved sfec asce methods are realized with zero attracting least mean square fourth error \( za lms f \) algorithm and reweighted za lms f \( rza lms f \) algorithm , respectively firstly , regularization parameters of the sfec asce methods are selected by means of monte carlo simulations secondly , lower bounds of the sfec asce methods are derived and analyzed finally , simulation results are given to show that the proposed sfec asce methods achieve better estimation performance than the conventional sec asce methods 1
we study deterministic distributed broadcasting in synchronous multiple access channels packets are injected into n nodes by a window type adversary that is constrained by a window w and injection rates individually assigned to all nodes we investigate what queue size and packet latency can be achieved with the maximum aggregate injection rate of one packet per round , depending on properties of channels and algorithms we give a non adaptive algorithm for channels with collision detection and an adaptive algorithm for channels without collision detection that achieve o \( min \( n w , w log n \) \) packet latency we show that packet latency has to be either omega \( w max \( 1 , log w n \) \) , when w le n , or omega \( w n \) , when w n , as a matching lower bound to these algorithms we develop a non adaptive algorithm for channels without collision detection that achieves o \( n w \) queue size and o \( nw \) packet latency this is in contrast with the adversarial model of global injection rates , in which non adaptive algorithms with bounded packet latency do not exist 18 our algorithm avoids collisions produced by simultaneous transmissions we show that any algorithm with this property must have omega \( nw \) packet latency
an infinite binary sequence has randomness rate at least sigma if , for almost every n , the kolmogorov complexity of its prefix of length n is at least sigma n it is known that for every rational sigma in \( 0 , 1 \) , on one hand , there exists sequences with randomness rate sigma that can not be effectively transformed into a sequence with randomness rate higher than sigma and , on the other hand , any two independent sequences with randomness rate sigma can be transformed into a sequence with randomness rate higher than sigma we show that the latter result holds even if the two input sequences have linear dependency \( which , informally speaking , means that all prefixes of length n of the two sequences have in common a constant fraction of their information \) the similar problem is studied for finite strings it is shown that from any two strings with sufficiently large kolmogorov complexity and sufficiently small dependence , one can effectively construct a string that is random even conditioned by any one of the input strings
a new application of duality relations of stochastic processes is demonstrated although conventional usages of the duality relations need analytical solutions for the dual processes , we here employ numerical solutions of the dual processes and investigate the usefulness as a demonstration , estimation problems of hidden variables in stochastic differential equations are discussed employing algebraic probability theory , a little complicated birth death process is derived from the stochastic differential equations , and an estimation method based on the ensemble kalman filter is proposed as a result , the possibility for making faster computational algorithms based on the duality concepts is shown
this work considers the secure and reliable information transmission in two hop relay wireless networks without the information of both eavesdropper channels and locations while the previous work on this problem mainly studied infinite networks and their asymptotic behavior and scaling law results , this papers focuses on a more practical network with finite number of system nodes and explores the corresponding exact results on the number of eavesdroppers the network can tolerant to ensure a desired secrecy and reliability for achieving secure and reliable information transmission in a finite network , two transmission protocols are considered in this paper , one adopts an optimal but complex relay selection process with less load balance capacity while the other adopts a random but simple relay selection process with good load balance capacity theoretical analysis is further provided to determine the exact and maximum number of independent and also uniformly distributed eavesdroppers one network can tolerate to satisfy a specified requirement in terms of the maximum secrecy outage probability and maximum transmission outage probability allowed
mutual information is widely used , in a descriptive way , to measure the stochastic dependence of categorical random variables in order to address questions such as the reliability of the descriptive value , one must consider sample to population inferential approaches this paper deals with the posterior distribution of mutual information , as obtained in a bayesian framework by a second order dirichlet prior distribution the exact analytical expression for the mean , and analytical approximations for the variance , skewness and kurtosis are derived these approximations have a guaranteed accuracy level of the order o \( 1 n 3 \) , where n is the sample size leading order approximations for the mean and the variance are derived in the case of incomplete samples the derived analytical expressions allow the distribution of mutual information to be approximated reliably and quickly in fact , the derived expressions can be computed with the same order of complexity needed for descriptive mutual information this makes the distribution of mutual information become a concrete alternative to descriptive mutual information in many applications which would benefit from moving to the inductive side some of these prospective applications are discussed , and one of them , namely feature selection , is shown to perform significantly better when inductive mutual information is used
among the many challenges of integrating renewable energy sources into the existing power grid , is the challenge of integrating renewable energy generators into the power systems economy electricity markets currently are run in a way that participating generators must supply contracted amounts and yet , renewable energy generators such as wind power generators cannot supply contracted amounts with certainty thus , alternative market architectures must be considered where there are aggregator entities who participate in the electricity market by buying power from the renewable energy generators , and assuming risk of any shortfall from contracted amounts in this paper , we propose auction mechanisms that can be used by the aggregators for procuring stochastic resources , such as wind power the nature of stochastic resources is different from classical resources in that such a resource is only available stochastically the distribution of the generation is private information , and the system objective is to truthfully elicit such information we introduce a variant of the vcg mechanism for this problem we also propose a non vcg mechanism with a contracted payment plus penalty payoff structure we generalize the basic mechanisms in various ways we then consider the setting where there are two classes of players to demonstrate the difficulty of auction design in such scenarios we also consider an alternative architecture where the generators need to fulfill any shortfall from the contracted amount by buying from the spot market
our paper approaches the parking assistance service in urban environments as an instance of service provision in non cooperative network environments we propose normative abstractions for the way drivers pursue parking space and the way they respond to partial or complete information for parking demand and supply as well as specific pricing policies on public and private parking facilities the drivers are viewed as strategic agents who make rational decisions attempting to minimize the cost of the acquired parking spot we formulate the resulting games as resource selection games and derive their equilibria under different expressions of uncertainty about the overall parking demand the efficiency of the equilibrium states is compared against the optimal assignment that could be determined by a centralized entity and conditions are derived for minimizing the related price of anarchy value our results provide useful hints for the pricing and practical management of on street and private parking resources more importantly , they exemplify counterintuitive less is more effects about the way information availability modulates the service cost , which underpin general competitive service provision settings and contribute to the better understanding of effective information mechanisms
learning object detectors requires massive amounts of labeled training samples from the specific data source of interest this is impractical when dealing with many different sources \( e g , in camera networks \) , or constantly changing ones such as mobile cameras \( e g , in robotics or driving assistant systems \) in this paper , we address the problem of self learning detectors in an autonomous manner , i e \( i \) detectors continuously updating themselves to efficiently adapt to streaming data sources \( contrary to transductive algorithms \) , \( ii \) without any labeled data strongly related to the target data stream \( contrary to self paced learning \) , and \( iii \) without manual intervention to set and update hyper parameters to that end , we propose an unsupervised , on line , and self tuning learning algorithm to optimize a multi task learning convex objective our method uses confident but laconic oracles \( high precision but low recall off the shelf generic detectors \) , and exploits the structure of the problem to jointly learn on line an ensemble of instance level trackers , from which we derive an adapted category level object detector our approach is validated on real world publicly available video object datasets
the numerical solution of partial differential equations using the finite element method is one of the key applications of high performance computing local assembly is its characteristic operation this entails the execution of a problem specific kernel to numerically evaluate an integral for each element in the discretized problem domain since the domain size can be huge , executing efficient kernels is fundamental their op timization is , however , a challenging issue even though affine loop nests are generally present , the short trip counts and the complexity of mathematical expressions make it hard to determine a single or unique sequence of successful transformations therefore , we present the design and systematic evaluation of cof fee , a domain specific compiler for local assembly kernels coffee manipulates abstract syntax trees generated from a high level domain specific language for pdes by introducing domain aware composable optimizations aimed at improving instruction level parallelism , especially simd vectorization , and register locality it then generates c code including vector intrinsics experiments using a range of finite element forms of increasing complexity show that significant performance improvement is achieved
we present a new outer bound for the sum capacity of general multi unicast deterministic networks intuitively , this bound can be understood as applying the cut set bound to concatenated copies of the original network with a special restriction on the allowed transmit signal distributions we first study applications to finite field networks , where we obtain a general outer bound expression in terms of ranks of the transfer matrices we then show that , even though our outer bound is for deterministic networks , a recent result relating the capacity of awgn kxkxk networks and the capacity of a deterministic counterpart allows us to establish an outer bound to the dof of kxkxk wireless networks with general connectivity this bound is tight in the case of the adjacent cell interference topology , and yields graph theoretic necessary and sufficient conditions for k dof to be achievable in general topologies
compressed sensing \( cs \) enables people to acquire the compressed measurements directly and recover sparse or compressible signals faithfully even when the sampling rate is much lower than the nyquist rate however , the pure random sensing matrices usually require huge memory for storage and high computational cost for signal reconstruction many structured sensing matrices have been proposed recently to simplify the sensing scheme and the hardware implementation in practice based on the restricted isometry property and coherence , couples of existing structured sensing matrices are reviewed in this paper , which have special structures , high recovery performance , and many advantages such as the simple construction , fast calculation and easy hardware implementation the number of measurements and the universality of different structure matrices are compared
existing secret key extraction techniques use quantization to map wireless channel amplitudes to secret bits this pa per shows that such techniques are highly prone to environ ment and local noise effects they have very high mismatch rates between the two nodes that measure the channel be tween them this paper advocates using the shape of the channel instead of the size \( or amplitude \) of the channel it shows that this new paradigm shift is significantly ro bust against environmental and local noises we refer to this shape based technique as puzzle implementation in a software defined radio \( sdr \) platform demonstrates that puzzle has a 63 reduction in bit mismatch rate than the state of art frequency domain approach \( csi 2bit \) exper iments also show that unlike the state of the art received signal strength \( rss \) based methods like asbg , puzzle is robust against an attack in which an eavesdropper can pre dict the secret bits using planned movements
in this paper , we look at the following question we consider cellular automata in the hyperbolic plane , \( see margenstern , 2000 , 2007 and margenstern , morita , 2001 \) and we consider the global function defined on all possible configurations is the injectivity of this function undecidable \? the problem was answered positively in the case of the euclidean plane by jarkko kari , in 1994 in the present paper , we show that the answer is also positive for the hyperbolic plane the problem is undecidable
previous research on spreadsheet risks has predominantly focussed on errors inadvertently introduced by spreadsheet writers i e it focussed on the end user aspects of spreadsheet development when analyzing a faulty spreadsheet , one might not be able to determine whether a particular error \( fault \) has been made by mistake or with fraudulent intentions however , the fences protecting against fraudulent errors have to be different from those shielding against inadvertent mistakes faults resulting from errors committed inadvertently can be prevented ab initio by tools that notify the spreadsheet writer about potential problems whereas faults that are introduced on purpose have to be discovered by auditors without the cooperation of their originators even worse , some spreadsheet writers will do their best to conceal fraudulent parts of their spreadsheets from auditors in this paper we survey the available means for fraud protection by contrasting approaches suitable for spreadsheets with those known from fraud protection for conventional software
error correcting codes are required to have a reliable communication through a medium that has an unacceptable bit error rate and low signal to noise ratio in ieee 802 15 6 2 4ghz wireless body area network \( wban \) , data gets corrupted during the transmission and reception due to noises and interferences ultra low power operation is crucial to prolong the life of implantable devices hence simple block codes like bch \( 63 , 51 , 2 \) can be employed in the transceiver design of 802 15 6 narrowband phy in this paper , implementation of bch \( 63 , 51 , t 2 \) encoder and decoder using vhdl is discussed the incoming 51 bits are encoded into 63 bit code word using \( 63 , 51 \) bch encoder it can detect and correct up to 2 random errors the design of an encoder is implemented using linear feed back shift register \( lfsr \) for polynomial division and the decoder design is based on syndrome calculator , inversion less berlekamp massey algorithm \( bma \) and chien search algorithm synthesis and simulation were carried out using xilinx ise 14 2 and modelsim 10 1c the codes are implemented over virtex 4 fpga device and tested on dn8000k10pcie logic emulation board to the best of our knowledge , it is the first time an implementation of \( 63 , 51 \) bch encoder and decoder carried out
we study the problem of sensor placement in environments in which localization is a necessity , such as ad hoc wireless sensor networks that allow the placement of a few anchors that know their location or sensor arrays that are tracking a target in most of these situations , the quality of localization depends on the relative angle between the target and the pair of sensors observing it in this paper , we consider placing a small number of sensors which ensure good angular alpha coverage given alpha in 0 , pi 2 , for each target location t , there must be at least two sensors s 1 and s 2 such that the angle \( s 1 t s 2 \) is in the interval alpha , pi alpha one of the main difficulties encountered in such problems is that since the constraints depend on at least two sensors , building a solution must account for the inherent dependency between selected sensors , a feature that generic set cover techniques do not account for we introduce a general framework that guarantees an angular coverage that is arbitrarily close to alpha for any alpha pi 3 and apply it to a variety of problems to get bi criteria approximations when the angular coverage is required to be at least a constant fraction of alpha , we obtain results that are strictly better than what standard geometric set cover methods give when the angular coverage is required to be at least \( 1 1 delta \) cdot alpha , we obtain a mathcal o \( log delta \) approximation for sensor placement with alpha coverage on the plane in the presence of additional distance or visibility constraints , the framework gives a mathcal o \( log delta cdot log k opt \) approximation , where k opt is the size of the optimal solution we also use our framework to give a mathcal o \( log delta \) approximation that ensures \( 1 1 delta \) cdot alpha coverage and covers every target within distance 3r
social networks provide a new perspective for enterprises to better understand their customers and have attracted substantial attention in industry however , inferring high quality customer social networks is a great challenge while there are no explicit customer relations in many traditional oltp environments in this paper , we study this issue in the field of passenger transport and introduce a new member to the family of social networks , which is named co travel networks , consisting of passengers connected by their co travel behaviors we propose a novel method to infer high quality co travel networks of civil aviation passengers from their co booking behaviors derived from the pnrs \( passenger naming records \) in our method , to accurately evaluate the strength of ties , we present a measure of co journey times to count the co travel times of complete journeys between passengers we infer a high quality co travel network based on a large encrypted pnr dataset and conduct a series of network analyses on it the experimental results show the effectiveness of our inferring method , as well as some special characteristics of co travel networks , such as the sparsity and high aggregation , compared with other kinds of social networks it can be expected that such co travel networks will greatly help the industry to better understand their passengers so as to improve their services more importantly , we contribute a special kind of social networks with high strength of ties generated from very close and high cost travel behaviors , for further scientific researches on human travel behaviors , group travel patterns , high end travel market evolution , etc , from the perspective of social networks
this paper considers rateless network error correction codes for reliable multicast in the presence of adversarial errors most existing network error correction codes are designed for a given network capacity and maximum number of errors known a priori to the encoder and decoder however , in certain practical settings it may be necessary to operate without such a priori knowledge we present rateless coding schemes for two adversarial models , where the source sends more redundancy over time , until decoding succeeds the first model assumes there is a secret channel between the source and the destination that the adversaries cannot overhear the rate of the channel is negligible compared to the main network in the second model , instead of a secret channel , the source and destination share random secrets independent of the input information the amount of secret information required is negligible compared to the amount of information sent both schemes are optimal in that decoding succeeds with high probability when the total amount of information received by the sink satisfies the cut set bound with respect to the amount of message and error information the schemes are distributed , polynomial time and end to end in that other than the source and destination nodes , other intermediate nodes carry out classical random linear network coding
liouville numbers were the first class of real numbers which were proven to be transcendental it is easy to construct non normal liouville numbers kano and bugeaud have proved , using analytic techniques , that there are normal liouville numbers here , for a given base k 2 , we give two simple constructions of a liouville number which is normal to the base k the first construction is combinatorial , and is based on de bruijn sequences a real number in the unit interval is normal if and only if its finite state dimension is 1 we generalize our construction to prove that for any rational r in the closed unit interval , there is a liouville number with finite state dimension r this refines staiger 's result that the set of liouville numbers has constructive hausdorff dimension zero , showing a new quantitative classification of liouville numbers can be attained using finite state dimension in the second number theoretic construction , we use an arithmetic property of numbers the existence of primitive roots to construct liouville numbers normal in finitely many bases , assuming a generalized artin 's conjecture on primitive roots
the problem of joint detection and lossless source coding is considered we derive asymptotically optimal decision rules for deciding whether or not a sequence of observations has emerged from a desired information source , and to compress it if has in particular , our decision rules asymptotically minimize the cost of compression in the case that the data has been classified as `desirable' , subject to given constraints on the two kinds of the probability of error in another version of this performance criterion , the constraint on the false alarm probability is replaced by the a constraint on the cost of compression in the false alarm event we then analyze the asymptotic performance of these decision rules and demonstrate that they may exhibit certain phase transitions we also derive universal decision rules for the case where the underlying sources \( under either hypothesis or both \) are unknown , and training sequences from each source may or may not be available finally , we discuss how our framework can be extended in several directions
scientists in many fields have the common and fundamental problem of dimensionality reduction visualizing the underlying structure of the massive multivariate data in a low dimensional space in this work , a method based on the in tree \( it \) structure , called it map , is proposed , aiming to map data points from high to low dimensional space , meanwhile revealing the underlying cluster structure the visualized structure proves to be very useful for clustering analysis , for which only simple interactive operations are needed
this paper provides fundamental limits on the sample complexity of estimating dictionaries for tensor data by proving lower bounds on the minimax risk these lower bound depend on the dimensions of the tensor and parameters of the generative model the focus of this work is on k dimensional tensor data , with the underlying dictionaries constructed by taking the kronecker product of k smaller dictionaries and the observed data generated by sparse linear combinations of dictionary atoms observed through white gaussian noise in this regard , the paper provides a general lower bound on the minimax risk and also adapts the proof techniques for equivalent results using sparse and gaussian coefficient models furthermore , a special scenario of 2 dimensional tensor data is introduced and an algorithm is proposed to learn kronecker structured dictionaries consisting of 2 smaller dictionaries for this scenario it is shown that the algorithm achieves one of the provided minimax lower bounds the reported results suggest that the sample complexity of dictionary learning for tensor data can be significantly lower than that for unstructured data
the analysis and counting of blood cells in a microscope image can provide useful information concerning to the health of a person in particular , morphological analysis of red blood cells deformations can effectively detect important disease like malaria blood images , obtained by the microscope , which is coupled with a digital camera , are analyzed by the computer for diagnosis or can be transmitted easily to clinical centers than liquid blood samples automatic analysis system for the presence of plasmodium in microscopic image of blood can greatly help pathologists and doctors that typically inspect blood films manually unfortunately , the analysis made by human experts is not rapid and not yet standardized due to the operators capabilities and tiredness the paper shows how effectively and accurately it is possible to identify the plasmodium in the blood film in particular , the paper presents how to enhance the microscopic image and filter out the unnecessary segments followed by the threshold based segmentation and recognize the presence of plasmodium the proposed system can be deployed in the remote area as a supporting aid for telemedicine technology and only basic training is sufficient to operate it this system achieved more than 98 percentage accuracy for the samples collected to test this system
an n gon is defined as a sequence p \( v 0 , , v n 1 \) of n points on the plane an n gon p is said to be convex if the boundary of the convex hull of the set v 0 , , v n 1 of the vertices of p coincides with the union of the edges v 0 , v 1 , , v n 1 , v 0 if at that no three vertices of p are collinear then p is called strictly convex we prove that an n gon p with n ge3 is strictly convex if and only if a cyclic shift of the sequence \( al 0 , , al n 1 \) in 0 , 2 pi \) n of the angles between the x axis and the vectors v 1 v 0 , , v 0 v n 1 is strictly monotone a ``non strict'' version of this result is also proved
we consider secure multi terminal source coding problems in the presence of a public helper two main scenarios are studied 1 \) source coding with a helper where the coded side information from the helper is eavesdropped by an external eavesdropper 2 \) triangular source coding with a helper where the helper is considered as a public terminal we are interested in how the helper can support the source transmission subject to a constraint on the amount of information leaked due to its public nature we characterize the tradeoff between transmission rate , incurred distortion , and information leakage rate at the helper eavesdropper in the form of a rate distortion leakage region for various classes of problems
this paper builds on an innovative information retrieval tool , ariadne the tool has been developed as an interactive network visualization and browsing tool for large scale bibliographic databases it basically allows to gain insights into a topic by contextualizing a search query \( koopman et al , 2015 \) in this paper , we apply the ariadne tool to a far smaller dataset of 111 , 616 documents in astronomy and astrophysics labeled as the berlin dataset , this data have been used by several research teams to apply and later compare different clustering algorithms the quest for this team effort is how to delineate topics this paper contributes to this challenge in two different ways first , we produce one of the different cluster solution and second , we use ariadne \( the method behind it , and the interface called littleariadne \) to display cluster solutions of the different group members by providing a tool that allows the visual inspection of the similarity of article clusters produced by different algorithms , we present a complementary approach to other possible means of comparison more particular , we discuss how we can with littleariadne browse through the network of topical terms , authors , journals and cluster solutions in the berlin dataset and compare cluster solutions as well as see their context
we address the problem of computing distances between rankings that take into account similarities between candidates the need for evaluating such distances is governed by applications as diverse as rank aggregation , bioinformatics , social sciences and data storage the problem may be summarized as follows given two rankings and a positive cost function on transpositions that depends on the similarity of the candidates involved , find a smallest cost sequence of transpositions that converts one ranking into another our focus is on costs that may be described via special metric tree structures and on complete rankings modeled as permutations the presented results include a quadratic time algorithm for finding a minimum cost decomposition for simple cycles , and a quadratic time , 4 3 approximation algorithm for permutations that contain multiple cycles the proposed methods rely on investigating a newly introduced balancing property of cycles embedded in trees , cycle merging methods , and shortest path optimization techniques
genetic algorithms are a population based meta heuristics they have been successfully applied to many optimization problems however , premature convergence is an inherent characteristic of such classical genetic algorithms that makes them incapable of searching numerous solutions of the problem domain a memetic algorithm is an extension of the traditional genetic algorithm it uses a local search technique to reduce the likelihood of the premature convergence the cryptanalysis of simplified data encryption standard can be formulated as np hard combinatorial problem in this paper , a comparison between memetic algorithm and genetic algorithm were made in order to investigate the performance for the cryptanalysis on simplified data encryption standard problems \( sdes \) the methods were tested and various experimental results show that memetic algorithm performs better than the genetic algorithms for such type of np hard combinatorial problem this paper represents our first effort toward efficient memetic algorithm for the cryptanalysis of sdes
when data analysts train a classifier and check if its accuracy is significantly different from random guessing , they are implicitly and indirectly performing a hypothesis test \( two sample testing \) and it is of importance to ask whether this indirect method for testing is statistically optimal or not given that hypothesis tests attempt to maximize statistical power subject to a bound on the allowable false positive rate , while prediction attempts to minimize statistical risk on future predictions on unseen data , we wish to study whether a predictive approach for an ultimate aim of testing is prudent we formalize this problem by considering the two sample mean testing setting where one must determine if the means of two gaussians \( with known and equal covariance \) are the same or not , but the analyst indirectly does so by checking whether the accuracy achieved by fisher 's lda classifier is significantly different from chance or not unexpectedly , we find that the asymptotic power of lda 's sample splitting classification accuracy is actually minimax rate optimal in terms of problem dependent parameters since prediction is commonly thought to be harder than testing , it might come as a surprise to some that solving a harder problem does not create a information theoretic bottleneck for the easier one on the flip side , even though the power is rate optimal , our derivation suggests that it may be worse by a small constant factor hence practitioners must be wary of using \( admittedly flexible \) prediction methods on disguised testing problems
methods for digital , phase coherent acoustic communication date to at least the work of stojanjovic , et al 20 , and the added robustness afforded by improved phase tracking and compensation of johnson , et al 21 this work explores the use of such methods for communications through tissue for potential biomedical applications , using the tremendous bandwidth available in commercial medical ultrasound transducers while long range ocean acoustic experiments have been at rates of under 100kbps , typically on the order of 1 10kbps , data rates in excess of 120mb s have been achieved over cm scale distances in ultrasonic testbeds 19 this paper describes experimental transmission of digital communication signals through samples of real pork tissue and beef liver , achieving data rates of 20 30mbps , demonstrating the possibility of real time video rate data transmission through tissue for inbody ultrasonic communications with implanted medical devices
in this paper , we present several mathematical properties that the solutions to darcy and darcy brinkman equations satisfy these properties can serve as robust a posteriori error estimation techniques to verify numerical solutions for these equations the mathematical properties include the total minimum mechanical power , the minimum dissipation theorem , a reciprocal relation , and a maximum principle for the vorticity in particular , we show that , for a given set of boundary conditions , darcy velocity has the minimum total mechanical power of all the kinematically admissible vector fields we will also show that a similar result holds for a darcy brinkman velocity we also show that for a conservative body force , the darcy velocity and the darcy brinkman velocity have the minimum total dissipation among their respective kinematically admissible vector fields using numerical examples , we show that the minimum dissipation theorem can be utilized identify pollution errors in numerical solutions we then show that the solutions to darcy and darcy brinkman equations satisfy a reciprocal relation , which has the potential to identify errors in the numerical implementation of boundary conditions we also show that the vorticity under both steady and transient darcy brinkman equations satisfy maximum principles if the body force is conservative body force and the permeability is homogeneous and isotropic a discussion on the nature of vorticity under steady and transient darcy equations is also presented using several numerical examples , we will demonstrate the predictive capabilities of the proposed a posteriori techniques in assessing the accuracy of numerical solutions for a general class of problems , which could involve complex domains and general computational grids
there are two ways to describe the interaction between classical and quantum information categorically one based on completely positive maps between frobenius algebras , the other using symmetric monoidal 2 categories this paper makes a first step towards combining the two the integrated approach allows a unified description of quantum teleportation and classical encryption in a single 2 category , as well as a universal security proof applicable simultaneously to both scenarios
in this paper , we study a polynomial decomposition model that arises in problems of system identification , signal processing and machine learning we show that this decomposition is a special case of the x rank decomposition a powerful novel concept in algebraic geometry that generalizes the tensor cp decomposition we prove new results on generic maximal rank and on identifiability of the polynomial decomposition model in the paper , we try to make results and basic tools accessible for a general audience \( assuming no knowledge of algebraic geometry or its prerequisites \)
let t 1 , ldots , t n l in mathbb r d and p 1 , ldots , p n s in mathbb r d and consider the bipartite location recovery problem given a subset of pairwise direction observations \( t i p j \) t i p j 2 i , j in n l times n s , where a constant fraction of these observations are arbitrarily corrupted , find t i i in n ll and p j j in n s up to a global translation and scale we study the recently introduced shapefit algorithm as a method for solving this bipartite location recovery problem in this case , shapefit consists of a simple convex program over d \( n l n s \) real variables we prove that this program recovers a set of n l n s i i d gaussian locations exactly and with high probability if the observations are given by a bipartite erd h o s r ' e nyi graph , d is large enough , and provided that at most a constant fraction of observations involving any particular location are adversarially corrupted this recovery theorem is based on a set of deterministic conditions that we prove are sufficient for exact recovery finally , we propose a modified pipeline for the structure for motion problem , based on this bipartite location recovery problem
high rate space time block codes \( stbc with code rate 1 \) in multi input multi output \( mimo \) systems are able to provide both spatial multiplexing gain and diversity gain , but have high maximum likelihood \( ml \) decoding complexity since group decodable \( quasi orthogonal \) code structure can reduce the decoding complexity , we present in this paper systematic methods to construct group decodable high rate stbc with full symbol wise diversity gain for arbitrary transmit antenna number and code length we show that the proposed group decodable stbc can achieve high code rate that increases almost linearly with the transmit antenna number , and the slope of this near linear dependence increases with the code length comparisons with existing low rate and high rate codes \( such as orthogonal stbc and algebraic stbc \) are conducted to show the decoding complexity reduction and good code performance achieved by the proposed codes
this paper studies an incentive structure for cooperation and its stability in peer assisted services when there exist multiple content providers , using a coalition game theoretic approach we first consider a generalized coalition structure consisting of multiple providers with many assisting peers , where peers assist providers to reduce the operational cost in content distribution to distribute the profit from cost reduction to players \( i e , providers and peers \) , we then establish a generalized formula for individual payoffs when a shapley like payoff mechanism is adopted we show that the grand coalition is unstable , even when the operational cost functions are concave , which is in sharp contrast to the recently studied case of a single provider where the grand coalition is stable we also show that irrespective of stability of the grand coalition , there always exist coalition structures which are not convergent to the grand coalition our results give us an important insight that a provider does not tend to cooperate with other providers in peer assisted services , and be separated from them to further study the case of the separated providers , three examples are presented \( i \) underpaid peers , \( ii \) service monopoly , and \( iii \) oscillatory coalition structure our study opens many new questions such as realistic and efficient incentive structures and the tradeoffs between fairness and individual providers' competition in peer assisted services
we survey existing scheduling hypotheses made in the literature in self stabilization , commonly referred to under the notion of daemon we show that four main characteristics \( distribution , fairness , boundedness , and enabledness \) are enough to encapsulate the various differences presented in existing work our naming scheme makes it easy to compare daemons of particular classes , and to extend existing possibility or impossibility results to new daemons we further examine existing daemon transformer schemes and provide the exact transformed characteristics of those transformers in our taxonomy
grouse \( grassmannian rank one update subspace estimation \) is an incremental algorithm for identifying a subspace of rn from a sequence of vectors in this subspace , where only a subset of components of each vector is revealed at each iteration recent analysis has shown that grouse converges locally at an expected linear rate , under certain assumptions grouse has a similar flavor to the incremental singular value decomposition algorithm , which updates the svd of a matrix following addition of a single column in this paper , we modify the incremental svd approach to handle missing data , and demonstrate that this modified approach is equivalent to grouse , for a certain choice of an algorithmic parameter
the problem of autonomous launch and landing of a tethered rigid aircraft for airborne wind energy generation is addressed the system operates with ground based power conversion and pumping cycles , where the tether is repeatedly reeled in and out of a winch installed on the ground and linked to an electric motor generator in order to accelerate the aircraft to take off speed , the ground station is augmented with a linear motion system composed by a slide translating on rails and controlled by a second motor an onboard propeller is used to sustain the forward velocity during the ascend of the aircraft during landing , a slight tension on the line is kept , while the onboard control surfaces are used to align the aircraft with the rails and to land again on them a model based , decentralized control approach is proposed , capable to carry out a full cycle of launch , low tension flight , and landing again on the rails the derived controller is tested via numerical simulations with a realistic dynamical model of the system , in presence of different wind speeds and turbulence , and its performance in terms of landing accuracy is assessed this study is part of a project aimed to experimentally verify the launch and landing approach on a small scale prototype
the index coding problem involves a sender with k messages to be transmitted across a broadcast channel , and a set of receivers each of which demands a subset of the k messages while having prior knowledge of a different subset as side information we consider the specific instance of noisy index coding where the broadcast channel is gaussian and every receiver demands all the messages from the source we construct lattice index codes for this channel by encoding the k messages individually using k modulo lattice constellations and transmitting their sum modulo a shaping lattice we introduce a design metric called side information gain that measures the advantage of a code in utilizing the side information at the receivers , and hence its goodness as an index code based on the chinese remainder theorem , we then construct lattice index codes with large side information gains using lattices over the following principal ideal domains rational integers , gaussian integers , eisenstein integers , and the hurwitz quaternions among all lattice index codes constructed using any densest lattice of a given dimension , our codes achieve the maximum side information gain
collaborations and citations within scientific research grow simultaneously and interact dynamically modelling the coevolution between them helps to study many phenomena that can be approached only through combining citation and coauthorship data a geometric graph for the coevolution is proposed , the mechanism of which synthetically expresses the interactive impacts of authors and papers in a geometrical way the model is validated against a data set of papers published in pnas during 2000 2015 the validation shows the ability to reproduce a range of features observed with citation and coauthorship data combined and separately particulary , in the empirical distribution of citations per author there exist two limits , in which the distribution appears as a generalized poisson and a power law respectively our model successfully reproduces the shape of the distribution , and provides an explanation for how the shape emerges the model also captures the empirically positive correlations between the numbers of authors' papers , citations and collaborators
we revisit the problem of inferring the overall ranking among entities in the framework of bradley terry luce \( btl \) model , based on available empirical data on pairwise preferences by a simple transformation , we can cast the problem as that of solving a noisy linear system , for which a ready algorithm is available in the form of the randomized kaczmarz method this scheme is provably convergent , has excellent empirical performance , and is amenable to on line , distributed and asynchronous variants convergence , convergence rate , and error analysis of the proposed algorithm are presented and several numerical experiments are conducted whose results validate our theoretical findings
this work gives a simultaneous analysis of both the ordinary least squares estimator and the ridge regression estimator in the random design setting under mild assumptions on the covariate response distributions in particular , the analysis provides sharp results on the ``out of sample'' prediction error , as opposed to the ``in sample'' \( fixed design \) error the analysis also reveals the effect of errors in the estimated covariance structure , as well as the effect of modeling errors , neither of which effects are present in the fixed design setting the proofs of the main results are based on a simple decomposition lemma combined with concentration inequalities for random vectors and matrices
although recent studies show that both topological structures and human dynamics can strongly affect information spreading on social networks , the complicated interplay of the two significant factors has not yet been clearly described in this work , we find a strong pairwise interaction based on analyzing the weighted network generated by the short message communication dataset within a chinese tele communication provider the pairwise interaction bridges the network topological structure and human interaction dynamics , which can promote local information spreading between pairs of communication partners and in contrast can also suppress global information \( e g , rumor \) cascade and spreading in addition , the pairwise interaction is the basic pattern of group conversations and it can greatly reduce the waiting time of communication events between a pair of intimate friends our findings are also helpful for communication operators to design novel tariff strategies and optimize their communication services
it is well known that quantum codes can be constructed through classical symplectic self orthogonal codes in this paper , we give a kind of gilbert varshamov bound for symplectic self orthogonal codes first and then obtain the gilbert varshamov bound for quantum codes the idea of obtaining the gilbert varshamov bound for symplectic self orthogonal codes follows from counting arguments
we introduce a new class of games where each player 's aim is to randomise her strategic choices in order to affect the other players' expectations aside from her own the way each player intends to exert this influence is expressed through a boolean combination of polynomial equalities and inequalities with rational coefficients we offer a logical representation of these games as well as a computational study of the existence of equilibria
correlated sources are present in communication systems where protocols ensure that there is some predetermined information for sources here correlated sources across an eavesdropped channel that incorporate a heterogeneous encoding scheme and their effect on the information leakage when some channel information and a source have been wiretapped is investigated the information leakage bounds for the slepian wolf scenario are provided thereafter , the shannon cipher system approach is presented further , an implementation method using a matrix partition approach is described
we consider the weighted version of the tron game on graphs where two players , alice and bob , each build their own path by claiming one vertex at a time , starting with alice the vertices carry non negative weights that sum up to 1 and either player tries to claim a path with larger total weight than the opponent we show that if the graph is a tree then alice can always ensure to get at most 1 5 less than bob , and that there exist trees where bob can ensure to get at least 1 5 more than alice
carrier sense multiple access \( csma \) protocols have been shown to reach the full capacity region for data communication in wireless networks , with polynomial complexity however , current literature achieves the throughput optimality with an exponential delay scaling with the network size , even in a simplified scenario for transmission jobs with uniform sizes although csma protocols with order optimal average delay have been proposed for specific topologies , no existing work can provide worst case delay guarantee for each job in general network settings , not to mention the case when the jobs have non uniform lengths while the throughput optimality is still targeted in this paper , we tackle on this issue by proposing a two timescale csma based data communication protocol with dynamic decisions on rate control , link scheduling , job transmission and dropping in polynomial complexity through rigorous analysis , we demonstrate that the proposed protocol can achieve a throughput utility arbitrarily close to its offline optima for jobs with non uniform sizes and worst case delay guarantees , with a tradeoff of longer maximum allowable delay
this paper describes a minimax state estimation approach for linear differential algebraic equations \( dae \) with uncertain parameters the approach addresses continuous time dae with non stationary rectangular matrices and uncertain bounded deterministic input an observation 's noise is supposed to be random with zero mean and unknown bounded correlation function main results are a generalized kalman duality \( gkd \) principle and sub optimal minimax state estimation algorithm gkd is derived by means of young fenhel duality theorem gkd proves that the minimax estimate coincides with a solution to a dual control problem \( dcp \) with dae constraints the latter is ill posed and , therefore , the dcp is solved by means of tikhonov regularization approach resulting a sub optimal state estimation algorithm in the form of filter we illustrate the approach by an synthetic example and we discuss connections with impulse observability
this is an elementary introduction to the hodge laplacian on a graph , a higher order generalization of the graph laplacian we will discuss basic properties including cohomology and hodge theory at the end we will also discuss the nonlinear laplacian on a graph , a nonlinear generalization of the graph laplacian as its name implies these generalized laplacians will be constructed out of coboundary operators , i e , discrete analogues of exterior derivatives the main feature of our approach is simplicity this article requires only knowledge of linear algebra and graph theory
we consider a general honest homogeneous continuous time markov process with restarts the process is forced to restart from a given distribution at time moments generated by an independent poisson process the motivation to study such processes comes from modeling human and animal mobility patterns , restart processes in communication protocols , and from application of restarting random walks in information retrieval we provide a connection between the transition probability functions of the original markov process and the modified process with restarts we give closed form expressions for the invariant probability measure of the modified process when the process evolves on the euclidean space there is also a closed form expression for the moments of the modified process we show that the modified process is always positive harris recurrent and exponentially ergodic with the index equal to \( or bigger than \) the rate of restarts finally , we illustrate the general results by the standard and geometric brownian motions
two way relaying can significantly improve performance of next generation wireless networks however , due to its dependence on multi node cooperation and transmission coordination , applying this technique to a wireless network in an effective and scalable manner poses a challenging problem to tackle this problem without relying on complicated scheduling or network optimization algorithms , we propose a scalable random access scheme that takes measures in both the physical layer and the medium access control layer specifically , we propose a two way relaying technique that supports fully asynchronous transmission and is modulation independent it also assumes no priori knowledge of channel conditions on the top of this new physical layer technique , a random access mac protocol is designed to dynamically form two way relaying cooperation in a wireless network to evaluate the scalable random access scheme , both theoretical analysis and simulations are carried out performance results illustrate that our scheme has achieved the goal of scalable two way relaying in a wireless network and significantly outperforms csma ca protocol
it is essential to find solar predictive methods to massively insert renewable energies on the electrical distribution grid the goal of this study is to find the best methodology allowing predicting with high accuracy the hourly global radiation the knowledge of this quantity is essential for the grid manager or the private pv producer in order to anticipate fluctuations related to clouds occurrences and to stabilize the injected pv power in this paper , we test both methodologies single and hybrid predictors in the first class , we include the multi layer perceptron \( mlp \) , auto regressive and moving average \( arma \) , and persistence models in the second class , we mix these predictors with bayesian rules to obtain ad hoc models selections , and bayesian averages of outputs related to single models if mlp and arma are equivalent \( nrmse close to 40 5 for the both \) , this hybridization allows a nrmse gain upper than 14 percentage points compared to the persistence estimation \( nrmse 37 versus 51 \)
we propose a randomized nonmonotone block proximal gradient \( rnbpg \) method for minimizing the sum of a smooth \( possibly nonconvex \) function and a block separable \( possibly nonconvex nonsmooth \) function at each iteration , this method randomly picks a block according to any prescribed probability distribution and solves typically several associated proximal subproblems that usually have a closed form solution , until a certain progress on objective value is achieved in contrast to the usual randomized block coordinate descent method 23 , 20 , our method has a nonmonotone flavor and uses variable stepsizes that can partially utilize the local curvature information of the smooth component of objective function we show that any accumulation point of the solution sequence of the method is a stationary point of the problem it almost surely and the method is capable of finding an approximate stationary point with high probability we also establish a sublinear rate of convergence for the method in terms of the minimal expected squared norm of certain proximal gradients over the iterations when the problem under consideration is convex , we show that the expected objective values generated by rnbpg converge to the optimal value of the problem under some assumptions , we further establish a sublinear and linear rate of convergence on the expected objective values generated by a monotone version of rnbpg finally , we conduct some preliminary experiments to test the performance of rnbpg on the ell 1 regularized least squares problem and a dual svm problem in machine learning the computational results demonstrate that our method substantially outperforms the randomized block coordinate it descent method with fixed or variable stepsizes
we propose a general framework for the construction and analysis of minimax estimators for a wide class of functionals of discrete distributions , where the alphabet size s is unknown and may be scaling with the number of observations n we treat the respective regions where the functional is nonsmooth and smooth separately in the nonsmooth regime , we apply an unbiased estimator for the best polynomial approximation of the functional whereas , in the smooth regime , we apply a bias corrected version of the maximum likelihood estimator \( mle \) we illustrate the merit of this approach by thoroughly analyzing the performance of the resulting schemes for estimating two important information measures the entropy h \( p \) sum i 1 s p i ln p i and f alpha \( p \) sum i 1 s p i alpha , alpha 0 we obtain the minimax l 2 rates for estimating these functionals in particular , we demonstrate that our estimator achieves the optimal sample complexity n theta \( s ln s \) for entropy estimation we also demonstrate that the sample complexity for estimating f alpha \( p \) , 0 alpha 1 is theta \( s 1 alpha ln s \) , which can be achieved by our estimator but not the mle for 1 alpha 3 2 , we show the minimax l 2 rate for estimating f alpha \( p \) is \( n ln n \) 2 \( alpha 1 \) regardless of the alphabet size , while the exact l 2 rate for the mle is n 2 \( alpha 1 \) for all the above cases , the behavior of the optimal estimators with n samples is essentially that of the mle with n ln n samples we highlight the practical advantages of our schemes for the estimation of entropy and mutual information we compare our performance with the popular mle and with the order optimal entropy estimator of valiant and valiant as we illustrate with a few experiments , our approach reduces running time and boosts the accuracy
for the random 2 sat formula f \( n , p \) , let f c \( n , p \) be the formula left after the pure literal algorithm applied to f \( n , p \) stops using the recently developed poisson cloning model together with the cut off line algorithm \( cola \) , we completely analyze the structure of f c \( n , p \) in particular , it is shown that , for gl p \( 2n 1 \) 1 gs with gs gg n 1 3 , the core of f \( n , p \) has thl 2 n o \( \( thl n \) 1 2 \) variables and thl 2 gl n o \( \( thl n \) \) 1 2 clauses , with high probability , where thl is the larger solution of the equation th \( 1 e thl gl \) 0 we also estimate the probability of f \( n , p \) being satisfiable to obtain pr f 2 \( n , sfrac gl 2n 1 \) is satisfiable caseth 1 frac 1 o \( 1 \) 16 gs 3 n if gl 1 gs with gs gg n 1 3 e theta \( gs 3n \) if gl 1 gs with gs gg n 1 3 , where o \( 1 \) goes to 0 as gs goes to 0 this improves the bounds of bollob 'as et al cite bbckw
this paper presents a tutorial style review on the recent results about the disturbance observer \( dob \) in view of robust stabilization and recovery of the nominal performance the analysis is based on the case when the bandwidth of q filter is large , and it is explained in a pedagogical manner that , even in the presence of plant uncertainties and disturbances , the behavior of real uncertain plant can be made almost similar to that of disturbance free nominal system both in the transient and in the steady state the conventional dob is interpreted in a new perspective , and its restrictions and extensions are discussed
relativistic cryptography exploits the fact that no information can travel faster than the speed of light in order to obtain security guarantees that cannot be achieved from the laws of quantum mechanics alone recently , lunghi et al phys rev lett 2015 presented a bit commitment scheme where each party uses two agents that exchange classical information in a synchronized fashion , and that is both hiding and binding a caveat is that the commitment time is intrinsically limited by the spatial configuration of the players , and increasing this time requires the agents to exchange messages during the whole duration of the protocol while such a solution remains computationally attractive , its practicality is severely limited in realistic settings since all communication must remain perfectly synchronized at all times in this work , we introduce a robust protocol for relativistic bit commitment that tolerates failures of the classical communication network this is done by adding a third agent to both parties our scheme provides a quadratic improvement in terms of expected sustain time compared to the original protocol , while retaining the same level of security
we analyze nonbinary spatially coupled low density parity check \( sc ldpc \) codes built on the general linear group for transmission over the binary erasure channel we prove threshold saturation of the belief propagation decoding to the potential threshold , by generalizing the proof technique based on potential functions recently introduced by yedla et al the existence of the potential function is also discussed for a vector sparse system in the general case , and some existence conditions are developed we finally give density evolution and simulation results for several nonbinary sc ldpc code ensembles
user contributions in the form of posts , comments , and votes are essential to the success of online communities however , allowing user participation also invites undesirable behavior such as trolling in this paper , we characterize antisocial behavior in three large online discussion communities by analyzing users who were banned from these communities we find that such users tend to concentrate their efforts in a small number of threads , are more likely to post irrelevantly , and are more successful at garnering responses from other users studying the evolution of these users from the moment they join a community up to when they get banned , we find that not only do they write worse than other users over time , but they also become increasingly less tolerated by the community further , we discover that antisocial behavior is exacerbated when community feedback is overly harsh our analysis also reveals distinct groups of users with different levels of antisocial behavior that can change over time we use these insights to identify antisocial users early on , a task of high practical importance to community maintainers
presented approach in polynomial time calculates large number of invariants for each vertex , which wo n't change with graph isomorphism and should fully determine the graph for example numbers of closed paths of length k for given starting vertex , what can be though as the diagonal terms of k th power of the adjacency matrix for k 2 we would get degree of verities invariant , higher describes local topology deeper now if two graphs are isomorphic , they have the same set of such vectors of invariants we can sort theses vectors lexicographically and compare them if they agree , permutations from sorting allow to reconstruct the isomorphism i'm presenting arguments that these invariants should fully determine the graph , but unfortunately i ca n't prove it in this moment this approach can give hope , that maybe p np instead of checking all instances , we should make arithmetics on these large numbers
data mining techniques have frequently been developed for spontaneous reporting databases these techniques aim to find adverse drug events accurately and efficiently spontaneous reporting databases are prone to missing information , under reporting and incorrect entries this often results in a detection lag or prevents the detection of some adverse drug events these limitations do not occur in electronic health care databases in this paper , existing methods developed for spontaneous reporting databases are implemented on both a spontaneous reporting database and a general practice electronic health care database and compared the results suggests that the application of existing methods to the general practice database may help find signals that have gone undetected when using the spontaneous reporting system database in addition the general practice database provides far more supplementary information , that if incorporated in analysis could provide a wealth of information for identifying adverse events more accurately
in this letter , we present an end to end performance analysis of dual hop project and forward relaying in a realistic scenario , where the source relay and the relay destination links are experiencing mimo pinhole and rayleigh channel conditions , respectively we derive the probability density function of both the relay post processing and the end to end signal to noise ratios , and the obtained expressions are used to derive the outage probability of the analyzed system as well as its end to end ergodic capacity in terms of generalized functions applying then the residue theory to mellin barnes integrals , we infer the system asymptotic behavior for different channel parameters as the bivariate meijer g function is involved in the analysis , we propose a new and fast matlab implementation enabling an automated definition of the complex integration contour extensive monte carlo simulations are invoked to corroborate the analytical results
in this paper , we propose a distributive queueaware intra cell user scheduling and inter cell interference \( ici \) management control design for a delay optimal celluar downlink system with m base stations \( bss \) , and k users in each cell each bs has k downlink queues for k users respectively with heterogeneous arrivals and delay requirements the ici management control is adaptive to joint queue state information \( qsi \) over a slow time scale , while the user scheduling control is adaptive to both the joint qsi and the joint channel state information \( csi \) over a faster time scale we show that the problem can be modeled as an infinite horizon average cost partially observed markov decision problem \( pomdp \) , which is np hard in general by exploiting the special structure of the problem , we shall derive an equivalent bellman equation to solve the pomdp problem to address the distributive requirement and the issue of dimensionality and computation complexity , we derive a distributive online stochastic learning algorithm , which only requires local qsi and local csi at each of the m bss we show that the proposed learning algorithm converges almost surely \( with probability 1 \) and has significant gain compared with various baselines the proposed solution only has linear complexity order o \( mk \)
effective capacity , which provides the maximum constant arrival rate that a given service process can support while satisfying statistical delay constraints , is analyzed in a multiuser scenario in particular , we study the achievable effective capacity region of the users in multiaccess fading channels \( mac \) in the presence of quality of service \( qos \) constraints we assume that channel side information \( csi \) is available at both the transmitters and the receiver , and superposition coding technique with successive decoding is used when the power is fixed at the transmitters , we show that varying the decoding order with respect to the channel state can significantly increase the achievable throughput region for a two user case , we obtain the optimal decoding strategy when the users have the same qos constraints meanwhile , it is shown that time division multiple access \( tdma \) can achieve better performance than superposition coding with fixed successive decoding order at the receiver side for certain qos constraints for power and rate adaptation , we determine the optimal power allocation policy with fixed decoding order at the receiver side numerical results are provided to demonstrate our results
this paper deals with denial of service attack overview of the existing attacks and methods is proposed classification scheme is presented for a different denial of service attacks there is considered agent based intrusion detection systems architecture considered main components and working principles for a systems of such kind
in this paper , we propose a selection region based multihop routing protocol with directional antennas for wireless mobile ad hoc networks , where the selection region is defined by two parameters a reference distance and the beamwidth of the directional antenna at each hop , we choose the nearest node to the transmitter within the selection region as the next hop relay by maximizing the expected density of progress , we present an upper bound for the optimum reference distance and derive the relationship between the optimum reference distance and the optimum transmission probability compared with the results with routing strategy using omnidirectional antennas in cite di relay region , we find interestingly that the optimum transmission probability is a constant independent of the beamwidth , the expected density of progress with the new routing strategy is increased significantly , and the computational complexity involved in the relay selection is also greatly reduced
fix g 1 every graph of large enough tree width contains a g x g grid as a minor but here we prove that every four edge connected graph of large enough tree width contains a g x g grid as an immersion \( and hence contains any fixed graph with maximum degree at most four as an immersion \) this result has a number of applications
in this work , we consider a method of searching of the direction of a wireless network development \( the places of new access points or base stations etc \) optimized with criteria of coverage of important territories and minimum cost of equipment and additional needed infrastructure which does not need the execution of special field testings and determination of the exact geometry of elements of rf propagation medium and their rf absorbing properties but takes into account the minimum accessible information obtained from built in measuring instruments of wireless hardware and approximate data of the medium elements shape the problem of search of a disposition and types of the infrastructure elements of the growing network is formulated as a multicriteria discrete constrained optimization problem solvable with variant probability method 1 the problem of a medium rf propagation properties modeling is also formulated and solved as a discrete optimization task
we calculate an extensive set of characteristics for internet as topologies extracted from the three data sources most frequently used by the research community traceroutes , bgp , and whois we discover that traceroute and bgp topologies are similar to one another but differ substantially from the whois topology among the widely considered metrics , we find that the joint degree distribution appears to fundamentally characterize internet as topologies as well as narrowly define values for other important metrics we discuss the interplay between the specifics of the three data collection mechanisms and the resulting topology views in particular , we show how the data collection peculiarities explain differences in the resulting joint degree distributions of the respective topologies finally , we release to the community the input topology datasets , along with the scripts and output of our calculations this supplement should enable researchers to validate their models against real data and to make more informed selection of topology data sources for their specific needs
recently an increasing amount of research is devoted to the question of how the most influential nodes \( seeds \) can be found effectively in a complex network there are a number of measures proposed for this purpose , for instance , high degree centrality measure reflects the importance of the network topology and has a reasonable runtime performance to find a set of nodes with highest degree , but they do not have a satisfactory dissemination potentiality in the network due to having many common neighbors \( mbox cn \( 1 \) \) and common neighbors of neighbors \( mbox cn \( 2 \) \) this flaw holds in other measures as well in this paper , we compare high degree centrality measure with other well known measures using ten datasets in order to find a proportion for the common seeds in the seed sets obtained by them we , thereof , propose an improved high degree centrality measure \( named textit degreedistance \) and improve it to enhance accuracy in two phases , fidd and sidd , by put a threshold on the number of common neighbors of already selected seed nodes and a non seed node which is under investigation to be selected as a seed as well as considering the influence score of seed nodes directly or through their common neighbors over the non seed node to evaluate the accuracy and runtime performance of degreedistance , fidd , and sidd , they are applied to eight large scale networks and it finally turns out that sidd dramatically outperforms other well known measures and evinces comparatively more accurate performance in identifying the most influential nodes
in this paper , pilot symbol assisted transmission in cognitive radio systems over time selective flat fading channels is studied it is assumed that causal and noncausal wiener filter estimators are used at the secondary receiver with the aid of training symbols to obtain the channel side information \( csi \) under an interference power constraint cognitive radio model is described together with detection and false alarm probabilities determined by using a neyman person detector for channel sensing subsequently , for both filters , the variances of estimate errors are calculated from the doppler power spectrum of the channel , and achievable rate expressions are provided considering the scenarios which are results of channel sensing numerical results are obtained in gauss markov modeled channels , and achievable rates obtained by using causal and noncausal filters are compared and it is shown that the difference is decreasing with increasing signal to noise ratio \( snr \) moreover , the optimal probability of detection and false alarm values are shown , and the tradeoff between these two parameters is discussed finally , optimal power distributions are provided
in this work , a new bayesian framework for ofdm channel estimation is proposed using jaynes' maximum entropy principle to derive prior information , we successively tackle the situations when only the channel delay spread is a priori known , then when it is not known exploitation of the time frequency dimensions are also considered in this framework , to derive the optimal channel estimation associated to some performance measure under any state of knowledge simulations corroborate the optimality claim and always prove as good or better in performance than classical estimators
in this paper , a method to generate permutations of a string under a set of constraints decided by the user is presented the required permutations are generated without generating all the permutations
we study sparse principal components analysis in the high dimensional setting , where p \( the number of variables \) can be much larger than n \( the number of observations \) we prove optimal , non asymptotic lower and upper bounds on the minimax estimation error for the leading eigenvector when it belongs to an ell q ball for q in 0 , 1 our bounds are sharp in p and n for all q in 0 , 1 over a wide class of distributions the upper bound is obtained by analyzing the performance of ell q constrained pca in particular , our results provide convergence rates for ell 1 constrained pca
most models in machine learning contain at least one hyperparameter to control for model complexity choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging in this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information an advantage of this method is that hyperparameters can be updated before model parameters have fully converged we also give sufficient conditions for the global convergence of this method , based on regularity conditions of the involved functions and summability of errors finally , we validate the empirical performance of this method on the estimation of regularization constants of l2 regularized logistic regression and kernel ridge regression empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods
collaborative filtering is an effective recommendation technique wherein the preference of an individual can potentially be predicted based on preferences of other members early algorithms often relied on the strong locality in the preference data , that is , it is enough to predict preference of a user on a particular item based on a small subset of other users with similar tastes or of other items with similar properties more recently , dimensionality reduction techniques have proved to be equally competitive , and these are based on the co occurrence patterns rather than locality this paper explores and extends a probabilistic model known as boltzmann machine for collaborative filtering tasks it seamlessly integrates both the similarity and co occurrence in a principled manner in particular , we study parameterisation options to deal with the ordinal nature of the preferences , and propose a joint modelling of both the user based and item based processes experiments on moderate and large scale movie recommendation show that our framework rivals existing well known methods
this paper studies the problem of line spectral estimation in the continuum of a bounded interval with one snapshot of array measurement the single snapshot measurement data is turned into a hankel data matrix which admits the vandermonde decomposition and is suitable for the music algorithm the music algorithm amounts to finding the null space \( the noise space \) of the hankel matrix , forming the noise space correlation function and identifying the s smallest local minima of the noise space correlation as the frequency set in the noise free case exact reconstruction is guaranteed for any arbitrary set of frequencies as long as the number of measurements is at least twice the number of distinct frequencies to be recovered in the presence of noise the stability analysis shows that the perturbation of the noise space correlation is proportional to the spectral norm of the noise matrix as long as the latter is smaller than the smallest \( nonzero \) singular value of the noiseless hankel data matrix under the assumption that frequencies are separated by at least twice the rayleigh length \( rl \) , the stability of the noise space correlation is proved by means of novel discrete ingham inequalities which provide bounds on nonzero singular values of the noiseless hankel data matrix the numerical performance of music is tested in comparison with other algorithms such as blo omp and sdp \( tv min \) while blo omp is the stablest algorithm for frequencies separated above 4 rl , music becomes the best performing one for frequencies separated between 2 rl and 3 rl also , music is more efficient than other methods music truly shines when the frequency separation drops to 1 rl or below when all other methods fail indeed , the resolution length of music decreases to zero as noise decreases to zero as a power law with an exponent much smaller than an upper bound established by donoho
we consider the massive multiple input multiple output \( mimo \) downlink with maximum ratio and zero forcing processing and time division duplex \( tdd \) operation to decode , the terminals must know their instantaneous effective channel gain conventionally , it is assumed that by virtue of channel hardening , this instantaneous gain is close to its average and hence that terminals can rely on knowledge of that average \( also known as statistical channel information \) however , in some propagation environments , such as keyhole channels , channel hardening does not hold we propose a blind algorithm to estimate the effective channel gain at each user , that does not require any downlink pilots we derive a capacity lower bound of each user for our proposed scheme , applicable to any propagation channel compared to the case of no downlink pilots \( relying on channel hardening \) , and compared to training based estimation using downlink pilots , our blind algorithm performs significantly better the difference is especially pronounced in environments that do not offer channel hardening
the textbook by doets and van eijck puts the haskell programming language systematically to work for presenting a major piece of logic and mathematics the reader is taken through chapters on basic logic , proof recipes , sets and lists , relations and functions , recursion and co recursion , the number systems , polynomials and power series , ending with cantor 's infinities the book uses haskell for the executable and strongly typed manifestation of various mathematical notions at the level of declarative programming the book adopts a systematic but relaxed mathematical style \( definition , example , exercise , \) the text is very pleasant to read due to a small amount of anecdotal information , and due to the fact that definitions are fluently integrated in the running text an important goal of the book is to get the reader acquainted with reasoning about programs
f boxes defined in 6 as hyper boxes in n infty discrete space were applied here for the geometric description of the cobweb posetes hasse diagrams tilings the f boxes edges sizes are taken to be values of terms of natural numbers' valued sequence f the problem of partitions of hyper boxes represented by graphs into blocks of special form is considered and these are to be called f tilings the proof of such tilings' existence for certain sub family of admissible sequences f is delivered the family of f tilings which we consider here includes among others f natural numbers , fibonacci numbers , gaussian integers with their corresponding f nomial \( binomial , fibonomial , gaussian \) coefficients extension of this tiling problem onto the general case multi f nomial coefficients is here proposed reformulation of the present cobweb tiling problem into a clique problem of a graph specially invented for that purpose is proposed here too to this end we illustrate the area of our reconnaissance by means of the venn type map of various cobweb sequences families
a machine translation system is said to be complete if all expressions that are correct according to the source language grammar can be translated into the target language this paper addresses the completeness issue for compositional machine translation in general , and for compositional machine translation of context free grammars in particular conditions that guarantee translation completeness of context free grammars are presented
wireless network coding \( wnc \) has been proposed for next generation networks in this contribution , we investigate wnc schemes with embedded strong secrecy by exploiting structured interference in relay networks with two users and a single relay in a practical scenario where both users employ finite , uniform signal input distributions we compute the corresponding strong secrecy capacity , and make this explicit when pam modems are used we then describe a simple triple binning encoder that can provide strong secrecy close to capacity with respect to an untrustworthy relay in the single antenna and single relay setting an explicit encoder construction is described when m pam or m qam modulators are employed at the users' transmitters lastly we generalize to a mimo relay channel where the relay has more antennas than the users , and optimal precoding matrices are studied our results establish that the design of wnc transmission schemes with enhanced throughput and guaranteed data confidentiality is feasible in next generation systems
there is analyzed a performance of optimal feedback communication systems with the analog transmitters in the forward channel \( afcs \) it is shown that measures and limit boundaries of afcs performance are similar but differ from those used in digital communications and information theory the causes of the differences are discussed
modeling the locations of nodes as a uniform binomial point process \( bpp \) , we present a generic mathematical framework to characterize the performance of an arbitrarily located reference receiver in a finite wireless network different from most of the prior works where the serving transmitter \( tx \) node is located at the fixed distance from the reference receiver , we consider two general tx selection policies i \) uniform tx selection the serving node is chosen uniformly at random amongst transmitting nodes , and ii \) k closest tx selection the serving node is the k th closest node out of transmitting nodes to the reference receiver the key intermediate step in our analysis is the derivation of a new set of distance distributions that lead not only to the tractable analysis of coverage probability but also enable the analyses of wide range of classical and currently trending problems in wireless networks using this new set of distance distributions , we first investigate the diversity loss due to sir correlation in a finite network we then obtain the optimal number of links that can be simultaneously activated to maximize network spectral efficiency finally , we evaluate optimal caching probability to maximize the total hit probability in cache enabled finite networks
a harmonious coloring of g is a proper vertex coloring of g such that every pair of colors appears on at most one pair of adjacent vertices the harmonious chromatic number of g , h \( g \) , is the minimum number of colors needed for a harmonious coloring of g we show that if t is a forest of order n with maximum degree delta \( t \) geq frac n 2 3 , then h \( t \) delta \( t \) 2 , if t has non adjacent vertices of degree delta \( t \) delta \( t \) 1 , otherwise moreover , the proof yields a polynomial time algorithm for an optimal harmonious coloring of such a forest
locality sensitive hashing \( lsh \) has emerged as the dominant algorithmic technique for similarity search with strong performance guarantees in high dimensional spaces a drawback of traditional lsh schemes is that they may have emph false negatives , i e , the recall is less than 100 this limits the applicability of lsh in settings requiring precise performance guarantees building on the recent theoretical coveringlsh construction that eliminates false negatives , we propose a fast and practical covering lsh scheme for hamming space called emph fast coveringlsh \( fclsh \) inheriting the design benefits of coveringlsh our method avoids false negatives and always reports all near neighbors compared to coveringlsh we achieve an asymptotic improvement to the hash function computation time from mathcal o \( dl \) to mathcal o \( d l log l \) , where d is the dimensionality of data and l is the number of hash tables our experiments on synthetic and real world data sets demonstrate that emph fclsh is comparable \( and often superior \) to traditional hashing based approaches for search radius up to 20 in high dimensional hamming space
we propose a new platform for implementing secure wireless ad hoc networks our proposal is based on a modular architecture , with the software stack constructed directly on the ethernet layer within our platform we use a new security protocol that we designed to ensure mutual authentication between nodes and a secure key exchange the correctness of the proposed security protocol is ensured by guttman 's authentication tests
the sparse representation problem of recovering an n dimensional sparse vector x from m n linear observations y dx given dictionary d is considered the standard approach is to let the elements of the dictionary be independent and identically distributed \( iid \) zero mean gaussian and minimize the l1 norm of x under the constraint y dx in this paper , the performance of l1 reconstruction is analyzed , when the dictionary is bi orthogonal d o1 o2 , where o1 , o2 are independent and drawn uniformly according to the haar measure on the group of orthogonal m x m matrices by an application of the replica method , we obtain the critical conditions under which perfect l1 recovery is possible with bi orthogonal dictionaries
let r f 2 u f 2 u 2 f 2 be a non chain finite commutative ring , where u 3 u in this paper , we mainly study the construction of quantum codes from cyclic codes over r we obtained self orthogonal codes over f 2 as gray images of linear and cyclic codes over r the parameters of quantum codes which are obtained from cyclic code over r are discussed
this paper deals with a method of tomographic reconstruction of radially symmetric objects from a single radiograph , in order to study the behavior of shocked material the usual tomographic reconstruction algorithms such as generalized inverse or filtered back projection cannot be applied here because data are very noisy and the inverse problem associated to single view tomographic reconstruction is highly unstable in order to improve the reconstruction , we propose here to add some a priori assumptions on the looked after object one of these assumptions is that the object is binary and consequently , the object may be described by the curves that separate the two materials we present a model that lives in bv space and leads to a non local hamilton jacobi equation , via a level set strategy numerical experiments are performed \( using level sets methods \) on synthetic objects
we conduct a detailed investigation of correlations between real time expressions of individuals made across the united states and a wide range of emotional , geographic , demographic , and health characteristics we do so by combining \( 1 \) a massive , geo tagged data set comprising over 80 million words generated over the course of several recent years on the social network service twitter and \( 2 \) annually surveyed characteristics of all 50 states and close to 400 urban populations among many results , we generate taxonomies of states and cities based on their similarities in word use estimate the happiness levels of states and cities correlate highly resolved demographic characteristics with happiness levels and connect word choice and message length with urban characteristics such as education levels and obesity rates our results show how social media may potentially be used to estimate real time levels and changes in population level measures such as obesity rates
in the study of the capacity problem for multiple access channels \( macs \) , a lower bound on the error probability obtained by han plays a crucial role in the converse parts of several kinds of channel coding theorems in the information spectrum framework recently , yagi and oohama showed a tighter bound than the han bound by means of polyanskiy 's converse in this paper , we give a new bound which generalizes and strengthens the yagi oohama bound , and demonstrate that the bound plays a fundamental role in deriving extensions of several known bounds in particular , the yagi oohama bound is generalized to two different directions i e , to general input distributions and to general encoders in addition we extend these bounds to the quantum macs and apply them to the converse problems for several information spectrum settings
understanding the structure and evolution of online bipartite networks is a significant task since they play a crucial role in various e commerce services nowadays recently , various attempts have been tried to propose different models , resulting in either power law or exponential degree distributions however , many empirical results show that the user degree distribution actually follows a shifted power law distribution , so called emph mandelbrot law , which cannot be fully described by previous models in this paper , we propose an evolving model , considering two different user behaviors random and preferential attachment extensive empirical results on two real bipartite networks , emph delicious and emph citeulike , show that the theoretical model can well characterize the structure of real networks for both user and object degree distributions in addition , we introduce a structural parameter p , to demonstrate that the hybrid user behavior leads to the shifted power law degree distribution , and the region of power law tail will increase with the increment of p the proposed model might shed some lights in understanding the underlying laws governing the structure of real online bipartite networks
common problem in signal processing is reconstruction of the missing signal samples missing samples can occur by intentionally omitting signal coefficients to reduce memory requirements , or to speed up the transmission process also , noisy signal coefficients can be considered as missing ones , since they have wrong values due to the noise the reconstruction of these coefficients is demanding task , considered within the compressive sensing area signal with large number of missing samples can be recovered , if certain conditions are satisfied there is a number of algorithms used for signal reconstruction in this paper we have analyzed the performance of iterative gradient based algorithm for sparse signal reconstruction the parameters influence on the optimal performances of this algorithm is tested two cases are observed non noisy and noisy signal case
the free space optical \( fso \) communications can achieve high capacity with huge unlicensed optical spectrum and low operational costs the corresponding performance analysis of fso systems over turbulence channels is very limited , especially when using multiple apertures at both transmitter and receiver sides this paper aim to provide the ergodic capacity characterization of multiple input multiple output \( mimo \) fso systems over atmospheric turbulence induced fading channels the fluctuations of the irradiance of optical channels distorted by atmospheric conditions is usually described by a gamma gamma \( gamma gamma \) distribution , and the distribution of the sum of gamma gamma random variables \( rvs \) is required to model the mimo optical links we use an alpha mu distribution to efficiently approximate the probability density function \( pdf \) of the sum of independent and identical distributed gamma gamma rvs through moment based estimators furthermore , the pdf of the sum of independent , but not necessarily identically distributed gamma gamma rvs can be efficiently approximated by a finite weighted sum of pdfs of gamma gamma distributions based on these reliable approximations , novel and precise analytical expressions for the ergodic capacity of mimo fso systems are derived additionally , we deduce the asymptotic simple expressions in high signal to noise ratio regimes , which provide useful insights into the impact of the system parameters on the ergodic capacity finally , our proposed results are validated via monte carlo simulations
we propose a method to identify all the nodes that are relevant to compute all the conditional probability distributions for a given set of nodes our method is simple , effcient , consistent , and does not require learning a bayesian network first therefore , our method can be applied to high dimensional databases , e g gene expression databases
this squib examines the role of limited attention in a theory of discourse structure and proposes a model of attentional state that relates current hierarchical theories of discourse structure to empirical evidence about human discourse processing capabilities first , i present examples that are not predicted by grosz and sidner 's stack model of attentional state then i consider an alternative model of attentional state , the cache model , which accounts for the examples , and which makes particular processing predictions finally i suggest a number of ways that future research could distinguish the predictions of the cache model and the stack model
an information theoretic framework is presented for estimating the number of labeled samples needed to train a classifier in a parametric bayesian setting ideas from rate distortion theory are used to derive bounds on the average l 1 or l infty distance between the learned classifier and the true maximum a posteriori classifier which are well established surrogates for the excess classification error due to imperfect learning in terms of the differential entropy of the posterior distribution , the fisher information of the parametric family , and the number of training samples available the maximum em a posteriori classifier is viewed as a random source , labeled training data are viewed as a finite rate encoding of the source , and the l 1 or l infty bayes risk is viewed as the average distortion the result is a complementary framework to the well known probably approximately correct \( pac \) framework pac bounds characterize worst case learning performance of a family of classifiers whose complexity is captured by the vapnik chervonenkis \( vc \) dimension the rate distortion framework , on the other hand , characterizes the average case performance of a family of data distributions in terms of a quantity called the interpolation dimension , which represents the complexity of the family of data distributions the resulting bounds do not suffer from the pessimism typical of the pac framework , particularly when the training set is small the framework also naturally accommodates multi class settings furthermore , monte carlo methods provide accurate estimates of the bounds even for complicated distributions the effectiveness of this framework is demonstrated in both a binary and multi class gaussian setting
the task of dialog management is commonly decomposed into two sequential subtasks dialog state tracking and dialog policy learning in an end to end dialog system , the aim of dialog state tracking is to accurately estimate the true dialog state from noisy observations produced by the speech recognition and the natural language understanding modules the state tracking task is primarily meant to support a dialog policy from a probabilistic perspective , this is achieved by maintaining a posterior distribution over hidden dialog states composed of a set of context dependent variables once a dialog policy is learned , it strives to select an optimal dialog act given the estimated dialog state and a defined reward function this paper introduces a novel method of dialog state tracking based on a bilinear algebric decomposition model that provides an efficient inference schema through collective matrix factorization we evaluate the proposed approach on the second dialog state tracking challenge \( dstc 2 \) dataset and we show that the proposed tracker gives encouraging results compared to the state of the art trackers that participated in this standard benchmark finally , we show that the prediction schema is computationally efficient in comparison to the previous approaches
protecting the network layer from malicious attacks is an important and challenging security issue in mobile ad hoc networks \( manets \) in this paper , a security mechanism is proposed to defend against a cooperative gray hole attack on the well known aodv routing protocol in manets a gray hole is a node that selectively drops and forwards data packets after it advertises itself as having the shortest path to the destination node in response to a route request message from a source node the proposed mechanism does not apply any cryptographic primitives on the routing messages instead , it protects the network by detecting and reacting to malicious activities of any node simulation results show that the scheme has a significantly high detection rate with moderate network traffic overhead
deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance however , each fraction of a percent of improved accuracy costs nearly doubling the number of layers , and so training very deep residual networks has a problem of diminishing feature reuse , which makes these networks very slow to train to tackle these problems , in this paper we conduct a detailed experimental study on the architecture of resnet blocks , based on which we propose a novel architecture where we decrease depth and increase width of residual networks we call the resulting network structures wide residual networks \( wrns \) and show that these are far superior over their commonly used thin and very deep counterparts for example , we demonstrate that even a simple 16 layer deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks , including thousand layer deep networks , achieving new state of the art results on cifar 10 , cifar 100 and svhn
the concept of the modulus in the cad system drawings is characterized , being a base of developing of the problem oriented extensions the modulus consists of visible geometric elements of the drawing and invisible parametric representation of the modelling object the technological advantages of moduluss in a complex cad system developing are described
many evolving complex systems can be modeled via dynamic networks an important problem in dynamic network research is community detection , which identifies groups of topologically related nodes typically , this problem is approached by assuming either that each time point has a distinct community organization or that all time points share one community organization in reality , the truth likely lies between these two extremes , since some time periods can have community organization that evolves while others can have community organization that stays the same to find the compromise , we consider community detection in the context of the problem of segment detection , which identifies contiguous time periods with consistent network structure consequently , we formulate a combined problem of segment community detection \( scd \) , which simultaneously partitions the network into contiguous time segments with consistent community organization and finds this community organization for each segment to solve scd , we introduce scout , an optimization framework that explicitly considers both segmentation quality and partition quality scout addresses limitations of existing methods that can be adapted to solve scd , which typically consider only one of segmentation quality or partition quality in a thorough evaluation , scout outperforms the existing methods in terms of both accuracy and computational complexity
optimization methods are at the core of many problems in signal image processing , computer vision , and machine learning for a long time , it has been recognized that looking at the dual of an optimization problem may drastically simplify its solution deriving efficient strategies which jointly brings into play the primal and the dual problems is however a more recent idea which has generated many important new contributions in the last years these novel developments are grounded on recent advances in convex analysis , discrete optimization , parallel processing , and non smooth optimization with emphasis on sparsity issues in this paper , we aim at presenting the principles of primal dual approaches , while giving an overview of numerical methods which have been proposed in different contexts we show the benefits which can be drawn from primal dual algorithms both for solving large scale convex optimization problems and discrete ones , and we provide various application examples to illustrate their usefulness
the structure and size of the interleaver used in a turbo code critically affect the distance spectrum and the covariance property of a component decoder 's information input and soft output this paper introduces a new class of interleavers , the inter block permutation \( ibp \) interleavers , that can be build on any existing good block wise interleaver by simply adding an ibp stage the ibp interleavers reduce the above mentioned correlation and increase the effective interleaving size the increased effective interleaving size improves the distance spectrum while the reduced covariance enhances the iterative decoder 's performance moreover , the structure of the ibp \( interleaved \) turbo codes \( ibptc \) is naturally fit for high rate applications that necessitate parallel decoding we present some useful bounds and constraints associated with the ibptc that can be used as design guidelines the corresponding codeword weight upper bounds for weight 2 and weight 4 input sequences are derived based on some of the design guidelines , we propose a simple ibp algorithm and show that the associated ibptc yields 0 3 to 1 2 db performance gain , or equivalently , an ibptc renders the same performance with a much reduced interleaving delay the exit and covariance behaviors provide another numerical proof of the superiority of the proposed ibptc
most state of the art satisfiability algorithms today are variants of the dpll procedure augmented with clause learning the main bottleneck for such algorithms , other than the obvious one of time , is the amount of memory used in the field of proof complexity , the resources of time and memory correspond to the length and space of resolution proofs there has been a long line of research trying to understand these proof complexity measures , but while strong results have been proven on length our understanding of space is still quite poor for instance , it remains open whether the fact that a formula is provable in short length implies that it is also provable in small space or whether on the contrary these measures are unrelated in the sense that short proofs can be arbitrarily complex with respect to space in this paper , we present some evidence that the true answer should be that the latter case holds we do this by proving a tight bound of theta \( sqrt \( n \) \) on the space needed for so called pebbling contradictions over pyramid graphs of size n this yields the first polynomial lower bound on space that is not a consequence of a corresponding lower bound on width , another well studied measure in resolution , as well as an improvement of the weak separation in \( nordstrom 2006 \) of space and width from logarithmic to polynomial also , continuing the line of research initiated by \( ben sasson 2002 \) into trade offs between different proof complexity measures , we present a simplified proof of the recent length space trade off result in \( hertel and pitassi 2007 \) , and show how our ideas can be used to prove a couple of other exponential trade offs in resolution
we present a 6 approximation algorithm for the minimum cost k node connected spanning subgraph problem , assuming that the number of nodes is at least k 3 \( k 1 \) k we apply a combinatorial preprocessing , based on the frank tardos algorithm for k outconnectivity , to transform any input into an instance such that the iterative rounding method gives a 2 approximation guarantee this is the first constant factor approximation algorithm even in the asymptotic setting of the problem , that is , the restriction to instances where the number of nodes is lower bounded by a function of k
compressed sensing is a technique for recovering a high dimensional signal from lower dimensional data , whose components represent partial information about the signal , utilizing prior knowledge on the sparsity of the signal for further reducing the data size of the compressed expression , a scheme to recover the original signal utilizing only the sign of each entry of the linearly transformed vector was recently proposed this approach is often termed the 1 bit compressed sensing here we analyze the typical performance of an l1 norm based signal recovery scheme for the 1 bit compressed sensing using statistical mechanics methods we show that the signal recovery performance predicted by the replica method under the replica symmetric ansatz , which turns out to be locally unstable for modes breaking the replica symmetry , is in a good consistency with experimental results of an approximate recovery algorithm developed earlier this suggests that the l1 based recovery problem typically has many local optima of a similar recovery accuracy , which can be achieved by the approximate algorithm we also develop another approximate recovery algorithm inspired by the cavity method numerical experiments show that when the density of nonzero entries in the original signal is relatively large the new algorithm offers better performance than the abovementioned scheme and does so with a lower computational cost
this article gives theoretical insights into the performance of k svd , a dictionary learning algorithm that has gained significant popularity in practical applications the particular question studied here is when a dictionary phi in mathbb r d times k can be recovered as local minimum of the minimisation criterion underlying k svd from a set of n training signals y n phi x n a theoretical analysis of the problem leads to two types of identifiability results assuming the training signals are generated from a tight frame with coefficients drawn from a random symmetric distribution first , asymptotic results showing , that in expectation the generating dictionary can be recovered exactly as a local minimum of the k svd criterion if the coefficient distribution exhibits sufficient decay second , based on the asymptotic results it is demonstrated that given a finite number of training samples n , such that n log n o \( k 3d \) , except with probability o \( n kd \) there is a local minimum of the k svd criterion within distance o \( kn 1 4 \) to the generating dictionary
the automatic design of controllers for mobile robots usually requires two stages in the first stage , sensorial data are preprocessed or transformed into high level and meaningful values of variables whichare usually defined from expert knowledge in the second stage , a machine learning technique is applied toobtain a controller that maps these high level variables to the control commands that are actually sent tothe robot this paper describes an algorithm that is able to embed the preprocessing stage into the learningstage in order to get controllers directly starting from sensorial raw data with no expert knowledgeinvolved due to the high dimensionality of the sensorial data , this approach uses quantified fuzzy rules \( qfrs \) , that are able to transform low level input variables into high level input variables , reducingthe dimensionality through summarization the proposed learning algorithm , called iterative quantifiedfuzzy rule learning \( iqfrl \) , is based on genetic programming iqfrl is able to learn rules with differentstructures , and can manage linguistic variables with multiple granularities the algorithm has been testedwith the implementation of the wall following behavior both in several realistic simulated environmentswith different complexity and on a pioneer 3 at robot in two real environments results have beencompared with several well known learning algorithms combined with different data preprocessingtechniques , showing that iqfrl exhibits a better and statistically significant performance moreover , three real world applications for which iqfrl plays a central role are also presented path and objecttracking with static and moving obstacles avoidance
streams are infinite sequences over a given data type a stream specification is a set of equations intended to define a stream we propose a transformation from such a stream specification to a term rewriting system \( trs \) in such a way that termination of the resulting trs implies that the stream specification is well defined , that is , admits a unique solution as a consequence , proving well definedness of several interesting stream specifications can be done fully automatically using present powerful tools for proving trs termination in order to increase the power of this approach , we investigate transformations that preserve semantics and well definedness we give examples for which the above mentioned technique applies for the ransformed specification while it fails for the original one
recommendation systems represent an important tool for news distribution on the internet in this work we modify a recently proposed social recommendation model in order to deal with no explicit ratings of users on news the model consists of a network of users which continually adapts in order to achieve an efficient news traffic to optimize network 's topology we propose different stochastic algorithms that are scalable with respect to the network 's size agent based simulations reveal the features and the performance of these algorithms to overcome the resultant drawbacks of each method we introduce two improved algorithms and show that they can optimize network 's topology almost as fast and effectively as other not scalable methods that make use of much more information
with the rapid growth of online social network sites \( sns \) , it has become imperative for platform owners and online marketers to investigate what drives content production on these platforms however , previous research has found it difficult to statistically model these factors using observational data due to the inability to separate the effects of network formation from those of network influence the inability to successfully separate these two mechanisms makes it difficult to interpret whether the observed behavior is a result of peer influence or merely indicative of a selection bias due to homophily in this paper , we propose an actor oriented continuous time model to jointly estimate the co evolution of the users' social network structure and their content production behavior using a markov chain monte carlo \( mcmc \) based simulation approach specifically , we offer a method to analyze non stationary and continuous behavior with network effects , similar to what is observed in social media ecosystems leveraging a unique dataset contributed by facebook , we apply our model to data on university students across six months to find that users tend to connect with others that have similar posting behavior however , after doing so , users tend to diverge in posting behavior further , we also discover that homophilous friend selection as well as susceptibility to peer influence are sensitive to the strength of the posting behaviour our results provide insights and recommendations for sns platforms to sustain an active and viable community
non convex optimization problems often arise from probabilistic modeling , such as estimation of posterior distributions non convexity makes the problems intractable , and poses various obstacles for us to design efficient algorithms in this work , we attack non convexity by first introducing the concept of emph probable convexity for analyzing convexity of real functions in practice we then use the new concept to analyze an inference problem in the emph correlated topic model \( ctm \) and related nonconjugate models contrary to the existing belief of intractability , we show that this inference problem is concave under certain conditions one consequence of our analyses is a novel algorithm for learning ctm which is significantly more scalable and qualitative than existing methods finally , we highlight that stochastic gradient algorithms might be a practical choice to resolve efficiently non convex problems this finding might find beneficial in many contexts which are beyond probabilistic modeling
the ieee 802 11b wireless ethernet standart has several serious security flaws this paper describes this flaws , surveys wireless networks in the cologne bonn area to get an assessment of the security configurations of fielded networks and analizes the legal protections provided to wireless ethernet operators by german law we conclude that wireless ethernets without additional security measures are not usable for any transmissions which are not meant for a public audience
we consider downlink precoding in a frequency selective multi user massive mimo system with highly efficient but non linear power amplifiers at the base station \( bs \) a low complexity precoding algorithm is proposed , which generates constant envelope \( ce \) signals at each bs antenna to achieve a desired per user information rate , the extra total transmit power required under the per antenna ce constraint when compared to the commonly used less stringent total average transmit power constraint , is small
we introduce and analyze stochastic optimization methods where the input to each gradient update is perturbed by bounded noise we show that this framework forms the basis of a unified approach to analyze asynchronous implementations of stochastic optimization algorithms in this framework , asynchronous stochastic optimization algorithms can be thought of as serial methods operating on noisy inputs using our perturbed iterate framework , we provide new analyses of the hogwild ! algorithm and asynchronous stochastic coordinate descent , that are simpler than earlier analyses , remove many assumptions of previous models , and in some cases yield improved upper bounds on the convergence rates we proceed to apply our framework to develop and analyze kromagnon a novel , parallel , sparse stochastic variance reduced gradient \( svrg \) algorithm we demonstrate experimentally on a 16 core machine that the sparse and parallel version of svrg is in some cases more than four orders of magnitude faster than the standard svrg algorithm
we investigate methods for parameter learning from incomplete data that is not missing at random likelihood based methods then require the optimization of a profile likelihood that takes all possible missingness mechanisms into account optimzing this profile likelihood poses two main difficulties multiple \( local \) maxima , and its very high dimensional parameter space in this paper a new method is presented for optimizing the profile likelihood that addresses the second difficulty in the proposed ai m \( adjusting imputation and mazimization \) procedure the optimization is performed by operations in the space of data completions , rather than directly in the parameter space of the profile likelihood we apply the ai m method to learning parameters for bayesian networks the method is compared against conservative inference , which takes into account each possible data completion , and against em the results indicate that likelihood based inference is still feasible in the case of unknown missingness mechanisms , and that conservative inference is unnecessarily weak on the other hand , our results also provide evidence that the em algorithm is still quite effective when the data is not missing at random
behavioural type systems ensure more than the usual safety guarantees of static analysis they are based on the idea of types as processes , providing dedicated type algebras for particular properties , ranging from protocol compatibility to race freedom , lock freedom , or even responsiveness two successful , although rather different , approaches , are session types and process types the former allows to specify and verify \( distributed \) communication protocols using specific type \( proof \) systems the latter allows to infer from a system specification a process abstraction on which it is simpler to verify properties , using a generic type \( proof \) system what is the relationship between these approaches \? can the generic one subsume the specific one \? at what price \? and can the former be used as a compiler for the latter \? the work presented herein is a step towards answers to such questions concretely , we define a stepwise encoding of a pi calculus with sessions and session types \( the system of gay and hole \) into a pi calculus with process types \( the generic type system of igarashi and kobayashi \) we encode session type environments , polarities \( which distinguish session channels end points \) , and labelled sums we show forward and reverse operational correspondences for the encodings , as well as typing correspondences to faithfully encode session subtyping in process types subtyping , one needs to add to the target language record constructors and new subtyping rules in conclusion , the programming convenience of session types as protocol abstractions can be combined with the simplicity and power of the pi calculus , taking advantage in particular of the framework provided by the generic type system
this paper has been withdrawn by the authors
we consider an edge weighted uniform random graph with a given degree sequence \( repeated configuration model \) which is a useful approximation for many real world networks it has been observed that the vertices which are separated from the rest of the graph by a distance exceeding certain threshold play an important role in determining some global properties of the graph like diameter , flooding time etc , in spite of being statistically rare we give a convergence result for the distribution of the number of such far out vertices we also make a conjecture about how this relates to the longest edge of the minimal spanning tree on the graph under consideration
cross layer scheduling is a promising way to improve quality of service \( qos \) given a power constraint in this paper , we investigate the system with random data arrival and adaptive transmission probabilistic scheduling strategies aware of the buffer state are applied to generalize conventional deterministic scheduling based on this , the average delay and power consumption are analysed by markov reward process the optimal delay power tradeoff curve is the pareto frontier of the feasible delay power region it is proved that the optimal delay power tradeoff is piecewise linear , whose vertices are obtained by deterministic strategies moreover , the corresponding strategies of the optimal tradeoff curve are threshold based , hence can be obtained by a proposed effective algorithm on the other hand , we formulate a linear programming to minimize the average delay given a fixed power constraint by varying the power constraint , the optimal delay power tradeoff curve can also be obtained it is demonstrated that the algorithm result and the optimization result match each other , and are further validated by monte carlo simulation
we developed a smartphone application , ha'midgam , to poll and forecast the results of the 2015 israeli elections the application was downloaded by over 7 , 500 people we present the method used to control bias in our sample and our forecasts we discuss limitations of our approach and suggest possible solutions to control bias in similar applications
in the network flow interdiction problem an adversary attacks a network in order to minimize the maximum s t flow very little is known about the approximatibility of this problem despite decades of interest in it we present the first approximation hardness , showing that network flow interdiction and several of its variants cannot be much easier to approximate than densest k subgraph in particular , any n o \( 1 \) approximation algorithm for network flow interdiction would imply an n o \( 1 \) approximation algorithm for densest k subgraph we complement this hardness results with the first approximation algorithm for network flow interdiction , which has approximation ratio 2 \( n 1 \) we also show that network flow interdiction is essentially the same as the budgeted minimum s t cut problem , and transferring our results gives the first approximation hardness and algorithm for that problem , as well
the kaczmarz algorithm is a popular solver for overdetermined linear systems due to its simplicity and speed in this paper , we propose a modification that speeds up the convergence of the randomized kaczmarz algorithm for systems of linear equations with sparse solutions the speedup is achieved by projecting every iterate onto a weighted row of the linear system while maintaining the random row selection criteria of strohmer and vershynin the weights are chosen to attenuate the contribution of row elements that lie outside of the estimated support of the sparse solution while the kaczmarz algorithm and its variants can only find solutions to overdetermined linear systems , our algorithm surprisingly succeeds in finding sparse solutions to underdetermined linear systems as well we present empirical studies which demonstrate the acceleration in convergence to the sparse solution using this modified approach in the overdetermined case we also demonstrate the sparse recovery capabilities of our approach in the underdetermined case and compare the performance with that of ell 1 minimization
in this work , the role of unidirectional limited rate transmitter cooperation is studied in the case of the 2 user z interference channel \( z ic \) with secrecy constraints at the receivers , on achieving two conflicting goals simultaneously mitigating interference and ensuring secrecy first , the problem is studied under the linear deterministic model the achievable schemes for the deterministic model use a fusion of cooperative precoding and transmission of a jamming signal the optimality of the proposed scheme is established for the deterministic model for all possible parameter settings using the outer bounds derived by the authors in a previous work using the insights obtained from the deterministic model , a lower bound on the secrecy capacity region of the 2 user gaussian z ic are obtained the achievable scheme in this case uses stochastic encoding in addition to cooperative precoding and transmission of a jamming signal the secure sum generalized degrees of freedom \( gdof \) is characterized and shown to be optimal for the weak moderate interference regime it is also shown that the secure sum capacity lies within 2 bits s hz of the outer bound for the weak moderate interference regime for all values of the capacity of the cooperative link interestingly , in case of the deterministic model , it is found that there is no penalty on the capacity region of the z ic due to the secrecy constraints at the receivers in the weak moderate interference regimes similarly , it is found that there is no loss in the secure sum gdof for the gaussian case due to the secrecy constraint at the receiver , in the weak moderate interference regimes the results highlight the importance of cooperation in facilitating secure communication over the z ic
despite a significant improvement in the educational aids in terms of effective teaching learning process , most of the educational content available to the students is less than optimal in the context of being up to date , exhaustive and easy to understand there is a need to iteratively improve the educational material based on the feedback collected from the students' learning experience this can be achieved by observing the students' interactions with the content , and then having the authors modify it based on this feedback hence , we aim to facilitate and promote communication between the communities of authors , instructors and students in order to gradually improve the educational material such a system will also help in students' learning process by encouraging student to student teaching underpinning these objectives , we provide the framework of a platform named crowdsourced annotation system \( cas \) where the people from these communities can collaborate and benefit from each other we use the concept of in context annotations , through which , the students can add their comments about the given text while learning it an experiment was conducted on 60 students who try to learn an article of a textbook by annotating it for four days according to the result of the experiment , most of the students were highly satisfied with the use of cas they stated that the system is extremely useful for learning and they would like to use it for learning other concepts in future
the key approaches for machine learning , especially learning in unknown probabilistic environments are new representations and computation mechanisms in this paper , a novel quantum reinforcement learning \( qrl \) method is proposed by combining quantum theory and reinforcement learning \( rl \) inspired by the state superposition principle and quantum parallelism , a framework of value updating algorithm is introduced the state \( action \) in traditional rl is identified as the eigen state \( eigen action \) in qrl the state \( action \) set can be represented with a quantum superposition state and the eigen state \( eigen action \) can be obtained by randomly observing the simulated quantum state according to the collapse postulate of quantum measurement the probability of the eigen action is determined by the probability amplitude , which is parallelly updated according to rewards some related characteristics of qrl such as convergence , optimality and balancing between exploration and exploitation are also analyzed , which shows that this approach makes a good tradeoff between exploration and exploitation using the probability amplitude and can speed up learning through the quantum parallelism to evaluate the performance and practicability of qrl , several simulated experiments are given and the results demonstrate the effectiveness and superiority of qrl algorithm for some complex problems the present work is also an effective exploration on the application of quantum computation to artificial intelligence
many learning tasks , such as cross validation , parameter search , or leave one out analysis , involve multiple instances of similar problems , each instance sharing a large part of learning data with the others we introduce a robust framework for solving multiple square root lasso problems , based on a sketch of the learning data that uses low rank approximations our approach allows a dramatic reduction in computational effort , in effect reducing the number of observations from m \( the number of observations to start with \) to k \( the number of singular values retained in the low rank model \) , while not sacrificing sometimes even improving the statistical performance theoretical analysis , as well as numerical experiments on both synthetic and real data , illustrate the efficiency of the method in large scale applications
in this paper , we present a method to optimise rough set partition sizes , to which rule extraction is performed on hiv data the genetic algorithm optimisation technique is used to determine the partition sizes of a rough set in order to maximise the rough sets prediction accuracy the proposed method is tested on a set of demographic properties of individuals obtained from the south african antenatal survey six demographic variables were used in the analysis , these variables are race , age of mother , education , gravidity , parity , and age of father , with the outcome or decision being either hiv positive or negative rough set theory is chosen based on the fact that it is easy to interpret the extracted rules the prediction accuracy of equal width bin partitioning is 57 7 while the accuracy achieved after optimising the partitions is 72 8 several other methods have been used to analyse the hiv data and their results are stated and compared to that of rough set theory \( rst \)
we design and analyze gossip algorithms for networks with correlated data in these networks , either the data to be distributed , the data already available at the nodes , or both , are correlated this model is applicable for a variety of modern networks , such as sensor , peer to peer and content distribution networks although coding schemes for correlated data have been studied extensively , the focus has been on characterizing the rate region in static memory free networks in a gossip based scheme , however , nodes communicate among each other by continuously exchanging packets according to some underlying communication model the main figure of merit in this setting is the stopping time the time required until nodes can successfully decode while gossip schemes are practical , distributed and scalable , they have only been studied for uncorrelated data we wish to close this gap by providing techniques to analyze network coded gossip in \( dynamic \) networks with correlated data we give a clean framework for oblivious network models that applies to a multitude of network and communication scenarios , specify a general setting for distributed correlated data , and give tight bounds on the stopping times of network coded protocols in this wide range of scenarios
retrieval tasks typically require a ranking of items given a query collaborative filtering tasks , on the other hand , learn to model user 's preferences over items in this paper we study the joint problem of recommending items to a user with respect to a given query , which is a surprisingly common task this setup differs from the standard collaborative filtering one in that we are given a query x user x item tensor for training instead of the more traditional user x item matrix compared to document retrieval we do have a query , but we may or may not have content features \( we will consider both cases \) and we can also take account of the user 's profile we introduce a factorized model for this new task that optimizes the top ranked items returned for the given query and user we report empirical results where it outperforms several baselines
in this paper , we give a polynomial time algorithm which determines if a given graph containing a triangle and no induced seven vertex path is 3 colorable , and gives an explicit coloring if one exists in previous work , we gave a polynomial time algorithm for three coloring triangle free graphs with no induced seven vertex path combined , our work shows that three coloring a graph with no induced seven vertex path can be done in polynomial time
cloud computing , as one of the most promising computing paradigms , has become increasingly accepted in industry numerous commercial providers have started to supply public cloud services , and corresponding performance evaluation is then inevitably required for cloud provider selection or cost benefit analysis unfortunately , inaccurate and confusing evaluation implementations can be often seen in the context of commercial cloud computing , which could severely interfere and spoil evaluation related comprehension and communication this paper introduces a taxonomy to help profile and standardize the details of performance evaluation of commercial cloud services through a systematic literature review , we constructed the taxonomy along two dimensions by arranging the atomic elements of cloud related performance evaluation as such , this proposed taxonomy can be employed both to analyze existing evaluation practices through decomposition into elements and to design new experiments through composing elements for evaluating performance of commercial cloud services moreover , through smooth expansion , we can continually adapt this taxonomy to the more general area of evaluation of cloud computing
we introduce the notion of delta complete decision procedures for solving smt problems over the real numbers , with the aim of handling a wide range of nonlinear functions including transcendental functions and solutions of lipschitz continuous odes given an smt problem varphi and a positive rational number delta , a delta complete decision procedure determines either that varphi is unsatisfiable , or that the delta weakening of varphi is satisfiable here , the delta weakening of varphi is a variant of varphi that allows delta bounded numerical perturbations on varphi we prove the existence of delta complete decision procedures for bounded smt over reals with functions mentioned above for functions in type 2 complexity class c , under mild assumptions , the bounded delta smt problem is in np c delta complete decision procedures can exploit scalable numerical methods for handling nonlinearity , and we propose to use this notion as an ideal requirement for numerically driven decision procedures as a concrete example , we formally analyze the dpll icp framework , which integrates interval constraint propagation \( icp \) in dpll \( t \) , and establish necessary and sufficient conditions for its delta completeness we discuss practical applications of delta complete decision procedures for correctness critical applications including formal verification and theorem proving
traditional database access control mechanisms use role based methods , with generally row based and attribute based constraints for granularity , and privacy is achieved mainly by using views however if only a set of views according to policy are made accessible to users , then this set should be checked against the policy for the whole probable query history the aim of this work is to define a proactive decomposition algorithm according to the attribute based policy rules and build a secure logical schema in which relations are decomposed into several ones in order to inhibit joins or inferences that may violate predefined privacy constraints the attributes whose association should not be inferred , are defined as having security dependency among them and they form a new kind of context dependent attribute based policy rule named as security dependent set the decomposition algorithm works on a logical schema with given security dependent sets and aims to prohibit the inference of the association among the elements of these sets it is also proven that the decomposition technique generates a secure logical schema that is in compliance with the given security dependent set constraints
we present tropical games , a generalization of combinatorial min max games based on tropical algebras our model breaks the traditional symmetry of rational zero sum games where players have exactly opposed goals \( min vs max \) , is more widely applicable than min max and also supports a form of pruning , despite it being less effective than alpha beta actually , min max games may be seen as particular cases where both the game and its dual are tropical when the dual of a tropical game is also tropical , the power of alpha beta is completely recovered we formally develop the model and prove that the tropical pruning strategy is correct , then conclude by showing how the problem of approximated parsing can be modeled as a tropical game , profiting from pruning
a graph g is said to be 1 perfectly orientable \( 1 p o for short \) if it admits an orientation such that the out neighborhood of every vertex is a clique in g the class of 1 p o graphs forms a common generalization of the classes of chordal and circular arc graphs even though 1 p o graphs can be recognized in polynomial time , no structural characterization of 1 p o graphs is known in this paper we consider the four standard graph products the cartesian product , the strong product , the direct product , and the lexicographic product for each of them , we characterize when a nontrivial product of two graphs is 1 p o
for an n t transmit , n r receive antenna system \( n t times n r system \) , a it full rate space time block code \( stbc \) transmits n min min \( n t , n r \) complex symbols per channel use and in general , has an ml decoding complexity of the order of m n tn min \( considering square designs \) , where m is the constellation size in this paper , a scheme to obtain a full rate stbc for 2 a transmit antennas and any n r , with reduced ml decoding complexity of the order of m n t \( n min 3 4 \) , is presented the weight matrices of the proposed stbc are obtained from the unitary matrix representations of a clifford algebra for any value of n r , the proposed design offers a reduction from the full ml decoding complexity by a factor of m 3n t 4 the well known silver code for 2 transmit antennas is a special case of the proposed scheme further , it is shown that the codes constructed using the scheme have higher ergodic capacity than the well known punctured perfect codes for n r n t simulation results of the symbol error rates are shown for 8 times 2 systems , where the comparison of the proposed code is with the punctured perfect code for 8 transmit antennas the proposed code matches the punctured perfect code in error performance , while having reduced ml decoding complexity and higher ergodic capacity
in the maximum scatter traveling salesman problem the objective is to find a tour that maximizes the shortest distance between any two consecutive nodes this model can be applied to manufacturing processes , particularly laser melting processes we extend an algorithm by arkin et al that yields optimal solutions for nodes on a line to a regular m times n grid the new algorithm textsc weave \( m , n \) takes linear time to compute an optimal tour in some cases it is asymptotically optimal and a frac sqrt 10 5 approximation for the 3 times 4 grid , which is the worst case
bandelt and mulder 's structural characterization of bipartite distance hereditary graphs asserts that such graphs can be built inductively starting from a single vertex and by repeatedly adding either pending vertices or twins \( i e , vertices with the same neighborhood as an existing one \) dirac and duffin 's structural characterization of 2 connected series parallel graphs asserts that such graphs can be built inductively starting from a single edge by adding either edges in series or in parallel in this paper we prove that the two constructions are the same construction when bipartite graphs are viewed as the fundamental graphs of a graphic matroid we then apply the result to re prove known results concerning bipartite distance hereditary graphs and series parallel graphs , to characterize self dual outer planar graphs and , finally , to provide a new class of polynomially solvable instances for the integer multi commodity flow of maximum value
an important source of high clustering coefficient in real world networks is transitivity however , existing approaches for modeling transitivity suffer from at least one of the following problems i \) they produce graphs from a specific class like bipartite graphs , ii \) they do not give an analytical argument for the high clustering coefficient of the model , and iii \) their clustering coefficient is still significantly lower than real world networks in this paper , we propose a new model for complex networks which is based on adding transitivity to scale free models we theoretically analyze the model and provide analytical arguments for its different properties in particular , we calculate a lower bound on the clustering coefficient of the model which is independent of the network size , as seen in real world networks more than theoretical analysis , the main properties of the model are evaluated empirically and it is shown that the model can precisely simulate real world networks from different domains with and different specifications
this paper investigates a method to improve buildings' thermal predictive control performance via online identification and excitation \( active learning process \) that minimally disrupts normal operations in previous studies we have demonstrated scalable methods to acquire multi zone thermal models of passive buildings using a gray box approach that leverages building topology and measurement data here we extend the method to multi zone actively controlled buildings and examine how to improve the thermal model estimation by using the controller to excite unknown portions of the building 's dynamics comparing against a baseline thermostat controller , we demonstrate the utility of both the initially acquired and improved thermal models within a model predictive control \( mpc \) framework , which anticipates weather uncertainty and time varying temperature set points a simulation study demonstrates self excitation improves model estimation , which corresponds to improved mpc energy savings and occupant comfort by coupling building topology , estimation , and control routines into a single online framework , we have demonstrated the potential for low cost scalable methods to actively learn and control buildings to ensure occupant comfort and minimize energy usage , all while using the existing building 's hvac sensors and hardware
we briefly introduce the memory based approaches to emulate machine intelligence in vlsi hardware , describing the challenges and advantages implementation of artificial intelligence techniques in vlsi hardware is a practical and difficult problem deep architectures , hierarchical temporal memories and memory networks are some of the contemporary approaches in this area of research the techniques attempt to emulate low level intelligence tasks and aim at providing scalable solutions to high level intelligence problems such as sparse coding and contextual processing
in this work , we investigate a multi source multi cast network with the aid of an arbitrary number of relays , where it is assumed that no direct link is available at each s d pair the aim is to find the fundamental limit on the maximal common multicast throughput of all source nodes if resource allocations are available a transmission protocol employing the relaying strategy , namely , compute and forward \( cpf \) , is proposed we also adjust the methods in the literature to obtain the integer network constructed coefficient matrix \( a naive method , a local optimal method as well as a global optimal method \) to fit for the general topology with an arbitrary number of relays two transmission scenarios are addressed the first scenario is delay stringent transmission where each message must be delivered within one slot the second scenario is delay tolerant transmission where no delay constraint is imposed the associated optimization problems to maximize the short term and long term common multicast throughput are formulated and solved , and the optimal allocation of power and time slots are presented performance comparisons show that the cpf strategy outperforms conventional decode and forward \( df \) strategy it is also shown that with more relays , the cpf strategy performs even better due to the increased diversity finally , by simulation , it is observed that for a large network in relatively high snr regime , cpf with the local optimal method for the network constructed matrix can perform close to that with the global optimal method
we present new adaptive format for storing sparse matrices on gpu we compare it with several other formats including cusparse which is today probably the best choice for processing of sparse matrices on gpu in cuda contrary to cusparse which works with common csr format , our new format requires conversion however , multiplication of sparse matrix and vector is significantly faster for many atrices we demonstrate it on set of 1 600 matrices and we show for what types of matrices our format is profitable
biogeography is the study of the geographical distribution of biological organisms the mindset of the engineer is that we can learn from nature biogeography based optimization is a burgeoning nature inspired technique to find the optimal solution of the problem satellite image classification is an important task because it is the only way we can know about the land cover map of inaccessible areas though satellite images have been classified in past by using various techniques , the researchers are always finding alternative strategies for satellite image classification so that they may be prepared to select the most appropriate technique for the feature extraction task in hand this paper is focused on classification of the satellite image of a particular land cover using the theory of biogeography based optimization the original bbo algorithm does not have the inbuilt property of clustering which is required during image classification hence modifications have been proposed to the original algorithm and the modified algorithm is used to classify the satellite image of a given region the results indicate that highly accurate land cover features can be extracted effectively when the proposed algorithm is used
we develop a general framework for proving rigorous guarantees on the performance of the em algorithm and a variant known as gradient em our analysis is divided into two parts a treatment of these algorithms at the population level \( in the limit of infinite data \) , followed by results that apply to updates based on a finite set of samples first , we characterize the domain of attraction of any global maximizer of the population likelihood this characterization is based on a novel view of the em updates as a perturbed form of likelihood ascent , or in parallel , of the gradient em updates as a perturbed form of standard gradient ascent leveraging this characterization , we then provide non asymptotic guarantees on the em and gradient em algorithms when applied to a finite set of samples we develop consequences of our general theory for three canonical examples of incomplete data problems mixture of gaussians , mixture of regressions , and linear regression with covariates missing completely at random in each case , our theory guarantees that with a suitable initialization , a relatively small number of em \( or gradient em \) steps will yield \( with high probability \) an estimate that is within statistical error of the mle we provide simulations to confirm this theoretically predicted behavior
the importance of innovation in the world 's economy , now undeniable , draws great attention to the need to improve organizations' creative potential in the last 60 years , hundreds of books have been written on the subject and hundreds of webpages display information on how to be more creative and achieve innovation several north american and european universities offer graduated programs in creativity however , building an effective and validated creativity training program is not without challenges because of the nature of their work , engineers are often asked to be innovative without aiming for a degree in creativity , could future engineers benefit from training programs in creativity \? this article presents the conceptual framework and pedagogical elements of a new course in creativity for engineering students
performance problems are often observed in embedded software systems the reasons for poor performance are frequently not obvious bottlenecks can occur in any of the software components along the execution path therefore it is important to instrument and monitor the different components contributing to the runtime behavior of an embedded software system performance analysis tools can help locate performance bottlenecks in embedded software systems by monitoring the software 's execution and producing easily understandable performance data we maintain and further develop a tool for analyzing the performance of nokia mobile phone software the user can select among four performance analysis reports to be generated average processor load , processor utilization , task execution time statistics , and task execution timeline each of these reports provides important information about where execution time is being spent the demo will show how the tool helps to identify performance bottlenecks in nokia mobile phone software and better understand areas of poor performance
it is known that the existential theory of equations in free groups is decidable this is a famous result of makanin on the other hand it has been shown that the scheme of his algorithm is not primitive recursive in this paper we present an algorithm that works in polynomial space , even in the more general setting where each variable has a rational constraint , that is , the solution has to respect a specification given by a regular word language our main result states that the existential theory of equations in free groups with rational constraints is pspace complete we obtain this result as a corollary of the corresponding statement about free monoids with involution
numerous applications demand communication schemes that minimize the transmission delay while achieving a given level of reliability an extreme case is high frequency trading whereby saving a fraction of millisecond over a route between chicago and new york can be a game changer while such communications are often carried by fiber , microwave links can reduce transmission delays over large distances due to more direct routes and faster wave propagation in order to bridge large distances , information is sent over a multihop relay network motivated by these applications , this papers present an information theoretic approach to the design of optimal multihop microwave networks that minimizes end to end transmission delay to characterize the delay introduced by coding , we derive error exponents achievable in multihop networks we formulate and solve an optimization problem that determines optimal selection of amplify and forward and decode and forward relays we present the optimal solution for several examples of networks we prove that in high snr the optimum transmission scheme is for all relays to perform amplify and forward we then analyze the impact of deploying noisy feedback
complex information processing systems , for example quantum circuits , cryptographic protocols , or multi player games , are naturally described as networks composed of more basic information processing systems a modular analysis of such systems requires a mathematical model of systems that is closed under composition , i e , a network of these objects is again an object of the same type we propose such a model and call the corresponding systems causal boxes causal boxes capture superpositions of causal structures , e g , messages sent by a causal box a can be in a superposition of different orders or in a superposition of being sent to box b and box c furthermore , causal boxes can model systems whose behavior depends on time by instantiating the abstract cryptography framework with causal boxes , we obtain the first composable security framework that can handle arbitrary quantum protocols and relativistic protocols
fixed broadband wireless access is a promising technology which can offer high speed data rate from transmitting end to customer end which can offer high speed text , voice , and video data ieee 802 16 wirelessman is a standard that specifies medium access control layer and a set of phy layer to fixed and mobile bwa in broad range of frequencies and it supports equipment manufacturers due to its robust performance in multipath environment consequently wimax forum has adopted this version to develop the network world wide in this paper the performance of ieee 802 16 ofdm phy layer has been investigated by using the simulation model in matlab the stanford university interim \( sui \) channel models are selected for the performance evaluation of this standard the ideal channel estimation is considered in this work and the performance evaluation is observed in the basis of ber
much current research in ai and games is being devoted to monte carlo search \( mcs \) algorithms while the quest for a single unified mcs algorithm that would perform well on all problems is of major interest for ai , practitioners often know in advance the problem they want to solve , and spend plenty of time exploiting this knowledge to customize their mcs algorithm in a problem driven way we propose an mcs algorithm discovery scheme to perform this in an automatic and reproducible way we first introduce a grammar over mcs algorithms that enables inducing a rich space of candidate algorithms afterwards , we search in this space for the algorithm that performs best on average for a given distribution of training problems we rely on multi armed bandits to approximately solve this optimization problem the experiments , generated on three different domains , show that our approach enables discovering algorithms that outperform several well known mcs algorithms such as upper confidence bounds applied to trees and nested monte carlo search we also show that the discovered algorithms are generally quite robust with respect to changes in the distribution over the training problems
an extension of polar codes is proposed , which allows some of the frozen symbols , called dynamic frozen symbols , to be data dependent a construction of polar codes with dynamic frozen symbols , being subcodes of extended bch codes , is proposed the proposed codes have higher minimum distance than classical polar codes , but still can be efficiently decoded using the successive cancellation algorithm and its extensions the codes with arikan , extended bch and reed solomon kernel are considered the proposed codes are shown to outperform ldpc and turbo codes , as well as polar codes with crc
this paper presents a model based , unsupervised algorithm for recovering word boundaries in a natural language text from which they have been deleted the algorithm is derived from a probability model of the source that generated the text the fundamental structure of the model is specified abstractly so that the detailed component models of phonology , word order , and word frequency can be replaced in a modular fashion the model yields a language independent , prior probability distribution on all possible sequences of all possible words over a given alphabet , based on the assumption that the input was generated by concatenating words from a fixed but unknown lexicon the model is unusual in that it treats the generation of a complete corpus , regardless of length , as a single event in the probability space accordingly , the algorithm does not estimate a probability distribution on words instead , it attempts to calculate the prior probabilities of various word sequences that could underlie the observed text experiments on phonemic transcripts of spontaneous speech by parents to young children suggest that this algorithm is more effective than other proposed algorithms , at least when utterance boundaries are given and the text includes a substantial number of short utterances keywords bayesian grammar induction , probability models , minimum description length \( mdl \) , unsupervised learning , cognitive modeling , language acquisition , segmentation
in most real world settings , due to limited time or other resources , an agent cannot perform all potentially useful deliberation and information gathering actions this leads to the metareasoning problem of selecting such actions decision theoretic methods for metareasoning have been studied in ai , but there are few theoretical results on the complexity of metareasoning we derive hardness results for three settings which most real metareasoning systems would have to encompass as special cases in the first , the agent has to decide how to allocate its deliberation time across anytime algorithms running on different problem instances we show this to be mathcal np complete in the second , the agent has to \( dynamically \) allocate its deliberation or information gathering resources across multiple actions that it has to choose among we show this to be mathcal np hard even when evaluating each individual action is extremely simple in the third , the agent has to \( dynamically \) choose a limited number of deliberation or information gathering actions to disambiguate the state of the world we show that this is mathcal np hard under a natural restriction , and mathcal pspace hard in general
we initiate the study of probabilistic parallel programs with dynamic process creation and synchronisation to this end , we introduce probabilistic split join systems \( psjss \) , a model for parallel programs , generalising both probabilistic pushdown systems \( a model for sequential probabilistic procedural programs which is equivalent to recursive markov chains \) and stochastic branching processes \( a classical mathematical model with applications in various areas such as biology , physics , and language processing \) our psjs model allows for a possibly recursive spawning of parallel processes the spawned processes can synchronise and return values we study the basic performance measures of psjss , especially the distribution and expectation of space , work and time our results extend and improve previously known results on the subsumed models we also show how to do performance analysis in practice , and present two case studies illustrating the modelling power of psjss
in this letter , we study the performance of hybrid radio frequency \( rf \) and free space optical \( fso \) links using automatic repeat request \( arq \) we derive closed form expressions for the message decoding probabilities , throughput , and outage probability with different relative coherence times of the rf and fso links we also evaluate the effect of adaptive power allocation between the arq retransmissions on the system performance the results show that joint implementation of the rf and fso links leads to substantial performance improvement , compared to the cases with only the rf or the fso link
contrasting to advances in street outdoor navigation , wall mounted maps and signs continue to be the primary reference indoor navigation in hospitals , malls , museums , etc the proliferation of mobile devices and the growing demand for location aware systems that filter information based on currently device location have led to an increase in research and product development in this field an attempt has been made to provide solution for indoor navigation on google maps and to provide location of a user in a building using wi fi signal strength on android smartphone
in this paper we explore visually the structure of the collection of a digital research data archive in terms of metadata for deposited datasets we look into the distribution of datasets over different scientific fields the role of main depositors \( persons and institutions \) in different fields , and main access choices for the deposited datasets we argue that visual analytics of metadata of collections can be used in multiple ways to inform the archive about structure and growth of its collection to foster collections strategies and to check metadata consistency we combine visual analytics and visual enhanced browsing introducing a set of web based , interactive visual interfaces to the archive 's collection we discuss how text based search combined with visual enhanced browsing enhances data access , navigation , and reuse
clustering analysis and datamining methodologies were applied to the problem of identifying illegal and fraud transactions the researchers independently developed model and software using data provided by a bank and using rapidminer modeling tool the research objectives are to propose dynamic model and mechanism to cover fraud detection system limitations kda model as proposed model can detect 68 75 of fraudulent transactions with online dynamic modeling and 81 25 in offline mode and the fraud detection system decision support system software propose a good supporting procedure to detect fraudulent transaction dynamically
we study the problem of personalized advertisement recommendation \( par \) , which consist of a user visiting a system \( website \) and the system displaying one of k ads to the user the system uses an internal ad recommendation policy to map the user 's profile \( context \) to one of the ads the user either clicks or ignores the ad and correspondingly , the system updates its recommendation policy par problem is usually tackled by scalable emph contextual bandit algorithms , where the policies are generally based on classifiers a practical problem in par is extreme click sparsity , due to very few users actually clicking on ads we systematically study the drawback of using contextual bandit algorithms based on classifier based policies , in face of extreme click sparsity we then suggest an alternate policy , based on rankers , learnt by optimizing the area under the curve \( auc \) ranking loss , which can significantly alleviate the problem of click sparsity we conduct extensive experiments on public datasets , as well as three industry proprietary datasets , to illustrate the improvement in click through rate \( ctr \) obtained by using the ranker based policy over classifier based policies
this paper has been withdrawn by the author this paper is now obsolete for a solution please see arxiv 1205 4265
we present a mathematical model for communication subject to both network interference and noise we introduce a framework where the interferers are scattered according to a spatial poisson process , and are operating asynchronously in a wireless environment subject to path loss , shadowing , and multipath fading we consider both cases of slow and fast varying interferer positions the paper is comprised of two separate parts in part i , we determine the distribution of the aggregate network interference at the output of a linear receiver we characterize the error performance of the link , in terms of average and outage probabilities the proposed model is valid for any linear modulation scheme \( e g , m ary phase shift keying or m ary quadrature amplitude modulation \) , and captures all the essential physical parameters that affect network interference our work generalizes the conventional analysis of communication in the presence of additive white gaussian noise and fast fading , allowing the traditional results to be extended to include the effect of network interference in part ii of the paper , we derive the capacity of the link when subject to network interference and noise , and characterize the spectrum of the aggregate interference
conformal prediction is a method of producing prediction sets that can be applied on top of a wide range of prediction algorithms the method has a guaranteed coverage probability under the standard iid assumption regardless of whether the assumptions \( often considerably more restrictive \) of the underlying algorithm are satisfied however , for the method to be really useful it is desirable that in the case where the assumptions of the underlying algorithm are satisfied , the conformal predictor loses little in efficiency as compared with the underlying algorithm \( whereas being a conformal predictor , it has the stronger guarantee of validity \) in this paper we explore the degree to which this additional requirement of efficiency is satisfied in the case of bayesian ridge regression we find that asymptotically conformal prediction sets differ little from ridge regression prediction intervals when the standard bayesian assumptions are satisfied
the spectrum of a first order logic sentence is the set of natural numbers that are cardinalities of its finite models in this paper we study the hierarchy of first order spectra based on the number of variables it has been conjectured that it collapses to three variable we show the opposite it forms an infinite hierarchy however , despite the fact that more variables can express more spectra , we show that to establish whether the class of first order spectra is closed under complement , it is sufficient to consider sentences using only three variables and binary relations
a novel architecture for mimo transmission and reception of filterbank multicarrier \( fbmc \) modulated signals under strong frequency selectivity is presented the proposed system seeks to approximate an ideal frequency selective precoder and linear receiver by taylor expansion , exploiting the structure of the analysis and synthesis filterbanks the resulting architecture is implemented by linearly combining conventional mimo linear transceivers , which are applied to sequential derivatives of the original filterbank the classical per subcarrier precoding linear receiver configuration is obtained as a special case of this architecture , when only one stage is fixed at both transmitter and receiver an asymptotic expression for the resulting intersymbol intercarrier \( isi ici \) distortion is derived assuming that the number of subcarriers grows large this expression can in practice be used in order to determine the number of parallel stages that need to be implemented in the proposed architecture performance evaluation studies confirm the substantial advantage of the proposed scheme in practical frequency selective mimo scenarios
estimating divergences in a consistent way is of great importance in many machine learning tasks although this is a fundamental problem in nonparametric statistics , to the best of our knowledge there has been no finite sample exponential inequality convergence bound derived for any divergence estimators the main contribution of our work is to provide such a bound for an estimator of r 'enyi alpha divergence for a smooth h older class of densities on the d dimensional unit cube 0 , 1 d we also illustrate our theoretical results with a numerical experiment
the dominant cost in solving least square problems using newton 's method is often that of factorizing the hessian matrix over multiple values of the regularization parameter \( lambda \) we propose an efficient way to interpolate the cholesky factors of the hessian matrix computed over a small set of lambda values this approximation enables us to optimally minimize the hold out error while incurring only a fraction of the cost compared to exact cross validation we provide a formal error bound for our approximation scheme and present solutions to a set of key implementation challenges that allow our approach to maximally exploit the compute power of modern architectures we present a thorough empirical analysis over multiple datasets to show the effectiveness of our approach
for some applications where the speed of decoding and the fault tolerance are important , like in video storing , one of the successful answers is fix free codes these codes have been applied in some standards like h 263 and mpeg 4 the cost of using fix free codes is to increase the redundancy of the code which means the increase in the amount of bits we need to represent any peace of information thus we investigated the use of huffman codes with low and negligible backward decoding delay we showed that for almost all cases there is always a minimum delay huffman code for a given length vector the average delay of this code for anti uniform sources is calculated , that is in agreement with the simulations , and it is shown that this delay is one bit for large alphabet sources also an algorithm is proposed which will find the minimum delay code with a good performance
let p \( z \) be a monic cubic complex polynomial with distinct roots and distinct critical points we say a critical point has the it voronoi property if it lies in the voronoi cell of a root theta , v \( theta \) , i e the set of points that are closer to theta than to the other roots we prove at least one critical point has the voronoi property and characterize the cases when both satisfy this property it is known that for any xi in v \( theta \) , the sequence b m \( xi \) xi p \( xi \) d m 2 d m 1 converges to theta , where d m satisfies the recurrence d m p' \( xi \) d m 1 0 5 p \( xi \) p'' \( xi \) d m 2 p 2 \( xi \) d m 3 , d 0 1 , d 1 d 2 0 thus by the voronoi property , there is a solution c of p' \( z \) 0 where b m \( c \) converges to a root of p \( z \) the speed of convergence is dependent on the ratio of the distances between c and the closest and the second closest roots of p \( z \) this results in a different algorithm for solving a cubic equation than the classical methods we give polynomiography for an example
coverage and connectivity both are important in wireless sensor network \( wsn \) coverage means how well an area of interest is being monitored by the deployed network it depends on sensing model that has been used to design the network model connectivity ensures the establishment of a wireless link between two nodes a link model studies the connectivity between two nodes the probability of establishing a wireless link between two nodes is a probabilistic phenomenon the connectivity between two nodes plays an important role in the determination of network connectivity in this paper , we investigate the impact of sensing model of nodes on the network coverage also , we investigate the dependency of the connectivity and coverage on the shadow fading parameters it has been observed that shadowing effect reduces the network coverage while it enhances connectivity in a multi hop wireless network
the karmarkar karp differencing algorithm is the best known polynomial time heuristic for the number partitioning problem , fundamental in both theoretical computer science and statistical physics we analyze the performance of the differencing algorithm on random instances by mapping it to a nonlinear rate equation our analysis reveals strong finite size effects that explain why the precise asymptotics of the differencing solution is hard to establish by simulations the asymptotic series emerging from the rate equation satisfies all known bounds on the karmarkar karp algorithm and projects a scaling n c ln n , where c 1 \( 2 ln2 \) 0 7213 our calculations reveal subtle relations between the algorithm and fibonacci like sequences , and we establish an explicit identity to that effect
the aim of the authors' research is to gain better insights into the effectiveness and user satisfaction of anthropomorphism at the user interface therefore , this paper presents a between users experiment and the results in the context of anthropomorphism at the user interface and the giving of instruction for learning sewing stitches two experimental conditions were used , where the information for learning sewing stitches was the same however the manner of presentation was varied therefore one condition was anthropomorphic and the other was non anthropomorphic also the work is closely linked with hartson 's theory of affordances applied to user interfaces the results suggest that facilitation of the affordances in an anthropomorphic user interface lead to statistically significant results in terms of effectiveness and user satisfaction in the sewing context further some violation of the affordances leads to an interface being less usable in terms of effectiveness and user satisfaction
given matrices a and b and vectors a , b , c and d , all with non negative entries , we consider the problem of computing min c x x in z n , ax a , bx b , x d we give a bicriteria approximation algorithm that , given epsilon in \( 0 , 1 , finds a solution of cost o \( ln \( m \) epsilon 2 \) times optimal , meeting the covering constraints \( ax a \) and multiplicity constraints \( x d \) , and satisfying bx \( 1 epsilon \) b beta , where beta is the vector of row sums beta i sum j b ij here m denotes the number of rows of a this gives an o \( ln m \) approximation algorithm for cip minimum cost covering integer programs with multiplicity constraints , i e , the special case when there are no packing constraints bx b the previous best approximation ratio has been o \( ln \( max j sum i a ij \) \) since 1982 cip contains the set cover problem as a special case , so o \( ln m \) approximation is the best possible unless p np
there are numerous contexts where one wishes to describe the state of a randomly evolving system effective solutions combine models that quantify the underlying uncertainty with available observational data to form scientifically reasonable estimates for the uncertainty in the system state stochastic differential equations are often used to mathematically model the underlying system the kusuoka lyons victoir \( klv \) approach is a higher order particle method for approximating the weak solution of a stochastic differential equation that uses a weighted set of scenarios to approximate the evolving probability distribution to a high order of accuracy the algorithm can be performed by integrating along a number of carefully selected bounded variation paths the iterated application of the klv method has a tendency for the number of particles to increase this can be addressed and , together with local dynamic recombination , which simplifies the support of discrete measure without harming the accuracy of the approximation , the klv method becomes eligible to solve the filtering problem in contexts where one desires to maintain an accurate description of the ever evolving conditioned measure in addition to the alternate application of the klv method and recombination , we make use of the smooth nature of the likelihood function and high order accuracy of the approximations to lead some of the particles immediately to the next observation time and to build into the algorithm a form of automatic high order adaptive importance sampling
secret key establishment leveraging the physical layer as a source of common randomness has been investigated in a range of settings we investigate the problem of establishing , in an information theoretic sense , a secret key between a user and a base station \( bs \) \( more generally , part of a wireless infrastructure \) , but for two such user bs pairs attempting the key establishment simultaneously the challenge in this novel setting lies in that a user can eavesdrop another bs user communications it is thus paramount to ensure the two keys are established with no leakage to the other user , in spite the interference across neighboring cells we model the system with bs user communication through an interference channel and user bs communication through a public channel we find the region including achievable secret key rates for the general case that the interference channel \( ic \) is discrete and memoryless our results are examined for a gaussian ic in this setup , we investigate the performance of different transmission schemes for power allocation the chosen transmission scheme by each bs essentially affects the secret key rate of the other bs user assuming base stations are trustworthy but that they seek to maximize the corresponding secret key rate , a game theoretic setting arises to analyze the interaction between the base stations we model our key agreement scenario in normal form for different power allocation schemes to understand performance without cooperation numerical simulations illustrate the inefficiency of the nash equilibrium outcome and motivate further research on cooperative or coordinated schemes
team description paper for robocup 2014 soccer simulation league 2d
complex systems made of interacting elements are commonly abstracted as networks , in which nodes are associated with dynamic state variables , whose evolution is driven by interactions mediated by the edges markov processes have been the prevailing paradigm to model such a network based dynamics , for instance in the form of random walks or other types of diffusions despite the success of this modelling perspective for numerous applications , it represents an over simplification of several real world systems importantly , simple markov models lack memory in their dynamics , an assumption often not realistic in practice here , we explore possibilities to enrich the system description by means of second order markov models , exploiting empirical pathway information we focus on the problem of community detection and show that standard network algorithms can be generalized in order to extract novel temporal information about the system under investigation we also apply our methodology to temporal networks , where we can uncover communities shaped by the temporal correlations in the system finally , we discuss relations of the framework of second order markov processes and the recently proposed formalism of using non backtracking matrices for community detection
it is well known that the linear secret sharing scheme \( lsss \) can be constructed from linear error correcting codes \( brickell 1 , r j mceliece and d v sarwate 2 , cramer , el , 3 \) the theory of linear codes from algebraic geometric curves \( algebraic geometric \( ag \) codes or geometric goppa code \) has been well developed since the work of v goppa and tsfasman , vladut , and zink \( see 17 , 18 and 19 \) in this paper the linear secret sharing scheme from algebraic geometric codes , which are non threshold scheme for curves of genus greater than 0 , are presented we analysis the minimal access structure , d min and d cheat \( 8 \) , \( strongly \) multiplicativity and the applications in verifiable secret sharing \( vss \) scheme and secure multi party computation \( mpc \) of this construction \( 3 and 10 11 \) our construction also offers many examples of the self dually gf \( q \) representable matroids and many examples of new ideal linear secret sharing schemes addressing to the problem of the characterization of the access structures for ideal secret sharing schemes \( 3 and 9 \) the access structures of the linear secret sharing schemes from the codes on elliptic curves are given explicitly from the work in this paper we can see that the algebraic geometric structure of the underlying algebraic curves is an important resource for secret sharing , matroid theory , verifiable secret sharing and secure multi party computation
we propose a framework for indexing of grain and sub grain structures in electron backscatter diffraction \( ebsd \) images of polycrystalline materials the framework is based on a previously introduced physics based forward model by callahan and de graef \( 2013 \) relating measured patterns to grain orientations \( euler angle \) the forward model is tuned to the microscope and the sample symmetry group we discretize the domain of the forward model onto a dense grid of euler angles and for each measured pattern we identify the most similar patterns in the dictionary these patterns are used to identify boundaries , detect anomalies , and index crystal orientations the statistical distribution of these closest matches is used in an unsupervised binary decision tree \( dt \) classifier to identify grain boundaries and anomalous regions the dt classifies a pattern as an anomaly if it has an abnormally low similarity to any pattern in the dictionary it classifies a pixel as being near a grain boundary if the highly ranked patterns in the dictionary differ significantly over the pixels 3x3 neighborhood indexing is accomplished by computing the mean orientation of the closest dictionary matches to each pattern the mean orientation is estimated using a maximum likelihood approach that models the orientation distribution as a mixture of von mises fisher distributions over the quaternionic 3 sphere the proposed dictionary matching approach permits segmentation , anomaly detection , and indexing to be performed in a unified manner with the additional benefit of uncertainty quantification we demonstrate the proposed dictionary based approach on a ni base in100 alloy
space time coding techniques have become common place in wireless communication standards as they provide an effective way to mitigate the fading phenomena inherent in wireless channels however , the use of space time block codes \( stbcs \) increases significantly the optimal detection complexity at the receiver unless the low complexity decodability property is taken into consideration in the stbc design in this letter we propose a new low complexity decodable rate 1 full diversity 4 x 4 stbc we provide an analytical proof that the proposed code has the non vanishing determinant \( nvd \) property , a property that can be exploited through the use of adaptive modulation which changes the transmission rate according to the wireless channel quality we compare the proposed code to existing low complexity decodable rate 1 full diversity 4 x 4 stbcs in terms of performance over quasi static rayleigh fading channels , detection complexity and peak to average power ratio \( papr \) our code is found to provide the best performance and the smallest papr which is that of the used qam constellation at the expense of a slight increase in detection complexity w r t certain previous codes but this will only penalize the proposed code for high order qam constellations
this paper considers a downlink cloud radio access network \( c ran \) in which all the base stations \( bss \) are connected to a central computing cloud via digital backhaul links with finite capacities each user is associated with a user centric cluster of bss the central processor shares the user 's data with the bss in the cluster , which then cooperatively serve the user through joint beamforming under this setup , this paper investigates the user scheduling , bs clustering and beamforming design problem from a network utility maximization perspective differing from previous works , this paper explicitly considers the per bs backhaul capacity constraints we formulate the network utility maximization problem for the downlink c ran under two different models depending on whether the bs clustering for each user is dynamic or static over different user scheduling time slots in the former case , the user centric bs cluster is dynamically optimized for each scheduled user along with the beamforming vector in each time frequency slot , while in the latter case the user centric bs cluster is fixed for each user and we jointly optimize the user scheduling and the beamforming vector to account for the backhaul constraints in both cases , the nonconvex per bs backhaul constraints are approximated using the reweighted l1 norm technique this approximation allows us to reformulate the per bs backhaul constraints into weighted per bs power constraints and solve the weighted sum rate maximization problem through a generalized weighted minimum mean square error approach this paper shows that the proposed dynamic clustering algorithm can achieve significant performance gain over existing naive clustering schemes this paper also proposes two heuristic static clustering schemes that can already achieve a substantial portion of the gain
given a probability distribution p , what is the minimum amount of bits needed to store a value x sampled according to p , such that x can later be recovered \( except with some small probability \) \? or , what is the maximum amount of uniform randomness that can be extracted from x \? answering these and similar information theoretic questions typically boils down to computing so called smooth entropies in this paper , we derive explicit and almost tight bounds on the smooth entropies of n fold product distributions
a novel approach to hpsg based natural language processing is described that uses an off line compiler to automatically prime a declarative grammar for generation or parsing , and inputs the primed grammar to an advanced earley style processor this way we provide an elegant solution to the problems with empty heads and efficient bidirectional processing which is illustrated for the special case of hpsg generation extensive testing with a large hpsg grammar revealed some important constraints on the form of the grammar
in this paper , for the purposes of information transmission and network error correction simultaneously , three classes of important linear network codes in network coding , linear multicast broadcast dispersion codes are generalized to linear network error correction coding , i e , linear network error correction multicast broadcast dispersion codes we further propose the \( weakly , strongly \) extended singleton bounds for these new classes of codes , and define the optimal codes satisfying the corresponding singleton bounds with equality , which are called multicast broadcast dispersion mds codes respectively the existence of such codes are proved by an algebraic method and one kind of constructive algorithm is also proposed
we prove that a particular pushing blocks puzzle is intractable in 2d , improving an earlier result that established intractability in 3d os99 the puzzle , inspired by the game pushpush , consists of unit square blocks on an integer lattice an agent may push blocks \( but never pull them \) in attempting to move between given start and goal positions in the pushpush version , the agent can only push one block at a time , and moreover , each block , when pushed , slides the maximal extent of its free range we prove this version is np hard in 2d by reduction from sat
for many it security measures exact costs and benefits are not known this makes it difficult to allocate resources optimally to different security measures we present a model for costs and benefits of so called honeynets this can foster informed reasoning about the deployment of honeynet technology
this paper addresses the problem of energy efficient resource allocation in the downlink of a cellular ofdma system three definitions of the energy efficiency are considered for system design , accounting for both the radiated and the circuit power user scheduling and power allocation are optimized across a cluster of coordinated base stations with a constraint on the maximum transmit power \( either per subcarrier or per base station \) the asymptotic noise limited regime is discussed as a special case the performance of both an isolated and a non isolated cluster of coordinated base stations is examined in the numerical experiments results show that the maximization of the energy efficiency is approximately equivalent to the maximization of the spectral efficiency for small values of the maximum transmit power , while there is a wide range of values of the maximum transmit power for which a moderate reduction of the data rate provides a large saving in terms of dissipated energy also , the performance gap among the considered resource allocation strategies reduces as the out of cluster interference increases
discrete fourier transforms \( dfts \) over finite fields have widespread applications in error correction coding hence , reducing the computational complexities of dfts is of great significance , especially for long dfts as increasingly longer error control codes are chosen for digital communication and storage systems since dfts involve both multiplications and additions over finite fields and multiplications are much more complex than additions , recently proposed cyclotomic fast fourier transforms \( cffts \) are promising due to their low multiplicative complexity unfortunately , they have very high additive complexity techniques such as common subexpression elimination \( cse \) can be used to reduce the additive complexity of cffts , but their effectiveness for long dfts is limited by their complexity in this paper , we propose prime factor cyclotomic fourier transforms \( pfcfts \) , which use cffts as sub dfts via the prime factor algorithm when the length of dfts is prime , our pfcfts reduce to cffts when the length has co prime factors , since the sub dfts have much shorter lengths , this allows us to use cse to significantly reduce their additive complexity in comparison to previously proposed fast fourier transforms , our pfcfts achieve reduced overall complexity when the length of dfts is at least 255 , and the improvement significantly increases as the length grows this approach also enables us to propose efficient dfts with very long length \( e g , 4095 point \) , first efficient dfts of such lengths in the literature finally , our pfcfts are also advantageous for hardware implementation due to their regular structure
complementation of b uchi automata has been studied for over five decades since the formalism was introduced in 1960 known complementation constructions can be classified into ramsey based , determinization based , rank based , and slice based approaches regarding the performance of these approaches , there have been several complexity analyses but very few experimental results what especially lacks is a comparative experiment on all of the four approaches to see how they perform in practice in this paper , we review the four approaches , propose several optimization heuristics , and perform comparative experimentation on four representative constructions that are considered the most efficient in each approach the experimental results show that \( 1 \) the determinization based safra piterman construction outperforms the other three in producing smaller complements and finishing more tasks in the allocated time and \( 2 \) the proposed heuristics substantially improve the safra piterman and the slice based constructions
we consider the dynamic map visitation problem \( dmvp \) , in which a team of agents must visit a collection of critical locations as quickly as possible , in an environment that may change rapidly and unpredictably during the agents' navigation we apply recent formulations of time varying graphs \( tvgs \) to dmvp , shedding new light on the computational hierarchy mathcal r supset mathcal b supset mathcal p of tvg classes by analyzing them in the context of graph navigation we provide hardness results for all three classes , and for several restricted topologies , we show a separation between the classes by showing severe inapproximability in mathcal r , limited approximability in mathcal b , and tractability in mathcal p we also give topologies in which dmvp in mathcal r is fixed parameter tractable , which may serve as a first step toward fully characterizing the features that make dmvp difficult
a mixture of factor analyzers is a semi parametric density estimator that generalizes the well known mixtures of gaussians model by allowing each gaussian in the mixture to be represented in a different lower dimensional manifold this paper presents a robust and parsimonious model selection algorithm for training a mixture of factor analyzers , carrying out simultaneous clustering and locally linear , globally nonlinear dimensionality reduction permitting different number of factors per mixture component , the algorithm adapts the model complexity to the data complexity we compare the proposed algorithm with related automatic model selection algorithms on a number of benchmarks the results indicate the effectiveness of this fast and robust approach in clustering , manifold learning and class conditional modeling
we study the properties of input consuming derivations of moded logic programs input consuming derivations can be used to model the behavior of logic programs using dynamic scheduling and employing constructs such as delay declarations we consider the class of nicely moded programs and queries we show that for these programs a weak version of the well known switching lemma holds also for input consuming derivations furthermore , we show that , under suitable conditions , there exists an algebraic characterization of termination of input consuming derivations
we show that the classical discrete logarithm problem over prime fields can be reduced to that of solving a system of linear modular equations
secure similar document detection \( ssdd \) identifies similar documents of two parties while each party does not disclose its own sensitive documents to another party in this paper , we propose an efficient 2 step protocol that exploits a feature selection as the lower dimensional transformation and presents discriminative feature selections to maximize the performance of the protocol for this , we first analyze that the existing 1 step protocol causes serious computation and communication overhead for high dimensional document vectors to alleviate the overhead , we next present the feature selection based 2 step protocol and formally prove its correctness the proposed 2 step protocol works as follows \( 1 \) in the filtering step , it uses low dimensional vectors obtained by the feature selection to filter out non similar documents \( 2 \) in the post processing step , it identifies similar documents only from the non filtered documents by using the 1 step protocol as the feature selection , we first consider the simplest one , random projection \( rp \) , and propose its 2 step solution ssdd rp we then present two discriminative feature selections and their solutions ssdd lf \( local frequency \) which selects a few dimensions locally frequent in the current querying vector and ssdd gf \( global frequency \) which selects ones globally frequent in the set of all document vectors we finally propose a hybrid one , ssdd hf \( hybrid frequency \) , that takes advantage of both ssdd lf and ssdd gf we empirically show that the proposed 2 step protocol outperforms the 1 step protocol by three or four orders of magnitude
the model checking problem for various fragments of first order logic has attracted much attention over the last two decades in particular , for the primitive positive and the positive horn fragments , which are better known as the constraint satisfaction problem and the quantified constraint satisfaction problem , respectively these two fragments are in fact the only ones for which there is currently no known complexity classification all other syntactic fragments can be easily classified , either directly or using schaefer 's dichotomy theorems for sat and qsat , with the exception of the positive equality free fragment this outstanding fragment can also be classified and enjoys a tetrachotomy according to the model , the corresponding model checking problem is either tractable , np complete , co np complete or pspace complete moreover , the complexity drop is always witnessed by a generic solving algorithm which uses quantifier relativisation furthermore , its complexity is characterised by algebraic means the presence or absence of specific surjective hyper operations among those that preserve the model characterise the complexity
we study a model for continuous opinion dynamics under bounded confidence in particular , we analyze the importance of the initial distribution of opinions in determining the asymptotic configuration thus , we sketch the structure of attractors of the dynamical system , by means of the numerical computation of the time evolution of the agents density we show that , for a given bound of confidence , a consensus can be encouraged or prevented by certain initial conditions furthermore , a noisy perturbation is added to the system with the purpose of modeling the free will of the agents as a consequence , the importance of the initial condition is partially replaced by that of the statistical distribution of the noise nevertheless , we still find evidence of the influence of the initial state upon the final configuration for a short range of the bound of confidence parameter
the dependence of the gaussian input information rate on the line of sight \( los \) matrix in multiple input multiple output coherent rician fading channels is explored it is proved that the outage probability and the mutual information induced by a multivariate circularly symmetric gaussian input with any covariance matrix are monotonic in the los matrix d , or more precisely , monotonic in d'd in the sense of the loewner partial order conversely , it is also demonstrated that this ordering on the los matrices is a necessary condition for the uniform monotonicity over all input covariance matrices this result is subsequently applied to prove the monotonicity of the isotropic gaussian input information rate and channel capacity in the singular values of the los matrix extensions to multiple access channels are also discussed
we show that the widely used homotopy method for solving fixpoint problems , as well as the harsanyi selten equilibrium selection process for games , are pspace complete to implement extending our result for the harsanyi selten process , we show that several other homotopy based algorithms for finding equilibria of games are also pspace complete to implement a further application of our techniques yields the result that it is pspace complete to compute any of the equilibria that could be found via the classical lemke howson algorithm , a complexity theoretic strengthening of the result in savani and von stengel these results show that our techniques can be widely applied and suggest that the pspace completeness of implementing homotopy methods is a general principle
this paper addresses problems on the robust structural design of complex networks more precisely , we address the problem of deploying the minimum number of dedicated sensors , i e , those measuring a single state variable , that ensure the network to be structurally observable under disruptive scenarios the disruptive scenarios considered are as follows \( i \) the malfunction loss of one arbitrary sensor , and \( ii \) the failure of connection \( either unidirectional or bidirectional communication \) between a pair of agents first , we show these problems to be np hard , which implies that efficient algorithms to determine a solution are unlikely to exist secondly , we propose an intuitive two step approach \( 1 \) we achieve an arbitrary minimum sensor placement ensuring structural observability \( 2 \) we develop a sequential process to find minimum number of additional sensors required for robust observability this step can be solved by recasting it as a weighted set covering problem although this is known to be an np hard problem , feasible approximations can be determined in polynomial time that can be used to obtain feasible approximations to the robust structural design problems with optimality guarantees
the mds property \( aka the k out of n property \) requires that if a file is split into several symbols and subsequently encoded into n coded symbols , each being stored in one storage node of a distributed storage system \( dss \) , then an user can recover the file by accessing any k nodes we study the so called p decodable mu secure erasure coding scheme \( 1 leq p leq k mu , 0 leq mu k , p \( k mu \) \) , which satisfies the mds property and the following additional properties \( p1 \) strongly secure up to a threshold an adversary which eavesdrops at most mu storage nodes gains no information \( in shannon 's sense \) about the stored file , \( p2 \) partially decodable a legitimate user can recover a subset of p file symbols by accessing some mu p storage nodes the scheme is perfectly p decodable mu secure if it satisfies the following additional property \( p3 \) weakly secure up to a threshold an adversary which eavesdrops more than mu but less than mu p storage nodes cannot reconstruct any part of the file most of the related work in the literature only focused on the case p k mu in other words , no partial decodability is provided an user cannot retrieve any part of the file by accessing less than k nodes we provide an explicit construction of p decodable mu secure coding schemes over small fields for all mu and p that construction also produces perfectly p decodable mu secure schemes over small fields when p 1 \( for every mu \) , and when mu 0 , 1 \( for every p \) we establish that perfect schemes exist over emph sufficiently large fields for almost all mu and p
we define bioscapel , a stochastic pi calculus in 3d space a novel aspect of bioscapel is that entities have programmable locations the programmer can specify a particular location where to place an entity , or a location relative to the current location of the entity the motivation for the extension comes from the need to describe the evolution of populations of biochemical species in space , while keeping a sufficiently high level description , so that phenomena like diffusion , collision , and confinement can remain part of the semantics of the calculus combined with the random diffusion movement inherited from bioscape , programmable locations allow us to capture the assemblies of configurations of polymers , oligomers , and complexes such as microtubules or actin filaments further new aspects of bioscapel include random translation and scaling random translation is instrumental in describing the location of new entities relative to the old ones for example , when a cell secretes a hydronium ion , the ion should be placed at a given distance from the originating cell , but in a random direction additionally , scaling allows us to capture at a high level events such as division and growth for example , daughter cells after mitosis have half the size of the mother cell
in this paper , we study and analyze cognitive radio networks in which secondary users \( sus \) are equipped with energy harvesting \( eh \) capability we design a random spectrum sensing and access protocol for the su that exploits the primary link 's feedback and requires less average sensing time unlike previous works proposed earlier in literature , we do not assume perfect feedback instead , we take into account the more practical possibilities of overhearing unreliable feedback signals and accommodate spectrum sensing errors moreover , we assume an interference based channel model where the receivers are equipped with multi packet reception \( mpr \) capability furthermore , we perform power allocation at the su with the objective of maximizing the secondary throughput under constraints that maintain certain quality of service \( qos \) measures for the primary user \( pu \)
for distributed systems to properly react to peaks of requests , their adaptation activities would benefit from the estimation of the amount of requests this paper proposes a solution to produce a short term forecast based on data characterising user behaviour of online services we use emph wavelet analysis , providing compression and denoising on the observed time series of the amount of past user requests and a emph recurrent neural network trained with observed data and designed so as to provide well timed estimations of future requests the said ensemble has the ability to predict the amount of future user requests with a root mean squared error below 0 06 thanks to prediction , advance resource provision can be performed for the duration of a request peak and for just the right amount of resources , hence avoiding over provisioning and associated costs moreover , reliable provision lets users enjoy a level of availability of services unaffected by load variations
in this paper we propose strategies for estimating performance of a classifier when labels cannot be obtained for the whole test set the number of test instances which can be labeled is very small compared to the whole test data size the goal then is to obtain a precise estimate of classifier performance using as little labeling resource as possible specifically , we try to answer , how to select a subset of the large test set for labeling such that the performance of a classifier estimated on this subset is as close as possible to the one on the whole test set we propose strategies based on stratified sampling for selecting this subset we show that these strategies can reduce the variance in estimation of classifier accuracy by a significant amount compared to simple random sampling \( over 65 in several cases \) hence , our proposed methods are much more precise compared to random sampling for accuracy estimation under restricted labeling resources the reduction in number of samples required \( compared to random sampling \) to estimate the classifier accuracy with only 1 error is high as 60 in some cases
in this paper , we explore the relationship between the topological characteristics of a complex network and its robustness to sustained targeted attacks using synthesised scale free , small world and random networks , we look at a number of network measures , including assortativity , modularity , average path length , clustering coefficient , rich club profiles and scale free exponent \( where applicable \) of a network , and how each of these influence the robustness of a network under targeted attacks we use an established robustness coefficient to measure topological robustness , and consider sustained targeted attacks by order of node degree with respect to scale free networks , we show that assortativity , modularity and average path length have a positive correlation with network robustness , whereas clustering coefficient has a negative correlation we did not find any correlation between scale free exponent and robustness , or rich club profiles and robustness the robustness of small world networks on the other hand , show substantial positive correlations with assortativity , modularity , clustering coefficient and average path length in comparison , the robustness of erdos renyi random networks did not have any significant correlation with any of the network properties considered a significant observation is that high clustering decreases topological robustness in scale free networks , yet it increases topological robustness in small world networks our results highlight the importance of topological characteristics in influencing network robustness , and illustrate design strategies network designers can use to increase the robustness of scale free and small world networks under sustained targeted attacks
this paper presents widely linear multi branch decision feedback detection techniques for large scale multiuser multiple antenna systems we consider a scenario with impairments in the radio frequency chain in which the in phase \( i \) and quadrature \( q \) components exhibit an imbalance , which degrades the receiver performance and originates non circular signals a widely linear multi branch decision feedback receiver is developed to mitigate both the multiuser interference and the i q imbalance effects an iterative detection and decoding scheme with the proposed receiver and convolutional codes is also devised simulation results show that the proposed techniques outperform existing algorithms
bi capacities arise as a natural generalization of capacities \( or fuzzy measures \) in a context of decision making where underlying scales are bipolar they are able to capture a wide variety of decision behaviours , encompassing models such as cumulative prospect theory \( cpt \) the aim of this paper in two parts is to present the machinery behind bi capacities , and thus remains on a rather theoretical level , although some parts are firmly rooted in decision theory , notably cooperative game theory the present first part is devoted to the introduction of bi capacities and the structure on which they are defined we define the m obius transform of bi capacities , by just applying the well known theory of m obius functions as established by rota to the particular case of bi capacities then , we introduce derivatives of bi capacities , by analogy with what was done for pseudo boolean functions \( another view of capacities and set functions \) , and this is the key point to introduce the shapley value and the interaction index for bi capacities this is done in a cooperative game theoretic perspective in summary , all familiar notions used for fuzzy measures are available in this more general framework
symbolic computation and satisfiability checking are viewed as individual research areas , but they share common interests in the development , implementation and application of decision procedures for arithmetic theories despite these commonalities , the two communities are currently only weakly connected we introduce a new project sc square to build a joint community in this area , supported by a newly accepted eu \( h2020 fetopen csa \) project of the same name we aim to strengthen the connection between these communities by creating common platforms , initiating interaction and exchange , identifying common challenges , and developing a common roadmap this abstract and accompanying poster describes the motivation and aims for the project , and reports on the first activities
program specialisation aims at improving the overall performance of programs by performing source to source transformations a common approach within functional and logic programming , known respectively as partial evaluation and partial deduction , is to exploit partial knowledge about the input it is achieved through a well automated application of parts of the burstall darlington unfold fold transformation framework the main challenge in developing systems is to design automatic control that ensures correctness , efficiency , and termination this survey and tutorial presents the main developments in controlling partial deduction over the past 10 years and analyses their respective merits and shortcomings it ends with an assessment of current achievements and sketches some remaining research challenges
this letter presents a novel detection strategy for spatially multiplexed generalized spatial modulation systems it is a multi stage detection that produces a list of candidates of the transmitted signal vector , sorted according to the proximity of the data vector to one of the possible vector subspaces the quality metric and list length metric selects the best candidate and manages the list length , respectively performance results show that it significantly reduces the performance gap to the optimal maximum likelihood detector , while maintaining significant computational cost reduction
fault tolerant quantum computation is a technique that is necessary to build a scalable quantum computer from noisy physical building blocks key for the implementation of fault tolerant computations is the ability to perform a universal set of quantum gates that act on the code space of an underlying quantum code to implement such a universal gate set fault tolerantly is an expensive task in terms of physical operations , and any possible shortcut to save operations is potentially beneficial and might lead to a reduction in overhead for fault tolerant computations we show how the automorphism group of a quantum code can be used to implement some operators on the encoded quantum states in a fault tolerant way by merely permuting the physical qubits we derive conditions that a code has to satisfy in order to have a large group of operations that can be implemented transversally when combining transversal cnot with automorphisms we give several examples for quantum codes with large groups , including codes with parameters 8 , 3 , 3 , 15 , 7 , 3 , 22 , 8 , 4 , and 31 , 11 , 5
the paper studies properties of functional dependencies between strategies of players in nash equilibria of multi player strategic games the main focus is on the properties of functional dependencies in the context of a fixed dependency graph for pay off functions a logical system describing properties of functional dependence for any given graph is proposed and is proven to be complete
groenendijk and stokhof \( 1984 , 1996 groenendijk 1999 \) provide a logically attractive theory of the semantics of natural language questions , commonly referred to as the partition theory two central notions in this theory are entailment between questions and answerhood for example , the question who is going to the party \? entails the question is john going to the party \? , and john is going to the party counts as an answer to both groenendijk and stokhof define these two notions in terms of partitions of a set of possible worlds we provide a syntactic characterization of entailment between questions and answerhood we show that answers are , in some sense , exactly those formulas that are built up from instances of the question this result lets us compare the partition theory with other approaches to interrogation both linguistic analyses , such as hamblin 's and karttunen 's semantics , and computational systems , such as prolog our comparison separates a notion of answerhood into three aspects equivalence \( when two questions or answers are interchangeable \) , atomic answers \( what instances of a question count as answers \) , and compound answers \( how answers compose \)
in 1990 , alon and kleitman proposed an elementary argument for the sum free subset theorem every set of n nonzero integers contains a subset a of size a n 3 , which is sum free , i e , there are no a 1 , a 2 , a 3 in a such that a 1 a 2 a 3 in this note , we show that the alon kleitman argument is flawed because it confused two kinds of randomness
in this paper , the required amount of feedback overhead for multiple input multiple output \( mimo \) beamforming over time varying channels is presented in terms of the entropy of the feedback messages in the case that each transmit antenna has its own power amplifier which has individual power limit , it has been known that only phase steering information is necessary to form the optimal transmit beamforming vector since temporal correlation exists for wireless fading channels , one can utilize the previous reported feedback messages as prior information to efficiently encode the current feedback message thus , phase tracking information , difference between two phase steering information in adjacent feedback slots , is sufficient as a feedback message we show that while the entropy of the phase steering information is a constant , the entropy of the phase tracking information is a function of the temporal correlation parameter for the phase tracking information , upperbounds on the entropy are presented in the gaussian entropy and the von mises entropy by using the theory on the maximum entropy distributions derived results can quantify the amount of reduction in feedback overhead of the phase tracking information over the phase steering information for application perspective , the signal to noise ratio \( snr \) gain of phase tracking beamforming over phase steering beamforming is evaluated by using monte carlo simulation also we show that the derived entropies can determine the appropriate duration of the feedback reports with respect to the degree of the channel variation rates
in this paper , we propose a new lower approximation scheme for pomdp with discounted and average cost criterion the approximating functions are determined by their values at a finite number of belief points , and can be computed efficiently using value iteration algorithms for finite state mdp while for discounted problems several lower approximation schemes have been proposed earlier , ours seems the first of its kind for average cost problems we focus primarily on the average cost case , and we show that the corresponding approximation can be computed efficiently using multi chain algorithms for finite state mdp we give a preliminary analysis showing that regardless of the existence of the optimal average cost j in the pomdp , the approximation obtained is a lower bound of the liminf optimal average cost function , and can also be used to calculate an upper bound on the limsup optimal average cost function , as well as bounds on the cost of executing the stationary policy associated with the approximation weshow the convergence of the cost approximation , when the optimal average cost is constant and the optimal differential cost is continuous
consider data transmission over a binary input additive white gaussian noise channel using a binary low density parity check code we ask the following question given a decoder that takes log likelihood ratios as input , does it help to modify the log likelihood ratios before decoding \? if we use an optimal decoder then it is clear that modifying the log likelihoods cannot possibly help the decoder 's performance , and so the answer is no however , for a suboptimal decoder like the linear programming decoder , the answer might be yes in this paper we prove that for certain interesting classes of low density parity check codes and large enough snrs , it is advantageous to truncate the log likelihood ratios before passing them to the linear programming decoder
many complex multi target prediction problems that concern large target spaces are characterised by a need for efficient prediction strategies that avoid the computation of predictions for all targets explicitly examples of such problems emerge in several subfields of machine learning , such as collaborative filtering , multi label classification , dyadic prediction and biological network inference in this article we analyse efficient and exact algorithms for computing the top k predictions in the above problem settings , using a general class of models that we refer to as separable linear relational models we show how to use those inference algorithms , which are modifications of well known information retrieval methods , in a variety of machine learning settings furthermore , we study the possibility of scoring items incompletely , while still retaining an exact top k retrieval experimental results in several application domains reveal that the so called threshold algorithm is very scalable , performing often many orders of magnitude more efficiently than the naive approach
this paper investigates a wireless powered cooperative communication network consisting of a source , a destination and a multi antenna decode and forward relay we consider the relay as a wireless powered node that has no external power supply but it is equipped with an energy harvesting \( eh \) unit and a rechargeable battery such that it can harvest and accumulate energy from radio frequency signals broadcast by the source by fully incorporating the eh feature of the relay , we develop an opportunistic relaying protocol , termed accumulate then forward \( atf \) , for the considered wpccn we then adopt the discrete markov chain to model the dynamic charging and discharging behaviors of the relay battery based on this , we derive a closed form expression for the exact outage probability of the proposed atf protocol numerical results show that the atf scheme can outperform the direct transmission one , especially when the amount of energy consumed by relay for information forwarding is optimized
in this paper we study a gaussian relay interference network , in which relay \( helper \) nodes are to facilitate competing information flows over a wireless network we focus on a two stage relay interference network where there are weak cross links , causing the networks to behave like a chain of z gaussian channels for these gaussian zz and zs networks , we establish an approximate characterization of the rate region the outer bounds to the capacity region are established using genie aided techniques that yield bounds sharper than the traditional cut set outer bounds for the inner bound of the zz network , we propose a new interference management scheme , termed interference neutralization , which is implemented using structured lattice codes this technique allows for over the air interference removal , without the transmitters having complete access the interfering signals for both the zz and zs networks , we establish a new network decomposition technique that \( approximately \) achieves the capacity region we use insights gained from an exact characterization of the corresponding linear deterministic version of the problems , in order to establish the approximate characterization for gaussian networks
we propose a novel neural attention architecture to tackle machine comprehension tasks , such as answering cloze style queries with respect to a document unlike previous models , we do not collapse the query into a single vector , instead we deploy an iterative alternating attention mechanism that allows a fine grained exploration of both the query and the document our model outperforms state of the art baselines in standard machine comprehension benchmarks such as cnn news articles and the children 's book test \( cbt \) dataset
the capacity regions are investigated for two relay broadcast channels \( rbcs \) , where relay links are incorporated into standard two user broadcast channels to support user cooperation in the first channel , the partially cooperative relay broadcast channel , only one user in the system can act as a relay and transmit to the other user through a relay link an achievable rate region is derived based on the relay using the decode and forward scheme an outer bound on the capacity region is derived and is shown to be tighter than the cut set bound for the special case where the partially cooperative rbc is degraded , the achievable rate region is shown to be tight and provides the capacity region gaussian partially cooperative rbcs and partially cooperative rbcs with feedback are further studied in the second channel model being studied in the paper , the fully cooperative relay broadcast channel , both users can act as relay nodes and transmit to each other through relay links this is a more general model than the partially cooperative rbc all the results for partially cooperative rbcs are correspondingly generalized to the fully cooperative rbcs it is further shown that the awgn fully cooperative rbc has a larger achievable rate region than the awgn partially cooperative rbc the results illustrate that relaying and user cooperation are powerful techniques in improving the capacity of broadcast channels
meshes composed of well centered simplices have nice orthogonal dual meshes \( the dual voronoi diagram \) this is useful for certain numerical algorithms that prefer such primal dual mesh pairs we prove that well centered meshes also have optimality properties and relationships to delaunay and minmax angle triangulations we present an iterative algorithm that seeks to transform a given triangulation in two or three dimensions into a well centered one by minimizing a cost function and moving the interior vertices while keeping the mesh connectivity and boundary vertices fixed the cost function is a direct result of a new characterization of well centeredness in arbitrary dimensions that we present ours is the first optimization based heuristic for well centeredness , and the first one that applies in both two and three dimensions we show the results of applying our algorithm to small and large two dimensional meshes , some with a complex boundary , and obtain a well centered tetrahedralization of the cube we also show numerical evidence that our algorithm preserves gradation and that it improves the maximum and minimum angles of acute triangulations created by the best known previous method
we develop an efficient algorithm for cooperative spectrum sensing in a relay based cognitive radio network we consider a stochastic model where data is sent from the base station \( bs \) of the primary user \( pu \) the data is relayed by the secondary users \( su \) to the su bs the su bs has only partial csi knowledge of the wireless channels in order to obtain the optimal decision rule based on likelihood ratio test \( lrt \) , the marginal likelihood under each hypothesis needs to be evaluated pointwise these , however , cannot be obtained analytically due to the intractability of the integrals instead , we approximate these quantities by utilising the laplace method performance is evaluated via numerical simulations and it is shown that the proposed spectrum sensing scheme can achieve superior results to the energy detection scheme
this paper introduces a new type of unsupervised learning algorithm , based on the alignment of sentences and harris 's \( 1951 \) notion of interchangeability the algorithm is applied to an untagged , unstructured corpus of natural language sentences , resulting in a labelled , bracketed version of the corpus firstly , the algorithm aligns all sentences in the corpus in pairs , resulting in a partition of the sentences consisting of parts of the sentences that are similar in both sentences and parts that are dissimilar this information is used to find \( possibly overlapping \) constituents next , the algorithm selects \( non overlapping \) constituents several instances of the algorithm are applied to the atis corpus \( marcus et al , 1993 \) and the ovis \( openbaar vervoer informatie systeem \( ovis \) stands for public transport information system \) corpus \( bonnema et al , 1997 \) apart from the promising numerical results , the most striking result is that even the simplest algorithm based on alignment learns recursion
deep neural network \( dnn \) acoustic models have yielded many state of the art results in automatic speech recognition \( asr \) tasks more recently , recurrent neural network \( rnn \) models have been shown to outperform dnns counterparts however , state of the art dnn and rnn models tend to be impractical to deploy on embedded systems with limited computational capacity traditionally , the approach for embedded platforms is to either train a small dnn directly , or to train a small dnn that learns the output distribution of a large dnn in this paper , we utilize a state of the art rnn to transfer knowledge to small dnn we use the rnn model to generate soft alignments and minimize the kullback leibler divergence against the small dnn the small dnn trained on the soft rnn alignments achieved a 3 93 wer on the wall street journal \( wsj \) eval92 task compared to a baseline 4 54 wer or more than 13 relative improvement
the presence of burstiness in temporal social networks , revealed by a power law form of the waiting time distribution of consecutive interactions , is expected to produce aging effects in the corresponding time integrated network here we propose an analytically tractable model , in which interactions among the agents are ruled by a renewal process , and that is able to reproduce this aging behavior we develop an analytic solution for the topological properties of the integrated network produced by the model , finding that the time translation invariance of the degree distribution is broken we validate our predictions against numerical simulations , and we check for the presence of aging effects in a empirical temporal network , ruled by bursty social interactions
we propose an information theoretic framework for analog signal separation specifically , we consider the problem of recovering two analog signals from a noiseless sum of linear measurements of the signals our framework is inspired by the groundbreaking work of wu and verd 'u \( 2010 \) on almost lossless analog compression the main results of the present paper are a general achievability bound for the compression rate in the analog signal separation problem , an exact expression for the optimal compression rate in the case of signals that have mixed discrete continuous distributions , and a new technique for showing that the intersection of generic subspaces with subsets of sufficiently small minkowski dimension is empty this technique can also be applied to obtain a simplified proof of a key result in wu and verd 'u \( 2010 \)
this paper considers a general gaussian relay network where a source transmits a message to a destination with the help of n half duplex relays it proves that the information theoretic cut set upper bound to the capacity can be achieved to within 2 021 \( n 2 \) bits with noisy network coding , thereby reducing the previously known gap further improved gap results are presented for more structured networks like diamond networks it is then shown that the generalized degrees of freedom of a general gaussian half duplex relay network is the solution of a linear program , where the coefficients of the linear inequality constraints are proved to be the solution of several linear programs , known in graph theory as the assignment problem , for which efficient numerical algorithms exist the optimal schedule , that is , the optimal value of the 2 n possible transmit receive configurations states for the relays , is investigated and known results for diamond networks are extended to general relay networks it is shown , for the case of 2 relays , that only 3 out of the 4 possible states have strictly positive probability extensive experimental results show that , for a general n relay network with n 9 , the optimal schedule has at most n 1 states with strictly positive probability as an extension of a conjecture presented for diamond networks , it is conjectured that this result holds for any hd relay network and any number of relays finally , a 2 relay network is studied to determine the channel conditions under which selecting the best relay is not optimal , and to highlight the nature of the rate gain due to multiple relays
optical switching has been considered as a natural choice to keep pace with growing fiber link capacity one key research issue of all optical switching is the design of optical queues by using optical crossbar switches and fiber delay lines \( sdl \) in this paper , we focus on the construction of an optical priority queue with a single \( m 2 \) times \( m 2 \) crossbar switch and m fiber delay lines , and evaluate it in terms of the buffer size of the priority queue currently , the best known upper bound of the buffer size is o \( 2 m \) , while existing methods can only construct a priority queue with buffer o \( m 3 \) in this paper , we make a great step towards closing the above huge gap we propose a very efficient construction of priority queues with buffer 2 theta \( sqrt m \) we use 4 to 1 multiplexers with different buffer sizes , which can be constructed efficiently with sdl , as intermediate building blocks to simplify the design the key idea in our construction is to route each packet entering the switch to some group of four 4 to 1 multiplexers according to its current priority , which is shown to be collision free
the analysis of bibliometric networks , such as co authorship , bibliographic coupling , and co citation networks , has received a considerable amount of attention much less attention has been paid to the construction of these networks we point out that different approaches can be taken to construct a bibliometric network normally the full counting approach is used , but we propose an alternative fractional counting approach this approach is based on the idea that each decision of a researcher , such as the decision to co author or to cite a publication , should have equal weight , regardless of the number of authors or the number of citations of a publication we present two empirical analyses in which the full and fractional counting approaches yield very different results these analyses deal with co authorship networks of universities and bibliographic coupling networks of journals based on theoretical considerations and on the empirical analyses , we conclude that for many purposes the fractional counting approach is preferable over the full counting one
in statistics , series of ordinary least squares problems \( ols \) are used to study the linear correlation among sets of variables of interest in many studies , the number of such variables is at least in the millions , and the corresponding datasets occupy terabytes of disk space as the availability of large scale datasets increases regularly , so does the challenge in dealing with them indeed , traditional solvers which rely on the use of black box routines optimized for one single ols are highly inefficient and fail to provide a viable solution for big data analyses as a case study , in this paper we consider a linear regression consisting of two dimensional grids of related ols problems that arise in the context of genome wide association analyses , and give a careful walkthrough for the development of sc ols grid , a high performance routine for shared memory architectures analogous steps are relevant for tailoring ols solvers to other applications in particular , we first illustrate the design of efficient algorithms that exploit the structure of the ols problems and eliminate redundant computations then , we show how to effectively deal with datasets that do not fit in main memory finally , we discuss how to cast the computation in terms of efficient kernels and how to achieve scalability importantly , each design decision along the way is justified by simple performance models sc ols grid enables the solution of 10 11 correlated ols problems operating on terabytes of data in a matter of hours
in this paper we discuss l policy iteration , a method for exact and approximate dynamic programming it is intermediate between the classical value iteration \( vi \) and policy iteration \( pi \) methods , and it is closely related to optimistic \( also known as modified \) pi , whereby each policy evaluation is done approximately , using a finite number of vi we review the theory of the method and associated questions of bias and exploration arising in simulation based cost function approximation we then discuss various implementations , which offer advantages over well established pi methods that use lspe \( l \) , lstd \( l \) , or td \( l \) for policy evaluation with cost function approximation one of these implementations is based on a new simulation scheme , called geometric sampling , which uses multiple short trajectories rather than a single infinitely long trajectory
in an earlier article j schubert , on nonspecific evidence , int j intell syst 8 \( 6 \) , 711 725 \( 1993 \) we established within dempster shafer theory a criterion function called the metaconflict function with this criterion we can partition into subsets a set of several pieces of evidence with propositions that are weakly specified in the sense that it may be uncertain to which event a proposition is referring each subset in the partitioning is representing a separate event the metaconflict function was derived as the plausibility that the partitioning is correct when viewing the conflict in dempster 's rule within each subset as a newly constructed piece of metalevel evidence with a proposition giving support against the entire partitioning in this article we extend the results of the previous article we will not only find the most plausible subset for each piece of evidence as was done in the earlier article in addition we will specify each piece of nonspecific evidence , in the sense that we find to which events the proposition might be referring , by finding the plausibility for every subset that this piece of evidence belong to the subset in doing this we will automatically receive indication that some evidence might be false we will then develop a new methodology to exploit these newly specified pieces of evidence in a subsequent reasoning process this will include methods to discount evidence based on their degree of falsity and on their degree of credibility due to a partial specification of affiliation , as well as a refined method to infer the event of each subset
in this paper , we derive the capacity of a special class of mesh networks a mesh network is defined as a heterogeneous wireless network in which the transmission among power limited nodes is assisted by powerful relays , which use the same wireless medium we find the capacity of the mesh network when there is one source , one destination , and multiple relays we call this channel the single source multiple relay single destination \( ssmrsd \) mesh network our approach is as follows we first look at an upper bound on the information theoretic capacity of these networks in the gaussian setting we then show that the bound is achievable asymptotically using the compress forward strategy for the multiple relay channel theoretically , the results indicate the value of cooperation and the utility of carefully deployed relays in wireless ad hoc and sensor networks the capacity characterization quantifies how the relays can be used to either conserve node energy or to increase transmission rate
in this paper we present videoset , a method for video summary evaluation through text that can evaluate how well a video summary is able to retain the semantic information contained in its original video we observe that semantics is most easily expressed in words , and develop a text based approach for the evaluation given a video summary , a text representation of the video summary is first generated , and an nlp based metric is then used to measure its semantic distance to ground truth text summaries written by humans we show that our technique has higher agreement with human judgment than pixel based distance metrics we also release text annotations and ground truth text summaries for a number of publicly available video datasets , for use by the computer vision community
we present new algorithms for personalized pagerank estimation and personalized pagerank search first , for the problem of estimating personalized pagerank \( ppr \) from a source distribution to a target node , we present a new bidirectional estimator with simple yet strong guarantees on correctness and performance , and 3x to 8x speedup over existing estimators in simulations on a diverse set of networks moreover , it has a clean algebraic structure which enables it to be used as a primitive for the personalized pagerank search problem given a network like facebook , a query like people named john , and a searching user , return the top nodes in the network ranked by ppr from the perspective of the searching user previous solutions require scoring all candidate nodes matching the query , which is prohibitively slow for large candidate sets we develop a new algorithm based on our bidirectional ppr estimator which allows us to sample nodes from any given subset based on their ppr this is the first solution to ppr search that can find the best candidates without iterating through the set of candidates finally , by combining ppr sampling with sequential ppr estimation and monte carlo , we develop practical algorithms for ppr search we show via experiments that our algorithms are efficient on networks with billions of edges
we present a new cycle flow based method for finding fuzzy partitions of weighted directed networks coming from time series data we show that this method overcomes essential problems of most existing clustering approaches , which tend to ignore important directional information by considering only one step , one directional node connections our method introduces a novel measure of communication between nodes using multi step , bidirectional transitions encoded by a cycle decomposition of the probability flow symmetric properties of this measure enable us to construct an undirected graph that captures information flow of the original graph seen by the data and apply clustering methods designed for undirected graphs finally , we demonstrate our algorithm by analyzing earthquake time series data , which naturally induce \( time \) directed networks this article has been published originally in epl , doi 10 1209 0295 5075 108 68008 this version differs from the published version by minor formatting details
whilst not all spreadsheet defects are structural in nature , poor layout choices can compromise spreadsheet quality these defects may be avoided at the development stage by some simple mistake prevention and detection devices poka yoke \( japanese for mistake proofing \) , which owes its genesis to the toyota production system \( the standard for manufacturing excellence throughout the world \) offers some principles that may be applied to reducing spreadsheet defects in this paper we examine spreadsheet structure and how it can lead to defects and illustrate some basic spreadsheet poka yokes to reduce them these include guidelines on how to arrange areas of cells so that whole rows and columns can be inserted anywhere without causing errors , and rules for when to use relative and absolute references with respect to what type of area is being referred to
smart distribution grids should efficiently integrate stochastic renewable resources while effecting voltage regulation the design of energy management schemes is challenging , one of the reasons being that energy management is a multistage problem where decisions are not all made at the same timescale and must account for the variability during real time operation the joint dispatch of slow and fast timescale controls in a smart distribution grid is considered here the substation voltage , the energy exchanged with a main grid , and the generation schedules for small diesel generators have to be decided on a slow timescale whereas optimal photovoltaic inverter setpoints are found on a more frequent basis while inverter and looser voltage regulation limits are imposed at all times , tighter bus voltage constraints are enforced on the average or in probability , thus enabling more efficient renewable integration upon reformulating the two stage grid dispatch as a stochastic convex concave problem , two distribution free schemes are put forth an average dispatch algorithm converges provably to the optimal two stage decisions via a sequence of convex quadratic programs its non convex probabilistic alternative entails solving two slightly different convex problems and is numerically shown to converge numerical tests on a real world distribution feeder verify that both novel data driven schemes yield lower costs over competing alternatives
with the outburst of smart phones today , the market is exploding with various mobile applications this paper proposes an application using which visually impaired people can type a note in grade 1 braille and save it in the external memory of their smart phone the application also shows intelligence by activating reminders and or calling certain contacts based on the content in the notes
the system presented here shows the feasibility of modeling the knowledge involved in a complex musical activity by integrating sub symbolic and symbolic processes this research focuses on the question of whether there is any advantage in integrating a neural network together with a distributed artificial intelligence approach within the music domain the primary purpose of our work is to design a model that describes the different aspects a user might be interested in considering when involved in a musical activity the approach we suggest in this work enables the musician to encode his knowledge , intuitions , and aesthetic taste into different modules the system captures these aspects by computing and applying three distinct functions rules , fuzzy concepts , and learning as a case study , we began experimenting with first species two part counterpoint melodies we have developed a hybrid system composed of a connectionist module and an agent based module to combine the sub symbolic and symbolic levels to achieve this task the technique presented here to represent musical knowledge constitutes a new approach for composing polyphonic music
we consider the design and analysis of the efficiently encodable rate compatible \( e 2rc \) irregular ldpc codes proposed in previous work in this work we introduce semi structured e 2rc like codes and protograph e 2rc codes exit chart based methods are developed for the design of semi structured e 2rc like codes that allow us to determine near optimal degree distributions for the systematic part of the code while taking into account the structure of the deterministic parity part , thus resolving one of the open issues in the original construction we develop a fast exit function computation method that does not rely on monte carlo simulations and can be used in other scenarios as well our approach allows us to jointly optimize code performance across the range of rates under puncturing we then consider protograph e 2rc codes \( that have a protograph representation \) and propose rules for designing a family of rate compatible punctured protographs with low thresholds for both the semi structured and protograph e 2rc families we obtain codes whose gap to capacity is at most 0 3 db across the range of rates when the maximum variable node degree is twenty
consider a non standard numeration system like the one built over the fibonacci sequence where nonnegative integers are represented by words over 0 , 1 without two consecutive 1 given a set x of integers such that the language of their greedy representations in this system is accepted by a finite automaton , we consider the problem of deciding whether or not x is a finite union of arithmetic progressions we obtain a decision procedure for this problem , under some hypothesis about the considered numeration system in a second part , we obtain an analogous decision result for a particular class of abstract numeration systems built on an infinite regular language
the capacity of the channel defined by the stochastic nonlinear schr odinger equation , which includes the effects of the kerr nonlinearity and amplified spontaneous emission noise , is considered in the case of zero dispersion in the absence of dispersion , this channel behaves as a collection of parallel per sample channels the conditional probability density function of the nonlinear per sample channels is derived using both a sum product and a fokker planck differential equation approach it is shown that , for a fixed noise power , the per sample capacity grows unboundedly with input signal the channel can be partitioned into amplitude and phase subchannels , and it is shown that the contribution to the total capacity of the phase channel declines for large input powers it is found that a two dimensional distribution with a half gaussian profile on the amplitude and uniform phase provides a lower bound for the zero dispersion optical fiber channel , which is simple and asymptotically capacity achieving at high signal to noise ratios \( snrs \) a lower bound on the capacity is also derived in the medium snr region the exact capacity subject to peak and average power constraints is numerically quantified using dense multiple ring modulation formats the differential model underlying the zero dispersion channel is reduced to an algebraic model , which is more tractable for digital communication studies , and in particular it provides a relation between the zero dispersion optical channel and a 2 times 2 multiple input multiple output rician fading channel it appears that the structure of the capacity achieving input distribution resembles that of the rician fading channel , i e , it is discrete in amplitude with a finite number of mass points , while continuous and uniform in phase
in this work , we show that reconstructing a sparse signal from quantized compressive measurement can be achieved in an unified formalism whatever the \( scalar \) quantization resolution , i e , from 1 bit to high resolution assumption this is achieved by generalizing the iterative hard thresholding \( iht \) algorithm and its binary variant \( biht \) introduced in previous works to enforce the consistency of the reconstructed signal with respect to the quantization model the performance of this algorithm , simply called quantized iht \( qiht \) , is evaluated in comparison with other approaches \( e g , iht , basis pursuit denoise \) for several quantization scenarios
recently , extensive efforts have been made on the application of expert system technique to solving the process planning task in the machining domain this paper introduces a new formal method to design capp expert systems the formal method is applied to provide a contour of the capp expert system building technology theoretical aspects of the formalism are described and illustrated by an example of know how analysis flexible facilities to utilize multiple knowledge types and multiple planning strategies within one system are provided by the technology
pushing files to users based on predicting the personal interest of each user may provide higher throughput gain than broadcasting popular files to users based on their common interests however , the energy consumed at base station for pushing files individually to each user is also higher than broadcast in this paper , we propose an energy saving transmission strategy for pre downloading the files to each user by exploiting the excess resources in the network during off peak time specifically , a power allocation and scheduling algorithm is designed aimed to minimize the extra energy consumed for pushing , where network and user level context information are exploited simulation results show that when the energy of both content placement and content delivery is taken into account , the proposed unicast strategy consumes less energy and achieves higher throughput than broadcasting when the files popularity is not uniform and the personal interest prediction is with less uncertainty
in this work , we propose two stochastic architectural models \( cmc and cmc m \) with two layers of classifiers applicable to datasets with one and multiple skewed classes this distinction becomes important when the datasets have a large number of classes therefore , we present a novel solution to imbalanced multiclass learning with several skewed majority classes , which improves minority classes identification this fact is particularly important for text classification tasks , such as event detection our models combined with pre processing sampling techniques improved the classification results on six well known datasets finally , we have also introduced a new metric sg mean to overcome the multiplication by zero limitation of g mean
the concept of effective complexity of an object as the minimal description length of its regularities has been initiated by gell mann and lloyd the regularities are modeled by means of ensembles , that is probability distributions on finite binary strings in our previous paper we propose a definition of effective complexity in precise terms of algorithmic information theory here we investigate the effective complexity of binary strings generated by stationary , in general not computable , processes we show that under not too strong conditions long typical process realizations are effectively simple our results become most transparent in the context of coarse effective complexity which is a modification of the original notion of effective complexity that uses less parameters in its definition a similar modification of the related concept of sophistication has been suggested by antunes and fortnow
offloading work to cloud is one of the proposed solutions for increasing the battery life of mobile devices most prior research has focused on computation intensive applications , even though such applications are not the most popular ones in this paper , we first study the feasibility of method level offloading in network intensive applications , using an open source twitter client as an example our key observation is that implementing offloading transparently to the developer is difficult various constraints heavily limit the offloading possibilities , and estimation of the potential benefit is challenging we then propose a toolkit , smartdiet , to assist mobile application developers in creating code which is suitable for energy efficient offloading smartdiet provides fine grained offloading constraint identification and energy usage analysis for android applications in addition to outlining the overall functionality of the toolkit , we study some of its key mechanisms and identify the remaining challenges
we prove general exponential moment inequalities for averages of 0 , 1 valued iid random variables and use them to tighten the pac bayesian theorem the logarithmic dependence on the sample count in the enumerator of the pac bayesian bound is halved
scenarios are pen pictures of plausible futures , used for strategic planning the aim of this investigation is to expand the horizon of scenario based planning through computational models that are able to aid the analyst in the planning process the investigation builds upon the advances of information and communication technology \( ict \) to create a novel , flexible and customizable computational capability based planning methodology that is practical and theoretically sound we will show how evolutionary computation , in particular evolutionary multi objective optimization , can play a central role both as an optimizer and as a source for innovation
in this paper , a novel concept called a textit uniquely factorable constellation pair \( ufcp \) is proposed for the systematic design of a noncoherent full diversity collaborative unitary space time block code by normalizing two alamouti codes for a wireless communication system having two transmitter antennas and a single receiver antenna it is proved that such a unitary ufcp code assures the unique identification of both channel coefficients and transmitted signals in a noise free case as well as full diversity for the noncoherent maximum likelihood \( ml \) receiver in a noise case to further improve error performance , an optimal unitary ufcp code is designed by appropriately and uniquely factorizing a pair of energy efficient cross quadrature amplitude modulation \( qam \) constellations to maximize the coding gain subject to a transmission bit rate constraint after a deep investigation of the fractional coding gain function , a technical approach developed in this paper to maximizing the coding gain is to carefully design an energy scale to compress the first three largest energy points in the corner of the qam constellations in the denominator of the objective as well as carefully design a constellation triple forming two ufcps , with one collaborating with the other two so as to make the accumulated minimum euclidean distance along the two transmitter antennas in the numerator of the objective as large as possible and at the same time , to avoid as many corner points of the qam constellations with the largest energy as possible to achieve the minimum of the numerator in other words , the optimal coding gain is attained by intelligent constellations collaboration and efficient energy compression
artificial chemistries \( acs \) are symbolic chemical metaphors for the exploration of artificial life , with specific focus on the origin of life in this work we define a p system based artificial graph chemistry to understand the principles leading to the evolution of life like structures in an ac set up and to develop a unified framework to characterize and classify symbolic artificial chemistries by devising appropriate formalism to capture semantic and organizational information an extension of p system is considered by associating probabilities with the rules providing the topological framework for the evolution of a labeled undirected graph based molecular reaction semantics
within a statistical learning setting , we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method in particular , we show that , if all other parameters are fixed a priori , the number of passes over the data \( epochs \) acts as a regularization parameter , and prove strong universal consistency , i e almost sure convergence of the risk , as well as sharp finite sample bounds for the iterates our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results
in this note we prove that every closed graph g is up to isomorphism a proper interval graph as a consequence we obtain that there exist linear time algorithms for closed graph recognition
the peer to peer \( p2p \) economy relies on establishing trust in distributed networked systems , where the reliability of a user is assessed through digital peer review processes that aggregate ratings into reputation scores here we present evidence of a network effect which biases the digital reputations of the users of p2p networks , showing that p2p networks display exceedingly high levels of reciprocity in fact , these are so large that they are close to the highest levels structurally compatible with the networks' reputation landscape this shows that the crowdsourcing process underpinning digital reputation is significantly distorted by the attempt of users to mutually boost reputation , or to retaliate , through the exchange of ratings we show that the least active users are predominantly responsible for such reciprocity induced bias , and that this fact can be exploited to suppress the bias itself
orthogonal designs are fundamental mathematical notions used in the construction of space time block codes for wireless transmissions designs have two important parameters , the rate and the decoding delay the main problem of the theory is to construct designs maximizing the rate and minimizing the decoding delay all known constructions of cods are inductive or algorithmic in this paper , we present an explicit construction of optimal cods we do not apply recurrent procedures and do calculate the matrix elements directly our formula is based on a cubic function in two binary n vectors in our previous work \( comm math phys , 2010 , and j pure and appl algebra , 2011 \) , we used this function to define a series of non associative algebras generalizing the classical algebra of octonions and to obtain sum of squares identities of hurwitz radon type
given a polytope p in mathbb r n , we say that p has a positive semidefinite lift \( psd lift \) of size d if one can express p as the linear projection of an affine slice of the positive semidefinite cone mathbf s d if a polytope p has symmetry , we can consider equivariant psd lifts , i e those psd lifts that respect the symmetry of p one of the simplest families of polytopes with interesting symmetries are regular polygons in the plane , which have played an important role in the study of linear programming lifts \( or extended formulations \) in this paper we study equivariant psd lifts of regular polygons we first show that the standard lasserre sum of squares hierarchy for the regular n gon requires exactly ceil \( n 4 \) iterations and thus yields an equivariant psd lift of size linear in n in contrast we show that one can construct an equivariant psd lift of the regular 2 n gon of size 2n 1 , which is exponentially smaller than the psd lift of the sum of squares hierarchy our construction relies on finding a sparse sum of squares certificate for the facet defining inequalities of the regular 2 n gon , i e , one that only uses a small \( logarithmic \) number of monomials since any equivariant lp lift of the regular 2 n gon must have size 2 n , this gives the first example of a polytope with an exponential gap between sizes of equivariant lp lifts and equivariant psd lifts finally we prove that our construction is essentially optimal by showing that any equivariant psd lift of the regular n gon must have size at least logarithmic in n
multiresolution matrix factorization \( mmf \) was recently introduced as a method for finding multiscale structure and defining wavelets on graphs matrices in this paper we derive pmmf , a parallel algorithm for computing the mmf factorization empirically , the running time of pmmf scales linearly in the dimension for sparse matrices we argue that this makes pmmf a valuable new computational primitive in its own right , and present experiments on using pmmf for two distinct purposes compressing matrices and preconditioning large sparse linear systems
this paper presents a super resolution method based on gradient based adaptive interpolation in this method , in addition to considering the distance between the interpolated pixel and the neighboring valid pixel , the interpolation coefficients take the local gradient of the original image into account the smaller the local gradient of a pixel is , the more influence it should have on the interpolated pixel and the interpolated high resolution image is finally deblurred by the application of wiener filter experimental results show that our proposed method not only substantially improves the subjective and objective quality of restored images , especially enhances edges , but also is robust to the registration error and has low computational complexity
discrete fourier transforms \( dfts \) over finite fields have widespread applications in digital communication and storage systems hence , reducing the computational complexities of dfts is of great significance recently proposed cyclotomic fast fourier transforms \( cffts \) are promising due to their low multiplicative complexities unfortunately , there are two issues with cffts \( 1 \) they rely on efficient short cyclic convolution algorithms , which has not been investigated thoroughly yet , and \( 2 \) they have very high additive complexities when directly implemented in this paper , we address both issues one of the main contributions of this paper is efficient bilinear 11 point cyclic convolution algorithms , which allow us to construct cffts over gf \( 2 11 \) the other main contribution of this paper is that we propose composite cyclotomic fourier transforms \( ccfts \) in comparison to previously proposed fast fourier transforms , our ccfts achieve lower overall complexities for moderate to long lengths , and the improvement significantly increases as the length grows our 2047 point and 4095 point ccfts are also first efficient dfts of such lengths to the best of our knowledge finally , our ccfts are also advantageous for hardware implementations due to their regular and modular structure
email service providers have employed many email classification and prioritization systems over the last decade to improve their services in order to assist email services , we propose a personalized email community detection method to discover the groupings of email users based on their structural and semantic intimacy we extract the personalized social graph from a set of emails by uniquely leveraging each node with communication behavior subsequently , collaborative similarity measure \( csm \) based intra graph clustering approach detects personalized communities the empirical analysis shows effectiveness of the resultant communities in terms of evaluation measures , i e density , entropy and f measure moreover , email strainer , dynamic group prediction , and fraudulent account detection are suggested as the potential applications from both the service provider and user 's point of view
the recent work by marcus , spielman and srivastava proves the existence of bipartite ramanujan \( multi \) graphs of all degrees and all sizes however , that paper did not provide a polynomial time algorithm to actually compute such graphs here , we provide a polynomial time algorithm to compute certain expected characteristic polynomials related to this construction this leads to a deterministic polynomial time algorithm to compute bipartite ramanujan \( multi \) graphs of all degrees and all sizes
this paper presents a theory and an empirical evaluation of higher order quantum inspired genetic algorithms fundamental notions of the theory have been introduced , and a novel order 2 quantum inspired genetic algorithm \( qiga2 \) has been presented contrary to all qiga algorithms which represent quantum genes as independent qubits , in higher order qigas quantum registers are used to represent genes strings which allows modelling of genes relations using quantum phenomena performance comparison has been conducted on a benchmark of 20 deceptive combinatorial optimization problems it has been presented that using higher quantum orders is beneficial for genetic algorithm efficiency , and the new qiga2 algorithm outperforms the old qiga algorithm which was tuned in highly compute intensive metaoptimization process
an accurate and fair assessment of the efficiency and impact of scientific work is , despite a lot of recent research effort , still an open problem the measurement of quality and success of individual scientists and research groups can be approached from many different directions , none of which is universal a reason for this is inherently different behavior of different scientists within the global research community a complex evaluation of ones publication activities requires a careful consideration of a wide variety of factors the well known h index is one of the most used bibliometric indices despite its many imperfections , its simplicity and ease of interpretation make it a popular scientometric method this short paper uses the ideas behind the h index to analyze communities of authors who cite publishing scientists a new author evaluation measure named ah index is proposed , and intuitive interpretations of its properties and semantics are presented preliminary experiments with authors with high h index active in the area of computer science are presented to demonstrate the properties of the proposed measure
this paper derives the outage probability and transmission capacity of ad hoc wireless networks with nodes employing multiple antenna diversity techniques , for a general class of signal distributions this analysis allows system performance to be quantified for fading or non fading environments the transmission capacity is given for interference limited uniformly random networks on the entire plane with path loss exponent alpha 2 in which nodes use \( 1 \) static beamforming through m sectorized antennas , for which the increase in transmission capacity is shown to be theta \( m 2 \) if the antennas are without sidelobes , but less in the event of a nonzero sidelobe level \( 2 \) dynamic eigen beamforming \( maximal ratio transmission combining \) , in which the increase is shown to be theta \( m frac 2 alpha \) \( 3 \) various transmit antenna selection and receive antenna selection combining schemes , which give appreciable but rapidly diminishing gains and \( 4 \) orthogonal space time block coding , for which there is only a small gain due to channel hardening , equivalent to nakagami m fading for increasing m it is concluded that in ad hoc networks , static and dynamic beamforming perform best , selection combining performs well but with rapidly diminishing returns with added antennas , and that space time block coding offers only marginal gains
in this paper , we propose a systematic design of space time block codes \( stbc \) which can achieve high rate and full diversity when the partial interference cancellation \( pic \) group decoding is used at receivers the proposed codes can be applied to any number of transmit antennas and admit a low decoding complexity while achieving full diversity for m transmit antennas , in each codeword real and imaginary parts of pm complex information symbols are parsed into p diagonal layers and then encoded , respectively with pic group decoding , it is shown that the decoding complexity can be reduced to a joint decoding of m 2 real symbols in particular , for 4 transmit antennas , the code has real symbol pairwise \( i e , single complex symbol \) decoding that achieves full diversity and the code rate is 4 3 simulation results demonstrate that the full diversity is offered by the newly proposed stbc with the pic group decoding
the proposed framework provides a general model of concurrent imperative programming programs are modeled as formal languages and concurrency as an interleaving \( or shuffle \) operator this yields a simple and elegant algebra of programs the framework supports the views program logic by dinsdale young and others , which generalizes various type systems and separation logic approaches to program correctness it also validates familiar operational calculi in small step and big step flavours the consistency of the program logic with respect to the operational rules is established directly and does not use induction on derivations in fact the whole framework uses only straightforward mathematics parametric in states , views and basic commands , it can be instantiated to a variety of concrete languages and settings
in this paper , the bit error performance of a family of likelihood ascent search \( las \) multiuser detectors is analyzed an upper bound on the ber of any las detector is obtained by bounding the fixed point region with the worst initial detector the concept of indecomposable errors developed by verdu is applied to tighten the upper bound in a special instance , the upper bound is reduced to that for all the local maximum likelihood detectors the upper bound is comparable with that of the optimum detector obtained by verdu a lower bound on the asymptotic multiuser efficiency \( ame \) is then obtained it is shown that there are nontrivial cdma channels such that a las detector can achieve unit ame regardless of user number the ame lower bound provides a means for further seeking a good set of spreading sequences and power distribution for spectral and power efficient cdma
while the linked data community provides many big and interlinked datasets , efficient usage of the knowledge is often limited by the need for domain experts to formulate the right sparql queries to answer questions for each new question they have to decide which datasets are suitable and in which terminology and modelling style to phrase the sparql query in this work we present an evolutionary algorithm to help with this challenging task our algorithm can learn sparql bgp queries common to a list of source target pairs from a given sparql endpoint amongst others , we apply our algorithm to a dataset of several hundred human associations \( such as circle square \) to find patterns for them in dbpedia we show the scalability of the algorithm by running it against a sparql endpoint loaded with 7 9 billion triples further , we use the resulting sparql queries to mimic human associations with a recall 10 63 9
to achieve high range resolution profile \( hrrp \) , the geometric theory of diffraction \( gtd \) parametric model is widely used in stepped frequency radar system in the paper , a fast synthetic range profile algorithm , called orthogonal matching pursuit with sensing dictionary \( omp sd \) , is proposed it formulates the traditional hrrp synthetic to be a sparse approximation problem over redundant dictionary as it employs a priori information that targets are sparsely distributed in the range space , the synthetic range profile \( srp \) can be accomplished even in presence of data lost besides , the computational complexity is reduced by introducing sensing dictionary \( sd \) and it mitigates the model mismatch at the same time the computation complexity decreases from o \( mndk \) flops for omp to o \( m \( n d \) k \) flops for omp sd simulation experiments illustrate its advantages both in additive white gaussian noise \( awgn \) and noiseless situation , respectively
we investigate the complexity of finding a winning strategy for the mis `ere version of three games played on graphs two variants of the game text nimg , introduced by stockmann in 2004 and the game text vertex geography on both directed and undirected graphs we show that on general graphs those three games are text pspace hard or complete for one text pspace hard variant of text nimg , we find an algorithm to compute an effective winning strategy in time mathcal o \( sqrt v \( g \) e \( g \) \) when g is a bipartite graph
we propose a robust elastic net \( ren \) model for high dimensional sparse regression and give its performance guarantees \( both the statistical error bound and the optimization bound \) a simple idea of trimming the inner product is applied to the elastic net model specifically , we robustify the covariance matrix by trimming the inner product based on the intuition that the trimmed inner product can not be significant affected by a bounded number of arbitrarily corrupted points \( outliers \) the ren model can also derive two interesting special cases robust lasso and robust soft thresholding comprehensive experimental results show that the robustness of the proposed model consistently outperforms the original elastic net and matches the performance guarantees nicely
this paper studies the resource allocation algorithm design for multiuser coordinated multipoint \( comp \) networks with simultaneous wireless information and power transfer \( swipt \) in particular , remote radio heads \( rrhs \) are connected to a central processor \( cp \) via capacity limited backhaul links to facilitate comp joint transmission besides , the cp transfers energy to the rrhs for more efficient network operation the considered resource allocation algorithm design is formulated as a non convex optimization problem with a minimum required signal to interference plus noise ratio \( sinr \) constraint at multiple information receivers and a minimum required power transfer constraint at the energy harvesting receivers by optimizing the transmit beamforming vectors at the cp and energy sharing between the cp and the rrhs , we aim at jointly minimizing the total network transmit power and the maximum capacity consumption per backhaul link the resulting non convex optimization problem is np hard in light of the intractability of the problem , we reformulate it by replacing the non convex objective function with its convex hull , which enables the derivation of an efficient iterative resource allocation algorithm in each iteration , a non convex optimization problem is solved by semi definite programming \( sdp \) relaxation and the proposed iterative algorithm converges to a local optimal solution of the original problem simulation results illustrate that our proposed algorithm achieves a close to optimal performance and provides a significant reduction in backhaul capacity consumption compared to full cooperation besides , the considered comp network is shown to provide superior system performance as far as power consumption is concerned compared to a traditional system with multiple antennas co located
we prove a conjecture of abdullah , cooper and draief 2015 , that for every connected graph on n vertices , the random walk with transition function based on the minimum degree weighting scheme has cover time at most o \( n 2 \)
we study the feasibility of short finite impulse response \( fir \) synthesis for perfect reconstruction \( pr \) in generic fir filter banks among all pr synthesis banks , we focus on the one with the minimum filter length for filter banks with oversampling factors of at least two , we provide prescriptions for the shortest filter length of the synthesis bank that would guarantee pr almost surely the prescribed length is as short or shorter than the analysis filters and has an approximate inverse relationship with the oversampling factor our results are in form of necessary and sufficient statements that hold generically , hence only fail for elaborately designed nongeneric examples we provide extensive numerical verification of the theoretical results and demonstrate that the gap between the derived filter length prescriptions and the true minimum is small the results have potential applications in synthesis fb design problems , where the analysis bank is given , and for analysis of fundamental limitations in blind signals reconstruction from data collected by unknown subsampled multi channel systems
optimal and suboptimal decentralized estimators in wireless sensor networks \( wsns \) over orthogonal multiple access fading channels are studied in this paper considering multiple bit quantization before digital transmission , we develop maximum likelihood estimators \( mles \) with both known and unknown channel state information \( csi \) when training symbols are available , we derive a mle that is a special case of the mle with unknown csi it implicitly uses the training symbols to estimate the channel coefficients and exploits the estimated csi in an optimal way to reduce the computational complexity , we propose suboptimal estimators these estimators exploit both signal and data level redundant information to improve the estimation performance the proposed mles reduce to traditional fusion based or diversity based estimators when communications or observations are perfect by introducing a general message function , the proposed estimators can be applied when various analog or digital transmission schemes are used the simulations show that the estimators using digital communications with multiple bit quantization outperform the estimator using analog and forwarding transmission in fading channels when considering the total bandwidth and energy constraints , the mle using multiple bit quantization is superior to that using binary quantization at medium and high observation signal to noise ratio levels
there has been a tremendous growth in publicly available digital video footage over the past decade this has necessitated the development of new techniques in computer vision geared towards efficient analysis , storage and retrieval of such data many mid level computer vision tasks such as segmentation , object detection , tracking , etc involve an inference problem based on the video data available video data has a high degree of spatial and temporal coherence the property must be intelligently leveraged in order to obtain better results graphical models , such as markov random fields , have emerged as a powerful tool for such inference problems they are naturally suited for expressing the spatial dependencies present in video data , it is however , not clear , how to extend the existing techniques for the problem of inference over time this thesis explores the path probability method , a variational technique in statistical mechanics , in the context of graphical models and approximate inference problems it extends the method to a general framework for problems involving inference in time , resulting in an algorithm , emph dynbp we explore the relation of the algorithm with existing techniques , and find the algorithm competitive with existing approaches the main contribution of this thesis are the extended gbp algorithm , the extension of path probability methods to the dynbp algorithm and the relationship between them we have also explored some applications in computer vision involving temporal evolution with promising results
awale games have become widely recognized across the world , for their innovative strategies and techniques which were used in evolving the agents \( player \) and have produced interesting results under various conditions this paper will compare the results of the two major machine learning techniques by reviewing their performance when using minimax , endgame database , a combination of both techniques or other techniques , and will determine which are the best techniques
we show that the first order theory of structural subtyping of non recursive types is decidable let sigma be a language consisting of function symbols \( representing type constructors \) and c a decidable structure in the relational language l containing a binary relation leq c represents primitive types leq represents a subtype ordering we introduce the notion of sigma term power of c , which generalizes the structure arising in structural subtyping the domain of the sigma term power of c is the set of sigma terms over the set of elements of c we show that the decidability of the first order theory of c implies the decidability of the first order theory of the sigma term power of c our decision procedure makes use of quantifier elimination for term algebras and feferman vaught theorem our result implies the decidability of the first order theory of structural subtyping of non recursive types
among the vast information available on the web , social media streams capture what people currently pay attention to and how they feel about certain topics awareness of such trending topics plays a crucial role in multimedia systems such as trend aware recommendation and automatic vocabulary selection for video concept detection systems correctly utilizing trending topics requires a better understanding of their various characteristics in different social media streams to this end , we present the first comprehensive study across three major online and social media streams , twitter , google , and wikipedia , covering thousands of trending topics during an observation period of an entire year our results indicate that depending on one 's requirements one does not necessarily have to turn to twitter for information about current events and that some media streams strongly emphasize content of specific categories as our second key contribution , we further present a novel approach for the challenging task of forecasting the life cycle of trending topics in the very moment they emerge our fully automated approach is based on a nearest neighbor forecasting technique exploiting our assumption that semantically similar topics exhibit similar behavior we demonstrate on a large scale dataset of wikipedia page view statistics that forecasts by the proposed approach are about 9 48k views closer to the actual viewing statistics compared to baseline methods and achieve a mean average percentage error of 45 19 for time periods of up to 14 days
a novel rateless coding assisted multi packet relaying \( rmpr \) protocol is proposed for large size data spreading in mobile wireless networks with this lightweight and robust protocol , the packet redundancy is reduced by a factor of sqrt n , while the spreading time is reduced at least by a factor of ln \( n \) closed form bounds and explicit non asymptotic results are presented , which are further validated through simulations besides , the packet duplication phenomenon in the network setting is analyzed for the first time
many algorithms in machine learning and computational geometry require , as input , the intrinsic dimension of the manifold that supports the probability distribution of the data this parameter is rarely known and therefore has to be estimated we characterize the statistical difficulty of this problem by deriving upper and lower bounds on the minimax rate for estimating the dimension first , we consider the problem of testing the hypothesis that the support of the data generating probability distribution is a well behaved manifold of intrinsic dimension d 1 versus the alternative that it is of dimension d 2 , with d 1 d 2 with an i i d sample of size n , we provide an upper bound on the probability of choosing the wrong dimension of o left \( n left \( d 2 d 1 1 epsilon right \) n right \) , where epsilon is an arbitrarily small positive number the proof is based on bounding the length of the traveling salesman path through the data points we also demonstrate a lower bound of omega left \( n \( 2d 2 2d 1 epsilon \) n right \) , by applying le cam 's lemma with a specific set of d 1 dimensional probability distributions we then extend these results to get minimax rates for estimating the dimension of well behaved manifolds we obtain an upper bound of order o left \( n \( frac 1 m 1 epsilon \) n right \) and a lower bound of order omega left \( n \( 2 epsilon \) n right \) , where m is the embedding dimension
we recently measured the average distance of users in the facebook graph , spurring comments in the scientific community as well as in the general press \( four degrees of separation \) a number of interesting criticisms have been made about the meaningfulness , methods and consequences of the experiment we performed in this paper we want to discuss some methodological aspects that we deem important to underline in the form of answers to the questions we have read in newspapers , magazines , blogs , or heard from colleagues we indulge in some reflections on the actual meaning of average distance and make a number of side observations showing that , yes , 3 74 degrees of separation are really few
strong typicality and the markov lemma have been used in the proofs of several multiterminal source coding theorems since these two tools can be applied to finite alphabets only , the results proved by them are subject to the same limitation recently , a new notion of typicality , namely unified typicality , has been defined it can be applied to both finite or countably infinite alphabets , and it retains the asymptotic equipartition property and the structural properties of strong typicality in this paper , unified typicality is used to derive a version of the markov lemma which works on both finite or countably infinite alphabets so that many results in multiterminal source coding can readily be extended furthermore , a simple way to verify whether some sequences are jointly typical is shown
multiuser switched diversity scheduling schemes were recently proposed in order to overcome the heavy feedback requirements of conventional opportunistic scheduling schemes by applying a threshold based , distributed , and ordered scheduling mechanism the main idea behind these schemes is that slight reduction in the prospected multiuser diversity gains is an acceptable trade off for great savings in terms of required channel state information feedback messages in this work , we characterize the achievable rate region of multiuser switched diversity systems and compare it with the rate region of full feedback multiuser diversity systems we propose also a novel proportional fair multiuser switched based scheduling scheme and we demonstrate that it can be optimized using a practical and distributed method to obtain the feedback thresholds we finally demonstrate by numerical examples that switched diversity scheduling schemes operate within 0 3 bits sec hz from the ultimate network capacity of full feedback systems in rayleigh fading conditions
in this paper , we consider a parallel relay network where two relays cooperatively help a source transmit to a destination we assume the source and the destination nodes are equipped with multiple antennas three basic schemes and their achievable rates are studied decode and forward \( df \) , amplify and forward \( af \) , and compress and forward \( cf \) for the df scheme , the source transmits two private signals , one for each relay , where dirty paper coding \( dpc \) is used between the two private streams , and a common signal for both relays the relays make efficient use of the common information to introduce a proper amount of correlation in the transmission to the destination we show that the df scheme achieves the capacity under certain conditions we also show that the cf scheme is asymptotically optimal in the high relay power limit , regardless of channel ranks it turns out that the af scheme also achieves the asymptotic optimality but only when the relays to destination channel is full rank the relative advantages of the three schemes are discussed with numerical results
in this paper , we specify a class of mathematical problems , which we refer to as function density problems \( fdps , in short \) , and point out novel connections of fdps to the following two cryptographic topics theoretical security evaluations of keyless hash functions \( such as sha 1 \) , and constructions of provably secure pseudorandom generators \( prgs \) with some enhanced security property introduced by dubrov and ishai stoc 2006 our argument aims at proposing new theoretical frameworks for these topics \( especially for the former \) based on fdps , rather than providing some concrete and practical results on the topics we also give some examples of mathematical discussions on fdps , which would be of independent interest from mathematical viewpoints finally , we discuss possible directions of future research on other cryptographic applications of fdps and on mathematical studies on fdps themselves
tensors provide natural representations for massive multi mode datasets and tensor methods also form the backbone of many machine learning , signal processing , and statistical algorithms the utility of tensors is mainly due to the ability to identify overcomplete , non orthogonal factors from tensor data , which is known as tensor decomposition this work develops theories and computational methods for guaranteed overcomplete , non orthogonal tensor decomposition using convex optimization we consider tensor decomposition as a problem of measure estimation from moments we develop the theory for guaranteed decomposition under three assumptions \( i \) incoherence \( ii \) bounded spectral norm and \( iii \) gram isometry we show that under these three assumptions , one can retrieve tensor decomposition by solving a convex , infinite dimensional analog of ell 1 minimization on the space of measures the optimal value of this optimization defines the tensor nuclear norm that can be used to regularize tensor inverse problems , including tensor completion , denoising , and robust tensor principal component analysis remarkably , all the three assumptions are satisfied with high probability if the rank one tensor factors are uniformly distributed on the unit spheres , implying exact decomposition for tensors with random factors we also present and numerically test two computational methods based respectively on burer monteiro low rank factorization reformulation and the sum of squares relaxations
this paper investigates interference cancellation schemes at the receiver , in which the original data of the interference is known a priori such a priori knowledge is common in wireless relay networks for example , a transmitting relay could be relaying data that was previously transmitted by a node , in which case the interference received by the node now is actually self information besides the case of self information , the node could also have overheard or received the interference data in a prior transmission by another node directly removing the known interference requires accurate estimate of the interference channel , which may be difficult in many situations in this paper , we propose a novel scheme , blind known interference cancellation \( bkic \) , to cancel known interference without interference channel information bkic consists of two steps the first step combines adjacent symbols to cancel the interference , exploiting the fact that the channel coefficients are almost the same between successive symbols after such interference cancellation , however , the signal of interest is also distorted the second step recovers the signal of interest amidst the distortion we propose two algorithms for the critical second steps the first algorithm \( bkic s \) is based on the principle of smoothing it is simple and has near optimal performance in the slow fading scenario the second algorithm \( bkic rbp \) is based on the principle of real valued belief propagation it can achieve map optimal performance with fast convergence , and has near optimal performance even in the fast fading scenario both bkic schemes outperform the traditional self interference cancellation schemes with perfect initial channel information by a large margin , while having lower complexities
it has often been taken as a working assumption that directed links in information networks are frequently formed by short cutting a two step path between the source and the destination a kind of implicit link copying analogous to the process of triadic closure in social networks despite the role of this assumption in theoretical models such as preferential attachment , it has received very little direct empirical investigation here we develop a formalization and methodology for studying this type of directed closure process , and we provide evidence for its important role in the formation of links on twitter we then analyze a sequence of models designed to capture the structural phenomena related to directed closure that we observe in the twitter data
a relay channel is one in which a source and destination use an intermediate relay station in order to improve communication rates we propose the study of relay channels with classical inputs and quantum outputs and prove that a partial decode and forward strategy is achievable we divide the channel uses into many blocks and build codes in a randomized , block markov manner within each block the relay performs a standard holevo schumacher westmoreland quantum measurement on each block in order to decode part of the source 's message and then forwards this partial message in the next block the destination performs a novel sliding window quantum measurement on two adjacent blocks in order to decode the source 's message this strategy achieves non trivial rates for classical communication over a quantum relay channel
source separation or demixing is the process of extracting multiple components entangled within a signal contemporary signal processing presents a host of difficult source separation problems , from interference cancellation to background subtraction , blind deconvolution , and even dictionary learning despite the recent progress in each of these applications , advances in high throughput sensor technology place demixing algorithms under pressure to accommodate extremely high dimensional signals , separate an ever larger number of sources , and cope with more sophisticated signal and mixing models these difficulties are exacerbated by the need for real time action in automated decision making systems recent advances in convex optimization provide a simple framework for efficiently solving numerous difficult demixing problems this article provides an overview of the emerging field , explains the theory that governs the underlying procedures , and surveys algorithms that solve them efficiently we aim to equip practitioners with a toolkit for constructing their own demixing algorithms that work , as well as concrete intuition for why they work
compact closed categories provide a foundational formalism for a variety of important domains , including quantum computation these categories have a natural visualisation as a form of graphs we present a formalism for equational reasoning about such graphs and develop this into a generic proof system with a fixed logical kernel for equational reasoning about compact closed categories automating this reasoning process is motivated by the slow and error prone nature of manual graph manipulation a salient feature of our system is that it provides a formal and declarative account of derived results that can include `ellipses' style notation we illustrate the framework by instantiating it for a graphical language of quantum computation and show how this can be used to perform symbolic computation
this work provides simple algorithms for multi class \( and multi label \) prediction in settings where both the number of examples n and the data dimension d are relatively large these robust and parameter free algorithms are essentially iterative least squares updates and very versatile both in theory and in practice on the theoretical front , we present several variants with convergence guarantees owing to their effective use of second order structure , these algorithms are substantially better than first order methods in many practical scenarios on the empirical side , we present a scalable stagewise variant of our approach , which achieves dramatic computational speedups over popular optimization packages such as liblinear and vowpal wabbit on standard datasets \( mnist and cifar 10 \) , while attaining state of the art accuracies
recent research on temporal networks has highlighted the limitations of a static network perspective for our understanding of complex systems with dynamic topologies in particular , recent works have shown that i \) the specific order in which links occur in real world temporal networks affects causality structures and thus the evolution of dynamical processes , and ii \) higher order aggregate representations of temporal networks can be used to analytically study the effect of these order correlations on dynamical processes in this article we analyze the effect of order correlations on path based centrality measures in real world temporal networks analyzing temporal equivalents of betweenness , closeness and reach centrality in six empirical temporal networks , we first show that an analysis of the commonly used static , time aggregated representation can give misleading results about the actual importance of nodes we further study higher order time aggregated networks , a recently proposed generalization of the commonly applied static , time aggregated representation of temporal networks here , we particularly define path based centrality measures based on second order aggregate networks , empirically validating that node centralities calculated in this way better capture the true temporal centralities of nodes than node centralities calculated based on the commonly used static \( first order \) representation apart from providing a simple and practical method for the approximation of path based centralities in temporal networks , our results highlight interesting perspectives for the use of higher order aggregate networks in the analysis of time stamped network data
in this paper , we propose a multi channel full duplex medium access control \( mac \) protocol for cognitive radio networks \( mfdc mac \) our design exploits the fact that full duplex \( fd \) secondary users \( sus \) can perform spectrum sensing and access simultaneously , and we employ the randomized dynamic channel selection for load balancing among channels and the standard backoff mechanism for contention resolution on each available channel then , we develop a mathematical model to analyze the throughput performance of the proposed mfdc mac protocol furthermore , we study the protocol configuration optimization to maximize the network throughput where we show that this optimization can be performed in two steps , namely optimization of access and transmission parameters on each channel and optimization of channel selection probabilities of the users such optimization aims at achieving efficient self interference management for fd transceivers , sensing overhead control , and load balancing among the channels numerical results demonstrate the impacts of different protocol parameters and the importance of parameter optimization on the throughput performance as well as the significant performance gain of the proposed design compared to traditional design
the potential risk of privacy leakage prevents users from sharing their honest opinions on social platforms this paper addresses the problem of privacy preservation if the query returns the histogram of rankings the framework of differential privacy is applied to rank aggregation the error probability of the aggregated ranking is analyzed as a result of noise added in order to achieve differential privacy upper bounds on the error rates for any positional ranking rule are derived under the assumption that profiles are uniformly distributed simulation results are provided to validate the probabilistic analysis
weighted graphs obtained from co occurrence in user item relations lead to non metric topologies we use this semi metric behavior to issue recommendations , and discuss its relationship to transitive closure on fuzzy graphs finally , we test the performance of this method against other item and user based recommender systems on the movielens benchmark we show that including highly semi metric edges in our recommendation algorithms leads to better recommendations
local field potentials \( lfps \) sampled with extracellular electrodes are frequently used as a measure of population neuronal activity however , relating such measurements to underlying neuronal behaviour and connectivity is non trivial to help study this link , we developed the virtual electrode recording tool for extracellular potentials \( vertex \) we first identified a reduced neuron model that retained the spatial and frequency filtering characteristics of extracellular potentials from neocortical neurons we then developed vertex as an easy to use matlab tool for simulating lfps from large populations \( 100 000 neurons \) a vertex based simulation successfully reproduced features of the lfps from an in vitro multi electrode array recording of macaque neocortical tissue our model , with virtual electrodes placed anywhere in 3d , allows direct comparisons with the in vitro recording setup we envisage that vertex will stimulate experimentalists , clinicians , and computational neuroscientists to use models to understand the mechanisms underlying measured brain dynamics in health and disease
certain applications require that the output of an information extraction system be probabilistic , so that a downstream system can reliably fuse the output with possibly contradictory information from other sources in this paper we consider the problem of assigning a probability distribution to alternative sets of coreference relationships among entity descriptions we present the results of initial experiments with several approaches to estimating such distributions in an application using sri 's fastus information extraction system
in statistical relational learning , the link prediction problem is key to automatically understand the structure of large knowledge bases as in previous studies , we propose to solve this problem through latent factorization however , here we make use of complex valued embeddings the composition of complex embeddings can handle a large variety of binary relations , among them symmetric and antisymmetric relations compared to state of the art models such as neural tensor network and holographic embeddings , our approach based on complex embeddings is arguably simpler , as it only uses the hermitian dot product , the complex counterpart of the standard dot product between real vectors our approach is scalable to large datasets as it remains linear in both space and time , while consistently outperforming alternative approaches on standard link prediction benchmarks
we derive one shot upper bounds for quantum noisy channel codes we do so by regarding a channel code as a bipartite operation with an encoder belonging to the sender and a decoder belonging to the receiver , and imposing constraints on the bipartite operation we investigate the power of codes whose bipartite operation is non signalling from alice to bob , positive partial transpose \( ppt \) preserving , or both , and derive a simple semidefinite program for the achievable entanglement fidelity using the semidefinite program , we show that the non signalling assisted quantum capacity for memoryless channels is equal to the entanglement assisted capacity we also relate our ppt preserving codes and the ppt preserving entanglement distillation protocols studied by rains applying these results to a concrete example , the 3 dimensional werner holevo channel , we find that codes that are non signalling and ppt preserving can be strictly less powerful than codes satisfying either one of the constraints , and therefore provide a tighter bound for unassisted codes furthermore , ppt preserving non signalling codes can send one qubit perfectly over two uses of the channel , which has no quantum capacity we discuss whether this can be interpreted as a form of superactivation of quantum capacity
three types of video surrogates visual \( keyframes \) , verbal \( keywords phrases \) , and combination of the two were designed and studied in a qualitative investigation of user cognitive processes the results favor the combined surrogates in which verbal information and images reinforce each other , lead to better comprehension , and may actually require less processing time the results also highlight image features users found most helpful these findings will inform the interface design and video representation for video retrieval and browsing
verifying properties of object oriented software requires a method for handling references in a simple and intuitive way , closely related to how o o programmers reason about their programs the method presented here , a calculus of object programs , combines four components compositional logic , a framework for describing program semantics and proving program properties negative variables to address the specifics of o o programming , in particular qualified calls the alias calculus , which determines whether reference expressions can ever have the same value and the calculus of object structures , a specification technique for the structures that arise during the execution of an object oriented program the article illustrates the calculus by proving the standard algorithm for reversing a linked list
with the dramatic growth in the number of application domains that generate probabilistic , noisy and uncertain data , there has been an increasing interest in designing algorithms for geometric or combinatorial optimization problems over such data in this paper , we initiate the study of constructing epsilon kernel coresets for uncertain points we consider uncertainty under the existential model where each point 's location is fixed but only occurs with a certain probability , and the locational model where each point has a probability distribution describing its location an epsilon kernel coreset approximates the width of a point set in any direction we consider approximating the expected width \( an emph epsilon exp kernel \) , as well as the probability distribution on the width \( an emph eps quant kernel \) for any direction we show that there exists a set of o \( 1 epsilon \( d 1 \) 2 \) deterministic points which approximate the expected width under the existential and locational models , and we provide efficient algorithms for constructing such coresets we show , however , it is not always possible to find a subset of the original uncertain points which provides such an approximation however , if the existential probability of each point is lower bounded by a constant , an epsilon exp kernel is still possible we also construct an epsilon quant kernel coreset under the existential model finally , combining with known techniques , we show a few applications to approximating the extent of uncertain functions , maintaining extent measures for stochastic moving points and some shape fitting problems under uncertainty
in this paper , we introduced a novel approach to computing the fewest turn map directions or routes based on the concept of natural roads natural roads are joined road segments that perceptually constitute good continuity this approach relies on the connectivity of natural roads rather than that of road segments for computing routes or map directions because of this , the derived routes posses the fewest turns however , what we intend to achieve are the routes that not only possess the fewest turns , but are also as short as possible this kind of map direction is more effective and favorable by people , because they bear less cognitive burden furthermore , the computation of the routes is more efficient , since it is based on the graph encoding the connectivity of roads , which is significantly smaller than the graph of road segments we made experiments applied to eight urban street networks from north america and europe in order to illustrate the above stated advantages the experimental results indicate that the fewest turn routes posses fewer turns and shorter distances than the simplest paths and the routes provided by google maps for example , the fewest turn and shortest routes are on average 15 shorter than the routes suggested by google maps , while the number of turns is just half as much this approach is a key technology behind fromtomap org a web mapping service using openstreetmap data
the design of unknown input decoupled observers and filters requires the assumption of an existence condition in the literature this paper addresses an unknown input filtering problem where the existence condition is not satisfied instead of designing a traditional unknown input decoupled filter , a double model adaptive estimation approach is extended to solve the unknown input filtering problem it is proved that the state and the unknown inputs can be estimated and decoupled using the extended double model adaptive estimation approach without satisfying the existence condition numerical examples are presented in which the performance of the proposed approach is compared to methods from literature
in this paper , the problem of multi view embedding from different visual cues and modalities is considered we propose a unified solution for subspace learning methods using the rayleigh quotient , which is extensible for multiple views , supervised learning , and non linear embeddings numerous methods including canonical correlation analysis , partial least sqaure regression and linear discriminant analysis are studied using specific intrinsic and penalty graphs within the same framework non linear extensions based on kernels and \( deep \) neural networks are derived , achieving better performance than the linear ones moreover , a novel variant of multi view linear discriminant analysis is proposed by taking the view difference into consideration we demonstrate the effectiveness of the proposed multi view embedding methods on visual object recognition and cross modal image retrieval , and obtain superior results in both applications compared to related methods
the paper presents a study of an inter stimulus interval \( isi \) influence on a tactile point pressure stimulus based brain computer interface 's \( tpbci \) classification accuracy a novel tactile pressure generating tpbci stimulator is also discussed , which is based on a three by three pins' matrix prototype the six pin linear patterns are presented to the user 's palm during the online tpbci experiments in an oddball style paradigm allowing for the aha responses elucidation , within the event related potential \( erp \) a subsequent classification accuracies' comparison is discussed based on two isi settings in an online tpbci application a research hypothesis of classification accuracies' non significant differences with various isis is confirmed based on the two settings of 120 ms and 300 ms , as well as with various numbers of erp response averaging scenarios
in this paper , we investigate the adaptive control problem for robotic systems with both the uncertain kinematics and dynamics by a new formulation of the unknown kinematic system , we propose an adaptive control scheme that includes a new kinematic parameter adaptation law to realize the objective of task space trajectory tracking irrespective of the uncertain kinematics and dynamics unlike most existing results that rely on the approximate transpose jacobian feedback , the proposed controller employs the inverse jacobian feedback the new kinematic parameter adaptation law and the inverse jacobian feedback supplies the proposed control scheme with the desirable decomposition property and the convenient accommodation of the performance issues the performance of the proposed control is shown by numerical simulations
given the continually increasing amount of commercial cloud services in the market , evaluation of different services plays a significant role in cost benefit analysis or decision making for choosing cloud computing in particular , employing suitable metrics is essential in evaluation implementations however , to the best of our knowledge , there is not any systematic discussion about metrics for evaluating cloud services by using the method of systematic literature review \( slr \) , we have collected the de facto metrics adopted in the existing cloud services evaluation work the collected metrics were arranged following different cloud service features to be evaluated , which essentially constructed an evaluation metrics catalogue , as shown in this paper this metrics catalogue can be used to facilitate the future practice and research in the area of cloud services evaluation moreover , considering metrics selection is a prerequisite of benchmark selection in evaluation implementations , this work also supplements the existing research in benchmarking the commercial cloud services
generative models for graphs have been typically committed to strong prior assumptions concerning the form of the modeled distributions moreover , the vast majority of currently available models are either only suitable for characterizing some particular network properties \( such as degree distribution or clustering coefficient \) , or they are aimed at estimating joint probability distributions , which is often intractable in large scale networks in this paper , we first propose a novel network statistic , based on the laplacian spectrum of graphs , which allows to dispense with any parametric assumption concerning the modeled network properties second , we use the defined statistic to develop the fiedler random graph model , switching the focus from the estimation of joint probability distributions to a more tractable conditional estimation setting after analyzing the dependence structure characterizing fiedler random graphs , we evaluate them experimentally in edge prediction over several real world networks , showing that they allow to reach a much higher prediction accuracy than various alternative statistical models
in this paper we propose a new efficient message passing algorithm for decoding ldpc transmitted over a channel with strong phase noise the algorithm performs approximate bayesian inference on a factor graph representation of the channel and code joint posterior the approximate inference is based on an improved canonical model for the messages of the sum product algorithm , and a method for clustering the messages using the directional statistics framework the proposed canonical model includes treatment for phase slips which can limit the performance of tracking algorithms we show simulation results and complexity analysis for the proposed algorithm demonstrating its superiority over some of the current state of the art algorithms
highly dynamic networks rarely offer end to end connectivity at a given time yet , connectivity in these networks can be established over time and space , based on temporal analogues of multi hop paths \( also called em journeys \) attempting to optimize the selection of the journeys in these networks naturally leads to the study of three cases shortest \( minimum hop \) , fastest \( minimum duration \) , and foremost \( earliest arrival \) journeys efficient centralized algorithms exists to compute all cases , when the full knowledge of the network evolution is given in this paper , we study the em distributed counterparts of these problems , i e shortest , fastest , and foremost broadcast with termination detection \( tdb \) , with minimal knowledge on the topology we show that the feasibility of each of these problems requires distinct features on the evolution , through identifying three classes of dynamic graphs wherein the problems become gradually feasible graphs in which the re appearance of edges is em recurrent \( class r \) , em bounded recurrent \( b \) , or em periodic \( p \) , together with specific knowledge that are respectively n \( the number of nodes \) , delta \( a bound on the recurrence time \) , and p \( the period \) in these classes it is not required that all pairs of nodes get in contact only that the overall em footprint of the graph is connected over time our results , together with the strict inclusion between p , b , and r , implies a feasibility order among the three variants of the problem , i e tdb foremost requires weaker assumptions on the topology dynamics than tdb shortest , which itself requires less than tdb fastest reversely , these differences in feasibility imply that the computational powers of r n , b delta , and p p also form a strict hierarchy
we consider a diamond shaped dual hop communication system consisting a source , two parallel half duplex relays and a destination in a single antenna configuration , it has been previously shown that a two phase node scheduling algorithm , along with the decode and forward strategy can achieve the capacity of the diamond channel for a certain symmetric channel gains 1 in this paper , we obtain a more general condition for the optimality of the scheme in terms of power resources and channel gains in particular , it is proved that if the product of the capacity of the simultaneously active links are equal in both transmission phases , the scheme achieves the capacity of the channel
with data sizes constantly expanding , and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space , the need to distribute computation and memory requirements among several computers has become apparent although substantial work has been done in developing distributed binary svm algorithms and multi class svm algorithms individually , the field of multi class distributed svms remains largely unexplored this research proposes a novel algorithm that implements the support vector machine over a multi class dataset and is efficient in a distributed environment \( here , hadoop \) the idea is to divide the dataset into half recursively and thus compute the optimal support vector machine for this half during the training phase , much like a divide and conquer approach while testing , this structure has been effectively exploited to significantly reduce the prediction time our algorithm has shown better computation time during the prediction phase than the traditional sequential svm methods \( one vs one , one vs rest \) and out performs them as the size of the dataset grows this approach also classifies the data with higher accuracy than the traditional multi class algorithms
in this paper , a new framework for one dimensional contour extraction from discrete two dimensional data sets is presented contour extraction is important in many scientific fields such as digital image processing , computer vision , pattern recognition , etc this novel framework includes \( but is not limited to \) algorithms for dilated contour extraction , contour displacement , shape skeleton extraction , contour continuation , shape feature based contour refinement and contour simplification many of the new techniques depend strongly on the application of a delaunay tessellation in order to demonstrate the versatility of this novel toolbox approach , the contour extraction techniques presented here are applied to scientific problems in material science , biology and heavy ion physics
we present necessary and sufficient conditions for a boolean function to be a negabent function for both even and odd number of variables , which demonstrate the relationship between negabent functions and bent functions by using these necessary and sufficient conditions for boolean functions to be negabent , we obtain that the nega spectrum of a negabent function has at most 4 values we determine the nega spectrum distribution of negabent functions further , we provide a method to construct bent negabent functions in n variables \( n even \) of algebraic degree ranging from 2 to frac n 2 , which implies that the maximum algebraic degree of an n variable bent negabent function is equal to frac n 2 thus , we answer two open problems proposed by parker and pott and by st v a nic v a textit et al respectively
we propose a universal analysis for static routings on networks and describe the congestion characteristics by the theory the relation between average transmission time and transmission capacity is described by inequality t0rc0 1 for large scale sparse networks , the non trivial upper bond of transmission capacity rc0 is limited by rc0 1 1 k in some approximate conditions the theoretical results agree with simulations on ba networks
in this paper b rank , an efficient ranking algorithm for recommender systems , is proposed b rank is based on a random walk model on hypergraphs depending on the setup , b rank outperforms other state of the art algorithms in terms of precision , recall \( 19 50 \) , and inter list diversity \( 20 60 \) b rank captures well the difference between popular and niche objects the proposed algorithm produces very promising results for sparse and dense voting matrices furthermore , a recommendation list update algorithm is introduced , to cope with new votes this technique significantly reduces computational complexity the implementation of the algorithm is simple , since b rank needs no parameter tuning
we propose a novel generalized cellular automaton \( gca \) model for discrete time pulse coupled oscillators and study the emergence of synchrony given a finite simple graph and an integer n ge 3 , each vertex is an identical oscillator of period n with the following weak coupling along the edges each oscillator inhibits its phase update if it has at least one neighboring oscillator at a particular blinking state and if its state is ahead of this blinking state we obtain conditions on initial configurations and on network topologies for which states of all vertices eventually synchronize we show that our gca model synchronizes arbitrary initial configurations on paths , trees , and with random perturbation , any connected graph in particular , our main result is the following local global principle for tree networks for n in 3 , 4 , 5 , 6 , any n periodic network on a tree synchronizes arbitrary initial configuration if and only if the maximum degree of the tree is less than the period n
we propose stochastic rank 1 bandits , a class of online learning problems where at each step a learning agent chooses a pair of row and column arms , and receives the product of their payoffs as a reward the main challenge of the problem is that the learning agent does not observe the payoffs of the individual arms , only their product the payoffs of the row and column arms are stochastic , and independent of each other we propose a computationally efficient algorithm for solving our problem , rank1elim , and derive a o \( \( k l \) \( 1 delta \) log n \) upper bound on its n step regret , where k is the number of rows , l is the number of columns , and delta is the minimum gap in the row and column payoffs to the best of our knowledge , this is the first bandit algorithm for stochastic rank 1 matrix factorization whose regret is linear in k l , 1 delta , and log n we evaluate rank1elim on a synthetic problem and show that its regret scales as suggested by our upper bound we also compare it to ucb1 , and show significant improvements as k and l increase
we study a novel machine learning \( ml \) problem setting of sequentially allocating small subsets of training data amongst a large set of classifiers the goal is to select a classifier that will give near optimal accuracy when trained on all data , while also minimizing the cost of misallocated samples this is motivated by large modern datasets and ml toolkits with many combinations of learning algorithms and hyper parameters inspired by the principle of optimism under uncertainty , we propose an innovative strategy , data allocation using upper bounds \( daub \) , which robustly achieves these objectives across a variety of real world datasets we further develop substantial theoretical support for daub in an idealized setting where the expected accuracy of a classifier trained on n samples can be known exactly under these conditions we establish a rigorous sub linear bound on the regret of the approach \( in terms of misallocated data \) , as well as a rigorous bound on suboptimality of the selected classifier our accuracy estimates using real world datasets only entail mild violations of the theoretical scenario , suggesting that the practical behavior of daub is likely to approach the idealized behavior
edge connectivity and vertex connectivity are two fundamental concepts in graph theory although by now there is a good understanding of the structure of graphs based on their edge connectivity , our knowledge in the case of vertex connectivity is much more limited an essential tool in capturing edge connectivity are edge disjoint spanning trees the famous results of tutte and nash williams show that a graph with edge connectivity lambda contains floor lambda 2 edge disjoint spanning trees we present connected dominating set \( cds \) partition and packing as tools that are analogous to edge disjoint spanning trees and that help us to better grasp the structure of graphs based on their vertex connectivity the objective of the cds partition problem is to partition the nodes of a graph into as many connected dominating sets as possible the cds packing problem is the corresponding fractional relaxation , where cdss are allowed to overlap as long as this is compensated by assigning appropriate weights cds partition and cds packing can be viewed as the counterparts of the well studied edge disjoint spanning trees , focusing on vertex disjointedness rather than edge disjointness we constructively show that every k vertex connected graph with n nodes has a cds packing of size omega \( k log n \) and a cds partition of size omega \( k log 5 n \) we prove that the omega \( k log n \) cds packing bound is existentially optimal using cds packing , we show that if vertices of a k vertex connected graph are independently sampled with probability p , then the graph induced by the sampled vertices has vertex connectivity tilde omega \( kp 2 \) moreover , using our omega \( k log n \) cds packing , we get a store and forward broadcast algorithm with optimal throughput in the networking model where in each round , each node can send one bounded size message to all its neighbors
in 3 a short proof is given that some strings have maximal plain kolmogorov complexity but not maximal prefix free complexity the proof uses levin 's symmetry of information , levin 's formula relating plain and prefix complexity and gacs' theorem that complexity of complexity given the string can be high we argue that the proof technique and results mentioned above are useful to simplify existing proofs and to solve open questions we present a short proof of solovay 's result 21 relating plain and prefix complexity k \( x \) c \( x \) cc \( x \) o \( ccc \( x \) \) and c \( x \) k \( x \) kk \( x \) o \( kkk \( x \) \) , \( here cc \( x \) denotes c \( c \( x \) \) , etc \) we show that there exist omega such that liminf c \( omega 1 dots omega n \) c \( n \) is infinite and liminf k \( omega 1 dots omega n \) k \( n \) is finite , i e the infinitely often c trivial reals are not the same as the infinitely often k trivial reals \( i e 1 , question 1 \) solovay showed that for infinitely many x we have x c \( x \) le o \( 1 \) and x k \( x \) k \( x \) ge log \( 2 \) x o \( log \( 3 \) x \) , \( here x denotes the length of x and log \( 2 \) log log , etc \) we show that this result holds for prefixes of some 2 random sequences finally , we generalize our proof technique and show that no monotone relation exists between expectation and probability bounded randomness deficiency \( i e 6 , question 1 \)
separable codes were defined by cheng and miao in 2011 , motivated by applications to the identification of pirates in a multimedia setting combinatorially , overline t separable codes lie somewhere between t frameproof and \( t 1 \) frameproof codes all t frameproof codes are overline t separable , and all overline t separable codes are \( t 1 \) frameproof results for frameproof codes show that \( when q is large \) there are q ary overline t separable codes of length n with approximately q n t codewords , and that no q ary overline t separable codes of length n can have more than approximately q lceil n \( t 1 \) rceil codewords the paper provides improved probabilistic existence results for overline t separable codes when t geq 3 more precisely , for all t geq 3 and all n geq 3 , there exists a constant kappa \( depending only on t and n \) such that there exists a q ary overline t separable code of length n with at least kappa q n \( t 1 \) codewords for all sufficiently large integers q this shows , in particular , that the upper bound \( derived from the bound on \( t 1 \) frameproof codes \) on the number of codewords in a overline t separable code is realistic the results above are more surprising after examining the situation when t 2 results due to gao and ge show that a q ary overline 2 separable code of length n can contain at most frac 3 2 q 2 lceil n 3 rceil frac 1 2 q lceil n 3 rceil codewords , and that codes with at least kappa q 2n 3 codewords exist thus optimal overline 2 separable codes behave neither like 2 frameproof nor 1 frameproof codes also , the bound of gao and ge is strengthened to show that a q ary overline 2 separable code of length n can have at most q lceil 2n 3 rceil tfrac 1 2 q lfloor n 3 rfloor \( q lfloor n 3 rfloor 1 \) codewords
nowadays government and private agencies use remote sensing imagery for a wide range of applications from military applications to farm development the images may be a panchromatic , multispectral , hyperspectral or even ultraspectral of terra bytes remote sensing image classification is one amongst the most significant application worlds for remote sensing a few number of image classification algorithms have proved good precision in classifying remote sensing data but , of late , due to the increasing spatiotemporal dimensions of the remote sensing data , traditional classification algorithms have exposed weaknesses necessitating further research in the field of remote sensing image classification so an efficient classifier is needed to classify the remote sensing images to extract information we are experimenting with both supervised and unsupervised classification here we compare the different classification methods and their performances it is found that mahalanobis classifier performed the best in our classification
we present an automated verification of the well known modal logic cube in isabelle hol , in which we prove the inclusion relations between the cube 's logics using automated reasoning tools prior work addresses this problem but without restriction to the modal logic cube , and using encodings in first order logic in combination with first order automated theorem provers in contrast , our solution is more elegant , transparent and effective it employs an embedding of quantified modal logic in classical higher order logic automated reasoning tools , such as sledgehammer with leo ii , satallax and cvc4 , metis and nitpick , are employed to achieve full automation though successful , the experiments also motivate some technical improvements in the isabelle hol tool
we present foamgrid , a new implementation of the dune grid interface foamgrid implements one and two dimensional grids in a physical space of arbitrary dimension , which allows for grids for curved domains even more , the grids are not expected to have a manifold structure , i e , more than two elements can share a common facet this makes foamgrid the grid data structure of choice for simulating structures such as foams , discrete fracture networks , or network flow problems foamgrid implements adaptive non conforming refinement with element parametrizations as an additional feature it allows removal and addition of elements in an existing grid , which makes foamgrid suitable for network growth problems we show how to use foamgrid , with particular attention to the extensions of the grid interface needed to handle non manifold topology and grid growth three numerical examples demonstrate the possibilities offered by foamgrid
we design optimal 2 times n \( 2 n \) matrices , with unit columns , so that the maximum condition number of all the submatrices comprising 3 columns is minimized the problem has two applications when estimating a 2 dimensional signal by using only three of n observations at a given time , this minimizes the worst case achievable estimation error it also captures the problem of optimum sensor placement for monitoring a source located in a plane , when only a minimum number of required sensors are active at any given time for arbitrary n geq3 , we derive the optimal matrices which minimize the maximum condition number of all the submatrices of three columns surprisingly , a uniform distribution of the columns is emph not the optimal design for odd n geq 7
we introduce a framework and methodology of cooperative simultaneous localization and tracking \( coslat \) in decentralized mobile agent networks coslat provides a consistent combination of cooperative self localization \( csl \) and distributed target tracking \( dtt \) multiple mobile targets and mobile agents are tracked using pairwise measurements between agents and targets and between agents we propose a distributed coslat algorithm that combines particle based belief propagation with the a consensus or gossip scheme and performs a bidirectional probabilistic information transfer between csl and dtt simulation results demonstrate significant improvements in both self localization and target tracking performance compared to separate csl and dtt
real world complex networks are scale free and possess meso scale properties like core periphery and community structure we study evolution of the core over time in real world networks this paper proposes evolving models for both unweighted and weighted scale free networks having local and global core periphery as well as community structure network evolves using topological growth , self growth , and weight distribution function to validate the correctness of proposed models , we use k shell and s shell decomposition methods simulation results show that the generated unweighted networks follow power law degree distribution with droop head and heavy tail similarly , generated weighted networks follow degree , strength , and edge weight power law distributions we further study other properties of complex networks , such as clustering coefficient , nearest neighbor degree , and strength degree correlation
in this paper we study a particular aspect of the urban community policing routine patrol route planning we seek routes that guarantee visibility , as this has a sizable impact on the community perceived safety , allowing quick emergency responses and providing surveillance of selected sites \( e g , hospitals , schools \) the planning is restricted to the availability of vehicles and strives to achieve balanced routes we study an adaptation of the model for the multi vehicle covering tour problem , in which a set of locations must be visited , whereas another subset must be close enough to the planned routes it constitutes an np complete integer programming problem suboptimal solutions are obtained with several heuristics , some adapted from the literature and others developed by us we solve some adapted instances from tsplib and an instance with real data , the former being compared with results from literature , and latter being compared with empirical data
according to zipf 's meaning frequency law , words that are more frequent tend to have more meanings here it is shown that a linear dependency between the frequency of a form and its number of meanings is found in a family of models of zipf 's law for word frequencies this is evidence for a weak version of the meaning frequency law interestingly , that weak law \( a \) is not an inevitable of property of the assumptions of the family and \( b \) is found at least in the narrow regime where those models exhibit zipf 's law for word frequencies
we resolve the question of optimality for a well studied packetized implementation of random linear network coding , called pnc in pnc , in contrast to the classical memoryless setting , nodes store received information in memory to later produce coded packets that reflect this information pnc is known to achieve order optimal stopping times for the many to all multicast problem in many settings we give a reduction that captures exactly how pnc and other network coding protocols use the memory of the nodes more precisely , we show that any such protocol implementation induces a transformation which maps an execution of the protocol to an instance of the classical memoryless setting this allows us to prove that , for any \( non adaptive dynamic \) network , pnc converges with high probability in optimal time in other words , it stops at exactly the first time in which in hindsight it was possible to route information from the sources to each receiver individually our technique also applies to variants of pnc , in which each node uses only a finite buffer we show that , even in this setting , pnc stops exactly within the time in which in hindsight it was possible to route packets given the memory constraint , i e , that the memory used at each node never exceeds its buffer size this shows that pnc , even without any feedback or explicit memory management , allows to keep minimal buffer sizes while maintaining its capacity achieving performance
in low rank matrix recovery , one aims to reconstruct a low rank matrix from a minimal number of linear measurements within the paradigm of compressed sensing , this is made computationally efficient by minimizing the nuclear norm as a convex surrogate for rank in this work , we identify an improved regularizer based on the so called diamond norm , a concept imported from quantum information theory we show that for a class of matrices saturating a certain norm inequality the descent cone of the diamond norm is contained in that of the nuclear norm this suggests superior reconstruction properties for these matrices and we explicitly characterize this set we demonstrate numerically that the diamond norm indeed outperforms the nuclear norm in a number of relevant applications these include signal analysis tasks such as blind matrix deconvolution or the retrieval of certain unitary basis changes , as well as the quantum information problem of process tomography with random measurements the diamond norm is defined for matrices that can be interpreted as order 4 tensors and it turns out that the above condition depends crucially on that tensorial structure in this sense , this work touches on an aspect of the notoriously difficult tensor completion problem
in this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where each original input datapoint is described by a set of vectors and their associated outputs may be given by soft labels indicating , for example , class probabilities we represent an input datapoint as a mixture of probabilities over the corresponding set of feature vectors where each probability indicates how likely each vector is to belong to an unknown prototype pattern we propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization more importantly , both the model parameters and the prototype patterns can be learned from data in a discriminative way we show that our model can be seen as a probabilistic generalization of learning vector quantization \( lvq \) we apply our method to the problems of shape classification , hyperspectral imaging classification and people 's work class categorization , showing the superior performance of our method compared to the standard prototype based classification approach and other competitive benchmark methods
the fusion of independently obtained stochastic maps by collaborating mobile agents is considered the proposed approach includes two parts matching of stochastic maps and maximum likelihood alignment in particular , an affine invariant hypergraph is constructed for each stochastic map , and a bipartite matching via a linear program is used to establish landmark correspondence between stochastic maps a maximum likelihood alignment procedure is proposed to determine rotation and translation between common landmarks in order to construct a global map within a common frame of reference a main feature of the proposed approach is its scalability with respect to the number of landmarks the matching step has polynomial complexity and the maximum likelihood alignment is obtained in closed form experimental validation of the proposed fusion approach is performed using the victoria park benchmark dataset
social organization and division of labor crucially influence the performance of collaborative software engineering efforts in this paper , we provide a quantitative analysis of the relation between social organization and performance in gentoo , an open source community developing a linux distribution we study the structure and dynamics of collaborations as recorded in the project 's bug tracking system over a period of ten years we identify a period of increasing centralization after which most interactions in the community were mediated by a single central contributor in this period of maximum centralization , the central contributor unexpectedly left the project , thus posing a significant challenge for the community we quantify how the rise , the activity as well as the subsequent sudden dropout of this central contributor affected both the social organization and the bug handling performance of the gentoo community we analyze social organization from the perspective of network theory and augment our quantitative findings by interviews with prominent members of the gentoo community which shared their personal insights
textit fake followers are those twitter accounts specifically created to inflate the number of followers of a target account fake followers are dangerous for the social platform and beyond , since they may alter concepts like popularity and influence in the twittersphere hence impacting on economy , politics , and society in this paper , we contribute along different dimensions first , we review some of the most relevant existing features and rules \( proposed by academia and media \) for anomalous twitter accounts detection second , we create a baseline dataset of verified human and fake follower accounts such baseline dataset is publicly available to the scientific community then , we exploit the baseline dataset to train a set of machine learning classifiers built over the reviewed rules and features our results show that most of the rules proposed by media provide unsatisfactory performance in revealing fake followers , while features proposed in the past by academia for spam detection provide good results building on the most promising features , we revise the classifiers both in terms of reduction of overfitting and cost for gathering the data needed to compute the features the final result is a novel textit class a classifier , general enough to thwart overfitting , lightweight thanks to the usage of the less costly features , and still able to correctly classify more than 95 of the accounts of the original training set we ultimately perform an information fusion based sensitivity analysis , to assess the global sensitivity of each of the features employed by the classifier the findings reported in this paper , other than being supported by a thorough experimental methodology and interesting on their own , also pave the way for further investigation on the novel issue of fake twitter followers
in this paper , the theoretical analysis of compressive sensing via random filter , firstly outlined by j romberg compressive sensing by random convolution , submitted to siam journal on imaging science on july 9 , 2008 , has been refined or generalized to the design of general random filter used for compressive sensing this universal cs measurement consists of two parts one is from the convolution of unknown signal with a random waveform followed by random time domain subsampling the other is from the directly time domain subsampling of the unknown signal it has been shown that the proposed approach is a universally efficient data acquisition strategy , which means that the n dimensional signal which is s sparse in any sparse representation can be exactly recovered from slogn measurements with overwhelming probability
widespread and extensive use of computers and their interconnections in almost all sectors like communications , finance , transportation , military , governance , education , energy etc , have made them attractive targets for adversaries to spy , disrupt or steal information by pressing of few keystrokes from any part of the world this paper presents a survey of major cyberattacks from 2001 to 2013 and analyzes these attacks to understand the motivation , targets and technique \( s \) employed by the attackers observed trends in cyberattacks have also been discussed in the paper
most current sampling algorithms for high dimensional distributions are based on mcmc techniques and are approximate in the sense that they are valid only asymptotically rejection sampling , on the other hand , produces valid samples , but is unrealistically slow in high dimension spaces the os algorithm that we propose is a unified approach to exact optimization and sampling , based on incremental refinements of a functional upper bound , which combines ideas of adaptive rejection sampling and of a optimization search we show that the choice of the refinement can be done in a way that ensures tractability in high dimension spaces , and we present first experiments in two different settings inference in high order hmms and in large discrete graphical models
an achievable rate region is obtained for a primary multiple access network coexisting with a secondary link of one transmitter and a corresponding receiver the rate region depicts the sum primary rate versus the secondary rate and is established assuming that the secondary link performs rate splitting the achievable rate region is the union of two types of achievable rate regions the first type is a rate region established assuming that the secondary receiver cannot decode any primary signal , whereas the second is established assuming that the secondary receiver can decode the signal of one primary receiver the achievable rate region is determined first assuming discrete memoryless channel \( dmc \) then the results are applied to a gaussian channel in the gaussian channel , the performance of rate splitting is characterized for the two types of rate regions moreover , a necessary and sufficient condition to determine which primary signal that the secondary receiver can decode without degrading the range of primary achievable sum rates is provided when this condition is satisfied by a certain primary user , the secondary receiver can decode its signal and achieve larger rates without reducing the primary achievable sum rates from the case in which it does not decode any primary signal it is also shown that , the probability of having at least one primary user satisfying this condition grows with the primary signal to noise ratio
recently , strong results have been demonstrated by deep recurrent neural networks on natural language transduction problems in this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation these experiments lead us to propose new memory based recurrent networks that implement continuously differentiable analogues of traditional data structures such as stacks , queues , and deques we show that these architectures exhibit superior generalisation performance to deep rnns and are often able to learn the underlying generating algorithms in our transduction experiments
deep generative models parameterized by neural networks have recently achieved state of the art performance in unsupervised and semi supervised learning we extend deep generative models with auxiliary variables which improves the variational approximation the auxiliary variables leave the generative model unchanged but make the variational distribution more expressive inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections our findings suggest that more expressive and properly specified deep generative models converge faster with better results we show state of the art performance within semi supervised learning on mnist \( 0 96 \) , svhn \( 16 61 \) and norb \( 9 40 \) datasets
we present a mechanism for reservations of bursty resources that is both truthful and robust it consists of option contracts whose pricing structure induces users to reveal the true likelihoods that they will purchase a given resource users are also allowed to adjust their options as their likelihood changes this scheme helps users save cost and the providers to plan ahead so as to reduce the risk of under utilization and overbooking the mechanism extracts revenue similar to that of a monopoly provider practicing temporal pricing discrimination with a user population whose preference distribution is known in advance
artificial chemistries \( acs \) are symbolic chemical metaphors for the exploration of artificial life , with specific focus on the problem of biogenesis or the origin of life this paper presents authors thoughts towards defining a unified framework to characterize and classify symbolic artificial chemistries by devising appropriate formalism to capture semantic and organizational information we identify three basic high level abstractions in initial proposal for this framework viz , information , computation , and communication we present an analysis of two important notions of information , namely , shannon 's entropy and algorithmic information , and discuss inductive and deductive approaches for defining the framework
we investigate the problem of approximate bayesian inference for a general class of observation models by means of the expectation propagation \( ep \) framework for large systems under some statistical assumptions our approach tries to overcome the numerical bottleneck of ep caused by the inversion of large matrices assuming that the measurement matrices are realizations of specific types of ensembles we use the concept of freeness from random matrix theory to show that the ep cavity variances exhibit an asymptotic self averaging property they can be pre computed using specific generating functions , i e the r and or s transforms in free probability , which do not require matrix inversions our approach extends the framework of \( generalized \) approximate message passing assumes zero mean iid entries of the measurement matrix to a general class of random matrix ensembles the generalization is via a simple formulation of the r and or s transforms of the limiting eigenvalue distribution of the gramian of the measurement matrix we demonstrate the performance of our approach on a signal recovery problem of nonlinear compressed sensing and compare it with that of ep
one of the key features of next generation wireless communication systems will be the use of frequencies in the range 10 100ghz \( aka mmwave band \) in densely populated indoor and outdoor scenarios due to the reduced wavelength , antenna arrays with a large number of antennas can be packed in very small volumes , making thus it possible to consider , at least in principle , communication links wherein not only the base station , but also the user device , are equipped with very large antenna arrays we denote this configuration as a doubly massive mimo wireless link this paper introduces the concept of doubly massive mimo systems at mmwave , showing that at mmwave the fundamentals of the massive mimo regime are completely different from what happens at conventional sub 6 ghz cellular frequencies it is shown for instance that the multiplexing capabilities of the channel and its rank are no longer ruled by the number of transmit and receive antennas , but rather by the number of scattering clusters in the surrounding environment the implications of the doubly massive mimo regime on the transceiver processing , on the system energy efficiency and on the system throughput are also discussed
linkedin is the largest professional network with more than 350 million members as the member base increases , searching for experts becomes more and more challenging in this paper , we propose an approach to address the problem of personalized expertise search on linkedin , particularly for exploratory search queries containing it skills in the offline phase , we introduce a collaborative filtering approach based on matrix factorization our approach estimates expertise scores for both the skills that members list on their profiles as well as the skills they are likely to have but do not explicitly list in the online phase \( at query time \) we use expertise scores on these skills as a feature in combination with other features to rank the results to learn the personalized ranking function , we propose a heuristic to extract training data from search logs while handling position and sample selection biases we tested our models on two products linkedin homepage and linkedin recruiter a b tests showed significant improvements in click through rates 31 for ctr 1 for recruiter \( 18 for homepage \) as well as downstream messages sent from search 37 for recruiter \( 20 for homepage \) as of writing this paper , these models serve nearly all live traffic for skills search on linkedin homepage as well as linkedin recruiter
defect detection by ultrasonic method is limited by the pulse width resolution can be improved through a deconvolution process with a priori information of the pulse or by its estimation in this paper a regularization of the wiener filter using wavelet shrinkage is presented for the estimation of the reflectivity function the final result shows an improved signal to noise ratio with better axial resolution
we propose a new approach , multi view laplacian support vector machines \( svms \) , for semi supervised learning under the multi view scenario it integrates manifold regularization and multi view regularization into the usual formulation of svms and is a natural extension of svms from supervised learning to multi view semi supervised learning the function optimization problem in a reproducing kernel hilbert space is converted to an optimization in a finite dimensional euclidean space after providing a theoretical bound for the generalization performance of the proposed method , we further give a formulation of the empirical rademacher complexity which affects the bound significantly from this bound and the empirical rademacher complexity , we can gain insights into the roles played by different regularization terms to the generalization performance experimental results on synthetic and real world data sets are presented , which validate the effectiveness of the proposed multi view laplacian svms approach
we survey the numerical stability of some fast algorithms for solving systems of linear equations and linear least squares problems with a low displacement rank structure for example , the matrices involved may be toeplitz or hankel we consider algorithms which incorporate pivoting without destroying the structure , and describe some recent results on the stability of these algorithms we also compare these results with the corresponding stability results for the well known algorithms of schur bareiss and levinson , and for algorithms based on the semi normal equations
assessing network security is a complex and difficult task attack graphs have been proposed as a tool to help network administrators understand the potential weaknesses of their network however , a problem has not yet been addressed by previous work on this subject namely , how to actually execute and validate the attack paths resulting from the analysis of the attack graph in this paper we present a complete pddl representation of an attack model , and an implementation that integrates a planner into a penetration testing tool this allows to automatically generate attack paths for penetration testing scenarios , and to validate these attacks by executing the corresponding actions including exploits against the real target network we present an algorithm for transforming the information present in the penetration testing tool to the planning domain , and show how the scalability issues of attack graphs can be solved using current planners we include an analysis of the performance of our solution , showing how our model scales to medium sized networks and the number of actions available in current penetration testing tools
this paper presents a method for the automatic extraction of subgrammars to control and speeding up natural language generation nlg the method is based on explanation based learning \( ebl \) the main advantage for the proposed new method for nlg is that the complexity of the grammatical decision making process during nlg can be vastly reduced , because the ebl method supports the adaption of a nlg system to a particular use of a language
we consider multi antenna cooperative spectrum sensing in cognitive radio networks , when there may be multiple primary users a noise uncertainty free detector that is optimal in the low signal to noise ratio regime is analyzed in such a scenario specifically , we derive the exact moments of the test statistics involved , which lead to simple and accurate analytical formulae for the false alarm probability and the decision threshold simulations are provided to examine the accuracy of the derived results , and to compare with other detectors in realistic sensing scenarios
nonnegative matrix factorization has been widely applied in face recognition , text mining , as well as spectral analysis this paper proposes an alternating proximal gradient method for solving this problem with a uniformly positive lower bound assumption on the iterates , any limit point can be proved to satisfy the first order optimality conditions a nesterov type extrapolation technique is then applied to accelerate the algorithm though this technique is at first used for convex program , it turns out to work very well for the non convex nonnegative matrix factorization problem extensive numerical experiments illustrate the efficiency of the alternating proximal gradient method and the accleration technique especially for real data tests , the accelerated method reveals high superiority to state of the art algorithms in speed with comparable solution qualities
we consider the np hard problem of map inference for graphical models we propose a polynomial time practically efficient algorithm for finding a part of its optimal solution specifically , our algorithm marks each label in each node of the considered graphical model either as \( i \) optimal , meaning that it belongs to all optimal solutions of the inference problem \( ii \) non optimal if it provably does not belong to any solution or \( iii \) undefined , which means our algorithm can not make a decision regarding the label moreover , we prove optimality of our approach it delivers in a certain sense the largest total number of labels marked as optimal or non optimal we demonstrate superiority of our approach on problems from machine learning and computer vision benchmarks
we describe a preliminary investigation into learning a chess player 's style from game records the method is based on attempting to learn features of a player 's individual evaluation function using the method of temporal differences , with the aid of a conventional chess engine architecture some encouraging results were obtained in learning the styles of two recent chess world champions , and we report on our attempt to use the learnt styles to discriminate between the players from game records by trying to detect who was playing white and who was playing black we also discuss some limitations of our approach and propose possible directions for future research the method we have presented may also be applicable to other strategic games , and may even be generalisable to other domains where sequences of agents' actions are recorded
the minimum rank of a simple graph g is the smallest possible rank over all symmetric real matrices a whose nonzero off diagonal entries correspond to the edges of g using the zero forcing number , we prove that the minimum rank of the butterfly network is frac19 left \( 3r 1 \) 2 r 1 2 \( 1 \) r right and that this is equal to the rank of its adjacency matrix
texture classification became one of the problems which has been paid much attention on by image processing scientists since late 80s consequently , since now many different methods have been proposed to solve this problem in most of these methods the researchers attempted to describe and discriminate textures based on linear and non linear patterns the linear and non linear patterns on any window are based on formation of grain components in a particular order grain component is a primitive unit of morphology that most meaningful information often appears in the form of occurrence of that the approach which is proposed in this paper could analyze the texture based on its grain components and then by making grain components histogram and extracting statistical features from that would classify the textures finally , to increase the accuracy of classification , proposed approach is expanded to color images to utilize the ability of approach in analyzing each rgb channels , individually although , this approach is a general one and it could be used in different applications , the method has been tested on the stone texture and the results can prove the quality of approach
common neighbor based method is simple yet effective to predict missing links , which assume that two nodes are more likely to be connected if they have more common neighbors in such method , each common neighbor of two nodes contributes equally to the connection likelihood in this letter , we argue that different common neighbors may play different roles and thus lead to different contributions , and propose a local na i ve bayes model accordingly extensive experiments were carried out on eight real networks compared with the common neighbor based methods , the present method can provide more accurate predictions finally , we gave a detailed case study on the us air transportation network
a new natural language understanding method for disambiguation of difficult pronouns is described difficult pronouns are those pronouns for which a level of world or domain knowledge is needed in order to perform anaphoral or other types of resolution resolution of difficult pronouns may in some cases require a prior step involving the application of inference to a situation that is represented by the natural language text a general method is described it performs entity resolution and pronoun resolution an extension to the general pronoun resolution method performs inference as an embedded commonsense reasoning method the general method and the embedded method utilize features of the ross representational scheme in particular the methods use ross ontology classes and the ross situation model the overall method is a working solution that solves the following winograd schemas a \) trophy and suitcase , b \) person lifts person , c \) person pays detective , and d \) councilmen and demonstrators
stochastic hype is a novel process algebra that models stochastic , instantaneous and continuous behaviour it develops the flow based approach of the hybrid process algebra hype by replacing non urgent events with events with exponentially distributed durations and also introduces random resets the random resets allow for general stochasticity , and in particular allow for the use of event durations drawn from distributions other than the exponential distribution to account for stochasticity , the semantics of stochastic hype target piecewise deterministic markov processes \( pdmps \) , via intermediate transition driven stochastic hybrid automata \( tdsha \) in contrast to the hybrid automata used as semantic target for hype stochastic hype models have a specific structure where the controller of a system is separate from the continuous aspect of this system providing separation of concerns and supporting reasoning a novel equivalence is defined which captures when two models have the same stochastic behaviour \( as in stochastic bisimulation \) , instantaneous behaviour \( as in classical bisimulation \) and continuous behaviour these techniques are illustrated via an assembly line example
the maude nrl protocol analyzer \( maude npa \) is a tool and inference system for reasoning about the security of cryptographic protocols in which the cryptosystems satisfy different equational properties it both extends and provides a formal framework for the original nrl protocol analyzer , which supported equational reasoning in a more limited way maude npa supports a wide variety of algebraic properties that includes many crypto systems of interest such as , for example , one time pads and diffie hellman maude npa , like the original npa , looks for attacks by searching backwards from an insecure attack state , and assumes an unbounded number of sessions because of the unbounded number of sessions and the support for different equational theories , it is necessary to develop ways of reducing the search space and avoiding infinite search paths in order for the techniques to prove useful , they need not only to speed up the search , but should not violate completeness , so that failure to find attacks still guarantees security in this paper we describe some state space reduction techniques that we have implemented in maude npa we also provide completeness proofs , and experimental evaluations of their effect on the performance of maude npa
the necessary and sufficient conditions for a class of functions f mathbb z 2 n rightarrow mathbb z q , where q geq 2 is an even positive integer , have been recently identified for q 4 and q 8 in this article we give an alternative characterization of the generalized walsh hadamard transform in terms of the walsh spectra of the component boolean functions of f , which then allows us to derive sufficient conditions that f is generalized bent for any even q the case when q is not a power of two , which has not been addressed previously , is treated separately and a suitable representation in terms of the component functions is employed consequently , the derived results lead to generic construction methods of this class of functions the main remaining task , which is not answered in this article , is whether the sufficient conditions are also necessary there are some indications that this might be true which is also formally confirmed for generalized bent functions that belong to the class of generalized maiorana mcfarland functions \( gmmf \) , but still we were unable to completely specify \( in terms of necessity \) gbent conditions
in this era of computerization , education has also revamped itself and is not limited to old lecture method the regular quest is on to find out new ways to make it more effective and efficient for students nowadays , lots of data is collected in educational databases , but it remains unutilized in order to get required benefits from such a big data , powerful tools are required data mining is an emerging powerful tool for analysis and prediction it is successfully applied in the area of fraud detection , advertising , marketing , loan assessment and prediction but , it is in nascent stage in the field of education considerable amount of work is done in this direction , but still there are many untouched areas moreover , there is no unified approach among these researches this paper presents a comprehensive survey , a travelogue \( 2002 2014 \) towards educational data mining and its scope in future
one of the essential and most complex components in the software development process is the database the complexity increases when the orientation of the interacting components differs a persistence framework moves the program data in its most natural form to and from a permanent data store , the database thus a persistence framework manages the database and the mapping between the database and the objects this paper compares the performance of two persistence frameworks \? hibernate and ibatis \? s sqlmaps using a banking database the performance of both of these tools in single and multi user environments are evaluated
we formulate the control of reactive power generation by photovoltaic inverters in a power distribution circuit as a constrained optimization that aims to minimize reactive power losses subject to finite inverter capacity and upper and lower voltage limits at all nodes in the circuit when voltage variations along the circuit are small and losses of both real and reactive powers are small compared to the respective flows , the resulting optimization problem is convex moreover , the cost function is separable enabling a distributed , on line implementation with node local computations using only local measurements augmented with limited information from the neighboring nodes communicated over cyber channels such an approach lies between the fully centralized and local policy approaches previously considered we explore protocols based on the dual ascent method and on the alternating direction method of multipliers \( admm \) and find that the admm protocol performs significantly better
in this paper we present a way of hiding the data in mobile devices from being compromised we use two level data hiding technique , where in its first level data is encrypted and stored in special records and the second level being a typical password protection scheme the second level is for secure access of information from the device in the first level , encryption of the data is done using the location coordinates as key location coordinates are rounded up figures of longitude and latitude information in the second phase the password entry differs from conventional schemes here we have used the patterns of traditional rangoli for specifying the password and gaining access , thus minimising the chances of data leak in hostile situations the proposed structure would be a better trade off in comparison with the previous models which use bio metric authentication a relatively costly way of authentication
we consider the most common variants of linear regression , including ridge , lasso and support vector regression , in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time we present simple and efficient algorithms for these problems for lasso and ridge regression they need the same total number of attributes \( up to constants \) as do full information algorithms , for reaching a certain accuracy for support vector regression , we require exponentially less attributes compared to the state of the art by that , we resolve an open problem recently posed by cesa bianchi et al \( 2010 \) experiments show the theoretical bounds to be justified by superior performance compared to the state of the art
this paper presents a new decoder for probabilistic binary traitor tracing codes under the marking assumption it is based on a binary hypothesis testing rule which integrates a collusion channel relaxation so as to obtain numerical and simple accusation functions this decoder is blind as no estimation of the collusion channel prior to the accusation is required experimentations show that using the proposed decoder gives better performance than the well known symmetric version of the tardos decoder for common attack channels
pairing based cryptographic schemes require so called pairing friendly elliptic curves , which have special properties the set of pairing friendly elliptic curves that are generated by given polynomials form a complete family although a complete family with a rho value of 1 is the ideal case , there is only one such example that is known , this was given by barreto and naehrig we prove that there are no ideal families with embedding degree 3 , 4 , or 6 and that many complete families with embedding degree 8 or 12 are nonideal , even if we chose noncyclotomic families
information theoretical measures , such as entropy , mutual information , and various divergences , exhibit robust characteristics in image registration applications however , the estimation of these quantities is computationally intensive in high dimensions on the other hand , consistent estimation from pairwise distances of the sample points is possible , which suits random projection \( rp \) based low dimensional embeddings we adapt the rp technique to this task by means of a simple ensemble method to the best of our knowledge , this is the first distributed , rp based information theoretical image registration approach the efficiency of the method is demonstrated through numerical examples
community detection has become an extremely active area of research in recent years , with researchers proposing various new metrics and algorithms to address the problem recently , the weighted community clustering \( wcc \) metric was proposed as a novel way to judge the quality of a community partitioning based on the distribution of triangles in the graph , and was demonstrated to yield superior results over other commonly used metrics like modularity the same authors later presented a parallel algorithm for optimizing wcc on large graphs in this paper , we propose a new distributed , vertex centric algorithm for community detection using the wcc metric results are presented that demonstrate the algorithm 's performance and scalability on up to 32 worker machines and real graphs of up to 1 8 billion vertices the algorithm scales best with the largest graphs , and to our knowledge , it is the first distributed algorithm for optimizing the wcc metric
as the development of distributed systems progresses , more and more challenges arise and the need for developing optimized systems and for optimizing existing systems from multiple perspectives becomes more stringent in this paper i present novel algorithmic techniques for solving several optimization problems regarding distributed systems with tree topologies i address topics like reliability improvement , partitioning , coloring , content delivery , optimal matchings , as well as some tree counting aspects some of the presented techniques are only of theoretical interest , while others can be used in practical settings
we consider single cell multi user ofdma downlink resource allocation on a flat fading channel such that average supply power is minimized while fulfilling a set of target rates available degrees of freedom are transmission power and duration this paper extends our previous work on power optimal resource allocation in the mobile downlink by detailing the optimal power control strategy investigation and extracting fundamental characteristics of power optimal operation in cellular downlink we find that only a system wide allocation of transmit powers is optimal rather than on link level the allocation strategy that minimizes overall power consumption requires the transmission power on all links to be increased if only one link degrades furthermore , we show that for mobile stations with equal channels but different rate requirements , it is power optimal to assign equal transmit powers with proportional transmit durations to relate the effectiveness of power control to live operation , we take the power model into consideration which maps transmit power to supply power we show that due to the affine mapping , the solution is independent of the power model however , the effectiveness of power control measures is completely dependent on the underlying hardware and the load dependence factor of a base station \( instead of absolute consumption values \) finally , we conclude that power control measures in base stations are most relevant in macro stations which have load dependence factor of more than 50
the line graphs are clustered and assortative they share these topological features with some social networks we argue that this similarity reveals the cliquey character of the social networks in the model proposed here , a social network is the line graph of an initial network of families , communities , interest groups , school classes and small companies these groups play the role of nodes , and individuals are represented by links between these nodes the picture is supported by the data on the livejournal network of about 8 x 10 6 people in particular , sharp maxima of the observed data of the degree dependence of the clustering coefficient c \( k \) are associated with cliques in the social network
an extended formulation of a polytope p is a polytope q which can be projected onto p extended formulations of small size \( i e , number of facets \) are of interest , as they allow to model corresponding optimization problems as linear programs of small sizes the main known lower bounds on the minimum sizes of extended formulations for fixed polytope p \( yannakakis 1991 \) are closely related to the concept of nondeterministic communication complexity we study the relative power and limitations of the bounds on several examples
we present a scheme able to protect k 3 qubits of information against the occurrence of multiple erasures , based on the code proposed by yang et al \( 2004 jetp letters 79 236 \) in this scheme redundant blocks are used and we restrict to the case that each erasure must occur in distinct blocks we explicitly characterize the encoding operation and the restoring operation required to implement this scheme the operators used in these operations can be adjusted to construct different quantum erasure correcting codes a special feature of this scheme is that no measurement is required to illustrate our scheme , we present an example in which five qubits of information are protected against the occurrence of two erasures
bandwidth starved multicore chips have become ubiquitous it is well known that the performance of stencil codes can be improved by temporal blocking , lessening the pressure on the memory interface we introduce a new pipelined approach that makes explicit use of shared caches in multicore environments and minimizes synchronization and boundary overhead benchmark results are presented for three current x86 based microprocessors , showing clearly that our optimization works best on designs with high speed shared caches and low memory bandwidth per core we furthermore demonstrate that simple bandwidth based performance models are inaccurate for this kind of algorithm and employ a more elaborate , synthetic modeling procedure finally we show that temporal blocking can be employed successfully in a hybrid shared distributed memory environment , albeit with limited benefit at strong scaling
the generalised colouring numbers mathrm col r \( g \) and mathrm wcol r \( g \) were introduced by kierstead and yang as a generalisation of the usual colouring number , and have since then found important theoretical and algorithmic applications in this paper , we dramatically improve upon the known upper bounds for generalised colouring numbers for graphs excluding a fixed minor , from the exponential bounds of grohe emph et al to a linear bound for the r colouring number mathrm col r and a polynomial bound for the weak r colouring number mathrm wcol r in particular , we show that if g excludes k t as a minor , for some fixed t ge4 , then mathrm col r \( g \) le binom t 1 2 , \( 2r 1 \) and mathrm wcol r \( g \) le binom r t 2 t 2 , \( t 3 \) \( 2r 1 \) in mathcal o \( r , t 1 \) in the case of graphs g of bounded genus g , we improve the bounds to mathrm col r \( g \) le \( 2g 3 \) \( 2r 1 \) \( and even mathrm col r \( g \) le5r 1 if g 0 , i e if g is planar \) and mathrm wcol r \( g \) le bigl \( 2g binom r 2 2 bigr \) , \( 2r 1 \)
we introduce a model for a market based economic system of cyber risk valuation to correct fundamental problems of incentives within the information technology and information processing industries we assess the makeup of the current day marketplace , identify incentives , identify economic reasons for current failings , and explain how a market based risk valuation system could improve these incentives to form a secure and robust information marketplace for all consumers by providing visibility into open , consensus based risk pricing and allowing all parties to make well informed decisions
in the past few years , like other fields , rapid expansion of digitization and globalization has influenced the medical field as well for progress of diagnostic results most of the reputed hospitals and diagnostic centres all over the world have started exchanging medical information in this proposed method , the calculated diagnostic parametric values of the original electrooculography \( eog \) signal are embedded as a watermark by using difference expansion \( de \) algorithm based reversible watermarking technique the extracted watermark provides the required parametric values at the recipient end without any post computation of the recovered eog signal by computing the parametric values from the recovered signal , the integrity of the extracted watermark can be validated the time domain features of eog signal are calculated for the generation of watermark in the current work , various features are studied and two major features related to blink frequency are used to generate the watermark the high signal to noise ratio \( snr \) and the bit error rate \( ber \) claim the robustness of the proposed method
the total number of patents produced by a country \( or the number of patents produced per capita \) is often used as an indicator for innovation here we present evidence that the distribution of patents amongst applicants within many oecd countries is well described by power laws with exponents that vary between 1 66 \( japan \) and 2 37 \( poland \) using simulations based on simple preferential attachment type rules that generate power laws , we find we can explain some of the variation in exponents between countries , with countries that have larger numbers of patents per applicant generally exhibiting smaller exponents in both the simulated and actual data similarly we find that the exponents for most countries are inversely correlated with other indicators of innovation , such as r d intensity or the ubiquity of export baskets this suggests that in more advanced economies , which tend to have smaller values of the exponent , a greater proportion of the total number of patents are filed by large companies than in less advanced countries
a framework for consensus modelling is introduced using kleene 's three valued logic as a means to express vagueness in agents' beliefs explicitly borderline cases are inherent to propositions involving vague concepts where sentences of a propositional language may be absolutely true , absolutely false or borderline by exploiting these intermediate truth values , we can allow agents to adopt a more vague interpretation of underlying concepts in order to weaken their beliefs and reduce the levels of inconsistency , so as to achieve consensus we consider a consensus combination operation which results in agents adopting the borderline truth value as a shared viewpoint if they are in direct conflict simulation experiments are presented which show that applying this operator to agents chosen at random \( subject to a consistency threshold \) from a population , with initially diverse opinions , results in convergence to a smaller set of more precise shared beliefs furthermore , if the choice of agents for combination is dependent on the payoff of their beliefs , this acting as a proxy for performance or usefulness , then the system converges to beliefs which , on average , have higher payoff
linear codes have been an interesting topic in both theory and practice for many years in this paper , for an odd prime p , we present the explicit complete weight enumerator of a family of p ary linear codes constructed with defining set the weight enumerator is an immediate result of the complete weight enumerator which shows that the codes proposed in this paper are three weight linear codes additionally , all nonzero codewords are minimal and thus they are suitable for secret sharing schemes
given a large set of measurement sensor data , in order to identify a simple function that captures the essence of the data gathered by the sensors , we suggest representing the data by \( spatial \) functions , in particular by polynomials given a \( sampled \) set of values , we interpolate the datapoints to define a polynomial that would represent the data the interpolation is challenging , since in practice the data can be noisy and even byzantine , where the byzantine data represents an adversarial value that is not limited to being close to the correct measured data we present two solutions , one that extends the welch berlekamp technique in the case of multidimensional data , and copes with discrete noise and byzantine data , and the other based on arora and khot techniques , extending them in the case of multidimensional noisy and byzantine data
in order to formally understand the power of neural computing , we first need to crack the frontier of threshold circuits with two and three layers , a regime that has been surprisingly intractable to analyze we prove the first super linear gate lower bounds and the first super quadratic wire lower bounds for depth two linear threshold circuits with arbitrary weights , and depth three majority circuits computing an explicit function bullet we prove that for all epsilon gg sqrt log \( n \) n , the linear time computable andreev 's function cannot be computed on a \( 1 2 epsilon \) fraction of n bit inputs by depth two linear threshold circuits of o \( epsilon 3 n 3 2 log 3 n \) gates , nor can it be computed with o \( epsilon 3 n 5 2 log 7 2 n \) wires this establishes an average case ``size hierarchy'' for threshold circuits , as andreev 's function is computable by uniform depth two circuits of o \( n 3 \) linear threshold gates , and by uniform depth three circuits of o \( n \) majority gates bullet we present a new function in p based on small biased sets , which we prove cannot be computed by a majority vote of depth two linear threshold circuits with o \( n 3 2 log 3 n \) gates , nor with o \( n 5 2 log 7 2 n \) wires bullet we give tight average case \( gate and wire \) complexity results for computing parity with depth two threshold circuits the answer turns out to be the same as for depth two majority circuits the key is a new random restriction lemma for linear threshold functions our main analytical tool is the littlewood offord lemma from additive combinatorics
statistics about n grams \( i e , sequences of contiguous words or other tokens in text documents or other string data \) are an important building block in information retrieval and natural language processing in this work , we study how n gram statistics , optionally restricted by a maximum n gram length and minimum collection frequency , can be computed efficiently harnessing mapreduce for distributed data processing we describe different algorithms , ranging from an extension of word counting , via methods based on the apriori principle , to a novel method suffix sigma that relies on sorting and aggregating suffixes we examine possible extensions of our method to support the notions of maximality closedness and to perform aggregations beyond occurrence counting assuming hadoop as a concrete mapreduce implementation , we provide insights on an efficient implementation of the methods extensive experiments on the new york times annotated corpus and clueweb09 expose the relative benefits and trade offs of the methods
in this paper , we propose an efficient algorithm for the parameter synthesis of pltl formulas with respect to parametric markov chains the pltl formula is translated to an almost fully partitioned b uchi automaton which is then composed with the parametric markov chain we then reduce the problem to solving an optimisation problem , allowing to decide the satisfaction of the formula using an smt solver the algorithm works also for interval markov chains the complexity is linear in the size of the markov chain , and exponential in the size of the formula we provide a prototype and show the efficiency of our approach on a number of benchmarks
we extend amp chain graphs by \( i \) relaxing the semidirected acyclity constraint so that only directed cycles are forbidden , and \( ii \) allowing up to two edges between any pair of nodes we introduce global , ordered local and pairwise markov properties for the new models we show the equivalence of these properties for strictly positive probability distributions we also show that , when the random variables are normally distributed , the new models can be interpreted as systems of linear equations with correlated errors finally , we describe an exact algorithm for learning the new models via answer set programming
in this paper , we consider the problem of the interference alignment for the k user siso interference channel with blind channel state information at transmitters \( csit \) our achievement in contrast to popular k user interference alignment \( ia \) scheme has more practical notions in this case every receiver is equipped with one reconfigurable antenna which tries to place its desired signal in a subspace which is linearly independent from interference signals we show that if the channel values are known to the receivers only , the sum degrees of freedom \( dof \) rate region of the linear bia with staggered antenna switching is frac kr r 2 r k , where r left lceil frac sqrt 1 4k 1 2 right rceil the result indicates that the optimum dof rate region of the k user interference channel is to achieve the dof of frac sqrt k 2 for an asymptotically large network thus , the dof of the k user interference channel using staggered antenna switching grows sub linearly with the number of the users , whereas it grows linearly in the case where transmitters access the csi in addition we propose both achievability and converse proof so as to show that this is the dof rate region of blind interference alignment \( bia \) with staggered antenna switching
in this paper , we consider the downlink of large scale multi cellular ofdma based networks and study performance bounds of the system as a function of the number of users k , the number of base stations b , and the number of resource blocks n here , a resource block is a collection of subcarriers such that all such collections , that are disjoint have associated independently fading channels we derive novel upper and lower bounds on the sum utility for a general spatial geometry of base stations , a truncated path loss model , and a variety of fading models \( rayleigh , nakagami m , weibull , and lognormal \) we also establish the associated scaling laws and show that , in the special case of fixed number of resource blocks , a grid based network of base stations , and rayleigh fading channels , the sum information capacity of the system scales as theta \( b log log k b \) for extended networks , and as o \( b log log k \) and omega \( log log k \) for dense networks interpreting these results , we develop some design principles for the service providers along with some guidelines for the regulators in order to achieve provisioning of various qos guarantees for the end users and , at the same time , maximize revenue for the service providers
compiling bayesian networks \( bns \) to junction trees and performing belief propagation over them is among the most prominent approaches to computing posteriors in bns however , belief propagation over junction tree is known to be computationally intensive in the general case its complexity may increase dramatically with the connectivity and state space cardinality of bayesian network nodes in this paper , we address this computational challenge using gpu parallelization we develop data structures and algorithms that extend existing junction tree techniques , and specifically develop a novel approach to computing each belief propagation message in parallel we implement our approach on an nvidia gpu and test it using bns from several applications experimentally , we study how junction tree parameters affect parallelization opportunities and hence the performance of our algorithm we achieve speedups ranging from 0 68 to 9 18 for the bns studied
it has been shown that cooperative localization is capable of improving both the positioning accuracy and coverage in scenarios where the global positioning system \( gps \) has a poor performance however , due to its potentially excessive computational complexity , at the time of writing the application of cooperative localization remains limited in practice in this paper , we address the efficient cooperative positioning problem in wireless sensor networks a space time hierarchical graph based scheme exhibiting fast convergence is proposed for localizing the agent nodes in contrast to conventional methods , agent nodes are divided into different layers with the aid of the space time hierarchical model and their positions are estimated gradually in particular , an information propagation rule is conceived upon considering the quality of positional information according to the rule , the information always propagates from the upper layers to a certain lower layer and the message passing process is further optimized at each layer hence , the potential error propagation can be mitigated additionally , both position estimation and position broadcasting are carried out by the sensor nodes furthermore , a sensor activation mechanism is conceived , which is capable of significantly reducing both the energy consumption and the network traffic overhead incurred by the localization process the analytical and numerical results provided demonstrate the superiority of our space time hierarchical graph based cooperative localization scheme over the benchmarking schemes considered
the maximum number of non crossing straight line perfect matchings that a set of n points in the plane can have is known to be o \( 10 0438 n \) and omega \( 3 n \) the lower bound , due to garc 'ia , noy , and tejel \( 2000 \) is attained by the double chain , which has theta \( 3 n n o \( 1 \) \) such matchings we reprove this bound in a simplified way that uses the novel notion of down free matching , and apply this approach on several other constructions as a result , we improve the lower bound first we show that double zigzag chain with n points has theta \( lambda n \) such matchings with lambda approx 3 0532 next we analyze further generalizations of double zigzag chains double r chains the best choice of parameters leads to a construction with theta \( nu n \) matchings , with nu approx 3 0930 the derivation of this bound requires an analysis of a coupled dynamic programming recursion between two infinite vectors
we use differential geometry techniques to study the classification problem to estimate the conditional label probability p \( y 1 mathbf x \) for learning a plug in classifier in particular , we propose a geometric regularization technique to find the optimal hypersurface corresponding to the estimator of p \( y 1 mathbf x \) the regularization term measures the total riemannian curvature of the hypersurface corresponding to the estimator of p \( y 1 mathbf x \) , based on the intuition that overfitting corresponds to fast oscillations and hence large curvature of the estimator we use gradient flow type methods to move from an initial estimator towards a minimizer of a penalty function that penalizes both the deviation of the hypersurface from the training data and the total curvature of the hypersurface we establish bayes consistency for our algorithm under mild initialization assumptions and implement a discrete version of this algorithm in experiments for binary classification , our implementation compares favorably to several widely used classification methods
we consider the problem of predicting winners in elections , for the case where we are given complete knowledge about all possible candidates , all possible voters \( together with their preferences \) , but where it is uncertain either which candidates exactly register for the election or which voters cast their votes under reasonable assumptions , our problems reduce to counting variants of election control problems we either give polynomial time algorithms or prove p completeness results for counting variants of control by adding deleting candidates voters for plurality , k approval , approval , condorcet , and maximin voting rules we consider both the general case , where voters' preferences are unrestricted , and the case where voters' preferences are single peaked
after a brief discussion of the history of the problem , we propose a generalization of the map colouring problem to higher dimensions
trust is a fundamental concept in many real world applications such as e commerce and peer to peer networks in these applications , users can generate local opinions about the counterparts based on direct experiences , and these opinions can then be aggregated to build trust among unknown users the mechanism to build new trust relationships based on existing ones is referred to as trust inference state of the art trust inference approaches employ the transitivity property of trust by propagating trust along connected users in this paper , we propose a novel trust inference model \( matrust \) by exploring an equally important property of trust , i e , the multi aspect property matrust directly characterizes multiple latent factors for each trustor and trustee from the locally generated trust relationships furthermore , it can naturally incorporate prior knowledge as specified factors these factors in turn serve as the basis to infer the unseen trustworthiness scores experimental evaluations on real data sets show that the proposed matrust significantly outperforms several benchmark trust inference models in both effectiveness and efficiency
we consider the problem of decision making with side information and unbounded loss functions inspired by probably approximately correct learning model , we use a slightly different model that incorporates the notion of side information in a more generic form to make it applicable to a broader class of applications including parameter estimation and system identification we address sufficient conditions for consistent decision making with exponential convergence behavior in this regard , besides a certain condition on the growth function of the class of loss functions , it suffices that the class of loss functions be dominated by a measurable function whose exponential orlicz expectation is uniformly bounded over the probabilistic model decay exponent , decay constant , and sample complexity are discussed example applications to method of moments , maximum likelihood estimation , and system identification are illustrated , as well
we consider a scenario where a monitor is interested in being up to date with respect to the status of some system which is not directly accessible to this monitor however , we assume a source node has access to the status and can send status updates as packets to the monitor through a communication system we also assume that the status updates are generated randomly as a poisson process the source node can manage the packet transmission to minimize the age of information at the destination node , which is defined as the time elapsed since the last successfully transmitted update was generated at the source we use queuing theory to model the source destination link and we assume that the time to successfully transmit a packet is a gamma distributed service time we consider two packet management schemes lcfs \( last come first served \) with preemption and lcfs without preemption we compute and analyze the average age and the average peak age of information under these assumptions moreover , we extend these results to the case where the service time is deterministic
traits allow decomposing programs into smaller parts and mixins are a form of composition that resemble multiple inheritance unfortunately , in the presence of traits , programming languages like scala give up on subtyping relation between objects in this paper , we present a method to check subtyping between objects based on entailment in separation logic we implement our method as a domain specific language in scala and apply it on the scala standard library we have verified that 67 of mixins used in the scala standard library do indeed conform to subtyping between the traits that are used to build them
this paper addresses the following question , which is of interest in the design and deployment of a multiuser decentralized network given a total system bandwidth of w hz and a fixed data rate constraint of r bps for each transmission , how many frequency slots n of size w n should the band be partitioned into to maximize the number of simultaneous transmissions in the network \? in an interference limited ad hoc network , dividing the available spectrum results in two competing effects on the positive side , it reduces the number of users on each band and therefore decreases the interference level which leads to an increased sinr , while on the negative side the sinr requirement for each transmission is increased because the same information rate must be achieved over a smaller bandwidth exploring this tradeoff between bandwidth and sinr and determining the optimum value of n in terms of the system parameters is the focus of the paper using stochastic geometry , we analytically derive the optimal sinr threshold \( which directly corresponds to the optimal spectral efficiency \) on this tradeoff curve and show that it is a function of only the path loss exponent furthermore , the optimal sinr point lies between the low sinr \( power limited \) and high sinr \( bandwidth limited \) regimes in order to operate at this optimal point , the number of frequency bands \( i e , the reuse factor \) should be increased until the threshold sinr , which is an increasing function of the reuse factor , is equal to the optimal value
futurlct is a fet flagship project using collective , participatory research , integrated across ict , the social sciences and complexity science , to design socio inspired technology and develop a science of global , socially interactive systems the project will bring together , on a global level , big data , new modelling techniques and new forms of interaction , leading to a new understanding of society and its coevolution with technology it aims to understand , explore and manage our complex , connected world in a more sustainable and resilient way futurict is motivated by the fact that ubiquitous communication and sensing blur the boundaries between the physical and digital worlds , creating unparalleled opportunities for understanding the socio economic fabric of our world , and for empowering humanity to make informed , responsible decisions for its future the intimate , complex and dynamic relationship between global , networked ict systems and human society directly influences the complexity and manageability of both this also opens up the possibility to fundamentally change the way ict will be designed , built and operated , to reflect the need for socially interactive , ethically sensitive , trustworthy , self organised and reliable systems futurict will create a new public resource value oriented tools and models to aggregate , access , query and understand vast amounts of data information from open sources , real time devices and mobile sensors will be integrated with multi scale models of the behaviour of social , technological , environmental and economic systems , which can be interrogated by policy makers , business people and citizens alike together , these will build an eco system that will lead to new business models , scientific paradigm shifts and more rapid and effective ways to create and disseminate new knowledge and social benefits thereby forming an innovation accelerator
since organizations have recognized that knowledge constitutes a valuable intangible asset for creating and sustaining competitive advantages , knowledge sharing has a vital role in present society it is an activity through which information is exchanged among people through different media many problems face the area of knowledge sharing and knowledge reuse currently , knowledge sharing between entities is achieved in a very ad hoc fashion , lacking proper understanding of the meaning of the data ontologies can potentially solve these problems by facilitating knowledge sharing and reuse through formal and real world semantics ontologies , through formal semantics , are machine understandable a computer can process data , annotated with references to ontologies , and through the knowledge encapsulated in the ontology , deduce facts from the original data the date fruit is the most enduring symbol of the sultanate 's rich heritage creating ontology for dates will enrich the farming group and research scholars in the agro farm area
in this paper , we present some results regarding the size complexity of accepting networks of evolutionary processors with filtered connections \( anepfcs \) we show that there are universal anepfcs of size 10 , by devising a method for simulating 2 tag systems this result significantly improves the known upper bound for the size of universal anepfcs which is 18 we also propose a new , computationally and descriptionally efficient simulation of nondeterministic turing machines by anepfcs more precisely , we describe \( informally , due to space limitations \) how anepfcs with 16 nodes can simulate in o \( f \( n \) \) time any nondeterministic turing machine of time complexity f \( n \) thus the known upper bound for the number of nodes in a network simulating an arbitrary turing machine is decreased from 26 to 16
new ranking algorithms are continually being developed and refined , necessitating the development of efficient methods for evaluating these rankers online ranker evaluation focuses on the challenge of efficiently determining , from implicit user feedback , which ranker out of a finite set of rankers is the best online ranker evaluation can be modeled by dueling ban dits , a mathematical model for online learning under limited feedback from pairwise comparisons comparisons of pairs of rankers is performed by interleaving their result sets and examining which documents users click on the dueling bandits model addresses the key issue of which pair of rankers to compare at each iteration , thereby providing a solution to the exploration exploitation trade off recently , methods for simultaneously comparing more than two rankers have been developed however , the question of which rankers to compare at each iteration was left open we address this question by proposing a generalization of the dueling bandits model that uses simultaneous comparisons of an unrestricted number of rankers we evaluate our algorithm on synthetic data and several standard large scale online ranker evaluation datasets our experimental results show that the algorithm yields orders of magnitude improvement in performance compared to stateof the art dueling bandit algorithms
we prove chi s' \( g \) leq 1 93 delta \( g \) 2 for graphs of sufficiently large maximum degree where chi s' \( g \) is the strong chromatic index of g this improves an old bound of molloy and reed as a by product , we present a talagrand type inequality where it is allowed to exclude unlikely bad outcomes that would otherwise render the inequality unusable
r 'enyi divergence is related to r 'enyi entropy much like information divergence \( also called kullback leibler divergence or relative entropy \) is related to shannon 's entropy , and comes up in many settings it was introduced by r 'enyi as a measure of information that satisfies almost the same axioms as information divergence we review the most important properties of r 'enyi divergence , including its relation to some other distances we show how r 'enyi divergence appears when the theory of majorization is generalized from the finite to the continuous setting finally , r 'enyi divergence plays a role in analyzing the number of binary questions required to guess the values of a sequence of random variables
social network sites allow users to publicly tag people in their posts these tagged posts allow users to share to both the general public and a targeted audience , dynamically assembled via notifications that alert the people mentioned we investigate people 's perceptions of this mixed sharing mode through a qualitative study with 120 participants we found that individuals like this sharing modality as they believe it strengthens their relationships individuals also report using tags to have more control of facebook 's ranking algorithm , and to expose one another to novel information and people this work helps us understand people 's complex relationships with the algorithms that mediate their interactions with each another we conclude by discussing the design implications of these findings
linear classifiers separate the data with a hyperplane in this paper we focus on the novel method of construction of multithreshold linear classifier , which separates the data with multiple parallel hyperplanes proposed model is based on the information theory concepts namely renyi 's quadratic entropy and cauchy schwarz divergence we begin with some general properties , including data scale invariance then we prove that our method is a multithreshold large margin classifier , which shows the analogy to the svm , while in the same time works with much broader class of hypotheses what is also interesting , proposed method is aimed at the maximization of the balanced quality measure \( such as matthew 's correlation coefficient \) as opposed to very common maximization of the accuracy this feature comes directly from the optimization problem statement and is further confirmed by the experiments on the uci datasets it appears , that our multithreshold entropy linear classifier \( melc \) obtaines similar or higher scores than the ones given by svm on both synthetic and real data we show how proposed approach can be benefitial for the cheminformatics in the task of ligands activity prediction , where despite better classification results , melc gives some additional insight into the data structure \( classes of underrepresented chemical compunds \)
nowadays it is very important to maintain a high level security to ensure safe and trusted communication of information between various organizations but secured data communication over internet and any other network is always under threat of intrusions and misuses so intrusion detection systems have become a needful component in terms of computer and network security there are various approaches being utilized in intrusion detections , but unfortunately any of the systems so far is not completely flawless so , the quest of betterment continues in this progression , here we present an intrusion detection system \( ids \) , by applying genetic algorithm \( ga \) to efficiently detect various types of network intrusions parameters and evolution processes for ga are discussed in details and implemented this approach uses evolution theory to information evolution in order to filter the traffic data and thus reduce the complexity to implement and measure the performance of our system we used the kdd99 benchmark dataset and obtained reasonable detection rate
we consider streaming , one pass principal component analysis \( pca \) , in the high dimensional regime , with limited memory here , p dimensional samples are presented sequentially , and the goal is to produce the k dimensional subspace that best approximates these points standard algorithms require o \( p 2 \) memory meanwhile no algorithm can do better than o \( kp \) memory , since this is what the output itself requires memory \( or storage \) complexity is most meaningful when understood in the context of computational and sample complexity sample complexity for high dimensional pca is typically studied in the setting of the em spiked covariance model , where p dimensional points are generated from a population covariance equal to the identity \( white noise \) plus a low dimensional perturbation \( the spike \) which is the signal to be recovered it is now well understood that the spike can be recovered when the number of samples , n , scales proportionally with the dimension , p yet , all algorithms that provably achieve this , have memory complexity o \( p 2 \) meanwhile , algorithms with memory complexity o \( kp \) do not have provable bounds on sample complexity comparable to p we present an algorithm that achieves both it uses o \( kp \) memory \( meaning storage of any kind \) and is able to compute the k dimensional spike with o \( p log p \) sample complexity the first algorithm of its kind while our theoretical analysis focuses on the spiked covariance model , our simulations show that our algorithm is successful on much more general models for the data
we consider the problem of partitioning the set of vertices of a given unit disk graph \( udg \) into a minimum number of cliques the problem is np hard and various constant factor approximations are known , with the current best ratio of 3 our main result is a em weakly robust polynomial time approximation scheme \( ptas \) for udgs expressed with edge lengths , it either \( i \) computes a clique partition or \( ii \) gives a certificate that the graph is not a udg for the case \( i \) that it computes a clique partition , we show that it is guaranteed to be within \( 1 eps \) ratio of the optimum if the input is udg however if the input is not a udg it either computes a clique partition as in case \( i \) with no guarantee on the quality of the clique partition or detects that it is not a udg noting that recognition of udg 's is np hard even if we are given edge lengths , our ptas is a weakly robust algorithm our algorithm can be transformed into an o \( frac log n eps o \( 1 \) \) time distributed ptas we consider a weighted version of the clique partition problem on vertex weighted udgs that generalizes the problem we note some key distinctions with the unweighted version , where ideas useful in obtaining a ptas breakdown yet , surprisingly , it admits a \( 2 eps \) approximation algorithm for the weighted case where the graph is expressed , say , as an adjacency matrix this improves on the best known 8 approximation for the em unweighted case for udgs expressed in standard form
information communication technology promotes collaborative environments like wikipedia where , however , controversiality and conflicts can appear to describe the rise , persistence , and resolution of such conflicts we devise an extended opinion dynamics model where agents with different opinions perform a single task to make a consensual product as a function of the convergence parameter describing the influence of the product on the agents , the model shows spontaneous symmetry breaking of the final consensus opinion represented by the medium in the case when agents are replaced with new ones at a certain rate , a transition from mainly consensus to a perpetual conflict occurs , which is in qualitative agreement with the scenarios observed in wikipedia
it is known that there exists a network which does not have a scalar linear solution over any finite field but has a vector linear solution when message dimension is 2 3 it is not known whether this result can be generalized for an arbitrary message dimension in this paper , we show that there exists a network which admits an m dimensional vector linear solution , where m is a positive integer greater than or equal to 2 , but does not have a vector linear solution over any finite field when the message dimension is less than m
recursive neural models , which use syntactic parse trees to recursively generate representations bottom up from parse children , are a popular new architecture , promising to capture structural properties like the scope of negation or long distance semantic dependencies but understanding exactly which tasks this parse based method is appropriate for remains an open question in this paper we benchmark recursive neural models against sequential recurrent neural models , which are structured solely on word sequences we investigate 5 tasks sentiment classification on \( 1 \) sentences and \( 2 \) syntactic phrases \( 3 \) question answering \( 4 \) discourse parsing \( 5 \) semantic relations \( e g , component whole between nouns \) we find that recurrent models have equal or superior performance to recursive models on all tasks except one semantic relations between nominals our analysis suggests that tasks relying on the scope of negation \( like sentiment \) are well handled by sequential models recursive models help only with tasks that require representing long distance relations between words our results offer insights on the design of neural architectures for representation learning
the yao graph for k 4 , y4 , is naturally partitioned into four subgraphs , one per quadrant we show that the subgraphs for one quadrant differ from the subgraphs for two adjacent quadrants in three properties planarity , connectedness , and whether the directed graphs are spanners
we consider a class of interdependent security games on networks where each node chooses a personal level of security investment the attack probability experienced by a node is a function of her own investment and the investment by her neighbors in the network most of the existing work in these settings consider players who are risk neutral or expected value maximizers in contrast , studies in behavioral decision theory have shown that individuals often deviate from risk neutral behavior while making decisions under uncertainty in particular , the true probabilities associated with uncertain outcomes are often transformed into perceived probabilities in a highly nonlinear fashion by the users , which then influence their decisions in this paper , we investigate the effects of such behavioral probability weightings by the nodes on their optimal investment strategies and the resulting security risk profiles that arise in the nash equilibria of interdependent network security games we characterize graph topologies that achieve the largest and smallest worst case average attack probabilities at nash equilibria in total effort games , and equilibrium investments in weakest link and best shot games
in this paper , we study the problem of approximately computing the product of two real matrices in particular , we analyze a dimensionality reduction based approximation algorithm due to sarlos 1 , introducing the notion of nuclear rank as the ratio of the nuclear norm over the spectral norm the presented bound has improved dependence with respect to the approximation error \( as compared to previous approaches \) , whereas the subspace on which we project the input matrices has dimensions proportional to the maximum of their nuclear rank and it is independent of the input dimensions in addition , we provide an application of this result to linear low dimensional embeddings namely , we show that any euclidean point set with bounded nuclear rank is amenable to projection onto number of dimensions that is independent of the input dimensionality , while achieving additive error guarantees
in this paper , we study the linear codes over the commutative ring r f q v f q v 2 f q and their gray images , where v 3 v we define the lee weight of the elements of r , we give a gray map from r n to f 3n q and we give the relation between the dual and the gray image of a code this allows us to investigate the structure and properties of self dual cyclic , formally self dual and the gray image of formally self dual codes over r further , we give several constructions of formally self dual codes over r
expectation is a central notion in probability theory the notion of expectation also makes sense for other notions of uncertainty we introduce a propositional logic for reasoning about expectation , where the semantics depends on the underlying representation of uncertainty we give sound and complete axiomatizations for the logic in the case that the underlying representation is \( a \) probability , \( b \) sets of probability measures , \( c \) belief functions , and \( d \) possibility measures we show that this logic is more expressive than the corresponding logic for reasoning about likelihood in the case of sets of probability measures , but equi expressive in the case of probability , belief , and possibility finally , we show that satisfiability for these logics is np complete , no harder than satisfiability for propositional logic
intelligent machines require basic information such as moving object detection from videos in order to deduce higher level semantic information in this paper , we propose a methodology that uses a texture measure to detect moving objects in video the methodology is computationally inexpensive , requires minimal parameter fine tuning and also is resilient to noise , illumination changes , dynamic background and low frame rate experimental results show that performance of the proposed approach is higher than those of state of the art approaches we also present a framework for vehicular traffic density estimation using the foreground object detection technique and present a comparison between the foreground object detection based framework and the classical density state modelling based framework for vehicular traffic density estimation
the two user multiple input single output \( miso \) broadcast channel with confidential messages \( bccm \) is studied in which the nature of channel state information at the transmitter \( csit \) from each user can be of the form i i , i 1 , 2 where i 1 , i 2 in mathsf p , mathsf d , mathsf n , and the forms mathsf p , mathsf d and mathsf n correspond to perfect and instantaneous , completely delayed , and no csit , respectively thus , the overall csit can alternate between 9 possible states corresponding to all possible values of i 1 i 2 , with each state occurring for lambda i 1 i 2 fraction of the total duration the main contribution of this paper is to establish the secure degrees of freedom \( s d o f \) region of the miso bccm with alternating csit with the symmetry assumption , where lambda i 1 i 2 lambda i 2 i 1 the main technical contributions include developing a \) novel achievable schemes for miso bccm with alternating csit with security constraints which also highlight the synergistic benefits of inter state coding for secrecy , b \) new converse proofs via local statistical equivalence and channel enhancement and c \) showing the interplay between various aspects of channel knowledge and their impact on s d o f
recently , a general tool called a holographic transformation , which transforms an expression of the partition function to another form , has been used for polynomial time algorithms and for improvement and understanding of the belief propagation in this work , the holographic transformation is generalized to quantum factor graphs
we consider the problem of approximating an affinely structured matrix , for example a hankel matrix , by a low rank matrix with the same structure this problem occurs in system identification , signal processing and computer algebra , among others we impose the low rank by modeling the approximation as a product of two factors with reduced dimension the structure of the low rank model is enforced by introducing a penalty term in the objective function the proposed local optimization algorithm is able to solve the weighted structured low rank approximation problem , as well as to deal with the cases of missing or fixed elements in contrast to approaches based on kernel representations \( in linear algebraic sense \) , the proposed algorithm is designed to address the case of small targeted rank we compare it to existing approaches on numerical examples of system identification , approximate greatest common divisor problem , and symmetric tensor decomposition and demonstrate its consistently good performance
locally repairable codes \( lrcs \) are a class of codes designed for the local correction of erasures they have received considerable attention in recent years due to their applications in distributed storage most existing results on lrcs do not explicitly take into consideration the field size q , i e , the size of the code alphabet in particular , for the binary case , only a few results are known in this work , we present an upper bound on the minimum distance d of linear lrcs with availability , based on the work of cadambe and mazumdar the bound takes into account the code length n , dimension k , locality r , availability t , and field size q then , we study binary linear lrcs in three aspects first , we focus on analyzing the locality of some classical codes , i e , cyclic codes and reed muller codes , and their modified versions , which are obtained by applying the operations of extend , shorten , expurgate , augment , and lengthen next , we construct lrcs using phantom parity check symbols and multi level tensor product structure , respectively compared to other previous constructions of binary lrcs with fixed locality or minimum distance , our construction is much more flexible in terms of code parameters , and gives various families of high rate lrcs , some of which are shown to be optimal with respect to their minimum distance finally , availability of lrcs is studied we investigate the locality and availability properties of several classes of one step majority logic decodable codes , including cyclic simplex codes , cyclic difference set codes , and 4 cycle free regular low density parity check \( ldpc \) codes we also show the construction of a long lrc with availability from a short one step majority logic decodable code
we study opportunistic scheduling and the sum capacity of cellular networks with a full duplex multi antenna base station and a large number of single antenna half duplex users simultaneous uplink and downlink over the same band results in uplink to downlink interference , degrading performance we present a simple opportunistic joint uplink downlink scheduling algorithm that exploits multiuser diversity and treats interference as noise we show that in homogeneous networks , our algorithm achieves the same sum capacity as what would have been achieved if there was no uplink to downlink interference , asymptotically in the number of users the algorithm does not require interference csi at the base station or uplink users it is also shown that for a simple class of heterogeneous networks without sufficient channel diversity , it is not possible to achieve the corresponding interference free system capacity we discuss the potential for using device to device side channels to overcome this limitation in heterogeneous networks
we consider dynamic pricing schemes in online settings where selfish agents generate online events previous work on online mechanisms has dealt almost entirely with the goal of maximizing social welfare or revenue in an auction settings this paper deals with quite general settings and minimizing social costs we show that appropriately computed posted prices allow one to achieve essentially the same performance as the best online algorithm this holds in a wide variety of settings unlike online algorithms that learn about the event , and then make enforceable decisions , prices are posted without knowing the future events or even the current event , and are thus inherently dominant strategy incentive compatible in particular we show that one can give efficient posted price mechanisms for metrical task systems , some instances of the k server problem , and metrical matching problems we give both deterministic and randomized algorithms such posted price mechanisms decrease the social cost dramatically over selfish behavior where no decision incurs a charge one alluring application of this is reducing the social cost of free parking exponentially
we consider solving a convex , possibly stochastic optimization problem over a randomly time varying multi agent network each agent has access to some local objective function , and it only has unbiased estimates of the gradients of the smooth component we develop a dynamic stochastic proximal gradient consensus \( dyspgc \) algorithm , with the following key features i \) it works for both the static and certain randomly time varying networks , ii \) it allows the agents to utilize either the exact or stochastic gradient information , iii \) it is convergent with provable rate in particular , we show that the proposed algorithm converges to a global optimal solution , with a rate of mathcal o \( 1 r \) resp mathcal o \( 1 sqrt r \) when the exact \( resp stochastic \) gradient is available , where r is the iteration counter interestingly , the developed algorithm bridges a number of \( seemingly unrelated \) distributed optimization algorithms , such as the extra \( shi et al 2014 \) , the pg extra \( shi et al 2015 \) , the ic idc admm \( chang et al 2014 \) , and the dlm \( ling et al 2015 \) and the classical distributed subgradient method identifying such relationship allows for significant generalization of these methods we also discuss one such generalization which accelerates the dyspgc \( hence accelerating extra , pg extra , ic admm \)
though network coding is traditionally performed over finite fields , recent work on nested lattice based network coding suggests that , by allowing network coding over certain finite rings , more efficient physical layer network coding schemes can be constructed this paper considers the problem of communication over a finite ring matrix channel y ax be , where x is the channel input , y is the channel output , e is random error , and a and b are random transfer matrices tight capacity results are obtained and simple polynomial complexity capacity achieving coding schemes are provided under the assumption that a is uniform over all full rank matrices and be is uniform over all rank t matrices , extending the work of silva , kschischang and k o tter \( 2010 \) , who handled the case of finite fields this extension is based on several new results , which may be of independent interest , that generalize concepts and methods from matrices over finite fields to matrices over finite chain rings
we propose a scheme to estimate the parameters b i and c j of the bilinear form z m sum i , j b i z m \( i , j \) c j from noisy measurements y m m 1 m , where y m and z m are related through an arbitrary likelihood function and z m \( i , j \) are known our scheme is based on generalized approximate message passing \( g amp \) it treats b i and c j as random variables and z m \( i , j \) as an i i d gaussian tensor in order to derive a tractable simplification of the sum product algorithm in the large system limit it generalizes previous instances of bilinear g amp , such as those that estimate matrices boldsymbol b and boldsymbol c from a noisy measurement of boldsymbol z boldsymbol bc , allowing the application of amp methods to problems such as self calibration , blind deconvolution , and matrix compressive sensing numerical experiments confirm the accuracy and computational efficiency of the proposed approach
we tackle the pac bayesian domain adaptation \( da \) problem this arrives when one desires to learn , from a source distribution , a good weighted majority vote \( over a set of classifiers \) on a different target distribution in this context , the disagreement between classifiers is known crucial to control in non da supervised setting , a theoretical bound the c bound involves this disagreement and leads to a majority vote learning algorithm mincq in this work , we extend mincq to da by taking advantage of an elegant divergence between distribution called the perturbed varation \( pv \) firstly , justified by a new formulation of the c bound , we provide to mincq a target sample labeled thanks to a pv based self labeling focused on regions where the source and target marginal distributions are closer secondly , we propose an original process for tuning the hyperparameters our framework shows very promising results on a toy problem
languages across the world exhibit zipf 's law of abbreviation , namely more frequent words tend to be shorter the generalized version of the law , namely an inverse relationship between the frequency of a unit and its magnitude , holds also for the behaviors of other species and the genetic code the apparent universality of this pattern in human language and its ubiquity in other domains calls for a theoretical understanding of its origins we generalize the information theoretic concept of mean code length as a mean energetic cost function over the probability and the magnitude of the symbols of the alphabet we show that the minimization of that cost function and a negative correlation between probability and the magnitude of symbols are intimately related
the task of outlier detection is to find small groups of data objects that are exceptional when compared with rest large amount of data in 38 , the problem of outlier detection in categorical data is defined as an optimization problem and a local search heuristic based algorithm \( lsa \) is presented however , as is the case with most iterative type algorithms , the lsa algorithm is still very time consuming on very large datasets in this paper , we present a very fast greedy algorithm for mining outliers under the same optimization model experimental results on real datasets and large synthetic datasets show that \( 1 \) our algorithm has comparable performance with respect to those state of art outlier detection algorithms on identifying true outliers and \( 2 \) our algorithm can be an order of magnitude faster than lsa algorithm
we study \( symbol pair \) codes for symbol pair read channels introduced recently by cassuto and blaum \( 2010 \) a singleton type bound on symbol pair codes is established and infinite families of optimal symbol pair codes are constructed these codes are maximum distance separable \( mds \) in the sense that they meet the singleton type bound in contrast to classical codes , where all known q ary mds codes have length o \( q \) , we show that q ary mds symbol pair codes can have length omega \( q 2 \) in addition , we completely determine the existence of mds symbol pair codes for certain parameters
the mammogrid project has deployed its service oriented architecture \( soa \) based grid application in a real environment comprising actual participating hospitals the resultant setup is currently being exploited to conduct rigorous in house tests in the first phase before handing over the setup to the actual clinicians to get their feedback this paper elaborates the deployment details and the experiences acquired during this phase of the project finally the strategy regarding migration to an upcoming middleware from egee project will be described this paper concludes by highlighting some of the potential areas of future work
this paper introduces cancer hybrid automata \( chas \) , a formalism to model the progression of cancers through discrete phenotypes the classification of cancer progression using discrete states like stages and hallmarks has become common in the biology literature , but primarily as an organizing principle , and not as an executable formalism the precise computational model developed here aims to exploit this untapped potential , namely , through automatic verification of progression models \( e g , consistency , causal connections , etc \) , classification of unreachable or unstable states and computer generated \( individualized or universal \) therapy plans the paper builds on a phenomenological approach , and as such does not need to assume a model for the biochemistry of the underlying natural progression rather , it abstractly models transition timings between states as well as the effects of drugs and clinical tests , and thus allows formalization of temporal statements about the progression as well as notions of timed therapies the model proposed here is ultimately based on hybrid automata , and we show how existing controller synthesis algorithms can be generalized to cha models , so that therapies can be generated automatically throughout this paper we use cancer hallmarks to represent the discrete states through which cancer progresses , but other notions of discretely or continuously varying state formalisms could also be used to derive similar therapies
brassard et al showed that shared nonlocal boxes with the chsh probability greater than frac 3 sqrt 6 6 yields trivial communication complexity there still exists the gap with the maximum chsh probability frac 2 sqrt 2 4 achievable by quantum mechanics it is an interesting open question to determine the exact threshold for the trivial communication complexity brassard et al 's idea is based on the recursive bias amplification by the 3 input majority function it was not obvious if other choice of function exhibits stronger bias amplification we show that the 3 input majority function is the unique optimal so that one cannot improve the threshold frac 3 sqrt 6 6 by brassard et al 's bias amplification
this paper has been withdrawn due to a critical error near equation \( 71 \) this error causes the entire argument of the paper to collapse emmanuel candes of stanford discovered the error , and has suggested a correct analysis , which will be reported in a separate publication
the basic goal in combinatorial group testing is to identify a set of up to d defective items within a large population of size n gg d using a pooling strategy namely , the items can be grouped together in pools , and a single measurement would reveal whether there are one or more defectives in the pool the threshold model is a generalization of this idea where a measurement returns positive if the number of defectives in the pool reaches a fixed threshold u 0 , negative if this number is no more than a fixed lower threshold ell u , and may behave arbitrarily otherwise we study non adaptive threshold group testing \( in a possibly noisy setting \) and show that , for this problem , o \( d g 2 \( log d \) log \( n d \) \) measurements \( where g u ell 1 and u is any fixed constant \) suffice to identify the defectives , and also present almost matching lower bounds this significantly improves the previously known \( non constructive \) upper bound o \( d u 1 log \( n d \) \) moreover , we obtain a framework for explicit construction of measurement schemes using lossless condensers the number of measurements resulting from this scheme is ideally bounded by o \( d g 3 \( log d \) log n \) using state of the art constructions of lossless condensers , however , we obtain explicit testing schemes with o \( d g 3 \( log d \) qpoly \( log n \) \) and o \( d g 3 beta poly \( log n \) \) measurements , for arbitrary constant beta 0
in this article , we examine several design and complexity aspects of the optimal physical layer resource allocation problem for a generic interference channel \( ic \) the latter is a natural model for multi user communication networks in particular , we characterize the computational complexity , the convexity as well as the duality of the optimal resource allocation problem moreover , we summarize various existing algorithms for resource allocation and discuss their complexity and performance tradeoff we also mention various open research problems throughout the article
in this paper we present a new algorithm , denoted as tep , to decode low density parity check \( ldpc \) codes over the binary erasure channel \( bec \) the tep decoder is derived applying the expectation propagation \( ep \) algorithm with a tree structured approximation expectation propagation \( ep \) is a generalization to belief propagation \( bp \) in two ways first , it can be used with any exponential family distribution over the cliques in the graph second , it can impose additional constraints on the marginal distributions we use this second property to impose pair wise marginal constraints in some check nodes of the ldpc code 's tanner graph the algorithm has the same computational complexity than bp , but it can decode a higher fraction of errors when applied over the bec in this paper , we focus on the asymptotic performance of the tep decoder , as the block size tends to infinity we describe the tep decoder by a set of differential equations that represents the residual graph evolution during the decoding process the solution of these equations yields the capacity of this decoder for a given ldpc ensemble over the bec we show that the achieved capacity with the tep is higher than the bp capacity , at the same computational complexity
advances in sensing and communication capabilities as well as power industry deregulation are driving the need for distributed state estimation in the smart grid at the level of the regional transmission organizations \( rtos \) this leads to a new competitive privacy problem amongst the rtos since there is a tension between sharing data to ensure network reliability \( utility benefit to all rtos \) and withholding data for profitability and privacy reasons the resulting tradeoff between utility , quantified via fidelity of its state estimate at each rto , and privacy , quantified via the leakage of the state of one rto at other rtos , is captured precisely using a lossy source coding problem formulation for a two rto network for a two rto model , it is shown that the set of all feasible utility privacy pairs can be achieved via a single round of communication when each rto communicates taking into account the correlation between the measured data at both rtos the lossy source coding problem and solution developed here is also of independent interest
in this paper we investigate the benefit of base station \( bs \) cooperation in the uplink of coordinated multi point \( comp \) networks our figure of merit is the required bs density required to meet a chosen rate coverage our model assumes a 2 d network of bss on a regular hexagonal lattice in which path loss , lognormal shadowing and rayleigh fading affect the signal received from users accurate closed form expressions are first presented for the sum rate coverage probability and ergodic sum rate at each point of the cooperation region then , for a chosen quality of user rate , the required density of bs is derived based on the minimum value of rate coverage probability in the cooperation region the approach guarantees that the achievable rate in the entire coverage region is above a target rate with chosen probability the formulation allows comparison between different orders of bs cooperation , quantifying the reduced required bs density from higher orders of cooperation
layer wise relevance propagation \( lrp \) is a recently proposed technique for explaining predictions of complex non linear classifiers in terms of input variables in this paper , we apply lrp for the first time to natural language processing \( nlp \) more precisely , we use it to explain the predictions of a convolutional neural network \( cnn \) trained on a topic categorization task our analysis highlights which words are relevant for a specific prediction of the cnn we compare our technique to standard sensitivity analysis , both qualitatively and quantitatively , using a word deleting perturbation experiment , a pca analysis , and various visualizations all experiments validate the suitability of lrp for explaining the cnn predictions , which is also in line with results reported in recent image classification studies
one clock alternating timed automata ocata have been recently introduced as natural extension of \( one clock \) timed automata to express the semantics of mtl \( ouaknine , worrell 2005 \) we consider the application of ocata to problem of model checking mitl formulas \( a syntactic fragment of mtl \) against timed automata we introduce a new semantics for ocata where , intuitively , clock valuations are intervals instead of single real values thanks to this new semantics , we show that we can bound the number of clock copies that are necessary to allow an ocata to recognise the models of an mitl formula equipped with this technique , we propose a new algorithm to translate an mitl formula into a timed automaton , and we sketch several ideas to define new model checking algorithms for mitl
we investigate multi round team competitions between two teams , where each team selects one of its players simultaneously in each round and each player can play at most once the competition defines an extensive form game with perfect recall and can be solved efficiently by standard methods we are interested in the properties of the subgame perfect equilibria of this game we first show that uniformly random strategy is a subgame perfect equilibrium strategy for both teams when there are no redundant players \( i e , the number of players in each team equals to the number of rounds of the competition \) secondly , a team can safely abandon its weak players if it has redundant players and the strength of players is transitive we then focus on the more interesting case where there are redundant players and the strength of players is not transitive in this case , we obtain several counterintuitive results first of all , a player might help improve the payoff of its team , even if it is dominated by the entire other team we give a necessary condition for a dominated player to be useful we also study the extent to which the dominated players can increase the payoff these results bring insights into playing and designing general team competitions
we propose new algorithms for computing triangular decompositions of polynomial systems incrementally with respect to previous works , our improvements are based on a em weakened notion of a polynomial gcd modulo a regular chain , which permits to greatly simplify and optimize the sub algorithms extracting common work from similar expensive computations is also a key feature of our algorithms in our experimental results the implementation of our new algorithms , realized with the regularchains library in maple , outperforms solvers with similar specifications by several orders of magnitude on sufficiently difficult problems
graph clustering is a fundamental problem with a number of applications in algorithm design , machine learning , data mining , and analysis of social networks over the past decades , researchers have proposed a number of algorithmic design methods for graph clustering however , most of these methods are based on complicated spectral techniques or convex optimization , and cannot be applied directly for clustering most real world networks , whose information is often collected on different sites designing a simple clustering algorithm that works in the distributed setting is of important interest , and has wide applications for processing big datasets in this paper we present a simple and distributed algorithm for graph clustering for a wide class of graphs with n nodes that can be characterized by a well defined cluster structure , our algorithm finishes in a poly logarithmic number of rounds , and recovers a partition of the graph with at most o \( n \) misclassified nodes the main component of our algorithm is an application of the random matching model of load balancing , which is a fundamental protocol in distributed computing and has been extensively studied in the past 20 years hence , our result highlights an intrinsic and interesting connection between graph clustering and load balancing
we consider a class of well known dynamic resource allocation models in loss network systems with advanced reservation the most important performance measure in any loss network system is to compute its blocking probability , i e , the probability of an arriving customer in equilibrium finds a fully utilized system \( thereby getting rejected by the system \) in this paper , we derive upper bounds on the asymptotic blocking probabilities for such systems in high volume regimes there have been relatively few results on loss network systems with advanced reservation due to its inherent complexity the theoretical results find applications in a wide class of revenue management problems in systems with reusable resources and advanced reservation , e g , hotel room , car rental and workforce management we propose a simple control policy called the improved class selection policy \( icsp \) based on solving a continuous knapsack problem , similar in spirit to the one proposed in levi and radovanovic \( 2010 \) using our results derived for loss network systems with advanced reservation , we show the icsp performs asymptotically near optimal in high volume regimes
the proliferation of internet enabled devices and services has led to a shifting balance between digital and analogue aspects of our everyday lives in the face of this development there is a growing demand for the study of privacy hazards , the potential for unique user de anonymization and information leakage between the various social media profiles many of us maintain to enable the structured study of such adversarial effects , this paper presents a dedicated dataset of cross platform social network personas \( i e , the same person has accounts on multiple platforms \) the corpus comprises 850 users who generate predominantly english content each user object contains the online footprint of the same person in three distinct social networks twitter , instagram and foursquare in total , it encompasses over 2 5m tweets , 340k check ins and 42k instagram posts we describe the collection methodology , characteristics of the dataset , and how to obtain it finally , we discuss a common use case , cross platform user identification
we consider learning the principal subspace of a large set of vectors from an extremely small number of compressive measurements of each vector our theoretical results show that even a constant number of measurements per column suffices to approximate the principal subspace to arbitrary precision , provided that the number of vectors is large this result is achieved by a simple algorithm that computes the eigenvectors of an estimate of the covariance matrix the main insight is to exploit an averaging effect that arises from applying a different random projection to each vector we provide a number of simulations confirming our theoretical results
we propose a new method for robust pca the task of recovering a low rank matrix from sparse corruptions that are of unknown value and support our method involves alternating between projecting appropriate residuals onto the set of low rank matrices , and the set of sparse matrices each projection is em non convex but easy to compute in spite of this non convexity , we establish exact recovery of the low rank matrix , under the same conditions that are required by existing methods \( which are based on convex optimization \) for an m times n input matrix \( m leq n \) , our method has a running time of o \( r 2mn \) per iteration , and needs o \( log \( 1 epsilon \) \) iterations to reach an accuracy of epsilon this is close to the running time of simple pca via the power method , which requires o \( rmn \) per iteration , and o \( log \( 1 epsilon \) \) iterations in contrast , existing methods for robust pca , which are based on convex optimization , have o \( m 2n \) complexity per iteration , and take o \( 1 epsilon \) iterations , i e , exponentially more iterations for the same accuracy experiments on both synthetic and real data establishes the improved speed and accuracy of our method over existing convex implementations
this study aims to develop and test an amalgamated conceptual framework based on technology acceptance model \( tam \) and other models from social psychology , such as theory of reasoned action and tam2 that captures the salient factors influencing the user adoption and acceptance of web based learning systems this framework has been applied to the study of higher educational institutions in the context of developing as well as developed countries \( e g lebanon and uk \) additionally , the framework investigates the moderating effect of hofstedes four cultural dimensions at the individual level and a set of individual differences \( age , gender , experience and educational level \) on the key determinants that affect the behavioral intention to use e learning a total of 1197 questionnaires were received from students who were using web based learning systems at higher educational institutions in lebanon and the uk with opposite scores on cultural dimensions confirmatory factor analysis \( cfa \) was used to perform reliability and validity checks , and structural equation modeling \( sem \) in conjunction with multi group analysis method was used to test the hypothesized conceptual model our findings suggest that individual , social , cultural and organizational factors are important to consider in explaining students behavioral intention and usage of e learning environments the findings of this research contribute to the literature by validating and supporting the applicability of our extended tam in the lebanese and british contexts and provide several prominent implications to both theory and practice on the individual , organizational and societal levels
databases in domains such as healthcare are routinely released to the public in aggregated form unfortunately , naive modeling with aggregated data may significantly diminish the accuracy of inferences at the individual level this paper addresses the scenario where features are provided at the individual level , but the target variables are only available as histogram aggregates or order statistics we consider a limiting case of generalized linear modeling when the target variables are only known up to permutation , and explore how this relates to permutation testing a standard technique for assessing statistical dependency based on this relationship , we propose a simple algorithm to estimate the model parameters and individual level inferences via alternating imputation and standard generalized linear model fitting our results suggest the effectiveness of the proposed approach when , in the original data , permutation testing accurately ascertains the veracity of the linear relationship the framework is extended to general histogram data with larger bins with order statistics such as the median as a limiting case our experimental results on simulated data and aggregated healthcare data suggest a diminishing returns property with respect to the granularity of the histogram when a linear relationship holds in the original data , the targets can be predicted accurately given relatively coarse histograms
complementarity problems and variational inequalities arise in a wide variety of areas , including machine learning , planning , game theory , and physical simulation in all of these areas , to handle large scale problem instances , we need fast approximate solution methods one promising idea is galerkin approximation , in which we search for the best answer within the span of a given set of basis functions bertsekas proposed one possible galerkin method for variational inequalities however , this method can exhibit two problems in practice its approximation error is worse than might be expected based on the ability of the basis to represent the desired solution , and each iteration requires a projection step that is not always easy to implement efficiently so , in this paper , we present a new galerkin method with improved behavior our new error bounds depend directly on the distance from the true solution to the subspace spanned by our basis , and the only projections we require are onto the feasible region or onto the span of our basis
an irregular ldgm ldpc code is studied as a sub code of an ldpc code with some randomly emph punctured output bits it is shown that the ldgm ldpc codes achieve rates arbitrarily close to the channel capacity of the binary input symmetric output memoryless \( bisom \) channel with bounded emph complexity the measure of complexity is the average degree \( per information bit \) of the check nodes for the factor graph of the code a lower bound on the average degree of the check nodes of the irregular ldgm ldpc codes is obtained the bound does not depend on the decoder used at the receiver the stability condition for decoding the irregular ldgm ldpc codes over the binary erasure channel \( bec \) under iterative decoding with message passing is described
given an n ary k valued function f , gap \( f \) denotes the minimal number of essential variables in f which become fictive when identifying any two distinct essential variables in f we particularly solve a problem concerning the explicit determination of n ary k valued functions f with 2 leq gap \( f \) leq n leq k our methods yield new combinatorial results about the number of such functions
compressed sensing is designed to measure sparse signals directly in a compressed form however , most signals of interest are only approximately sparse , i e even though the signal contains only a small fraction of relevant \( large \) components the other components are not strictly equal to zero , but are only close to zero in this paper we model the approximately sparse signal with a gaussian distribution of small components , and we study its compressed sensing with dense random matrices we use replica calculations to determine the mean squared error of the bayes optimal reconstruction for such signals , as a function of the variance of the small components , the density of large components and the measurement rate we then use the g amp algorithm and we quantify the region of parameters for which this algorithm achieves optimality \( for large systems \) finally , we show that in the region where the gamp for the homogeneous measurement matrices is not optimal , a special seeding design of a spatially coupled measurement matrix allows to restore optimality
we consider a gi h n queueing system in this system , there are multiple servers in the queue the inter arrival time is general and independent , and the service time follows hyper exponential distribution instead of stochastic differential equations , we propose two heavy traffic limits for this system , which can be easily applied in practical systems in applications , we show how to use these heavy traffic limits to design a power efficient cloud computing environment based on different qos requirements
common statistical prediction models often require and assume stationarity in the data however , in many practical applications , changes in the relationship of the response and predictor variables are regularly observed over time , resulting in the deterioration of the predictive performance of these models this paper presents linear four rates \( lfr \) , a framework for detecting these concept drifts and subsequently identifying the data points that belong to the new concept \( for relearning the model \) unlike conventional concept drift detection approaches , lfr can be applied to both batch and stream data is not limited by the distribution properties of the predictor variable \( e g , datasets with imbalanced labels \) is independent of the underlying statistical model and uses user specified parameters that are intuitively comprehensible the performance of lfr is compared to benchmark approaches using both simulated and commonly used public datasets that span the gamut of concept drift types the results show lfr significantly outperforms benchmark approaches in terms of recall , accuracy and delay in detection of concept drifts across datasets
submodular functions are well studied in combinatorial optimization , game theory and economics the natural diminishing returns property makes them suitable for many applications we study an extension of monotone submodular functions , which we call em weakly submodular functions our extension includes some \( mildly \) supermodular functions we show that several natural functions belong to this class and relate our class to some other recent submodular function extensions we consider the optimization problem of maximizing a weakly submodular function subject to uniform and general matroid constraints for a uniform matroid constraint , the standard greedy algorithm achieves a constant approximation ratio where the constant \( experimentally \) converges to 5 95 as the cardinality constraint increases for a general matroid constraint , a simple local search algorithm achieves a constant approximation ratio where the constant \( analytically \) converges to 10 22 as the rank of the matroid increases
in this paper , space time block codes \( stbcs \) with reduced sphere decoding complexity \( sdc \) are constructed for two user multiple input multiple output \( mimo \) fading multiple access channels in this set up , both the users employ identical stbcs and the destination performs sphere decoding for the symbols of the two users first , we identify the positions of the zeros in the textbf r matrix arising out of the q r decomposition of the lattice generator such that \( i \) the worst case sdc \( wsdc \) and \( ii \) the average sdc \( asdc \) are reduced then , a set of necessary and sufficient conditions on the lattice generator is provided such that the textbf r matrix has zeros at the identified positions subsequently , explicit constructions of stbcs which results in the reduced asdc are presented the rate \( in complex symbols per channel use \) of the proposed designs is at most 2 n t where n t denotes the number of transmit antennas for each user we also show that the class of stbcs from complex orthogonal designs \( other than the alamouti design \) reduce the wsdc but not the asdc
we examine a naming game on an adaptive weighted network a weight of connection for a given pair of agents depends on their communication success rate and determines the probability with which the agents communicate in some cases , depending on the parameters of the model , the preference toward successfully communicating agents is basically negligible and the model behaves similarly to the naming game on a complete graph in particular , it quickly reaches a single language state , albeit some details of the dynamics are different from the complete graph version in some other cases , the preference toward successfully communicating agents becomes much more relevant and the model gets trapped in a multi language regime in this case gradual coarsening and extinction of languages lead to the emergence of a dominant language , albeit with some other languages still being present a comparison of distribution of languages in our model and in the human population is discussed
gaussian graphical models are widely utilized to infer and visualize networks of dependencies between continuous variables however , inferring the graph is difficult when the sample size is small compared to the number of variables to reduce the number of parameters to estimate in the model , we propose a non asymptotic model selection procedure supported by strong theoretical guarantees based on an oracle inequality and a minimax lower bound the covariance matrix of the model is approximated by a block diagonal matrix the structure of this matrix is detected by thresholding the sample covariance matrix , where the threshold is selected using the slope heuristic based on the block diagonal structure of the covariance matrix , the estimation problem is divided into several independent problems subsequently , the network of dependencies between variables is inferred using the graphical lasso algorithm in each block the performance of the procedure is illustrated on simulated data an application to a real gene expression dataset with a limited sample size is also presented the dimension reduction allows attention to be objectively focused on interactions among smaller subsets of genes , leading to a more parsimonious and interpretable modular network
a novel compressive sensing based signal multiplexing scheme is proposed in this paper to further improve the multiplexing gain for multiple input multiple output \( mimo \) system at the transmitter side , a gaussian random measurement matrix in compressive sensing is employed before the traditional spatial multiplexing in order to carry more data streams on the available spatial multiplexing streams of the underlying mimo system at the receiver side , it is proposed to reformulate the detection of the multiplexing signal into two steps in the first step , the traditional mimo equalization can be used to restore the transmitted spatial multiplexing signal of the mimo system while in the second step , the standard optimization based detection algorithm assumed in the compressive sensing framework is utilized to restore the cs multiplexing data streams , wherein the exhaustive over complete dictionary is used to guarantee the sparse representation of the cs multiplexing signal in order to avoid the excessive complexity , the sub block based dictionary and the sub block based cs restoration is proposed finally , simulation results are presented to show the feasibility of the proposed cs based enhanced mimo multiplexing scheme and our efforts in this paper shed some lights on the great potential in further improving the spatial multiplexing gain for the mimo system
his study presents a novel technique to estimate the computational complexity of sequential decoding using the berry esseen theorem unlike the theoretical bounds determined by the conventional central limit theorem argument , which often holds only for sufficiently large codeword length , the new bound obtained from the berry esseen theorem is valid for any blocklength the accuracy of the new bound is then examined for two sequential decoding algorithms , an ordering free variant of the generalized dijkstra 's algorithm \( gda \) \( or simplified gda \) and the maximum likelihood sequential decoding algorithm \( mlsda \) empirically investigating codes of small blocklength reveals that the theoretical upper bound for the simplified gda almost matches the simulation results as the signal to noise ratio \( snr \) per information bit \( gamma b \) is greater than or equal to 8 db however , the theoretical bound may become markedly higher than the simulated average complexity when gamma b is small for the mlsda , the theoretical upper bound is quite close to the simulation results for both high snr \( gamma b geq 6 db \) and low snr \( gamma b leq 2 db \) even for moderate snr , the simulation results and the theoretical bound differ by at most makeblue 0 8 on a log 10 scale
greedy algorithms for minimizing l0 norm of sparse decomposition have profound application impact on many signal processing problems in the sparse coding setup , given the observations mathrm y and the redundant dictionary mathbf phi , one would seek the most sparse coefficient \( signal \) mathrm x with a constraint on approximation fidelity in this work , we propose a greedy algorithm based on the classic orthogonal matching pursuit \( omp \) with improved sparsity on mathrm x and better recovery rate , which we name as eomp the key ingredient of the eomp is recursively performing one step orthonormalization on the remaining atoms , and evaluating correlations between residual and orthonormalized atoms we show a proof that the proposed eomp guarantees to maximize the residual reduction at each iteration through extensive simulations , we show the proposed algorithm has better exact recovery rate on i i d gaussian ensembles with gaussian signals , and more importantly yields smaller l0 norm under the same approximation fidelity compared to the original omp , for both synthetic and practical scenarios the complexity analysis and real running time result also show a manageable complexity increase over the original omp we claim that the proposed algorithm has better practical perspective for finding more sparse representations than existing greedy algorithms
the fast fourier transform \( fft \) is the most efficiently known way to compute the discrete fourier transform \( dft \) of an arbitrary n length signal , and has a computational complexity of o \( n log n \) if the dft x of the signal x has only k non zero coefficients \( where k n \) , can we do better \? in 1 , we addressed this question and presented a novel ffast \( fast fourier aliasing based sparse transform \) algorithm that cleverly induces sparse graph alias codes in the dft domain , via a chinese remainder theorem \( crt \) guided sub sampling operation of the time domain samples the resulting sparse graph alias codes are then exploited to devise a fast and iterative onion peeling style decoder that computes an n length dft of a signal using only o \( k \) time domain samples and o \( klog k \) computations the ffast algorithm is applicable whenever k is sub linear in n \( i e k o \( n \) \) , but is obviously most attractive when k is much smaller than n in this paper , we adapt the ffast framework of 1 to the case where the time domain samples are corrupted by a white gaussian noise in particular , we show that the extended noise robust algorithm r ffast computes an n length k sparse dft x using o \( klog 3 n \) noise corrupted time domain samples , in o \( klog 4n \) computations , i e , sub linear time complexity while our theoretical results are for signals with a uniformly random support of the non zero dft coefficients and additive white gaussian noise , we provide simulation results which demonstrates that the r ffast algorithm performs well even for signals like mr images , that have an approximately sparse fourier spectrum with a non uniform support for the dominant dft coefficients
we describe the construction of quantum gates \( unitary operators \) from boolean functions and give a number of applications both non reversible and reversible boolean functions are considered the construction of the hamilton operator for a quantum gate is also described computer algebra implementations are provided
this paper addresses the modeling and digital cancellation of self interference in in band full duplex \( fd \) transceivers with multiple transmit and receive antennas the self interference modeling and the proposed nonlinear spatio temporal digital canceller structure takes into account , by design , the effects of i q modulator imbalances and power amplifier \( pa \) nonlinearities with memory , in addition to the multipath self interference propagation channels and the analog rf cancellation stage the proposed solution is the first cancellation technique in the literature which can handle such a self interference scenario it is shown by comprehensive simulations with realistic rf component parameters and with two different pa models to clearly outperform the current state of the art digital self interference cancellers , and to clearly extend the usable transmit power range
learning in networks of binary synapses is known to be an np complete problem a combined stochastic local search strategy in the synaptic weight space is constructed to further improve the learning performance of a single random walker we apply two correlated random walkers guided by their hamming distance and associated energy costs \( the number of unlearned patterns \) to learn a same large set of patterns each walker first learns a small part of the whole pattern set \( partially different for both walkers but with the same amount of patterns \) and then both walkers explore their respective weight spaces cooperatively to find a solution to classify the whole pattern set correctly the desired solutions locate at the common parts of weight spaces explored by these two walkers the efficiency of this combined strategy is supported by our extensive numerical simulations and the typical hamming distance as well as energy cost is estimated by an annealed computation
aspect oriented programming \( aop \) improves modularity by encapsulating crosscutting concerns into aspects some mechanisms to compose aspects allow invasiveness as a mean to integrate concerns invasiveness means that aop languages have unrestricted access to program properties such kind of languages are interesting because they allow performing complex operations and better introduce functionalities in this report we present a classification of invasive patterns in aop this classification characterizes the aspects invasive behavior and allows developers to abstract about the aspect incidence over the program they crosscut
we prove exact bounds on the time complexity of distributed graph colouring if we are given a directed path that is properly coloured with n colours , by prior work it is known that we can find a proper 3 colouring in frac 1 2 log \( n \) pm o \( 1 \) communication rounds we close the gap between upper and lower bounds we show that for infinitely many n the time complexity is precisely frac 1 2 log n communication rounds
this paper advocates the use of the emerging distributed compressive sensing \( dcs \) paradigm in order to deploy energy harvesting \( eh \) wireless sensor networks \( wsn \) with practical network lifetime and data gathering rates that are substantially higher than the state of the art in particular , we argue that there are two fundamental mechanisms in an eh wsn i \) the energy diversity associated with the eh process that entails that the harvested energy can vary from sensor node to sensor node , and ii \) the sensing diversity associated with the dcs process that entails that the energy consumption can also vary across the sensor nodes without compromising data recovery we also argue that such mechanisms offer the means to match closely the energy demand to the energy supply in order to unlock the possibility for energy neutral wsns that leverage eh capability a number of analytic and simulation results are presented in order to illustrate the potential of the approach
in this paper we propose a new approach to the description of a network of interacting processes in a traditional programming language special programming languages or extensions to sequential languages are usually designed to express the semantics of concurrent execution using libraries in c , java , c , and other languages is more practical way of concurrent programming however , this method leads to an increase in workload of a manual coding besides , stock compilers can not detect semantic errors related to the programming model in such libraries the new markup language and a special technique of automatic programming based on the marked code can solve these problems the article provides a detailed specification of the markup language without discussing its implementation details the language is used for programming of current and prospective multi core and many core systems
a new approach to the construction of general persistent polyhierarchical classifications is proposed it is based on implicit description of category polyhierarchy by a generating polyhierarchy of classification criteria similarly to existing approaches , the classification categories are defined by logical functions encoded by attributive expressions however , the generating hierarchy explicitly predefines domains of criteria applicability , and the semantics of relations between categories is invariant to changes in the universe composition , extending variety of criteria , and increasing their cardinalities the generating polyhierarchy is an independent , compact , portable , and re usable information structure serving as a template classification it can be associated with one or more particular sets of objects , included in more general classifications as a standard component , or used as a prototype for more comprehensive classifications the approach dramatically simplifies development and unplanned modifications of persistent hierarchical classifications compared with tree , dag , and faceted schemes it can be efficiently implemented in common dbms , while considerably reducing amount of computer resources required for storage , maintenance , and use of complex polyhierarchies
we investigate em multidimensional covering mechanism design problems , wherein there are m items that need to be covered and n agents who provide covering objects , with each agent i having a private cost for the covering objects he provides the goal is to select a set of covering objects of minimum total cost that together cover all the items we focus on two representative covering problems uncapacitated facility location \( ufl \) and vertex cover \( vcp \) for multidimensional ufl , we give a black box method to transform any em lagrangian multiplier preserving rho approximation algorithm for ufl to a truthful in expectation , rho approx mechanism this yields the first result for multidimensional ufl , namely a truthful in expectation 2 approximation mechanism for multidimensional vcp \( mvcp \) , we develop a em decomposition method that reduces the mechanism design problem into the simpler task of constructing em threshold mechanisms , which are a restricted class of truthful mechanisms , for simpler \( in terms of graph structure or problem dimension \) instances of mvcp by suitably designing the decomposition and the threshold mechanisms it uses as building blocks , we obtain truthful mechanisms with the following approximation ratios \( n is the number of nodes \) \( 1 \) o \( r 2 log n \) for r dimensional vcp and \( 2 \) o \( r log n \) for r dimensional vcp on any proper minor closed family of graphs \( which improves to o \( log n \) if no two neighbors of a node belong to the same player \) these are the first truthful mechanisms for mvcp with non trivial approximation guarantees
we first introduce a class of divergence measures between power spectral density matrices these are derived by comparing the suitability of different models in the context of optimal prediction distances between infinitesimally close power spectra are quadratic , and hence , they induce a differential geometric structure we study the corresponding riemannian metrics and , for a particular case , provide explicit formulae for the corresponding geodesics and geodesic distances the close connection between the geometry of power spectra and the geometry of the fisher rao metric is noted
let h \( d \) be the parity check matrix of an ldpc convolutional code corresponding to the parity check matrix h of a qc code obtained using the method of tanner et al we see that the entries in h \( d \) are all monomials and several rows \( columns \) have monomial factors let us cyclically shift the rows of h then the parity check matrix h' \( d \) corresponding to the modified matrix h' defines another convolutional code however , its free distance is lower bounded by the minimum distance of the original qc code also , each row \( column \) of h' \( d \) has a factor different from the one in h \( d \) we show that the state space complexity of the error trellis associated with h' \( d \) can be significantly reduced by controlling the row shifts applied to h with the error correction capability being preserved
adaboost is one of the most popular machine learning algorithms it is simple to implement and often found very effective by practitioners , while still being mathematically elegant and theoretically sound adaboost 's behavior in practice , and in particular the test error behavior , has puzzled many eminent researchers for over a decade it seems to defy our general intuition in machine learning regarding the fundamental trade off between model complexity and generalization performance in this paper , we establish the convergence of optimal adaboost , a term coined by rudin , daubechies , and schapire in 2004 we prove the convergence , with the number of rounds , of the classifier itself , its generalization error , and its resulting margins for fixed data sets , under certain reasonable conditions more generally , we prove that the time per round average of almost any function of the example weights converges our approach is to frame adaboost as a dynamical system , to provide sufficient conditions for the existence of an invariant measure , and to employ tools from ergodic theory unlike previous work , we do not assume adaboost cycles actually , we present empirical evidence against it on real world datasets our main theoretical results hold under a weaker condition we show sufficient empirical evidence that optimal adaboost always met the condition on every real world dataset we tried our results formally ground future convergence rate analyses , and may even provide opportunities for slight algorithmic modifications to optimize the generalization ability of adaboost classifiers , thus reducing a practitioner 's burden of deciding how long to run the algorithm
the simple knowledge organization system \( skos \) is a standard model for controlled vocabularies on the web however , skos vocabularies often differ in terms of quality , which reduces their applicability across system boundaries here we investigate how we can support taxonomists in improving skos vocabularies by pointing out quality issues that go beyond the integrity constraints defined in the skos specification we identified potential quantifiable quality issues and formalized them into computable quality checking functions that can find affected resources in a given skos vocabulary we implemented these functions in the qskos quality assessment tool , analyzed 15 existing vocabularies , and found possible quality issues in all of them
njexl is a derivative of apache jexl project 1 the motivation for njexl are domain specific languages \( dsl \) like verilog 2 , vhdl 3 , spice 4 dsl for soft ware testing is not a new idea , many commercial tools like silk suite 5 use them , while selenese , the dsl for selenium ide 6 is open source njexl is a functionally motivated , embeddable , turing complete 7 8 language it 's philosophy is to expose the existing java echo system in a declarative fashion for the purpose of software validation 9 by design njexl script size is meagre compared to python or even to scala for validation problems author 's firm heavily uses njexl for software automation purpose which drastically reduced the effort for automatic verification for a year now
online matching has received significant attention over the last 15 years due to its close connection to internet advertising as the seminal work of karp , vazirani , and vazirani has an optimal \( 1 1 e \) competitive ratio in the standard adversarial online model , much effort has gone into developing useful online models that incorporate some stochasticity in the arrival process one such popular model is the known i i d model where different customer types arrive online from a known distribution we develop algorithms with improved competitive ratios for some basic variants of this model with integral arrival rates , including \( a \) the case of general weighted edges , where we improve the best known ratio of 0 667 due to haeupler , mirrokni and zadimoghaddam to 0 705 and \( b \) the vertex weighted case , where we improve the 0 7250 ratio of jaillet and lu to 0 7299 we also consider two extensions , one is known i i d with non integral arrival rate and stochastic rewards the other is known i i d b matching with non integral arrival rate and stochastic rewards we present a simple non adaptive algorithm which works well simultaneously on the two extensions
this paper investigates domain generalization how to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains \? we propose domain invariant component analysis \( dica \) , a kernel based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains , whilst preserving the functional relationship between input and output variables a learning theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains , motivating the proposed algorithm experimental results on synthetic and real world datasets demonstrate that dica successfully learns invariant features and improves classifier performance in practice
parity games are games that are played on directed graphs whose vertices are labeled by natural numbers , called priorities the players push a token along the edges of the digraph the winner is determined by the parity of the greatest priority occurring infinitely often in this infinite play a motivation for studying parity games comes from the area of formal verification of systems by model checking deciding the winner in a parity game is polynomial time equivalent to the model checking problem of the modal mu calculus another strong motivation lies in the fact that the exact complexity of solving parity games is a long standing open problem , the currently best known algorithm being subexponential it is known that the problem is in the complexity classes up and coup in this paper we identify restricted classes of digraphs where the problem is solvable in polynomial time , following an approach from structural graph theory we consider three standard graph operations the join of two graphs , repeated pasting along vertices , and the addition of a vertex given a class c of digraphs on which we can solve parity games in polynomial time , we show that the same holds for the class obtained from c by applying once any of these three operations to its elements these results provide , in particular , polynomial time algorithms for parity games whose underlying graph is an orientation of a complete graph , a complete bipartite graph , a block graph , or a block cactus graph these are classes where the problem was not known to be efficiently solvable previous results concerning restricted classes of parity games which are solvable in polynomial time include classes of bounded tree width , bounded dag width , and bounded clique width we also prove that recognising the winning regions of a parity game is not easier than computing them from scratch
in imitation learning , an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems such algorithms are therefore not directly applicable to large , high dimensional environments , and their performance can significantly degrade if the planning problems are not solved to optimality under the apprenticeship learning formalism , we develop alternative model free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function , based on sample trajectories from the expert our approach , based on policy gradients , scales to large continuous environments with guaranteed convergence to local minima
in this paper , we study the symmetry of polar codes on symmetric binary input discrete memoryless channels \( b dmc \) the symmetry property of polar codes is originally pointed out in arikan 's work for general b dmc channels with the symmetry , the output vector y 1 n \( n be the block length \) can be divided into equivalence classes in terms of their transition probabilities in this paper , we present a new frame of analysis on the symmetry of polar codes for b dmc channels theorems are provided to characterize the symmetries among the received vectors
we consider the distributed source coding problem in which correlated data picked up by scattered sensors has to be encoded separately and transmitted to a common receiver , subject to a rate distortion constraint although near tooptimal solutions based on turbo and ldpc codes exist for this problem , in most cases the proposed techniques do not scale to networks of hundreds of sensors we present a scalable solution based on the following key elements \( a \) distortion optimized index assignments for low complexity distributed quantization , \( b \) source optimized hierarchical clustering based on the kullback leibler distance and \( c \) sum product decoding on specific factor graphs exploiting the correlation of the data
in modern society , the flow of information has become the lifeblood of commerce and social interaction this movement of data supports most aspects of the united states economy in particular , as well as , serving as the vehicle upon which governmental agencies react to social conditions in addition , it is understood that the continuance of efficient and reliable data communications during times of national or regional disaster remains a priority in the united states the coordination of emergency response and area revitalization rehabilitation efforts between local , state , and federal emergency response is increasingly necessary as agencies strive to work more seamlessly between the affected organizations additionally , international support is often made available to react to such adverse conditions as wildfire suppression scenarios and therefore require the efficient management of workforce and associated logistics support it is through the examination of the issues related to un tethered data transmission during infrastructure contingencies that responders may best tailor a unified approach to the rapid recovery after disasters occur
with the proliferation of social networking sites \( snss \) such as facebook and google , investigating the impact of snss on our lives has become an important research area in recent years though sns usage plays a key role in connecting people with friends and families from distant places , snss also bring concern for families we focus on imbalance sns usage , i e , an individual remains busy in using snss when her family member is expecting to spend time with her more specifically , we investigate the cause and pattern of imbalance sns usage and how the emotion of family members may become affected , if they use snss in an imbalanced way in a regular manner this paper is the first attempt to identify the relationship between an individual 's imbalance sns usage and the emotion of her family member in the context of a developing country
we consider the reachability problem for timed automata a standard solution to this problem involves computing a search tree whose nodes are abstractions of zones these abstractions preserve underlying simulation relations on the state space of the automaton for both effectiveness and efficiency reasons , they are parametrized by the maximal lower and upper bounds \( lu bounds \) occurring in the guards of the automaton we consider the alu abstraction defined by behrmann et al since this abstraction can potentially yield non convex sets , it has not been used in implementations we prove that alu abstraction is the biggest abstraction with respect to lu bounds that is sound and complete for reachability we also provide an efficient technique to use the alu abstraction to solve the reachability problem
the ability to monitor the progress of students academic performance is a critical issue to the academic community of higher learning a system for analyzing students results based on cluster analysis and uses standard statistical algorithms to arrange their scores data according to the level of their performance is described in this paper , we also implemented k mean clustering algorithm for analyzing students result data the model was combined with the deterministic model to analyze the students results of a private institution in nigeria which is a good benchmark to monitor the progression of academic performance of students in higher institution for the purpose of making an effective decision by the academic planners
we analyze the minimax regret of the adversarial bandit convex optimization problem focusing on the one dimensional case , we prove that the minimax regret is widetilde theta \( sqrt t \) and partially resolve a decade old open problem our analysis is non constructive , as we do not present a concrete algorithm that attains this regret rate instead , we use minimax duality to reduce the problem to a bayesian setting , where the convex loss functions are drawn from a worst case distribution , and then we solve the bayesian version of the problem with a variant of thompson sampling our analysis features a novel use of convexity , formalized as a local to global property of convex functions , that may be of independent interest
top k queries allow end users to focus on the most important \( top k \) answers amongst those which satisfy the query in traditional databases , a user defined score function assigns a score value to each tuple and a top k query returns k tuples with the highest score in uncertain database , top k answer depends not only on the scores but also on the membership probabilities of tuples several top k definitions covering different aspects of score probability interplay have been proposed in recent past cite r10 , r4 , r2 , r8 most of the existing work in this research field is focused on developing efficient algorithms for answering top k queries on static uncertain data any change \( insertion , deletion of a tuple or change in membership probability , score of a tuple \) in underlying data forces re computation of query answers such re computations are not practical considering the dynamic nature of data in many applications in this paper , we propose a fully dynamic data structure that uses ranking function prf e \( alpha \) proposed by li et al cite r8 under the generally adopted model of x relations cite r11 prf e can effectively approximate various other top k definitions on uncertain data based on the value of parameter alpha an x relation consists of a number of x tuples , where x tuple is a set of mutually exclusive tuples \( up to a constant number \) called alternatives each x tuple in a relation randomly instantiates into one tuple from its alternatives for an uncertain relation with n tuples , our structure can answer top k queries in o \( k log n \) time , handles an update in o \( log n \) time and takes o \( n \) space finally , we evaluate practical efficiency of our structure on both synthetic and real data
high sidelobe level is a major disadvantage of the capon beamforming to suppress the sidelobe , this paper introduces a mainlobe to sidelobe power ratio constraint to the capon beamforming it minimizes the sidelobe power while keeping the mainlobe power constant simulations show that the obtained beamformer outperforms the capon beamformer
we present qhipster , the quantum high performance software testing environment qhipster is a distributed high performance implementation of a quantum simulator on a classical computer , that can simulate general single qubit gates and two qubit controlled gates we perform a number of single and multi node optimizations , including vectorization , multi threading , cache blocking , as well as overlapping computation with communication using the tacc stampede supercomputer , we simulate quantum circuits \( quantum software \) of up to 40 qubits we carry out a detailed performance analysis to show that our simulator achieves both high performance and high hardware efficiency , limited only by the sustainable memory and network bandwidth of the machine
relaying is an effective technique to achieve reliable wireless connectivity in harsh communication environment however , most of the existing relaying schemes are based on relays with fixed locations , or emph static relaying in this paper , we consider a novel emph mobile relaying technique , where the relay nodes are assumed to be capable of moving at high speed compared to static relaying , mobile relaying offers a new degree of freedom for performance enhancement via careful relay trajectory design we study the throughput maximization problem in mobile relaying systems by optimizing the source relay transmit power along with the relay trajectory , subject to practical mobility constraints \( on the relay speed and initial final relay locations \) , as well as the emph information causality constraint at the relay owing to its decode store and forward \( dsf \) strategy it is shown that for fixed relay trajectory , the throughput optimal source relay power allocations over time follow a staircase water filling \( wf \) structure , with emph non increasing and emph non decreasing water levels at the source and relay , respectively on the other hand , with given power allocations , the throughput can be further improved by optimizing the relay trajectory via successive convex optimization an iterative algorithm is thus proposed to optimize the power allocations and relay trajectory alternately furthermore , for the special case with free initial and final relay locations , the jointly optimal power allocation and relay trajectory are derived numerical results show that by optimizing the trajectory of the relay and power allocations adaptive to its induced channel variation , mobile relaying is able to achieve significant throughput gains over the conventional static relaying
this paper presents the first in depth analysis on the energy efficiency of lt codes with non coherent m ary frequency shift keying \( nc mfsk \) , known as green modulation 1 , in a proactive wireless sensor network \( wsn \) over rayleigh flat fading channels with path loss we describe the proactive system model according to a pre determined time based process utilized in practical sensor nodes the present analysis is based on realistic parameters including the effect of channel bandwidth used in the ieee 802 15 4 standard , and the active mode duration a comprehensive analysis , supported by some simulation studies on the probability mass function of the lt code rate and coding gain , shows that among uncoded nc mfsk and various classical channel coding schemes , the optimized lt coded nc mfsk is the most energy efficient scheme for distance d greater than the pre determined threshold level d t , where the optimization is performed over coding and modulation parameters in addition , although uncoded nc mfsk outperforms coded schemes for d d t , the energy gap between lt coded and uncoded nc mfsk is negligible for d d t compared to the other coded schemes these results come from the flexibility of the lt code to adjust its rate to suit instantaneous channel conditions , and suggest that lt codes are beneficial in practical low power wsns with dynamic position sensor nodes
in this paper , two q learning \( ql \) methods are proposed and their convergence theories are established for addressing the model free optimal control problem of general nonlinear continuous time systems by introducing the q function for continuous time systems , policy iteration based ql \( piql \) and value iteration based ql \( viql \) algorithms are proposed for learning the optimal control policy from real system data rather than using mathematical system model it is proved that both piql and viql methods generate a nonincreasing q function sequence , which converges to the optimal q function for implementation of the ql algorithms , the method of weighted residuals is applied to derived the parameters update rule the developed piql and viql algorithms are essentially off policy reinforcement learning approachs , where the system data can be collected arbitrary and thus the exploration ability is increased with the data collected from the real system , the ql methods learn the optimal control policy offline , and then the convergent control policy will be employed to real system the effectiveness of the developed ql algorithms are verified through computer simulation
we explore the resiliency and robustness of systems while viewing them as complex , multi genre networks the term complex , multi genre networks refers to networks that combine several distinct genres networks of physical resources , communication networks , information networks , and social and cognitive networks we show that this perspective is fruitful and adds to our understanding of fundamental challenges and tradeoffs in robustness and resiliency , as well as potential solutions to the challenges study of systems as multi genre networks is relatively uncommon instead , it is customary in research and engineering literature to focus on a view of a network comprised of homogeneous elements , \( e g , a network of communication devices , or a network of social beings \) yet , most if not all real world networks are multi genre it is hard to find any real system of a significant complexity that does not include a combination of interconnected physical elements , communication devices and channels , data collections , and human users forming an integrated , inter dependent whole most approaches to improving resiliency and robustness involve compromises , and the key challenge is to find a favorable compromise such compromises involve reducing or managing the complexity of the network coupling , rigidity and dependency we discuss several of these compromises , e g , performance vs resiliency resiliency to one type of disruption vs resiliency to another disruption type and complexity vs resiliency
this paper describes a linear time algorithm that finds the longest stretch in a sequence of real numbers \( ``scores'' \) in which the sum exceeds an input parameter the algorithm also solves the problem of finding the longest interval in which the average of the scores is above a fixed threshold the problem originates from molecular sequence analysis for instance , the algorithm can be employed to identify long gc rich regions in dna sequences the algorithm can also be used to trim low quality ends of shotgun sequences in a preprocessing step of whole genome assembly
interval temporal logics \( itls \) are logics for reasoning about temporal statements expressed over intervals , i e , periods of time the most famous itl studied so far is halpern and shoham 's hs , which is the logic of the thirteen allen 's interval relations unfortunately , hs and most of its fragments have an undecidable satisfiability problem this discouraged the research in this area until recently , when a number non trivial decidable itls have been discovered this paper is a contribution towards the complete classification of all different fragments of hs we consider different combinations of the interval relations begins , after , later and their inverses abar , bbar , and lbar we know from previous works that the combination abbbarabar is decidable only when finite domains are considered \( and undecidable elsewhere \) , and that abbbar is decidable over the natural numbers we extend these results by showing that decidability of abbar can be further extended to capture the language abbbarlbar , which lays in between abbar and abbbarabar , and that turns out to be maximal w r t decidability over strongly discrete linear orders \( e g finite orders , the naturals , the integers \) we also prove that the proposed decision procedure is optimal with respect to the complexity class
we seek decision rules for prediction time cost reduction , where complete data is available for training , but during prediction time , each feature can only be acquired for an additional cost we propose a novel random forest algorithm to minimize prediction error for a user specified it average feature acquisition budget while random forests yield strong generalization performance , they do not explicitly account for feature costs and furthermore require low correlation among trees , which amplifies costs our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost weighted impurity splits theoretically , we establish near optimal acquisition cost guarantees for our algorithm empirically , on a number of benchmark datasets we demonstrate superior accuracy cost curves against state of the art prediction time algorithms
inverse reinforcement learning \( irl \) describes the problem of learning an unknown reward function of a markov decision process \( mdp \) from observed behavior of an agent since the agent 's behavior originates in its policy and mdp policies depend on both the stochastic system dynamics as well as the reward function , the solution of the inverse problem is significantly influenced by both current irl approaches assume that if the transition model is unknown , additional samples from the system 's dynamics are accessible , or the observed behavior provides enough samples of the system 's dynamics to solve the inverse problem accurately these assumptions are often not satisfied to overcome this , we present a gradient based irl approach that simultaneously estimates the system 's dynamics by solving the combined optimization problem , our approach takes into account the bias of the demonstrations , which stems from the generating policy the evaluation on a synthetic mdp and a transfer learning task shows improvements regarding the sample efficiency as well as the accuracy of the estimated reward functions and transition models
areas where artificial intelligence \( ai \) related fields are finding their applications are increasing day by day , moving from core areas of computer science they are finding their applications in various other domains in recent times machine learning i e a sub domain of ai has been widely used in order to assist medical experts and doctors in the prediction , diagnosis and prognosis of various diseases and other medical disorders in this manuscript the authors applied various machine learning algorithms to a problem in the domain of medical diagnosis and analyzed their efficiency in predicting the results the problem selected for the study is the diagnosis of the chronic kidney disease the dataset used for the study consists of 400 instances and 24 attributes the authors evaluated 12 classification techniques by applying them to the chronic kidney disease data in order to calculate efficiency , results of the prediction by candidate methods were compared with the actual medical results of the subject the various metrics used for performance evaluation are predictive accuracy , precision , sensitivity and specificity the results indicate that decision tree performed best with nearly the accuracy of 98 6 , sensitivity of 0 9720 , precision of 1 and specificity of 1
there is a conception that boltzmann gibbs statistics cannot yield the long tail distribution this is the justification for the intensive research of nonextensive entropies \( i e tsallis entropy and others \) here the error that caused this misconception is explained and it is shown that a long tail distribution exists in equilibrium thermodynamics for more than a century
we explore optimization methods for planning the placement , sizing and operations of flexible alternating current transmission system \( facts \) devices installed into the grid to relieve congestion created by load growth or fluctuations of intermittent renewable generation we limit our selection of facts devices to those that can be represented by modification of the inductance of the transmission lines our master optimization problem minimizes the l 1 norm of the facts associated inductance correction subject to constraints enforcing that no line of the system exceeds its thermal limit we develop off line heuristics that reduce this non convex optimization to a succession of linear programs \( lp \) where at each step the constraints are linearized analytically around the current operating point the algorithm is accelerated further with a version of the cutting plane method greatly reducing the number of active constraints during the optimization , while checking feasibility of the non active constraints post factum this hybrid algorithm solves a typical single contingency problem over the mathpower polish grid model \( 3299 lines and 2746 nodes \) in 40 seconds per iteration on a standard laptop a speed up that allows the sizing and placement of a family of facts devices to correct a large set of anticipated contingencies from testing of multiple examples , we observe that our algorithm finds feasible solutions that are always sparse , i e , facts devices are placed on only a few lines the optimal facts are not always placed on the originally congested lines , however typically the correction \( s \) is made at line \( s \) positioned in a relative proximity of the overload line \( s \)
in this work we relate the deterministic complexity of factoring polynomials \( over finite fields \) to certain combinatorial objects we call m schemes we extend the known conditional deterministic subexponential time polynomial factoring algorithm for finite fields to get an underlying m scheme we demonstrate how the properties of m schemes relate to improvements in the deterministic complexity of factoring polynomials over finite fields assuming the generalized riemann hypothesis \( grh \) in particular , we give the first deterministic polynomial time algorithm \( assuming grh \) to find a nontrivial factor of a polynomial of prime degree n where \( n 1 \) is a smooth number
we prove that a random word of length n over a k ary fixed alphabet contains , on expectation , theta \( sqrt n \) distinct palindromic factors we study this number of factors , e \( n , k \) , in detail , showing that the limit lim n to infty e \( n , k \) sqrt n does not exist for any k ge2 , liminf n to infty e \( n , k \) sqrt n theta \( 1 \) , and limsup n to infty e \( n , k \) sqrt n theta \( sqrt k \) such a complicated behaviour stems from the asymmetry between the palindromes of even and odd length we show that a similar , but much simpler , result on the expected number of squares in random words holds we also provide some experimental data on the number of palindromic factors in random words
we call a cnf formula linear if any two clauses have at most one variable in common let linear k sat be the problem of deciding whether a given linear k cnf formula is satisfiable here , a k cnf formula is a cnf formula in which every clause has size exactly k it was known that for k 3 , linear k sat is np complete if and only if an unsatisfiable linear k cnf formula exists , and that they do exist for k 4 we prove that unsatisfiable linear k cnf formulas exist for every k let f \( k \) be the minimum number of clauses in an unsatisfiable linear k cnf formula we show that f \( k \) is omega \( k2 k \) and o \( 4 k k 4 \) , i e , minimum size unsatisfiable linear k cnf formulas are significantly larger than minimum size unsatisfiable k cnf formulas finally , we prove that , surprisingly , linear k cnf formulas do not allow for a larger fraction of clauses to be satisfied than general k cnf formulas
in this paper we analyze a specific class of rateless codes , called lt codes with unequal recovery time these codes provide the option of prioritizing different segments of the transmitted data over other the result is that segments are decoded in stages during the rateless transmission , where higher prioritized segments are decoded at lower overhead our analysis focuses on quantifying the expected amount of received symbols , which are redundant already upon arrival , i e all input symbols contained in the received symbols have already been decoded this analysis gives novel insights into the probabilistic mechanisms of lt codes with unequal recovery time , which has not yet been available in the literature we show that while these rateless codes successfully provide the unequal recovery time , they do so at a significant price in terms of redundancy in the lower prioritized segments we propose and analyze a modification where a single intermediate feedback is transmitted , when the first segment is decoded in a code with two segments our analysis shows that this modification provides a dramatic improvement on the decoding performance of the lower prioritized segment
this paper presents the results of a laboratory study involving mailvelope , a modern pgp client that integrates tightly with existing webmail providers in our study , we brought in pairs of participants and had them attempt to use mailvelope to communicate with each other our results shown that more than a decade and a half after textit why johnny ca n't encrypt , modern pgp tools are still unusable for the masses we finish with a discussion of pain points encountered using mailvelope , and discuss what might be done to address them in future pgp systems
we extend the study by ornstein and weiss on the asymptotic behavior of the normalized version of recurrence times and establish the large deviation property for a certain class of mixing processes further , an estimator for entropy based on recurrence times is proposed for which large deviation behavior is proved for stationary and ergodic sources satisfying similar mixing conditions
this project presents the analysis , design , implementation and results of reconstruction xicocotitlan tollan through augmented reality , which will release information about the toltec culture supplemented by presenting an overview of the main premises of the xicocotitlan tollan city supported dimensional models based on the augmented reality technique showing the user a virtual representation of buildings in tollan
in this paper we study the state complexity of catenation combined with symmetric difference first , an upper bound is computed using some combinatoric tools then , this bound is shown to be tight by giving a witness for it moreover , we relate this work with the study of state complexity for two other combinations catenation with union and catenation with intersection and we extract a unified approach which allows to obtain the state complexity of any combination involving catenation and a binary boolean operation
for the general problem of minimizing a convex function over a compact convex domain , we will investigate a simple iterative approximation algorithm based on the method by frank wolfe 1956 , that does not need projection steps in order to stay inside the optimization domain instead of a projection step , the linearized problem defined by a current subgradient is solved , which gives a step direction that will naturally stay in the domain our framework generalizes the sparse greedy algorithm of frank wolfe and its primal dual analysis by clarkson 2010 \( and the low rank sdp approach by hazan 2008 \) to arbitrary convex domains we give a convergence proof guaranteeing epsilon small duality gap after o \( 1 epsilon \) iterations the method allows us to understand the sparsity of approximate solutions for any l1 regularized convex optimization problem \( and for optimization over the simplex \) , expressed as a function of the approximation quality we obtain matching upper and lower bounds of theta \( 1 epsilon \) for the sparsity for l1 problems the same bounds apply to low rank semidefinite optimization with bounded trace , showing that rank o \( 1 epsilon \) is best possible here as well as another application , we obtain sparse matrices of o \( 1 epsilon \) non zero entries as epsilon approximate solutions when optimizing any convex function over a class of diagonally dominant symmetric matrices we show that our proposed first order method also applies to nuclear norm and max norm matrix optimization problems for nuclear norm regularized optimization , such as matrix completion and low rank recovery , we demonstrate the practical efficiency and scalability of our algorithm for large matrix problems , as e g the netflix dataset for general convex optimization over bounded matrix max norm , our algorithm is the first with a convergence guarantee , to the best of our knowledge
we describe a method for predicting a classification of an object given classifications of the objects in the training set , assuming that the pairs object classification are generated by an i i d process from a continuous probability distribution our method is a modification of vapnik 's support vector machine its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction we also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine some experimental results are presented , and possible extensions of the algorithms are discussed
our work studies the enumeration and random generation of unlabeled combinatorial classes of unrooted graphs while the technique of vertex pointing provides a straightforward procedure for analyzing a labeled class of unrooted graphs by first studying its rooted counterpart , the existence of nontrivial symmetries in the unlabeled case causes this technique to break down instead , techniques such as the dissymmetry theorem \( of otter \) and cycle pointing \( of bodirsky et al \) have emerged in the unlabeled case , with the former providing an enumeration of the class and the latter providing both an enumeration and an unbiased sampler in this work , we extend the power of the dissymmetry theorem by showing that it in fact provides a boltzmann sampler for the class in question we then present an exposition of the cycle pointing technique , with a focus on the enumeration and random generation of the underlying unpointed class finally , we apply cycle pointing to enumerate and implement samplers for the classes of distance hereditary graphs and three leaf power graphs
a substantial volume of research has been devoted to studies of community structure in networks , but communities are not the only possible form of large scale network structure here we describe a broad extension of community structure that encompasses traditional communities but includes a wide range of generalized structural patterns as well we describe a principled method for detecting this generalized structure in empirical network data and demonstrate with real world examples how it can be used to learn new things about the shape and meaning of networks
the derivation of the maximum entropy distribution of particles in boxes yields two kinds of distributions a bell like distribution and a long tail distribution the first one is obtained when the ratio between particles and boxes is low , and the second one when the ratio is high the obtained long tail distribution yields correctly the empirical zipf law , pareto 's 20 80 rule and benford 's law therefore , it is concluded that the long tail and the bell like distributions are outcomes of the tendency of statistical systems to maximize entropy
though high redundancy rate of a tight frame can improve performance in applications , as the dimension increases , it also makes the computational cost skyrocket and the storage of frame coefficients increase exponentially this seriously restricts the usefulness of such tight frames for problems in moderately high dimensions such as video processing in dimension three inspired by the directional tensor product complex tight framelets tp ctf m with m ge 3 in 14 , 18 and their impressive performance for image processing in 18 , 30 in this paper we introduce a directional tensor product complex tight framelet tp ctf ! 6 \( called reduced tp ctf 6 \) with low redundancy such tp ctf 6 ! is a particular example of tight framelet filter banks with mixed sampling factors the tp ctf ! 6 in d dimensions not only offers good directionality but also has the low redundancy rate frac 3 d 1 2 d 1 \( e g , the redundancy rates are 2 , 2 mathord frac 2 3 , 3 mathord frac 5 7 , 5 mathord frac 1 3 and 7 mathord frac 25 31 for dimension d 1 , , 5 , respectively \) moreover , our numerical experiments on image video denoising and inpainting show that the performance using our proposed tp ctf ! 6 is often comparable or sometimes better than several state of the art frame based methods which have much higher redundancy rates than that of tpctf ! 6
the border algorithm and the ipred algorithm find the hasse diagrams of fca lattices we show that they can be generalized to arbitrary lattices in the case of ipred , this requires the identification of a join semilattice homomorphism into a distributive lattice
we analyze a class of estimators based on convex relaxation for solving high dimensional matrix decomposition problems the observations are noisy realizations of a linear transformation mathfrak x of the sum of an approximately \) low rank matrix theta star with a second matrix gamma star endowed with a complementary form of low dimensional structure this set up includes many statistical models of interest , including factor analysis , multi task regression , and robust covariance estimation we derive a general theorem that bounds the frobenius norm error for an estimate of the pair \( theta star , gamma star \) obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer our results utilize a spikiness condition that is related to but milder than singular vector incoherence we specialize our general result to two cases that have been studied in past work low rank plus an entrywise sparse matrix , and low rank plus a columnwise sparse matrix for both models , our theory yields non asymptotic frobenius error bounds for both deterministic and stochastic noise matrices , and applies to matrices theta star that can be exactly or approximately low rank , and matrices gamma star that can be exactly or approximately sparse moreover , for the case of stochastic noise matrices and the identity observation operator , we establish matching lower bounds on the minimax error the sharpness of our predictions is confirmed by numerical simulations
given a binary nonlinear code , we provide a deterministic algorithm to compute its weight and distance distribution , and in particular its minimum weight and its minimum distance , which takes advantage of fast fourier techniques this algorithm 's performance is similar to that of best known algorithms for the average case , while it is especially efficient for codes with low information rate we provide complexity estimates for several cases of interest
a common approach to data analysis involves understanding and manipulating succinct representations of data in earlier work , we put forward a succinct representation system for relational data called factorised databases and reported on the main memory query engine fdb for select project join queries on such databases in this paper , we extend fdb to support a larger class of practical queries with aggregates and ordering this requires novel optimisation and evaluation techniques we show how factorisation coupled with partial aggregation can effectively reduce the number of operations needed for query evaluation we also show how factorisations of query results can support enumeration of tuples in desired orders as efficiently as listing them from the unfactorised , sorted results we experimentally observe that fdb can outperform off the shelf relational engines by orders of magnitude
in addition to the theoretical value of challenging optimal control problmes , recent progress in autonomous vehicles mandates further research in optimal motion planning for wheeled vehicles since current numerical optimal control techniques suffer from either the curse of dimens ionality , e g the hamilton jacobi bellman equation , or the curse of complexity , e g pseudospectral optimal control and max plus methods , analytical characterization of geodesics for wheeled vehicles becomes important not only from a theoretical point of view but also from a prac tical one such an analytical characterization provides a fast motion planning algorithm that can be used in robust feedback loops in this work , we use the pontryagin maximum principle to characterize extremal trajectories , i e candidate geodesics , for a car like robot with one trailer we use time as the distance function in spite of partial progress , this problem has remained open in the past two decades besides straight motion and turn with maximum allowed curvature , we identify planar elastica as the third piece of motion that occurs along our extr emals we give a detailed characterization of such curves , a special case of which , called emph merging curve , connects maximum curvature turns to straight line segments the structure of extremals in our case is revealed through analytical integration of the system and adjoint equations
information hierarchies are organizational structures that often used to organize and present large and complex information as well as provide a mechanism for effective human navigation fortunately , many statistical and computational models exist that automatically generate hierarchies however , the existing approaches do not consider linkages in information em networks that are increasingly common in real world scenarios current approaches also tend to present topics as an abstract probably distribution over words , etc rather than as tangible nodes from the original network furthermore , the statistical techniques present in many previous works are not yet capable of processing data at web scale in this paper we present the hierarchical document topic model \( hdtm \) , which uses a distributed vertex programming process to calculate a nonparametric bayesian generative model experiments on three medium size data sets and the entire wikipedia dataset show that hdtm can infer accurate hierarchies even over large information networks
in this paper , we propose a radio frequency \( rf \) lens embedded massive multiple input multiple output \( mimo \) system and evaluate the performance of limited feedback by utilizing a technique for generating a suitable codebook for the system we fabricate an rf lens that operates at a 77 ghz \( mmwave \) band experimental results show a proper value of amplitude gain and an appropriate focusing property in addition , using a simple numerical method beam propagation method \( bpm \) we estimate the power profile of the rf lens and verify its accordance with experimental results numerical results confirm that the proposed system shows significant performance enhancement over a conventional massive mimo system without an rf lens
we study quantum protocols among two distrustful parties by adopting a rather strict definition of correctness guaranteeing that honest players obtain their correct outcomes only we can show that every strictly correct quantum protocol implementing a non trivial classical primitive necessarily leaks information to a dishonest player this extends known impossibility results to all non trivial primitives we provide a framework for quantifying this leakage and argue that leakage is a good measure for the privacy provided to the players by a given protocol our framework also covers the case where the two players are helped by a trusted third party we show that despite the help of a trusted third party , the players cannot amplify the cryptographic power of any primitive all our results hold even against quantum honest but curious adversaries who honestly follow the protocol but purify their actions and apply a different measurement at the end of the protocol as concrete examples , we establish lower bounds on the leakage of standard universal two party primitives such as oblivious transfer
we study the problem of deinterleaving a set of finite memory \( markov \) processes over disjoint finite alphabets , which have been randomly interleaved by a finite memory switch the deinterleaver has access to a sample of the resulting interleaved process , but no knowledge of the number or structure of the component markov processes , or of the switch we study conditions for uniqueness of the interleaved representation of a process , showing that certain switch configurations , as well as memoryless component processes , can cause ambiguities in the representation we show that a deinterleaving scheme based on minimizing a penalized maximum likelihood cost function is strongly consistent , in the sense of reconstructing , almost surely as the observed sequence length tends to infinity , a set of component and switch markov processes compatible with the original interleaved process furthermore , under certain conditions on the structure of the switch \( including the special case of a memoryless switch \) , we show that the scheme recovers emph all possible interleaved representations of the original process experimental results are presented demonstrating that the proposed scheme performs well in practice , even for relatively short input samples
opinions are very important in the life of human beings these opinions helped the humans to carry out the decisions as the impact of the web is increasing day by day , web documents can be seen as a new source of opinion for human beings web contains a huge amount of information generated by the users through blogs , forum entries , and social networking websites and so on to analyze this large amount of information it is required to develop a method that automatically classifies the information available on the web this domain is called sentiment analysis and opinion mining opinion mining or sentiment analysis is a natural language processing task that mine information from various text forms such as reviews , news , and blogs and classify them on the basis of their polarity as positive , negative or neutral but , from the last few years , enormous increase has been seen in hindi language on the web research in opinion mining mostly carried out in english language but it is very important to perform the opinion mining in hindi language also as large amount of information in hindi is also available on the web this paper gives an overview of the work that has been done hindi language
in this paper a new method for information hiding in club music is introduced the method called stegibiza is based on using the music tempo as a carrier the tempo is modulated by hidden messages with a 3 value coding scheme , which is an adoption of morse code for stegibiza the evaluation of the system was performed for several music samples \( with and without stegibiza enabled \) on a selected group of testers who had a music background finally , for the worst case scenario , none of them could identify any differences in the audio with a 1 margin of changed tempo
network simulation is the most useful and common methodology used to evaluate different network to pologies without real world implementation network simulators are widely used by the research community to evaluate new theories and hypotheses there are a number of network simulators , for instance , ns 2 , ns 3 , omnet , swan , opnet , jist , and glomosim etc therefore , the selection of a network simulator for evaluating research work is a crucial task for researchers the main focus of this paper is to compare the state of the art , open source network simulators based on the following parameters cpu utilization , memory usage , computational time , and scalability by simulating a manet routing protocol , to identify an optimal network simulator for the research community
we study the computational problem of checking whether a quantified conjunctive query \( a first order sentence built using only conjunction as boolean connective \) is true in a finite poset \( a reflexive , antisymmetric , and transitive directed graph \) we prove that the problem is already np hard on a certain fixed poset , and investigate structural properties of posets yielding fixed parameter tractability when the problem is parameterized by the query our main algorithmic result is that model checking quantified conjunctive queries on posets of bounded width is fixed parameter tractable \( the width of a poset is the maximum size of a subset of pairwise incomparable elements \) we complement our algorithmic result by complexity results with respect to classes of finite posets in a hierarchy of natural poset invariants , establishing its tightness in this sense
in this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns as described in grosz , joshi and weinstein \( 1986 \) , the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing , retaining and shifting we propose an extension to these states which handles some additional cases of multiple ambiguous pronouns the algorithm has been implemented in an hpsg natural language system which serves as the interface to a database query application
in this work we study encoding decoding schemes for the transmission of a discrete time analog gaussian source over a gaussian wiretap channel the intended receiver is assumed to have a certain minimum signal to noise ratio \( snr \) and the eavesdropper is assumed to have a strictly lower snr compared to the intended receiver for a fixed information leakage to the eavesdropper , we are interested in minimizing the distortion in source reconstruction at the intended receiver , and we propose joint source channel coding \( jscc \) schemes for this setup for a fixed information leakage to the eavesdropper , we also show that the schemes considered give a graceful degradation of distortion with snr under snr mismatch , i e , when the actual channel snr is observed to be different from the design snr
multi variant execution environments \( mvees \) are a promising technique to protect software against memory corruption attacks they transparently execute multiple , diversified variants \( often referred to as replicae \) of the software receiving the same inputs by enforcing and monitoring the lock step execution of the replicae 's system calls , and by deploying diversity techniques that prevent an attacker from simultaneously compromising multiple replicae , mvees can block attacks before they succeed existing mvees cannot handle non trivial multi threaded programs because their undeterministic behavior introduces benign system call inconsistencies in the replicae , which trigger false positive detections and deadlocks in the mvees this paper for the first time extends the generality of mvees to protect multi threaded software by means of secure and efficient synchronization replication agents on the parsec 2 1 parallel benchmarks running with four worker threads , our prototype mvee incurs a run time overhead of only 1 32x
we prove that every weighted graph contains a spanning tree subgraph of average stretch o \( \( log n log log n \) 2 \) moreover , we show how to construct such a tree in time o \( m log 2 n \)
we propose an automated technique for inferring software contracts from programs that are written in a non trivial fragment of c , called kernelc , that supports pointer based structures and heap manipulation starting from the semantic definition of kernelc in the k framework , we enrich the symbolic execution facilities recently provided by k with novel capabilities for assertion synthesis that are based on abstract subsumption roughly speaking , we define an abstract symbolic technique that explains the execution of a \( modifier \) c function by using other \( observer \) routines in the same program we implemented our technique in the automated tool kindspec 2 0 , which generates logical axioms that express pre and post condition assertions by defining the precise input output behaviour of the c routines
the paper addresses a new class of combinatorial problems which consist in restructuring of solutions \( as structures \) in combinatorial optimization two main features of the restructuring process are examined \( i \) a cost of the restructuring , \( ii \) a closeness to a goal solution this problem corresponds to redesign \( improvement , upgrade \) of modular systems or solutions the restructuring approach is described and illustrated for the following combinatorial optimization problems knapsack problem , multiple choice problem , assignment problem , spanning tree problems examples illustrate the restructuring processes
the goal of this project is to \( i \) accumulate annotated informal formal mathematical corpora suitable for training semi automated translation between informal and formal mathematics by statistical machine translation methods , \( ii \) to develop such methods oriented at the formalization task , and in particular \( iii \) to combine such methods with learning assisted automated reasoning that will serve as a strong semantic component we describe these ideas , the initial set of corpora , and some initial experiments done over them
given an argumentation framework af , we introduce a mapping function that constructs a disjunctive logic program p , such that the preferred extensions of af correspond to the stable models of p , after intersecting each stable model with the relevant atoms the given mapping function is of polynomial size w r t af in particular , we identify that there is a direct relationship between the minimal models of a propositional formula and the preferred extensions of an argumentation framework by working on representing the defeated arguments then we show how to infer the preferred extensions of an argumentation framework by using unsat algorithms and disjunctive stable model solvers the relevance of this result is that we define a direct relationship between one of the most satisfactory argumentation semantics and one of the most successful approach of non monotonic reasoning i e , logic programming with the stable model semantics
we investigate joint network and channel coding schemes for networks when relay nodes are not capable of performing channel coding operations rather , channel encoding is performed at the source node while channel decoding is done only at the destination nodes we examine three different decoding strategies independent network then channel decoding , serial network and channel decoding , and joint network and channel decoding furthermore , we describe how to implement such joint network and channel decoding using iteratively decodable error correction codes using simple networks as a model , we derive achievable rate regions and use simulations to demonstrate the effectiveness of the three decoders
we present an analysis of user conversations in on line social media and their evolution over time we propose a dynamic model that accurately predicts the growth dynamics and structural properties of conversation threads the model successfully reconciles the differing observations that have been reported in existing studies by separating artificial factors from user behaviors , we show that there are actually underlying rules in common for on line conversations in different social media websites results of our model are supported by empirical measurements throughout a number of different social media websites
verification of temporal logic properties plays a crucial role in proving the desired behaviors of hybrid systems in this paper , we propose an interval method for verifying the properties described by a bounded linear temporal logic we relax the problem to allow outputting an inconclusive result when verification process cannot succeed with a prescribed precision , and present an efficient and rigorous monitoring algorithm that demonstrates that the problem is decidable this algorithm performs a forward simulation of a hybrid automaton , detects a set of time intervals in which the atomic propositions hold , and validates the property by propagating the time intervals a continuous state at a certain time computed in each step is enclosed by an interval vector that is proven to contain a unique solution in the experiments , we show that the proposed method provides a useful tool for formal analysis of nonlinear and complex hybrid systems
multiscale entropy \( mse \) was proposed to overcome the deficiencies of conventional entropy methods when quantifying the complexity of time series however , mse is not fast enough for real time applications because of the use of sample entropy \( sampen \) here , we introduce multiscale disen \( mde \) as a very fast and powerful method to quantify the complexity of signals mde is based on our recently developed disen , which has a computation cost of o \( n \) , compared with o \( n 2 \) for sampen we also propose the refined composite mde \( rcmde \) to improve the reliability and stability of mde , especially for short signals to evaluate the proposed methods , we employ white gaussian and 1 f noises to discriminate the concept of irregularity from complexity moreover , we use logistic map to understand the ability of mde and rcmde to detect changes from periodicity to non periodic nonlinearity we also employ two publicly available datasets to illustrate the application of our method to real world signals the results show a similar behaviour of rcmse and rcmde , although the latter is significantly faster the results also show more stability and reliability of rcmde than mde , especially for short signals thus , this novel measure is expected to be useful for the analysis of real signals thanks to its ability to distinguish different types of dynamics
the most advanced implementation of adaptive constraint processing with constraint handling rules \( chr \) allows the application of intelligent search strategies to solve constraint satisfaction problems \( csp \) this presentation compares an improved version of conflict directed backjumping and two variants of dynamic backtracking with respect to chronological backtracking on some of the aim instances which are a benchmark set of random 3 sat problems a chr implementation of a boolean constraint solver combined with these different search strategies in java is thus being compared with a chr implementation of the same boolean constraint solver combined with chronological backtracking in sicstus prolog this comparison shows that the addition of ``intelligence'' to the search process may reduce the number of search steps dramatically furthermore , the runtime of their java implementations is in most cases faster than the implementations of chronological backtracking more specifically , conflict directed backjumping is even faster than the sicstus prolog implementation of chronological backtracking , although our java implementation of chr lacks the optimisations made in the sicstus prolog system to appear in theory and practice of logic programming \( tplp \)
this paper focuses on analyzing the finite time convergence of a nonlinear consensus algorithm for multi agent networks with unknown inherent nonlinear dynamics due to the existence of the unknown inherent nonlinear dynamics , the stability analysis and the finite time convergence analysis of the closed loop system under the proposed consensus algorithm are more challenging than those under the well studied consensus algorithms for known linear systems for this purpose , we propose a novel stability tool based on a generalized comparison lemma with the aid of the novel stability tool , it is shown that the proposed nonlinear consensus algorithm can guarantee finite time convergence if the directed switching interaction graph has a directed spanning tree at each time interval specifically , the finite time convergence is shown by comparing the closed loop system under the proposed consensus algorithm with some well designed closed loop system whose stability properties are easier to obtain moreover , the stability and the finite time convergence of the closed loop system using the proposed consensus algorithm under a \( general \) directed switching interaction graph can even be guaranteed by the stability and the finite time convergence of some special well designed nonlinear closed loop system under some special directed switching interaction graph , where each agent has at most one neighbor whose state is either the maximum of those states that are smaller than its own state or the minimum of those states that are larger than its own state this provides a stimulating example for the potential applications of the proposed novel stability tool in the stability analysis of linear nonlinear closed loop systems by making use of known results in linear nonlinear systems for illustration of the theoretical result , we provide a simulation example
orc is a theory of orchestration of services that allows structured programming of distributed and timed computations several formal semantics have been proposed for orc , including a rewriting logic semantics developed by the authors orc also has a fully fledged implementation in java with functional programming features however , as with descriptions of most distributed languages , there exists a fairly substantial gap between orc 's formal semantics and its implementation , in that \( i \) programs in orc are not easily deployable in a distributed implementation just by using orc 's formal semantics , and \( ii \) they are not readily formally analyzable at the level of a distributed orc implementation in this work , we overcome problems \( i \) and \( ii \) for orc specifically , we describe an implementation technique based on rewriting logic and maude that narrows this gap considerably the enabling feature of this technique is maude 's support for external objects through tcp sockets we describe how sockets are used to implement orc site calls and returns , and to provide real time timing information to orc expressions and sites we then show how orc programs in the resulting distributed implementation can be formally analyzed at a reasonable level of abstraction by defining an abstract model of time and the socket communication infrastructure , and discuss the assumptions under which the analysis can be deemed correct finally , the distributed implementation and the formal analysis methodology are illustrated with a case study
existing work on online learning for decision making takes the information available as a given and focuses solely on choosing best actions given this information instead , in this paper , the decision maker needs to simultaneously learn both what decisions to make and what source \( s \) of contextual information to gather data from in order to inform its decisions such that its reward is maximized we propose algorithms that obtain costly source \( s \) of contextual information over time , while simultaneously learning what actions to take based on the contextual information revealed by the selected source \( s \) we prove that our algorithms achieve regret that is logarithmic in time we demonstrate the performance of our algorithms using a medical dataset the proposed algorithm can be applied in many applications including clinical decision assist systems for medical diagnosis , recommender systems , actionable intelligence etc , where observing the complete information in every instance or consulting all the available sources to gather intelligence before making decisions is costly
in recent years because of the advances in computer vision research , free hand gestures have been explored as means of human computer interaction \( hci \) together with improved speech processing technology it is an important step toward natural multimodal hci however , inclusion of non predefined continuous gestures into a multimodal framework is a challenging problem in this paper , we propose a structured approach for studying patterns of multimodal language in the context of a 2d display control we consider systematic analysis of gestures from observable kinematical primitives to their semantics as pertinent to a linguistic structure proposed semantic classification of co verbal gestures distinguishes six categories based on their spatio temporal deixis we discuss evolution of a computational framework for gesture and speech integration which was used to develop an interactive testbed \( imap \) the testbed enabled elicitation of adequate , non sequential , multimodal patterns in a narrative mode of hci conducted user studies illustrate significance of accounting for the temporal alignment of gesture and speech parts in semantic mapping furthermore , co occurrence analysis of gesture speech production suggests syntactic organization of gestures at the lexical level
this paper presents a practical writing reading scheme in nonvolatile memories , called balanced modulation , for minimizing the asymmetric component of errors the main idea is to encode data using a balanced error correcting code when reading information from a block , it adjusts the reading threshold such that the resulting word is also balanced or approximately balanced balanced modulation has suboptimal performance for any cell level distribution and it can be easily implemented in the current systems of nonvolatile memories furthermore , we studied the construction of balanced error correcting codes , in particular , balanced ldpc codes it has very efficient encoding and decoding algorithms , and it is more efficient than prior construction of balanced error correcting codes
we introduce the shifted legendre symbol problem and some variants along with efficient quantum algorithms to solve them the problems and their algorithms are different from previous work on quantum computation in that they do not appear to fit into the framework of the hidden subgroup problem the classical complexity of the problem is unknown despite the various results on the irregularity of legendre sequences
this paper presents networks of influence diagrams \( nid \) , a compact , natural and highly expressive language for reasoning about agents beliefs and decision making processes nids are graphical structures in which agents mental models are represented as nodes in a network a mental model for an agent may itself use descriptions of the mental models of other agents nids are demonstrated by examples , showing how they can be used to describe conflicting and cyclic belief structures , and certain forms of bounded rationality in an opponent modeling domain , nids were able to outperform other computational agents whose strategies were not known in advance nids are equivalent in representation to bayesian games but they are more compact and structured than this formalism in particular , the equilibrium definition for nids makes an explicit distinction between agents optimal strategies , and how they actually behave in reality
we consider the setting of ontological database access , where an abox is given in form of a relational database d and where a boolean conjunctive query q has to be evaluated against d modulo a tbox t formulated in dl lite or linear datalog it is well known that \( t , q \) can be rewritten into an equivalent nonrecursive datalog program p that can be directly evaluated over d however , for linear datalog \? or for dl lite versions that allow for role inclusion , the rewriting methods described so far result in a nonrecursive datalog program p of size exponential in the joint size of t and q this gives rise to the interesting question of whether such a rewriting necessarily needs to be of exponential size in this paper we show that it is actually possible to translate \( t , q \) into a polynomially sized equivalent nonrecursive datalog program p
planning problems are hard , motion planning , for example , ispspace hard such problems are even more difficult in the presence of uncertainty although , markov decision processes \( mdps \) provide a formal framework for such problems , finding solutions to high dimensional continuous mdps is usually difficult , especially when the actions and time measurements are continuous fortunately , problem specific knowledge allows us to design controllers that are good locally , though having no global guarantees we propose a method of nonparametrically combining local controllers to obtain globally good solutions we apply this formulation to two types of problems motion planning \( stochastic shortest path \) and discounted mdps for motion planning , we argue that usual mdp optimality criterion \( expected cost \) may not be practically relevant wepropose an alternative finding the minimum cost path , subject to the constraint that the robot must reach the goal withhigh probability for this problem , we prove that a polynomial number of samples is sufficient to obtain a high probability path for discounted mdps , we propose a formulation that explicitly deals with model uncertainty , i e , the problem introduced when transition probabilities are not known exactly we formulate the problem as a robust linear program which directly incorporates this type of uncertainty
axelrod 's model describes the dissemination of a set of cultural traits in a society constituted by individual agents in a social context , nevertheless , individual choices toward a specific attitude are also at the basis of the formation of communities , groups and parties the membership in a group changes completely the behavior of single agents who start acting according to a social identity groups act and interact among them as single entities , but still conserve an internal dynamics we show that , under certain conditions of social dynamics , the introduction of group dynamics in a cultural dissemination process avoids the flattening of the culture into a single entity and preserves the multiplicity of cultural attitudes we also considered diffusion processes on this dynamical background , showing the conditions under which information as well as innovation can spread through the population in a scenario where the groups' choices determine the social structure
the new yorker publishes a weekly captionless cartoon more than 5 , 000 readers submit captions for it the editors select three of them and ask the readers to pick the funniest one we describe an experiment that compares a dozen automatic methods for selecting the funniest caption we show that negative sentiment , human centeredness , and lexical centrality most strongly match the funniest captions , followed by positive sentiment these results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal \( vision text \) systems as part of this work , a large set of cartoons and captions is being made available to the community
the exploration of social conversations for addressing patient 's needs is an important analytical task in which many scholarly publications are contributing to fill the knowledge gap in this area the main difficulty remains the inability to turn such contributions into pragmatic processes the pharmaceutical industry can leverage in order to generate insight from social media data , which can be considered as one of the most challenging source of information available today due to its sheer volume and noise this study is based on the work by scott spangler and jeffrey kreulen and applies it to identify structure in social media through the extraction of a topical taxonomy able to capture the latent knowledge in social conversations in health related sites the mechanism for automatically identifying and generating a taxonomy from social conversations is developed and pressured tested using public data from media sites focused on the needs of cancer patients and their families moreover , a novel method for generating the category 's label and the determination of an optimal number of categories is presented which extends scott and jeffrey 's research in a meaningful way we assume the reader is familiar with taxonomies , what they are and how they are used
we show np hardness of the minimum latency scheduling \( mls \) problem under the physical model of wireless networking in this model a transmission is received successfully if the signal to interference plus noise ratio \( sinr \) , is above a given threshold in the minimum latency scheduling problem , the goal is to assign a time slot and power level to each transmission , so that all the messages are received successfully , and the number of distinct times slots is minimized despite its seeming simplicity and several previous hardness results for various settings of the minimum latency scheduling problem , it has remained an open question whether or not the minimum latency scheduling problem is np hard , when the nodes are placed in the euclidean plane and arbitrary power levels can be chosen for the transmissions we resolve this open question for all path loss exponent values alpha geq 3
new families of fisher information and entropy power inequalities for sums of independent random variables are presented these inequalities relate the information in the sum of n independent random variables to the information contained in sums over subsets of the random variables , for an arbitrary collection of subsets as a consequence , a simple proof of the monotonicity of information in central limit theorems is obtained , both in the setting of i i d summands as well as in the more general setting of independent summands with variance standardized sums
intuitionistic dependence logic was introduced by abramsky and vaananen \( 2009 \) as a variant of dependence logic under a general construction of hodges' \( trump \) team semantics it was proven that there is a translation from intuitionistic dependence logic sentences into second order logic sentences in this paper , we prove that the other direction is also true , therefore intuitionistic dependence logic is equivalent to second order logic on the level of sentences
in this paper , we proposed an alternating projection based algorithm to solve a class of distributed min max convex optimization problems we firstly transform this minmax problem into the problem of searching for the minimum distance between some hyper plane and the intersection of the epigraphs of convex functions the bregman 's alternating method is employed in our algorithm to achieve the distance by iteratively projecting onto the hyper plane and the intersection the projection onto the intersection is obtained by cyclic dykstra 's projection method we further apply our algorithm to the minimum time multi agent consensus problem the attainable states set for the agent can be transformed into the epigraph of some convex functions , and the search for time optimal state for consensus satisfies the min max problem formulation finally , the numerous simulation proves the validity of our algorithm
a large number of streaming applications use reliable transport protocols such as tcp to deliver content over the internet however , head of line blocking due to packet loss recovery can often result in unwanted behavior and poor application layer performance transport layer coding can help mitigate this issue by helping to recover from lost packets without waiting for retransmissions we consider the use of an on line network code that inserts coded packets at strategic locations within the underlying packet stream if retransmissions are necessary , additional coding packets are transmitted to ensure the receiver 's ability to decode an analysis of this scheme is provided that helps determine both the expected in order packet delivery delay and its variance numerical results are then used to determine when and how many coded packets should be inserted into the packet stream , in addition to determining the trade offs between reducing the in order delay and the achievable rate the analytical results are finally compared with experimental results to provide insight into how to minimize the delay of existing transport layer protocols
space time block codes from square complex orthogonal designs \( scod \) have been extensively studied and most of the existing scods contain large number of zero the zeros in the designs result in high peak to average power ratio \( papr \) and also impose a severe constraint on hardware implementation of the code when turning off some of the transmitting antennas whenever a zero is transmitted recently , rate 1 2 scods with no zero entry have been reported for 8 transmit antennas in this paper , scods with no zero entry for 2 a transmit antennas whenever a 1 is a power of 2 , are constructed which includes the 8 transmit antennas case as a special case more generally , for arbitrary values of a , explicit construction of 2 a times 2 a rate frac a 1 2 a scods with the ratio of number of zero entries to the total number of entries equal to 1 frac a 1 2 a 2 lfloor log 2 \( frac 2 a a 1 \) rfloor is reported , whereas for standard known constructions , the ratio is 1 frac a 1 2 a the codes presented do not result in increased signaling complexity simulation results show that the codes constructed in this paper outperform the codes using the standard construction under peak power constraint while performing the same under average power constraint
as we grow highly dependent on data for making predictions , we translate these predictions into models that help us make informed decisions but what are the guarantees we have \? can we optimize decisions on models learned from data and be guaranteed that we achieve desirable outcomes \? in this paper we formalize this question through a novel model called optimization from samples \( ops \) in the ops model , we are given sampled values of a function drawn from some distribution and our objective is to optimize the function under some constraint our main interest is in the following question are functions that are learnable \( from samples \) and approximable \( given oracle access to the function \) also optimizable from samples \? we show that there are classes of submodular functions which have desirable approximation and learnability guarantees and for which no reasonable approximation for optimizing from samples is achievable in particular , our main result shows that even for maximization of coverage functions under a cardinality constraint k , there exists a hypothesis class of functions that cannot be approximated within a factor of n 1 4 epsilon \( for any constant epsilon 0 \) of the optimal solution , from samples drawn from the uniform distribution over all sets of size at most k in the general case of monotone submodular functions , we show an n 1 3 epsilon lower bound and an almost matching tilde omega \( n 1 3 \) optimization from samples algorithm additive and unit demand functions can be optimized from samples to within arbitrarily good precision finally , we also consider a corresponding notion of additive approximation for continuous optimization from samples , and show near optimal hardness for concave maximization and convex minimization
link prediction in large knowledge graphs has received a lot of attention recently because of its importance for inferring missing relations and for completing and improving noisily extracted knowledge graphs over the years a number of machine learning researchers have presented various models for predicting the presence of missing relations in a knowledge base although all the previous methods are presented with empirical results that show high performance on select datasets , there is almost no previous work on understanding the connection between properties of a knowledge base and the performance of a model in this paper we analyze the rescal method and prove that it can not encode asymmetric transitive relations in knowledge bases
the theory of the tight span , a cell complex that can be associated to every metric d , offers a unifying view on existing approaches for analyzing distance data , in particular for decomposing a metric d into a sum of simpler metrics as well as for representing it by certain specific edge weighted graphs , often referred to as realizations of d many of these approaches involve the explicit or implicit computation of the so called cutpoints of \( the tight span of \) d , such as the algorithm for computing the building blocks of optimal realizations of d recently presented by a hertz and s varone the main result of this paper is an algorithm for computing the set of these cutpoints for a metric d on a finite set with n elements in o \( n 3 \) time as a direct consequence , this improves the run time of the aforementioned o \( n 6 \) algorithm by hertz and varone by ``three orders of magnitude''
we offer a new understanding of some aspects of practical sat solvers that are based on dpll with unit clause propagation , clause learning , and restarts we do so by analyzing a concrete algorithm which we claim is faithful to what practical solvers do in particular , before making any new decision or restart , the solver repeatedly applies the unit resolution rule until saturation , and leaves no component to the mercy of non determinism except for some internal randomness we prove the perhaps surprising fact that , although the solver is not explicitly designed for it , with high probability it ends up behaving as width k resolution after no more than o \( n 2k 2 \) conflicts and restarts , where n is the number of variables in other words , width k resolution can be thought of as o \( n 2k 2 \) restarts of the unit resolution rule with learning
metrics on the space of sets of trajectories are important for scientists in the field of computer vision , machine learning , robotics and general artificial intelligence yet existing notions of closeness are either mathematically inconsistent or of limited practical use in this paper we outline the limitations in the existing mathematically consistent metrics , which are based on schuhmacher et al 2008 , and the inconsistencies in the heuristic notions of closeness used in practice , whose main ideas are common to the clear mot measures widely used in computer vision in two steps we then propose a new intuitive metric between sets of trajectories and address these problems first we explain a natural solution that leads to a metric that is hard to compute then we modify this formulation to obtain a metric that is easy to compute and keeps all the good properties of the previous metric in particular , our notion of closeness is the first that has the following three properties it can be quickly computed , it incorporates confusion of trajectories' identity in an optimal way and it is a metric in the mathematical sense
when two or more self interested agents put their plans to execution in the same environment , conflicts may arise as a consequence , for instance , of a common utilization of resources in this case , an agent can postpone the execution of a particular action , if this punctually solves the conflict , or it can resort to execute a different plan if the agent 's payoff significantly diminishes due to the action deferral in this paper , we present a game theoretic approach to non cooperative planning that helps predict before execution what plan schedules agents will adopt so that the set of strategies of all agents constitute a nash equilibrium we perform some experiments and discuss the solutions obtained with our game theoretical approach , analyzing how the conflicts between the plans determine the strategic behavior of the agents
in accessibility tests for digital preservation , over time we experience drifts of localized and labelled content in statistical models of evolving semantics represented as a vector field this articulates the need to detect , measure , interpret and model outcomes of knowledge dynamics to this end we employ a high performance machine learning algorithm for the training of extremely large emergent self organizing maps for exploratory data analysis the working hypothesis we present here is that the dynamics of semantic drifts can be modeled on a relaxed version of newtonian mechanics called social mechanics by using term distances as a measure of semantic relatedness vs their pagerank values indicating social importance and applied as variable `term mass' , gravitation as a metaphor to express changes in the semantic content of a vector field lends a new perspective for experimentation from `term gravitation' over time , one can compute its generating potential whose fluctuations manifest modifications in pairwise term similarity vs social importance , thereby updating osgood 's semantic differential the dataset examined is the public catalog metadata of tate galleries , london
multi output dependence \( mod \) learning is a generalization of standard classification problems that allows for multiple outputs that are dependent on each other a primary issue that arises in the context of mod learning is that for any given input pattern there can be multiple correct output patterns this changes the learning task from function approximation to relation approximation previous algorithms do not consider this problem , and thus cannot be readily applied to mod problems to perform mod learning , we introduce the hierarchical multi output nearest neighbor model \( hmonn \) that employs a basic learning model for each output and a modified nearest neighbor approach to refine the initial results
using agent directed simulations , we investigate fluctuations in the collective emotional states on a chat network where agents interchange messages with a fixed number of moderators and emotional bot to design a realistic chat system , the interaction rules and some statistical parameters , as well as the agent 's attributes , are inferred from the empirical chat channel texttt ubuntu in the simulations , the bot 's emotion is fixed the moderators tune the level of its activity by passing a fraction epsilon of messages to the bot at epsilon gtrsim 0 , the collective emotional state matching the bot 's emotion polarity gradually arises the average growth rate of the dominant emotional charge serves as an order parameter due to self organizing effects , the collective dynamics is more explosive when positive emotions arise by positive bot than the onset of negative emotions in the presence of negative bot at the same epsilon furthermore , when the emotions matching the bot 's emotion polarity are spread over the system , the underlying fractal processes exhibit higher persistence and stronger clustering of events than the processes spreading of emotion polarity opposite to the bot 's emotion on the other hand , the relaxation dynamics is controlled by the external noise the related nonextensive parameter , estimated from the statistics of returns , is virtually independent of the bot 's activity level and emotion contents
we study how the behavior of viral spreading processes is influenced by local structural properties of the network over which they propagate for a wide variety of spreading processes , the largest eigenvalue of the adjacency matrix of the network plays a key role on their global dynamical behavior for many real world large scale networks , it is unfeasible to exactly retrieve the complete network structure to compute its largest eigenvalue instead , one usually have access to myopic , egocentric views of the network structure , also called egonets in this paper , we propose a mathematical framework , based on algebraic graph theory and convex optimization , to study how local structural properties of the network constrain the interval of possible values in which the largest eigenvalue must lie based on this framework , we present a computationally efficient approach to find this interval from a collection of egonets our numerical simulations show that , for several social and communication networks , local structural properties of the network strongly constrain the location of the largest eigenvalue and the resulting spreading dynamics from a practical point of view , our results can be used to dictate immunization strategies to tame the spreading of a virus , or to design network topologies that facilitate the spreading of information virally
in this paper we analyze the effect of the calibration period , or lack of , on self interference channel estimation in the digital domain of in band full duplex radio transceivers in particular , we consider a scenario where the channel estimation must be performed without a separate calibration period , which means that the received signal of interest will act as an additional noise source from the estimation perspective we will explicitly analyze its effect , and quantify the increase in the parameter estimation variance , or sample size , if similar accuracy for the channel estimate is to be achieved as with a separate calibration period in addition , we will analyze how the calibration period , or its absence , affects the overall achievable rates full waveform simulations are then used to determine the validity of the obtained results , as well as provide numerical results regarding the achievable rates it is shown that , even though a substantial increase in the parameter sample size is required if there is no calibration period , the achievable rates are still comparable for the two scenarios
in this paper , we investigate the performance of cooperative spectrum sensing \( css \) with multiple antenna nodes over composite and generalized fading channels we model the probability density function \( pdf \) of the signal to noise ratio \( snr \) using the mixture gamma \( mg \) distribution we then derive a generalized closed form expression for the probability of energy detection , which can be used efficiently for generalized multipath as well as composite \( multipath and shadowing \) fading channels the composite effect of fading and shadowing scenarios in css is mitigated by applying an optimal fusion rule that minimizes the total error rate \( ter \) , where the optimal number of nodes is derived under the bayesian criterion , assuming erroneous feedback channels for imperfect feedback channels , we demonstrate the existence of a ter floor as the number of antennas of the cr nodes increases accordingly , we derive the optimal rule for the number of antennas that minimizes the ter numerical and monte carlo simulations are presented to corroborate the analytical results and to provide illustrative performance comparisons between different composite fading channels
regenerating codes \( rcs \) can significantly reduce the repair bandwidth of distributed storage networks initially , the analysis of rcs was based on the assumption that during the repair process , the newcomer does not distinguish \( among all surviving nodes \) which nodes to access , i e , the newcomer is oblivious to the set of helpers being used such a scheme is termed the blind helper selection \( bhs \) scheme nonetheless , it is intuitive in practice that the newcomer should choose to access only those good helpers in this two part paper , a new characterization of the effect of choosing the helper nodes in terms of the storage bandwidth tradeoff is given specifically , the answer to the following fundamental question is provided under what condition does proactively choosing the helper nodes improve the storage bandwidth tradeoff \? through a graph based analysis , this part i paper answers this question by providing a necessary and sufficient condition under which optimally choosing good helpers strictly improves the storage bandwidth tradeoff a low complexity helper selection solution , termed the family helper selection \( fhs \) scheme , is proposed and the corresponding storage repair bandwidth curve is characterized this part i paper also proves that under some design parameters , the fhs scheme is indeed optimal among all helper selection schemes in the part ii paper , an explicit construction of an exact repair code is proposed that achieves the minimum bandwidth regenerating \( mbr \) point of the fhs scheme the new exact repair code can be viewed as a generalization of the existing fractional repetition code
this paper considers an example of object oriented programming \( oop \) leading to subtle errors that break separation of interface and implementations a comprehensive principle that guards against such errors is undecidable the paper introduces a set of mechanically verifiable rules that prevent these insidious problems although the rules seem restrictive , they are powerful and expressive , as we show on several familiar examples the rules contradict both the spirit and the letter of the oop the present examples as well as available theoretical and experimental results pose a question if oop is conducive to software development at all
in this paper , we investigate mutual information as a cost function for clustering , and show in which cases hard , i e , deterministic , clusters are optimal using convexity properties of mutual information , we show that certain formulations of the information bottleneck problem are solved by hard clusters similarly , hard clusters are optimal for the information theoretic co clustering problem that deals with simultaneous clustering of two dependent data sets if both data sets have to be clustered using the same cluster assignment , hard clusters are not optimal in general we point at interesting and practically relevant special cases of this so called pairwise clustering problem , for which we can either prove or have evidence that hard clusters are optimal our results thus show that one can relax the otherwise combinatorial hard clustering problem to a real valued optimization problem with the same global optimum
the rank problem in succinct data structures asks to preprocess an array a 1 n of bits into a data structure using as close to n bits as possible , and answer queries of the form rank \( k \) sum i 1 k a i the problem has been intensely studied , and features as a subroutine in a majority of succinct data structures we show that in the cell probe model with w bit cells , if rank takes t time , the space of the data structure must be at least n n w o \( t \) bits this redundancy query trade off is essentially optimal , matching our upper bound from focs'08
big data research is currently split on whether and to what extent twitter can be characterised as an informational or social network we contribute to this line of inquiry through an investigation of digital humanities scholars' uses and gratifications of twitter we conducted a thematic analysis of 25 semistructured interview transcripts to learn about these scholars' professional use of twitter our findings show that twitter is considered a critical tool for informal communication within dh invisible colleges , functioning at varying levels as both an informational network \( learning to 'twitter' and maintaining awareness \) and a social network \( imagining audiences and engaging other digital humanists \) we find that twitter follow relationships reflect common academic interests and are closely tied to scholars' preexisting social ties and conference or event co attendance the concept of the invisible college continues to be relevant but requires revisiting the invisible college formed on twitter is messy , consisting of overlapping social contexts \( professional , personal , and public \) , scholars with different habits of engagement , and both formal and informal ties our research illustrates the value of using multiple methods to explore the complex questions arising from big data studies and points toward future research that could implement big data techniques on a small scale , focusing on sub topics or emerging fields , to expose the nature of scholars' invisible colleges made visible on twitter
robust optimization is a common framework in optimization under uncertainty when the problem parameters are not known , but it is rather known that the parameters belong to some given uncertainty set in the robust optimization framework the problem solved is a min max problem where a solution is judged according to its performance on the worst possible realization of the parameters in many cases , a straightforward solution of the robust optimization problem of a certain type requires solving an optimization problem of a more complicated type , and in some cases even np hard for example , solving a robust conic quadratic program , such as those arising in robust svm , ellipsoidal uncertainty leads in general to a semidefinite program in this paper we develop a method for approximately solving a robust optimization problem using tools from online convex optimization , where in every stage a standard \( non robust \) optimization program is solved our algorithms find an approximate robust solution using a number of calls to an oracle that solves the original \( non robust \) problem that is inversely proportional to the square of the target accuracy
we present an algorithm of clustering of many dimensional objects , where only the distances between objects are used centers of classes are found with the aid of neuron like procedure with lateral inhibition the result of clustering does not depend on starting conditions our algorithm makes it possible to give an idea about classes that really exist in the empirical data the results of computer simulations are presented
in this report , we try to improve the performance of existing approaches for search operations in multi robot context we propose three novel algorithms that are using a triangular grid pattern , i e , robots certainly go through the vertices of a triangular grid during the search procedure the main advantage of using a triangular grid pattern is that it is asymptotically optimal in terms of the minimum number of robots required for the complete coverage of an arbitrary bounded area we use a new topological map which is made and shared by robots during the search operation we consider an area that is unknown to the robots a priori with an arbitrary shape , containing some obstacles unlike many current heuristic algorithms , we give mathematically proofs of convergence of the algorithms the computer simulation results for the proposed algorithms are presented using a simulator of real robots and environment we evaluate the performance of the algorithms via experiments with real robots we compare the performance of our own algorithms with three existing algorithms from other researchers the results demonstrate the merits of our proposed solution a further study on formation building with obstacle avoidance for a team of mobile robots is presented in this report we propose a decentralized formation building with obstacle avoidance algorithm for a group of mobile robots to move in a defined geometric configuration furthermore , we consider a more complicated formation problem with a group of anonymous robots these robots are not aware of their position in the final configuration and need to reach a consensus during the formation process we propose a randomized algorithm for the anonymous robots that achieves the convergence to a desired configuration with probability 1 we also propose a novel obstacle avoidance rule , used in the formation building algorithm
the development of discursive knowledge presumes the communication of meaning as analytically different from the communication of information knowledge can then be considered as a meaning which makes a difference whereas the communication of information is studied in the information sciences and scientometrics , the communication of meaning has been central to luhmann 's attempts to make the theory of autopoiesis relevant for sociology analytical techniques such as semantic maps and the simulation of anticipatory systems enable us to operationalize the distinctions which luhmann proposed as relevant to the elaboration of husserl 's horizons of meaning in empirical research interactions among communications , the organization of meaning in instantiations , and the self organization of interhuman communication in terms of symbolically generalized media such as truth , love , and power horizons of meaning , however , remain uncertain orders of expectations , and one should caution against reification from the meta biological perspective of systems theory
we present a scalable and efficient iterative solver for high order hybridized discontinuous galerkin \( hdg \) discretizations of hyperbolic partial differential equations it is an interplay between domain decomposition methods and hdg discretizations in particular , the method is a fixed point approach that requires only independent element by element local solves in each iteration as such , it is well suited for current and future computing systems with massive concurrencies we rigorously show that the proposed method is exponentially convergent in the number of iterations for transport and linearized shallow water equations furthermore , the convergence is independent of the solution order various 2d and 3d numerical results for steady and time dependent problems are presented to verify our theoretical findings
device to device \( d2d \) communication integrated into cellular networks is a means to take advantage of the proximity of devices and allow for reusing cellular resources and thereby to increase the user bitrates and the system capacity however , when d2d \( in the 3rd generation partnership project also called long term evolution \( lte \) direct \) communication in cellular spectrum is supported , there is a need to revisit and modify the existing radio resource management \( rrm \) and power control \( pc \) techniques to realize the potential of the proximity and reuse gains and to limit the interference at the cellular layer in this paper , we examine the performance of the flexible lte pc tool box and benchmark it against a utility optimal iterative scheme we find that the open loop pc scheme of lte performs well for cellular users both in terms of the used transmit power levels and the achieved signal to interference and noise ratio \( sinr \) distribution however , the performance of the d2d users as well as the overall system throughput can be boosted by the utility optimal scheme , because the utility maximizing scheme takes better advantage of both the proximity and the reuse gains therefore , in this paper we propose a hybrid pc scheme , in which cellular users employ the open loop path compensation method of lte , while d2d users use the utility optimizing distributed pc scheme in order to protect the cellular layer , the hybrid scheme allows for limiting the interference caused by the d2d layer at the cost of having a small impact on the performance of the d2d layer to ensure feasibility , we limit the number of iterations to a practically feasible level we make the point that the hybrid scheme is not only near optimal , but it also allows for a distributed implementation for the d2d users , while preserving the lte pc scheme for the cellular users
authentication is the act of confirming the truth of an attribute of a datum or entity this might involve confirming the identity of a person , tracing the origins of an artefact , ensuring that a product is what it 's packaging and labelling claims to be , or assuring that a computer program is a trusted one the authentication of information can pose special problems \( especially man in the middle attacks \) , and is often wrapped up with authenticating identity password authentication using brain state in a box is presented in this paper here in this paper we discuss brain state in a box scheme for textual and graphical passwords which will be converted in to probabilistic values password we observe how to get password authentication probabilistic values for text and graphical image this study proposes the use of a brain state in a box technique for password authentication in comparison to existing layered neural network techniques , the proposed method provides better accuracy and quicker response time to registration and password changes
this paper describes a scalable algorithm for solving multiobjective decomposable problems by combining the hierarchical bayesian optimization algorithm \( hboa \) with the nondominated sorting genetic algorithm \( nsga ii \) and clustering in the objective space it is first argued that for good scalability , clustering or some other form of niching in the objective space is necessary and the size of each niche should be approximately equal multiobjective hboa \( mohboa \) is then described that combines hboa , nsga ii and clustering in the objective space the algorithm mohboa differs from the multiobjective variants of boa and hboa proposed in the past by including clustering in the objective space and allocating an approximately equally sized portion of the population to each cluster the algorithm mohboa is shown to scale up well on a number of problems on which standard multiobjective evolutionary algorithms perform poorly
two isometry groups of combinatorial codes are described the group of automorphisms and the group of monomial automorphisms , which is the group of those automorphisms that extend to monomial maps unlike the case of classical linear codes , where these groups are the same , it is shown that for combinatorial codes the groups can be arbitrary different particularly , there exist codes with the full automorphism group and the trivial monomial automorphism group in the paper the two groups are characterized and codes with predefined isometry groups are constructed
let mathcal r k \( n \) be the number of representations of an integer n as the sum of a prime and a k th power define e k \( x \) n le x , n in i k , n text not a sum of a prime and a k th power hardy and littlewood conjectured that for k 2 and k 3 , e k \( x \) ll k 1 in this note we present an alternative approach grounded in the theory of diophantine equations towards a proof of the conjecture for all k ge 2
the generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents previous results with aspect models have been promising , but hindered by the computational difficulty of carrying out inference and learning this paper demonstrates that the simple variational methods of blei et al \( 2001 \) can lead to inaccurate inferences and biased learning for the generative aspect model we develop an alternative approach that leads to higher accuracy at comparable cost an extension of expectation propagation is used for inference and then embedded in an em algorithm for learning experimental results are presented for both synthetic and real data sets
image segmentation and image restoration are two important topics in image processing with great achievements in this paper , we propose a new multiphase segmentation model by combining image restoration and image segmentation models utilizing image restoration aspects , the proposed segmentation model can effectively and robustly tackle high noisy images , blurry images , images with missing pixels , and vector valued images in particular , one of the most important segmentation models , the piecewise constant mumford shah model , can be extended easily in this way to segment gray and vector valued images corrupted for example by noise , blur or missing pixels after coupling a new data fidelity term which comes from image restoration topics it can be solved efficiently using the alternating minimization algorithm , and we prove the convergence of this algorithm with three variables under mild condition experiments on many synthetic and real world images demonstrate that our method gives better segmentation results in comparison to others state of the art segmentation models especially for blurry images and images with missing pixels values
a poset game is a two player game played over a partially ordered set \( poset \) in which the players alternate choosing an element of the poset , removing it and all elements greater than it the first player unable to select an element of the poset loses polynomial time algorithms exist for certain restricted classes of poset games , such as the game of nim however , until recently the complexity of arbitrary finite poset games was only known to exist somewhere between nc 1 and pspace we resolve this discrepancy by showing that deciding the winner of an arbitrary finite poset game is pspace complete to this end , we give an explicit reduction from node kayles , a pspace complete game in which players vie to chose an independent set in a graph
this paper proposes a hilbert space embedding for dirichlet process mixture models via a stick breaking construction of sethuraman although bayesian nonparametrics offers a powerful approach to construct a prior that avoids the need to specify the model size complexity explicitly , an exact inference is often intractable on the other hand , frequentist approaches such as kernel machines , which suffer from the model selection comparison problems , often benefit from efficient learning algorithms this paper discusses the possibility to combine the best of both worlds by using the dirichlet process mixture model as a case study
this paper presents a case study of formalizing a normalization proof for leivant 's predicative system f using the equations package leivant 's predicative system f is a stratified version of system f , where type quantification is annotated with kinds representing universe levels a weaker variant of this system was studied by stump eades , employing the hereditary substitution method to show normalization we improve on this result by showing normalization for leivant 's original system using hereditary substitutions and a novel multiset ordering on types our development is done in the coq proof assistant using the equations package , which provides an interface to define dependently typed programs with well founded recursion and full dependent pattern matching equations allows us to define explicitly the hereditary substitution function , clarifying its algorithmic behavior in presence of term and type substitutions from this definition , consistency can easily be derived the algorithmic nature of our development is crucial to reflect languages with type quantification , enlarging the class of languages on which reflection methods can be used in the proof assistant
in this paper , we present an analysis of the outage probability for fixed gain amplify and forward \( af \) multihop relay links operating in the high snr regime our analysis exploits properties of mellin transforms to derive an asymptotic approximation that is accurate even when the per hop channel gains adhere to completely different fading models the main result contained in the paper is a general expression for the outage probability , which is a functional of the mellin transforms of the per hop channel gains furthermore , we explicitly calculate the asymptotic outage probability for four different systems , whereby in each system the per hop channels adhere to either a nakagami m , weibull , rician , or hoyt fading profile , but where the distributional parameters may differ from hop to hop this analysis leads to our second main result , which is a semi general closed form formula for the outage probability of general fixed gain af multihop systems we exploit this formula to analyze an example scenario for a four hop system where the per hop channels follow the four aforementioned fading models , i e , the first channel is nakagami m fading , the second is weibull fading , and so on finally , we provide simulation results to corroborate our analysis
an algorithm of improving the performance of iterative decoding on perpendicular magnetic recording is presented this algorithm follows on the authors' previous works on the parallel and serial concatenated turbo codes and low density parity check codes the application of this algorithm with signal to noise ratio mismatch technique shows promising results in the presence of media noise we also show that , compare to the standard iterative decoding algorithm , an improvement of within one order of magnitude can be achieved
the behaviors of patients with depression are usually difficult to predict because the patients demonstrate the symptoms of a depressive episode without a warning at unexpected times the goal of this research is to build algorithms that detect signals of such unusual moments so that doctors can be proactive in approaching already diagnosed patients before they fall in depression each patient is equipped with a smartphone with the capability to track its sensors we first find the home location of a patient , which is then augmented with other sensor data to identify sleep patterns and select communication patterns the algorithms require two to three weeks of training data to build standard patterns , which are considered normal behaviors and then , the methods identify any anomalies in day to day data readings of sensors four smartphone sensors , including the accelerometer , the gyroscope , the location probe and the communication log probe are used for anomaly detection in sleeping and communication patterns
this paper presents theory for normalized random measures \( nrms \) , normalized generalized gammas \( nggs \) , a particular kind of nrm , and dependent hierarchical nrms which allow networks of dependent nrms to be analysed these have been used , for instance , for time dependent topic modelling in this paper , we first introduce some mathematical background of completely random measures \( crms \) and their construction from poisson processes , and then introduce nrms and nggs slice sampling is also introduced for posterior inference the dependency operators in poisson processes and for the corresponding crms and nrms is then introduced and posterior inference for the ngg presented finally , we give dependency and composition results when applying these operators to nrms so they can be used in a network with hierarchical and dependent relations
the multiple description \( md \) problem has received considerable attention as a model of information transmission over unreliable channels a general framework for designing efficient multiple description quantization schemes is proposed in this paper we provide a systematic treatment of the el gamal cover \( egc \) achievable md rate distortion region , and show that any point in the egc region can be achieved via a successive quantization scheme along with quantization splitting for the quadratic gaussian case , the proposed scheme has an intrinsic connection with the gram schmidt orthogonalization , which implies that the whole gaussian md rate distortion region is achievable with a sequential dithered lattice based quantization scheme as the dimension of the \( optimal \) lattice quantizers becomes large moreover , this scheme is shown to be universal for all i i d smooth sources with performance no worse than that for an i i d gaussian source with the same variance and asymptotically optimal at high resolution a class of low complexity md scalar quantizers in the proposed general framework also is constructed and is illustrated geometrically the performance is analyzed in the high resolution regime , which exhibits a noticeable improvement over the existing md scalar quantization schemes
we take two new approaches to design efficient algorithms for transmitter optimization under rate constraints to guarantee the quality of service in general mimo interference networks , named b mac networks , which is a combination of multiple interfering broadcast channels \( bc \) and multiaccess channels \( mac \) two related optimization problems , maximizing the minimum of weighted rates under a sum power constraint and minimizing the sum power under rate constraints , are considered the first approach takes advantage of existing efficient algorithms for sinr problems by building a bridge between rate and sinr through the design of optimal mappings between them so that the problems can be converted to sinr constraint problems the approach can be applied to other optimization problems as well the second approach employs polite water filling , which is the optimal network version of water filling that we recently found it replaces almost all generic optimization algorithms currently used for networks and reduces the complexity while demonstrating superior performance even in non convex cases both centralized and distributed algorithms are designed and the performance is analyzed in addition to numeric examples
the possibility of detecting which camera has been used to shoot a specific picture is of paramount importance for many forensics tasks this is extremely useful for copyright infringement cases , ownership attribution , as well as for detecting the authors of distributed illicit material \( e g , pedo pornographic shots \) due to its importance , the forensics community has developed a series of robust detectors that exploit characteristic traces left by each camera on the acquired images during the acquisition pipeline these traces are reverse engineered in order to attribute a picture to a camera in this paper , we investigate an alternative approach to solve camera identification problem indeed , we propose a data driven algorithm based on convolutional neural networks , which learns features characterizing each camera directly from the acquired pictures the proposed approach is tested on both instance attribution and model attribution , providing an accuracy greater than 94 in discriminating 27 camera models
after a decade of research in the field of wireless sensor networks \( wsns \) there are still open issues wsns impose several severe requirements regarding energy consumption , processing capabilities , mobility , and robustness of wireless transmissions simulation has shown to be the most cost efficient approach for evaluation of wsns , thus a number of simulators are available unfortunately , these simulation environments typically consider wsns from a special point of view in this work we present the integration of three such specialized frameworks , namely mixim , pawis , and steam sim this integration combines the strengths of the single frameworks such as realistic channel models , mobility patterns , accurate energy models , and inclusion of real life application code the result is a new simulation environment which enables a more general consideration of wsns we implemented and verified our proposed concept by means of static and mobile scenarios as the presented results show , the combined framework gives the same results regarding the functionality and energy consumption as our golden model therefore the system integration was successful and the framework is ready to be used by the community
we introduce stochastic variational inference for gaussian process models this enables the application of gaussian process \( gp \) models to data sets containing millions of data points we show how gps can be vari ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference our ap proach is readily extended to models with non gaussian likelihoods and latent variable models based around gaussian processes we demonstrate the approach on a simple toy problem and two real world data sets
in this paper , outage performance of network topology aware distributed opportunistic relay selection strategies is studied with focus on the impact of different levels of channel state information \( csi \) available at relays specifically , two scenarios with \( a \) exact instantaneous and \( b \) only statistical csi are compared with explicit account for both small scale rayleigh fading and path loss due to random inter node distances analytical results , matching closely to simulations , suggest that although similar diversity order can be achieved in both cases , the lack of precise csi to support relay selection translates into significant increase in the power required to achieve the same level of qos in addition , when only statistical csi is available , achieving the same diversity order is associated with a clear performance degradation at low snr due to splitting of system resources between multiple relays
the capacity or approximations to capacity of various single source single destination relay network models has been characterized in terms of the cut set upper bound in principle , a direct computation of this bound requires evaluating the cut capacity over exponentially many cuts we show that the minimum cut capacity of a relay network under some special assumptions can be cast as a minimization of a submodular function , and as a result , can be computed efficiently we use this result to show that the capacity , or an approximation to the capacity within a constant gap for the gaussian , wireless erasure , and avestimehr diggavi tse deterministic relay network models can be computed in polynomial time we present some empirical results showing that computing constant gap approximations to the capacity of gaussian relay networks with around 300 nodes can be done in order of minutes for gaussian networks , cut set capacities are also functions of the powers assigned to the nodes we consider a family of power optimization problems and show that they can be solved in polynomial time in particular , we show that the minimization of the sum of powers assigned to the nodes subject to a minimum rate constraint \( measured in terms of cut set bounds \) can be computed in polynomial time we propose an heuristic algorithm to solve this problem and measure its performance through simulations on random gaussian networks we observe that in the optimal allocations most of the power is assigned to a small subset of relays , which suggests that network simplification may be possible without excessive performance degradation
we introduce the problem of reconstructing a sequence of multidimensional real vectors where some of the data are missing this problem contains regression and mapping inversion as particular cases where the pattern of missing data is independent of the sequence index the problem is hard because it involves possibly multivalued mappings at each vector in the sequence , where the missing variables can take more than one value given the present variables and the set of missing variables can vary from one vector to the next to solve this problem , we propose an algorithm based on two redundancy assumptions vector redundancy \( the data live in a low dimensional manifold \) , so that the present variables constrain the missing ones and sequence redundancy \( e g continuity \) , so that consecutive vectors constrain each other we capture the low dimensional nature of the data in a probabilistic way with a joint density model , here the generative topographic mapping , which results in a gaussian mixture candidate reconstructions at each vector are obtained as all the modes of the conditional distribution of missing variables given present variables the reconstructed sequence is obtained by minimising a global constraint , here the sequence length , by dynamic programming we present experimental results for a toy problem and for inverse kinematics of a robot arm
multi user multi input multi output \( mu mimo \) systems transmit data to multiple users simultaneously using the spatial degrees of freedom with user feedback channel state information \( csi \) most of the existing literatures on the reduced feedback user scheduling focus on the throughput performance and the user queueing delay is usually ignored as the delay is very important for real time applications , a low feedback queue aware user scheduling algorithm is desired for the mu mimo system this paper proposed a two stage queue aware user scheduling algorithm , which consists of a queue aware mobile driven feedback filtering stage and a sinr based user scheduling stage , where the feedback filtering policy is obtained from the solution of an optimization problem we evaluate the queueing performance of the proposed scheduling algorithm by using the sample path large deviation analysis we show that the large deviation decay rate for the proposed algorithm is much larger than that of the csi only user scheduling algorithm the numerical results also demonstrate that the proposed algorithm performs much better than the csi only algorithm requiring only a small amount of feedback
unanticipated connections between different fragments of lambda calculus and different families of embedded graphs \( a k a maps \) motivate the problem of enumerating beta normal linear lambda terms in this brief note , it is shown \( by appeal to a theorem of arqu `es and beraud \) that the sequence counting isomorphism classes of beta normal linear lambda terms up to free exchange of adjacent lambda abstractions coincides with the sequence counting isomorphism classes of rooted maps on oriented surfaces \( a000698 \)
we present an efficient algorithm for standard ica that needs only a nearly linear number of samples and has polynomial time complexity the algorithm is a recursive version of the fourier pca method of goyal et al its analysis is based on properties of random polynomials , namely the spacings of an ensemble of polynomials
this paper discusses the problem of marrying structural similarity with semantic relatedness for information extraction from text aiming at accurate recognition of relations , we introduce local alignment kernels and explore various possibilities of using them for this task we give a definition of a local alignment \( la \) kernel based on the smith waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences we show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge our experiments suggest that the la kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin additional series of experiments have been conducted on the data sets of seven general relation types , where the performance of the la kernel is comparable to the current state of the art results
we study the problem of checking whether an existential sentence \( that is , a first order sentence in prefix form built using existential quantifiers and all boolean connectives \) is true in a finite partially ordered set \( in short , a poset \) a poset is a reflexive , antisymmetric , and transitive digraph the problem encompasses the fundamental embedding problem of finding an isomorphic copy of a poset as an induced substructure of another poset model checking existential logic is already np hard on a fixed poset thus we investigate structural properties of posets yielding conditions for fixed parameter tractability when the problem is parameterized by the sentence we identify width as a central structural property \( the width of a poset is the maximum size of a subset of pairwise incomparable elements \) our main algorithmic result is that model checking existential logic on classes of finite posets of bounded width is fixed parameter tractable we observe a similar phenomenon in classical complexity , where we prove that the isomorphism problem is polynomial time tractable on classes of posets of bounded width this settles an open problem in order theory we surround our main algorithmic result with complexity results on less restricted , natural neighboring classes of finite posets , establishing its tightness in this sense we also relate our work with \( and demonstrate its independence of \) fundamental fixed parameter tractability results for model checking on digraphs of bounded degree and bounded clique width
for a fading gaussian multiple access channel with user cooperation , we obtain the optimal power allocation policies that maximize the rates achievable by block markov superposition coding the optimal policies result in a coding scheme that is simpler than the one for a general multiple access channel with generalized feedback this simpler coding scheme also leads to the possibility of formulating an otherwise non concave optimization problem as a concave one using the channel state information at the transmitters to adapt the powers , we demonstrate significant gains over the achievable rates for existing cooperative systems
the experience sampling method \( esm \) introduces in situ sampling of human behaviour , and provides researchers and behavioural therapists with ecologically valid and timely assessments of a person 's psychological state this , in turn , opens up new opportunities for understanding behaviour at a scale and granularity that was not possible just a few years ago the practical applications are many , such as the delivery of personalised and agile behaviour interventions mobile computing devices represent a revolutionary platform for improving esm they are an inseparable part of our daily lives , context aware , and can interact with people at suitable moments furthermore , these devices are equipped with sensors , and can thus take part of the reporting burden off the participant , and collect data automatically the goal of this survey is to discuss recent advancements in using mobile technologies for esm \( mesm \) , and present our vision of the future of mobile experience sampling
we establish a new extremal inequality , which is further leveraged to give a complete characterization of the rate region of the vector gaussian ceo problem with the trace distortion constraint the proof of this extremal inequality hinges on a careful analysis of the karush kuhn tucker necessary conditions for the non convex optimization problem associated with the berger tung scheme , which enables us to integrate the perturbation argument by wang and chen with the distortion projection method by rahman and wagner
we explore the striking mathematical connections that exist between market scoring rules , cost function based prediction markets , and no regret learning we show that any cost function based prediction market can be interpreted as an algorithm for the commonly studied problem of learning from expert advice by equating trades made in the market with losses observed by the learning algorithm if the loss of the market organizer is bounded , this bound can be used to derive an o \( sqrt \( t \) \) regret bound for the corresponding learning algorithm we then show that the class of markets with convex cost functions exactly corresponds to the class of follow the regularized leader learning algorithms , with the choice of a cost function in the market corresponding to the choice of a regularizer in the learning problem finally , we show an equivalence between market scoring rules and prediction markets with convex cost functions this implies that market scoring rules can also be interpreted naturally as follow the regularized leader algorithms , and may be of independent interest these connections provide new insight into how it is that commonly studied markets , such as the logarithmic market scoring rule , can aggregate opinions into accurate estimates of the likelihood of future events
we show that for any given norm ball or proper cone , weak membership in its dual ball or dual cone is polynomial time reducible to weak membership in the given ball or cone a consequence is that the weak membership or membership problem for a ball or cone is np hard if and only if the corresponding problem for the dual ball or cone is np hard in a similar vein , we show that computation of the dual norm of a given norm is polynomial time reducible to computation of the given norm this extends to convex functions satisfying a polynomial growth condition for such a given function , computation of its fenchel dual conjugate is polynomial time reducible to computation of the given function hence the computation of a norm or a convex function of polynomial growth is np hard if and only if the computation of its dual norm or fenchel dual is np hard we discuss implications of these results on the weak membership problem for a symmetric convex body and its polar dual , the polynomial approximability of mahler volume , and the weak membership problem for the epigraph of a convex function with polynomial growth and that of its fenchel dual
to detect m ary pulse amplitude modulation signals reliably in an fso communication system , the receiver requires accurate knowledge about the instantaneous channel attenuation on the signal we derive here an optimum , symbol by symbol receiver that jointly estimates the attenuation with the help of past detected data symbols and detects the data symbols accordingly few pilot symbols are required , resulting in high spectral efficiency detection can be performed with a very low complexity from both theoretical analysis and simulation , we show that as the number of the detected data symbols used for estimating the channel attenuation increases , the bit error probability of our receiver approaches that of detection with perfect channel knowledge
the nowadays massive amounts of generated and communicated data present major challenges in their processing while capable of successfully classifying nonlinearly separable objects in various settings , subspace clustering \( sc \) methods incur prohibitively high computational complexity when processing large scale data inspired by the random sampling and consensus \( ransac \) approach to robust regression , the present paper introduces a randomized scheme for sc , termed sketching and validation \( skeva \) sc , tailored for large scale data at the heart of skeva sc lies a randomized scheme for approximating the underlying probability density function of the observed data by kernel smoothing arguments sparsity in data representations is also exploited to reduce the computational burden of sc , while achieving high clustering accuracy performance analysis as well as extensive numerical tests on synthetic and real data corroborate the potential of skeva sc and its competitive performance relative to state of the art scalable sc approaches keywords subspace clustering , big data , kernel smoothing , randomization , sketching , validation , sparsity
we present our extension of acl2 with satisfiability modulo theories \( smt \) solvers using acl2 's trusted clause processor mechanism we are particularly interested in the verification of physical systems including analog and mixed signal \( ams \) designs acl2 offers strong induction abilities for reasoning about sequences and smt complements deduction methods like acl2 with fast nonlinear arithmetic solving procedures while sat solvers have been integrated into acl2 in previous work , smt methods raise new issues because of their support for a broader range of domains including real numbers and uninterpreted functions this paper presents smtlink , our clause processor for integrating smt solvers into acl2 we describe key design and implementation issues and describe our experience with its use
quantum normalizer circuits were recently introduced as generalizations of clifford circuits arxiv 1201 4867 a normalizer circuit over a finite abelian group g is composed of the quantum fourier transform \( qft \) over g , together with gates which compute quadratic functions and automorphisms in arxiv 1201 4867 it was shown that every normalizer circuit can be simulated efficiently classically this result provides a nontrivial example of a family of quantum circuits that cannot yield exponential speed ups in spite of usage of the qft , the latter being a central quantum algorithmic primitive here we extend the aforementioned result in several ways most importantly , we show that normalizer circuits supplemented with intermediate measurements can also be simulated efficiently classically , even when the computation proceeds adaptively this yields a generalization of the gottesman knill theorem \( valid for n qubit clifford operations quant ph 9705052 , quant ph 9807006 to quantum circuits described by arbitrary finite abelian groups moreover , our simulations are twofold we present efficient classical algorithms to sample the measurement probability distribution of any adaptive normalizer computation , as well as to compute the amplitudes of the state vector in every step of it finally we develop a generalization of the stabilizer formalism quant ph 9705052 , quant ph 9807006 relative to arbitrary finite abelian groups for example we characterize how to update stabilizers under generalized pauli measurements and provide a normal form of the amplitudes of generalized stabilizer states using quadratic functions and subgroup cosets
noisy channels are a powerful resource for cryptography as they can be used to obtain information theoretically secure key agreement , commitment and oblivious transfer protocols , among others oblivious transfer \( ot \) is a fundamental primitive since it is complete for secure multi party computation , and the ot capacity characterizes how efficiently a channel can be used for obtaining string oblivious transfer ahlswede and csisz ' a r \( emph isit'07 \) presented upper and lower bounds on the ot capacity of generalized erasure channels \( gec \) against passive adversaries in the case of gec with erasure probability at least 1 2 , the upper and lower bounds match and therefore the ot capacity was determined it was later proved by pinto et al \( emph ieee trans inf theory 57 \( 8 \) \) that in this case there is also a protocol against malicious adversaries achieving the same lower bound , and hence the ot capacity is identical for passive and malicious adversaries in the case of gec with erasure probability smaller than 1 2 , the known lower bound against passive adversaries that was established by ahlswede and csisz ' a r does not match their upper bound and it was unknown whether this ot rate could be achieved against malicious adversaries as well in this work we show that there is a protocol against malicious adversaries achieving the same ot rate that was obtained against passive adversaries in order to obtain our results we introduce a novel use of interactive hashing that is suitable for dealing with the case of low erasure probability \( p 1 2 \)
nurse rostering is a complex scheduling problem that affects hospital personnel on a daily basis all over the world this paper presents a new component based approach with evolutionary eliminations , for a nurse scheduling problem arising at a major uk hospital the main idea behind this technique is to decompose a schedule into its components \( i e the allocated shift pattern of each nurse \) , and then to implement two evolutionary elimination strategies mimicking natural selection and natural mutation process on these components respectively to iteratively deliver better schedules the worthiness of all components in the schedule has to be continuously demonstrated in order for them to remain there this demonstration employs an evaluation function which evaluates how well each component contributes towards the final objective two elimination steps are then applied the first elimination eliminates a number of components that are deemed not worthy to stay in the current schedule the second elimination may also throw out , with a low level of probability , some worthy components the eliminated components are replenished with new ones using a set of constructive heuristics using local optimality criteria computational results using 52 data instances demonstrate the applicability of the proposed approach in solving real world problems
network robustness against attacks is one of the most fundamental researches in network science as it is closely associated with the reliability and functionality of various networking paradigms however , despite the study on intrinsic topological vulnerabilities to node removals , little is known on the network robustness when network defense mechanisms are implemented , especially for networked engineering systems equipped with detection capabilities in this paper , a sequential defense mechanism is firstly proposed in complex networks for attack inference and vulnerability assessment , where the data fusion center sequentially infers the presence of an attack based on the binary attack status reported from the nodes in the network the network robustness is evaluated in terms of the ability to identify the attack prior to network disruption under two major attack schemes , i e , random and intentional attacks we provide a parametric plug in model for performance evaluation on the proposed mechanism and validate its effectiveness and reliability via canonical complex network models and real world large scale network topology the results show that the sequential defense mechanism greatly improves the network robustness and mitigates the possibility of network disruption by acquiring limited attack status information from a small subset of nodes in the network
take the internet of things , a piece of cloud computing , a handful of smart cities , do n't forget social platforms , flavour it with mobile technologies and ever changing environments , shake it up and voila ! what a wonderful service ! oops ! wait a minute , where did my requirements go \?
transitive text mining also named swanson linking \( sl \) after its primary and principal researcher tries to establish meaningful links between literature sets which are virtually disjoint in the sense that each does not mention the main concept of the other if successful , sl may give rise to the development of new hypotheses in this communication we describe our approach to transitive text mining which employs co occurrence analysis of the medical subject headings \( mesh \) , the descriptors assigned to papers indexed in pubmed in addition , we will outline the current state of our web based information system which will enable our users to perform literature driven hypothesis building on their own
the challenge of self optimization for orthogonal frequency division multiple access \( ofdma \) interference channels is that users inherently compete harmfully and simultaneous water filling \( wf \) would lead to a pareto inefficient equilibrium to overcome this , we first introduce the role of environmental interference derivative in the wf optimization of the interactive ofdma game and then study the environmental interference derivative properties of stackelberg equilibrium \( se \) such properties provide important insights to devise free ofdma games for achieving various ses , realizable by simultaneous wf regulated by specifically chosen operational interference derivatives we also present a definition of all stackelberg leader equilibrium \( ase \) where users are all foresighted to each other , albeit each with only local channel state information \( csi \) , and can thus most effectively reconcile their competition to maximize the user rates we show that under certain environmental conditions , the free games are both unique and optimal simulation results reveal that our distributed ase game achieves the performance very close to the near optimal centralized iterative spectrum balancing \( isb \) method in 5
imaging neuroscience links brain activation maps to behavior and cognition via correlational studies due to the nature of the individual experiments , based on eliciting neural response from a small number of stimuli , this link is incomplete , and unidirectional from the causal point of view to come to conclusions on the function implied by the activation of brain regions , it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function we rely on a large corpus of imaging studies and a predictive engine technically , the challenges are to find commonality between the studies without denaturing the richness of the corpus the key elements that we contribute are labeling the tasks performed with a cognitive ontology , and modeling the long tail of rare paradigms in the corpus to our knowledge , our approach is the first demonstration of predicting the cognitive content of completely new brain images to that end , we propose a method that predicts the experimental paradigms across different studies
one of the first graph theoretical problems which got serious attention \( already in the fifties of the last century \) was to decide whether a given integer sequence is equal to the degree sequence of a simple graph \( or it is em graphical for short \) one method to solve this problem is the greedy algorithm of havel and hakimi , which is based on the em swap operation another , closely related question is to find a sequence of swap operations to transform one graphical realization into another one of the same degree sequence this latter problem got particular emphases in connection of fast mixing markov chain approaches to sample uniformly all possible realizations of a given degree sequence \( this becomes a matter of interest in connection of among others the study of large social networks \) earlier there were only crude upper bounds on the shortest possible length of such swap sequences between two realizations in this paper we develop formulae \( gallai type identities \) for these em swap distance s of any two realizations of simple undirected or directed degree sequences these identities improves considerably the known upper bounds on the swap distances
in this paper we propose a graph community detection approach to identify cross document relationships at the topic segment level given a set of related documents , we automatically find these relationships by clustering segments with similar content \( topics \) in this context , we study how different weighting mechanisms influence the discovery of word communities that relate to the different topics found in the documents finally , we test different mapping functions to assign topic segments to word communities , determining which topic segments are considered equivalent by performing this task it is possible to enable efficient multi document browsing , since when a user finds relevant content in one document we can provide access to similar topics in other documents we deploy our approach in two different scenarios one is an educational scenario where equivalence relationships between learning materials need to be found the other consists of a series of dialogs in a social context where students discuss commonplace topics results show that our proposed approach better discovered equivalence relationships in learning material documents and obtained close results in the social speech domain , where the best performing approach was a clustering technique
physical symbol systems are needed for open ended cognition a good way to understand physical symbol systems is by comparison of thought to chemistry both have systematicity , productivity and compositionality the state of the art in cognitive architectures for open ended cognition is critically assessed i conclude that a cognitive architecture that evolves symbol structures in the brain is a promising candidate to explain open ended cognition part 2 of the paper presents such a cognitive architecture
this paper has been withdrawn by the author due to a crucial sign error in equation 1
adaptive trials are now mainstream science recently , researchers have taken the adaptive trial concept to its natural conclusion , proposing what we call global cumulative treatment analysis \( gcta \) similar to the adaptive trial , decision making and data collection and analysis in the gcta are continuous and integrated , and treatments are ranked in accord with the statistics of this information , combined with what offers the most information gain where gcta differs from an adaptive trial , or , for that matter , from any trial design , is that all patients are implicitly participants in the gcta process , regardless of whether they are formally enrolled in a trial this paper discusses some of the theoretical and practical issues that arise in the design of a gcta , along with some preliminary thoughts on how they might be approached
in severe outbreaks such as ebola , bird flu and sars , people share news , and their thoughts and responses regarding the outbreaks on social media understanding how people perceive the severe outbreaks , what their responses are , and what factors affect these responses become important in this paper , we conduct a comprehensive study of understanding and mining the spread of ebola related information on social media in particular , we \( i \) conduct a large scale data driven analysis of geotagged social media messages to understand citizen reactions regarding ebola \( ii \) build information propagation models which measure locality of information and \( iii \) analyze spatial , temporal and social properties of ebola related information our work provides new insights into ebola outbreak by understanding citizen reactions and topic based information propagation , as well as providing a foundation for analysis and response of future public health crises
in this paper we discuss about full duplex operation in particular targeting mobile devices an overview of the problem is given and then methods both in analog and digital domain are presented in order to attain full duplex operation for the analog domain cancellation a novel analog rf canceller is presented finally , the methods are evaluated by measuring typical scenario for full duplex transceiver
reasoning about knowledge seems to play a fundamental role in distributed systems indeed , such reasoning is a central part of the informal intuitive arguments used in the design of distributed protocols communication in a distributed system can be viewed as the act of transforming the system 's state of knowledge this paper presents a general framework for formalizing and reasoning about knowledge in distributed systems we argue that states of knowledge of groups of processors are useful concepts for the design and analysis of distributed protocols in particular , distributed knowledge corresponds to knowledge that is ``distributed'' among the members of the group , while common knowledge corresponds to a fact being ``publicly known'' the relationship between common knowledge and a variety of desirable actions in a distributed system is illustrated furthermore , it is shown that , formally speaking , in practical systems common knowledge cannot be attained a number of weaker variants of common knowledge that are attainable in many cases of interest are introduced and investigated
this paper studies the stability of communication protocols that deal with transmission errors we consider a coordination game between an informed sender and an uninformed decision maker , the receiver , who communicate over a noisy channel the sender 's strategy , called a code , maps states of nature to signals the receiver 's best response is to decode the received channel output as the state with highest expected receiver payoff given this decoding , an equilibrium or nash code results if the sender encodes every state as prescribed we show two theorems that give sufficient conditions for nash codes first , a receiver optimal code defines a nash code a second , more surprising observation holds for communication over a binary channel which is used independently a number of times , a basic model of information transmission under a minimal monotonicity requirement for breaking ties when decoding , which holds generically , every code is a nash code
the edit distance is a way of quantifying how similar two strings are to one another by counting the minimum number of character insertions , deletions , and substitutions required to transform one string into the other in this paper we study the computational problem of computing the edit distance between a pair of strings where their distance is bounded by a parameter k ll n we present two streaming algorithms for computing edit distance one runs in time o \( n k 2 \) and the other n o \( k 3 \) by writing n o \( k 3 \) we want to emphasize that the number of operations per an input symbol is a small constant in particular , the running time does not depend on the alphabet size , and the algorithm should be easy to implement previously a streaming algorithm with running time o \( n k 4 \) was given in the paper by the current authors \( stoc'16 \) the best off line algorithm runs in time o \( n k 2 \) \( landau et al , 1998 \) which is known to be optimal under the strong exponential time hypothesis
this paper proposes an analysis of classifiers into four major types unit , metric , group and species , based on properties of both japanese and english the analysis makes possible a uniform and straightforward treatment of noun phrases headed by classifiers in japanese to english machine translation , and has been implemented in the mt system alt j e although the analysis is based on the characteristics of , and differences between , japanese and english , it is shown to be also applicable to the unrelated language thai
in silico investigations of skin permeation are an important but also computationally demanding problem to resolve all involved scales in full detail will probably not only require exascale computing capacities but also suitable parallel algorithms this article investigates the applicability of the time parallel parareal algorithm to a brick and mortar setup , a precursory problem to skin permeation a c library implementing parareal is combined with the ug4 simulation framework , which provides the spatial discretization and parallelization the combination 's performance is studied with respect to convergence and speedup while limited speedup from the time parallelization is shown to be possible , load balancing is identified as an important issue in the application of parareal with implicit integrators to complex pdes
accurate software development effort estimation is critical to the success of software projects although many techniques and algorithmic models have been developed and implemented by practitioners , accurate software development effort prediction is still a challenging endeavor in the field of software engineering , especially in handling uncertain and imprecise inputs and collinear characteristics in this paper , a hybrid in telligent model combining a neural network model integrated with fuzzy model \( neuro fuzzy model \) has been used to improve the accuracy of estimating software cost the performance of the proposed model is assessed by designing and conducting evaluation with published project and industrial data results have shown that the proposed model demonstrates the ability of improving the estimation accuracy by 18 based on the mean magnitude of relative error \( mmre \) criterion
in this paper , we adopt a cross layer design approach for analyzing the throughput delay tradeoff of the multicast channel in a single cell system to illustrate the main ideas , we start with the single group case , i e , pure multicast , where a common information stream is requested by all the users we consider three classes of scheduling algorithms with progressively increasing complexity the first class strives for minimum complexity by resorting to a static scheduling strategy along with memoryless decoding our analysis for this class of scheduling algorithms reveals the existence of a static scheduling policy that achieves the optimal scaling law of the throughput at the expense of a delay that increases exponentially with the number of users the second scheduling policy resorts to a higher complexity incremental redundancy encoding decoding strategy to achieve a superior throughput delay tradeoff the third , and most complex , scheduling strategy benefits from the cooperation between the different users to minimize the delay while achieving the optimal scaling law of the throughput in particular , the proposed cooperative multicast strategy is shown to simultaneously achieve the optimal scaling laws of both throughput and delay then , we generalize our scheduling algorithms to exploit the multi group diversity available when different information streams are requested by different subsets of the user population finally , we discuss the effect of the potential gains of equipping the base station with multi transmit antennas and present simulation results that validate our theoretical claims
the nonparametric problem of detecting existence of an anomalous interval over a one dimensional line network is studied nodes corresponding to an anomalous interval \( if exists \) receive samples generated by a distribution q , which is different from the distribution p that generates samples for other nodes if anomalous interval does not exist , then all nodes receive samples generated by p it is assumed that the distributions p and q are arbitrary , and are unknown in order to detect whether an anomalous interval exists , a test is built based on mean embeddings of distributions into a reproducing kernel hilbert space \( rkhs \) and the metric of maximummean discrepancy \( mmd \) it is shown that as the network size n goes to infinity , if the minimum length of candidate anomalous intervals is larger than a threshold which has the order o \( log n \) , the proposed test is asymptotically successful , i e , the probability of detection error approaches zero asymptotically an efficient algorithm to perform the test with substantial computational complexity reduction is proposed , and is shown to be asymptotically successful if the condition on the minimum length of candidate anomalous interval is satisfied numerical results are provided , which are consistent with the theoretical results
we study the list decoding problem of alternant codes , with the notable case of classical goppa codes the major consideration here is to take into account the size of the alphabet , which shows great influence on the list decoding radius this amounts to compare the emph generic johnson bound to the emph q ary johnson bound this difference is important when q is very small essentially , the most favourable case is q 2 , for which the decoding radius is greatly improved , notably when the relative minimum distance gets close to 1 2 even though the announced result , which is the list decoding radius of binary goppa codes , is new , it can be rather easily made up from previous sources \( v guruswami , r m roth and i tal , r m roth \) , which may be a little bit unknown , and in which the case of binary goppa codes has apparently not been thought at only d j bernstein treats the case of binary goppa codes in a preprint references are given in the introduction we propose an autonomous treatment and also a complexity analysis of the studied algorithm , which is quadratic in the blocklength n , when decoding at some distance of the relative maximum decoding radius , and in o \( n 7 \) when reaching the maximum radius
in a range of scientific coauthorship networks , tipping points are detected in degree distributions , correlations between degrees and local clustering coefficients , etc the existence of those tipping points could be treated as a result of the diversity of collaboration behaviours in scientific field a growing geometric hypergraph built on a cluster of concentric circles is proposed to model two typical collaboration behaviours , namely the behaviour of leaders and that of other members in research teams the model successfully predicts the tipping points , as well as many common features of coauthorship networks for example , it realizes a process of deriving the complex scale free property from the simple yes no experiments moreover , it gives a reasonable explanation for the emergence of tipping points by the difference of collaboration behaviours between leaders and other members , which emerges in the evolution of research teams the evolution synthetically addresses typical factors of generating collaborations , such as academic impacts , the homophily of authors , and the communications between research teams , etc
trilateration has recently become one of the well known threat models to the user 's location privacy in location based applications \( aka location based services or lbs \) , especially those containing highly sensitive information such as dating applications the threat model mainly depends on the distance shown from the targeted victim to the adversary to pinpoint the victim 's position as a countermeasure , most of location based applications have already implemented the hide distance function to protect their user 's location privacy the effectiveness of such approaches however is still questionable therefore , in this paper , we first investigate how popular location based dating applications are currently protecting their user 's privacy by testing the two most popular glbt focused applications jack 'd and grindr
in modern data science problems , techniques for extracting value from big data require performing large scale optimization over heterogenous , irregularly structured data much of this data is best represented as multi relational graphs , making vertex programming abstractions such as those of pregel and graphlab ideal fits for modern large scale data analysis in this paper , we describe a vertex programming implementation of a popular consensus optimization technique known as the alternating direction of multipliers \( admm \) admm consensus optimization allows elegant solution of complex objectives such as inference in rich probabilistic models we also introduce a novel hypergraph partitioning technique that improves over state of the art partitioning techniques for vertex programming and significantly reduces the communication cost by reducing the number of replicated nodes up to an order of magnitude we implemented our algorithm in graphlab and measure scaling performance on a variety of realistic bipartite graph distributions and a large synthetic voter opinion analysis application in our experiments , we are able to achieve a 50 improvement in runtime over the current state of the art graphlab partitioning scheme
describing the collective activity of neural populations is a daunting task the number of possible patterns grows exponentially with the number of cells , resulting in practically unlimited complexity recent empirical studies , however , suggest a vast simplification in how multi neuron spiking occurs the activity patterns of some circuits are nearly completely captured by pairwise interactions among neurons why are such pairwise models so successful in some instances , but insufficient in others \? here , we study the emergence of higher order interactions in simple circuits with different architectures and inputs we quantify the impact of higher order interactions by comparing the responses of mechanistic circuit models vs null descriptions in which all higher than pairwise correlations have been accounted for by lower order statistics , known as pairwise maximum entropy models we find that bimodal input signals produce larger deviations from pairwise predictions than unimodal inputs for circuits with local and global connectivity moreover , recurrent coupling can accentuate these deviations , if coupling strengths are neither too weak nor too strong a circuit model based on intracellular recordings from on parasol retinal ganglion cells shows that a broad range of light signals induce unimodal inputs to spike generators , and that coupling strengths produce weak effects on higher order interactions this provides a novel explanation for the success of pairwise models in this system overall , our findings identify circuit level mechanisms that produce and fail to produce higher order spiking statistics in neural ensembles
the ability to accurately model a sentence at varying stages \( e g , word phrase sentence \) plays a central role in natural language processing as an effort towards this goal we propose a self adaptive hierarchical sentence model \( adasent \) adasent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments we design a competitive mechanism \( through gating networks \) to allow the representations of the same sentence to be engaged in a particular learning task \( e g , classification \) , therefore effectively mitigating the gradient vanishing problem persistent in other recursive models both qualitative and quantitative analysis shows that adasent can automatically form and select the representations suitable for the task at hand during training , yielding superior classification performance over competitor models on 5 benchmark data sets
this paper presents distributed adaptive algorithms based on the conjugate gradient \( cg \) method for distributed networks both incremental and diffusion adaptive solutions are all considered the distributed conventional \( cg \) and modified cg \( mcg \) algorithms have an improved performance in terms of mean square error as compared with least mean square \( lms \) based algorithms and a performance that is close to recursive least squares \( rls \) algorithms the resulting algorithms are distributed , cooperative and able to respond in real time to changes in the environment
we propose the concept of adaptable processes as a way of overcoming the limitations that process calculi have for describing patterns of dynamic process evolution such patterns rely on direct ways of controlling the behavior and location of running processes , and so they are at the heart of the adaptation capabilities present in many modern concurrent systems adaptable processes have a location and are sensible to actions of dynamic update at runtime this allows to express a wide range of evolvability patterns for concurrent processes we introduce a core calculus of adaptable processes and propose two verification problems for them bounded and eventual adaptation while the former ensures that the number of consecutive erroneous states that can be traversed during a computation is bound by some given number k , the latter ensures that if the system enters into a state with errors then a state without errors will be eventually reached we study the \( un \) decidability of these two problems in several variants of the calculus , which result from considering dynamic and static topologies of adaptable processes as well as different evolvability patterns rather than a specification language , our calculus intends to be a basis for investigating the fundamental properties of evolvable processes and for developing richer languages with evolvability capabilities
the likelihood decoder is a stochastic decoder that selects the decoded message at random , using the posterior distribution of the true underlying message given the channel output in this work , we study a generalized version of this decoder where the posterior is proportional to a general function that depends only on the joint empirical distribution of the output vector and the codeword this framework allows both mismatched versions and universal \( mmi \) versions of the likelihood decoder , as well as the corresponding ordinary deterministic decoders , among many others we provide a direct analysis method that yields the exact random coding exponent \( as opposed to separate upper bounds and lower bounds that turn out to be compatible , which were derived earlier by scarlett et al we also extend the result from pure channel coding to combined source and channel coding \( random binning followed by random channel coding \) with side information available to the decoder finally , returning to pure channel coding , we derive also an expurgated exponent for the stochastic likelihood decoder , which turns out to be at least as tight \( and in some cases , strictly so \) as the classical expurgated exponent of the maximum likelihood decoder , even though the stochastic likelihood decoder is suboptimal
once one has enriched lfg 's formal machinery with the linear logic mechanisms needed for semantic interpretation as proposed by dalrymple et al , it is natural to ask whether these make any existing components of lfg redundant as dalrymple and her colleagues note , lfg 's f structure completeness and coherence constraints fall out as a by product of the linear logic machinery they propose for semantic interpretation , thus making those f structure mechanisms redundant given that linear logic machinery or something like it is independently needed for semantic interpretation , it seems reasonable to explore the extent to which it is capable of handling feature structure constraints as well r lfg represents the extreme position that all linguistically required feature structure dependencies can be captured by the resource accounting machinery of a linear or similiar logic independently needed for semantic interpretation , making lfg 's unification machinery redundant the goal is to show that lfg linguistic analyses can be expressed as clearly and perspicuously using the smaller set of mechanisms of r lfg as they can using the much larger set of unification based mechanisms in lfg if this is the case then we will have shown that positing these extra f structure mechanisms is not linguistically warranted
since its invention , polar code has received a lot of attention because of its capacity achieving performance and low encoding and decoding complexity successive cancellation decoding \( scd \) and belief propagation decoding \( bpd \) are two of the most popular approaches for decoding polar codes scd is able to achieve good error correcting performance and is less computationally expensive as compared to bpd however scds suffer from long latency and low throughput due to the serial nature of the successive cancellation algorithm bpd is parallel in nature and hence is more attractive for high throughput applications however since it is iterative in nature , the required latency and energy dissipation increases linearly with the number of iterations in this work , we borrow the idea of scd and propose a novel scheme based on sub factor graph freezing to reduce the average number of computations as well as the average number of iterations required by bpd , which directly translates into lower latency and energy dissipation simulation results show that the proposed scheme has no performance degradation and achieves significant reduction in computation complexity over the existing methods
the treatment of both aleatory and epistemic uncertainty by recent methods often requires an high computational effort in this abstract , we propose a numerical sampling method allowing to lighten the computational burden of treating the information by means of so called fuzzy random variables
this paper derives an improved sphere packing \( isp \) bound for finite length codes whose transmission takes place over symmetric memoryless channels we first review classical results , i e , the 1959 sphere packing \( sp59 \) bound of shannon for the gaussian channel , and the 1967 sphere packing \( sp67 \) bound of shannon et al for discrete memoryless channels a recent improvement on the sp67 bound , as suggested by valembois and fossorier , is also discussed these concepts are used for the derivation of a new lower bound on the decoding error probability \( referred to as the isp bound \) which is uniformly tighter than the sp67 bound and its recent improved version the isp bound is applicable to symmetric memoryless channels , and some of its applications are exemplified its tightness is studied by comparing it with bounds on the ml decoding error probability , and computer simulations of iteratively decoded turbo like codes the paper also presents a technique which performs the entire calculation of the sp59 bound in the logarithmic domain , thus facilitating the exact calculation of this bound for moderate to large block lengths without the need for the asymptotic approximations provided by shannon
in this paper , we propose a novel framework called rigid body localization for joint position and orientation estimation of a rigid body we consider a setup in which a few sensors are mounted on a rigid body the absolute position of the sensors on the rigid body , or the absolute position of the rigid body itself is not known however , we know how the sensors are mounted on the rigid body , i e , the sensor topology is known using range only measurements between the sensors and a few anchors \( nodes with known absolute positions \) , and without using any inertial measurements \( e g , accelerometers \) , we estimate the position and orientation of the rigid body for this purpose , the absolute position of the sensors is expressed as an affine function of the stiefel manifold in other words , we represent the orientation as a rotation matrix , and absolute position as a translation vector we propose a least squares \( ls \) , simplified unitarily constrained ls \( suc ls \) , and optimal unitarily constrained least squares \( ouc ls \) estimator , where the latter is based on newton 's method as a benchmark , we derive a unitarily constrained cram 'er rao bound \( uc crb \) the known topology of the sensors can sometimes be perturbed during fabrication to take these perturbations into account , a simplified unitarily constrained total least squares \( suc tls \) , and an optimal unitarily constrained total least squares \( ouc tls \) estimator are also proposed
we propose a new approach to the problem of searching a space of policies for a markov decision process \( mdp \) or a partially observable markov decision process \( pomdp \) , given a model our approach is based on the following observation any \( po \) mdp can be transformed into an equivalent pomdp in which all state transitions \( given the current state and action \) are deterministic this reduces the general problem of policy search to one in which we need only consider pomdps with deterministic transitions we give a natural way of estimating the value of all policies in these transformed pomdps policy search is then simply performed by searching for a policy with high estimated value we also establish conditions under which our value estimates will be good , recovering theoretical results similar to those of kearns , mansour and ng \( 1999 \) , but with sample complexity bounds that have only a polynomial rather than exponential dependence on the horizon time our method applies to arbitrary pomdps , including ones with infinite state and action spaces we also present empirical results for our approach on a small discrete problem , and on a complex continuous state continuous action problem involving learning to ride a bicycle
we aim at providing a foundation of a theory of good sat representations f of boolean functions f we argue that the hierarchy uc k of unit refutation complete clause sets of level k , introduced by the authors , provides the most basic target classes , that is , f in uc k is to be achieved for k as small as feasible if f does not contain new variables , i e , f is equivalent \( as a cnf \) to f , then f in uc 1 is similar to achieving \( generalised \) arc consistency known from the literature \( it is somewhat weaker , but theoretically much nicer to handle \) we show that for polysize representations of boolean functions in this sense , the hierarchy uc k is strict the boolean functions for these separations are doped minimally unsatisfiable clause sets of deficiency 1 these functions have been introduced in sloan , soerenyi , turan , 2007 , and we generalise their construction and show a correspondence to a strengthened notion of irredundant sub clause sets turning from lower bounds to upper bounds , we believe that many common cnf representations fit into the uc k scheme , and we give some basic tools to construct representations in uc 1 with new variables , based on the tseitin translation note that regarding new variables the uc 1 representations are stronger than mere arc consistency , since the new variables are not excluded from consideration
we define a class of stochastic processes based on evolutions and measurements of quantum systems , and consider the complexity of predicting their long term behavior it is shown that a very general class of decision problems regarding these stochastic processes can be efficiently solved classically in the space bounded case the following corollaries are implied by our main result \( 1 \) any space o \( s \) uniform family of quantum circuits acting on s qubits and consisting of unitary gates and measurement gates defined in a typical way by matrices of algebraic numbers can be simulated by an unbounded error space o \( s \) ordinary \( i e , fair coin flipping \) probabilistic turing machine , and hence by space o \( s \) uniform classical \( deterministic \) circuits of depth o \( s 2 \) and size 2 \( o \( s \) \) the quantum circuits are not required to operate with bounded error and may have depth exponential in s \( 2 \) any \( unbounded error \) quantum turing machine running in space s , having arbitrary algebraic transition amplitudes , allowing unrestricted measurements during its computation , and having no restrictions on running time can be simulated by an unbounded error space o \( s \) ordinary probabilistic turing machine , and hence deterministically in space o \( s 2 \)
broadcasting k independent messages to multiple users where each user demands all the messages and has a subset of the messages as side information is studied recently , natarajan , hong , and viterbo proposed a novel broadcasting strategy called lattice index coding which uses lattices constructed over some principal ideal domains \( pids \) for transmission and showed that this scheme provides uniform side information gains in this paper , we generalize this strategy to general rings of algebraic integers of number fields which may not be pids upper and lower bounds on the side information gains for the proposed scheme constructed over some interesting classes of number fields are provided and are shown to coincide asymptotically in message rates this generalization substantially enlarges the design space and partially includes the scheme by natarajan , hong , and viterbo as a special case perhaps more importantly , in addition to side information gains , the proposed lattice index codes benefit from diversity gains inherent in constellations carved from number fields when used over rayleigh fading channel some interesting examples are also provided for which the proposed scheme allows all the messages to be from the same field
we predict the popularity of short messages called tweets created in the micro blogging site known as twitter we measure the popularity of a tweet by the time series path of its retweets , which is when people forward the tweet to others we develop a probabilistic model for the evolution of the retweets using a bayesian approach , and form predictions using only observations on the retweet times and the local network or graph structure of the retweeters we obtain good step ahead forecasts and predictions of the final total number of retweets even when only a small fraction \( i e , less than one tenth \) of the retweet path is observed this translates to good predictions within a few minutes of a tweet being posted , and has potential implications for understanding the spread of broader ideas , memes , or trends in social networks
this note presents variations on the fibonacci universal code , that may also be called the gopala hemachandra code , that can have applications in source coding as well as in cryptography
recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers such research is difficult because it requires making sense of non linear computations performed by millions of learned parameters , but valuable because it increases our ability to understand current models and training algorithms and thus create improved versions of them in this paper we investigate the extent to which neural networks exhibit what we call convergent learning , which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low dimensional spaces we propose a specific method of probing representations training multiple networks and then comparing and contrasting their individual , learned representations at the level of neurons or groups of neurons we begin research into this question using three techniques to approximately align different neural networks on a feature level a bipartite matching approach that makes one to one assignments between neurons , a sparse prediction approach that finds one to many mappings , and a spectral clustering approach that finds many to many mappings this initial investigation reveals a few previously unknown properties of neural networks , and we argue that future research into the question of convergent learning will yield many more the insights described here include \( 1 \) that some features are learned reliably in multiple networks , yet other features are not consistently learned \( 2 \) that units learn to span low dimensional subspaces and , while these subspaces are common to multiple networks , the specific basis vectors learned are not \( 3 \) that the representation codes are a mix between a local code and slightly , but not fully , distributed codes across multiple units
we consider a problem where a memoryless bi variate gaussian source is to be transmitted over an additive white gaussian multiple access channel with two transmitting terminals and one receiving terminal the first transmitter only sees the first source component and the second transmitter only sees the second source component we are interested in the pair of mean squared error distortions at which the receiving terminal can reproduce each of the source components it is demonstrated that in the symmetric case , below a certain signal to noise ratio \( snr \) threshold , which is determined by the source correlation , uncoded communication is optimal for snrs above this threshold we present outer and inner bounds on the achievable distortions
modern signal processing \( sp \) methods rely very heavily on probability and statistics to solve challenging sp problems expectations and demands are constantly rising , and sp methods are now expected to deal with ever more complex models , requiring ever more sophisticated computational inference techniques this has driven the development of statistical sp methods based on stochastic simulation and optimization stochastic simulation and optimization algorithms are computationally intensive tools for performing statistical inference in models that are analytically intractable and beyond the scope of deterministic inference methods they have been recently successfully applied to many difficult problems involving complex statistical models and sophisticated \( often bayesian \) statistical inference techniques this paper presents a tutorial on stochastic simulation and optimization methods in signal and image processing and points to some interesting research problems the paper addresses a variety of high dimensional markov chain monte carlo \( mcmc \) methods as well as deterministic surrogate methods , such as variational bayes , the bethe approach , belief and expectation propagation and approximate message passing algorithms it also discusses a range of optimization methods that have been adopted to solve stochastic problems , as well as stochastic methods for deterministic optimization subsequently , areas of overlap between simulation and optimization , in particular optimization within mcmc and mcmc driven optimization are discussed
we investigate the combination between causal zero delay source coding and information theoretic secrecy two source coding models with secrecy constraints are considered we start by considering zero delay perfectly secret lossless transmission of a memoryless source we derive bounds on the key rate and coding rate needed for perfect zero delay secrecy in this setting , we consider two models which differ by the ability of the eavesdropper to parse the bit stream passing from the encoder to the legitimate decoder into separate messages we also consider causal source coding with a fidelity criterion and side information at the decoder and the eavesdropper unlike the zero delay setting where variable length coding is traditionally used but might leak information on the source through the length of the codewords , in this setting , since delay is allowed , block coding is possible we show that in this setting , separation of encryption and causal source coding is optimal
in this paper , we study the quantity of computational resources \( state machine states and or probabilistic transition precision \) needed to solve specific problems in a single hop network where nodes communicate using only beeps we begin by focusing on randomized leader election we prove a lower bound on the states required to solve this problem with a given error bound , probability precision , and \( when relevant \) network size lower bound we then show the bound tight with a matching upper bound noting that our optimal upper bound is slow , we describe two faster algorithms that trade some state optimality to gain efficiency we then turn our attention to more general classes of problems by proving that once you have enough states to solve leader election with a given error bound , you have \( within constant factors \) enough states to simulate correctly , with this same error bound , a logspace tm with a constant number of unary input tapes allowing you to solve a large and expressive set of problems these results identify a key simplicity threshold beyond which useful distributed computation is possible in the beeping model
it is well known that shannon 's rate distortion function \( rdf \) in the colored quadratic gaussian \( qg \) case can be parametrized via a single lagrangian variable \( the water level in the reverse water filling solution \) in this work , we show that the symmetric colored qg multiple description \( md \) rdf in the case of two descriptions can be parametrized in the spectral domain via two lagrangian variables , which control the trade off between the side distortion , the central distortion , and the coding rate this spectral domain analysis is complemented by a time domain scheme design approach we show that the symmetric colored qg md rdf can be achieved by combining ideas of delta sigma modulation and differential pulse code modulation specifically , two source prediction loops , one for each description , are embedded within a common noise shaping loop , whose parameters are explicitly found from the spectral domain characterization
we investigate the power of graph isomorphism algorithms based on algebraic reasoning techniques like gr obner basis computation the idea of these algorithms is to encode two graphs into a system of equations that are satisfiable if and only if if the graphs are isomorphic , and then to \( try to \) decide satisfiability of the system using , for example , the gr obner basis algorithm in some cases this can be done in polynomial time , in particular , if the equations admit a bounded degree refutation in an algebraic proof systems such as nullstellensatz or polynomial calculus we prove linear lower bounds on the polynomial calculus degree over all fields of characteristic different from 2 and also linear lower bounds for the degree of positivstellensatz calculus derivations we compare this approach to recently studied linear and semidefinite programming approaches to isomorphism testing , which are known to be related to the combinatorial weisfeiler lehman algorithm we exactly characterise the power of the weisfeiler lehman algorithm in terms of an algebraic proof system that lies between degree k nullstellensatz and degree k polynomial calculus
systems that employ network coding for content distribution convey to the receivers linear combinations of the source packets if we assume randomized network coding , during this process the network nodes collect random subspaces of the space spanned by the source packets we establish several fundamental properties of the random subspaces induced in such a system , and show that these subspaces implicitly carry topological information about the network and its state that can be passively collected and inferred we leverage this information towards a number of applications that are interesting in their own right , such as topology inference , bottleneck discovery in peer to peer systems and locating byzantine attackers we thus argue that , randomized network coding , apart from its better known properties for improving information delivery rate , can additionally facilitate network management and control
in this article , we study some new characterizations of primitive recursive functions based on restricted forms of primitive recursion , improving the pioneering work of r m robinson and m d gladstone in this area we reduce certain recursion schemes \( mixed pure iteration without parameters \) and we characterize one argument primitive recursive functions as the closure under substitution and iteration of certain optimal sets
the paper presents a solution for endcoding decoding dna information in 2d barcodes first part focuses on the existing techniques and symbologies in 2d barcodes field the 2d barcode pdf417 is presented as starting point the adaptations and optimizations on pdf417 and on datamatrix lead to the solution dna2dbc deoxyribonucleic acid two dimensional barcode the second part shows the dna2dbc encoding decoding process step by step in conclusions are enumerated the most important features of 2d barcode implementation for dna
the relation between self awareness and intelligence is an open problem these days despite the fact that self awarness is usually related to emotional intelligence , this is not the case here the problem described in this paper is how to model an agent which knows \( cognitive \) binary logic and which is also able to pass \( without any mistake \) a certain family of turing tests designed to verify its knowledge and its discourse about the modal states of truth corresponding to well formed formulae within the language of propositional binary logic
a large number of computational scientific research projects make use of open source software packages however , the development process of such tools frequently differs from conventional software development partly because of the nature of research , where the problems being addressed are not always fully understood partly because the majority of the development is often carried out by scientists with limited experience and exposure to best practices of software engineering often the software development suffers from the pressure to publish scientific results and that credit for software development is limited in comparison fundamental components of software engineering like modular and reusable design , validation , documentation , and software integration as well as effective maintenance and user support tend to be disregarded due to lack of resources and qualified specialists thus innovative developments are often hindered by steep learning curves required to master development for legacy software packages full of ad hoc solutions the growing complexity of research , however , requires suitable and maintainable computational tools , resulting in a widening gap between the potential users \( often growing in number \) and contributors to the development of such a package in this paper we share our experiences aiming to improve the situation by training particularly young scientists , through disseminating our own experiences at contributing to open source software packages and practicing key components of software engineering adapted for scientists and scientific software development specifically we summarize the outcome of the workshop in advanced techniques for scientific programming and collaborative development of open source software packages run at the abdus salam international centre for theoretical physics in march 2013 , and discuss our conclusions for future efforts
in recent years codes that are not uniquely decipherable \( ud \) are been studied partitioning them in classes that localize the ambiguities of the code a natural question is how we can extend the notion of maximality to codes that are not ud in this paper we give an answer to this question to do this we introduce a partial order in the set of submonoids of a monoid showing the existence , in this poset , of maximal elements that we call full monoids then a set of generators of a full monoid is , by definition , a maximal code we show how this definition extends , in a natural way , the existing definition concerning ud codes and we find a characteristic property of a monoid generated by a maximal ud code
we study the behavior of the clustering coefficient in tagged networks the rich variety of tags associated with the nodes in the studied systems provide additional information about the entities represented by the nodes which can be important for practical applications like searching in the networks here we examine how the clustering coefficient changes when narrowing the network to a sub graph marked by a given tag , and how does it correlate with various other properties of the sub graph another interesting question addressed in the paper is how the clustering coefficient of the individual nodes is affected by the tags on the node we believe these sort of analysis help acquiring a more complete description of the structure of large complex systems
this paper describes a systems architecture for a hybrid centralised swarm based multi agent system the issue of local goal assignment for agents is investigated through the use of a global agent which teaches the agents responses to given situations we implement a test problem in the form of a pursuit game , where the multi agent system is a set of captor agents the agents learn solutions to certain board positions from the global agent if they are unable to find a solution the captor agents learn through the use of multi layer perceptron neural networks the global agent is able to solve board positions through the use of a genetic algorithm the cooperation between agents and the results of the simulation are discussed here
data grids have been adopted as the platform for scientific communities that need to share , access , transport , process and manage large data collections distributed worldwide they combine high end computing technologies with high performance networking and wide area storage management techniques in this paper , we discuss the key concepts behind data grids and compare them with other data sharing and distribution paradigms such as content delivery networks , peer to peer networks and distributed databases we then provide comprehensive taxonomies that cover various aspects of architecture , data transportation , data replication and resource allocation and scheduling finally , we map the proposed taxonomy to various data grid systems not only to validate the taxonomy but also to identify areas for future exploration through this taxonomy , we aim to categorise existing systems to better understand their goals and their methodology this would help evaluate their applicability for solving similar problems this taxonomy also provides a gap analysis of this area through which researchers can potentially identify new issues for investigation finally , we hope that the proposed taxonomy and mapping also helps to provide an easy way for new practitioners to understand this complex area of research
the discrete memoryless interference channel is modelled as a conditional probability distribution with two outputs depending on two inputs and has widespread applications in practical communication scenarios in this paper , we introduce and study the quantum interference channel , a generalization of a two input , two output memoryless channel to the setting of quantum shannon theory we discuss three different coding strategies and obtain corresponding achievable rate regions for quantum interference channels we calculate the capacity regions in the special cases of very strong and strong interference the achievability proof in the case of strong interference exploits a novel quantum simultaneous decoder for two sender quantum multiple access channels we formulate a conjecture regarding the existence of a quantum simultaneous decoder in the three sender case and use it to state the rates achievable by a quantum han kobayashi strategy
we give the first non trivial upper bounds on the average sensitivity and noise sensitivity of polynomial threshold functions more specifically , for a boolean function f on n variables equal to the sign of a real , multivariate polynomial of total degree d we prove 1 \) the average sensitivity of f is at most o \( n 1 1 \( 4d 6 \) \) \( we also give a combinatorial proof of the bound o \( n 1 1 2 d \) 2 \) the noise sensitivity of f with noise rate delta is at most o \( delta 1 \( 4d 6 \) \) previously , only bounds for the linear case were known along the way we show new structural theorems about random restrictions of polynomial threshold functions obtained via hypercontractivity these structural results may be of independent interest as they provide a generic template for transforming problems related to polynomial threshold functions defined on the boolean hypercube to polynomial threshold functions defined in gaussian space
in 1991 , gnanajothi 4 proved that the path graph p n with n vertex and n 1 edge is odd graceful , and the cycle graph c m with m vertex and m edges is odd graceful if and only if m even , she proved the cycle graph is not graceful if m odd in this paper , firstly , we studied the graph c m cup p m when m 4 , 6 , 8 , 10 and then we proved that the graph c cup p n is odd graceful if m is even finally , we described an algorithm to label the vertices and the edges of the vertex set v \( c m cup p n \) and the edge set e \( c m cup p n \)
the golden code is a full rate full diversity space time code for two transmit antennas that has a maximal coding gain because each codeword conveys four information symbols from an m ary quadrature amplitude modulation alphabet , the complexity of an exhaustive search decoder is proportional to m 2 in this paper we present a new fast algorithm for maximum likelihood decoding of the golden code that has a worst case complexity of only o \( 2m 2 5 \) we also present an efficient implementation of the fast decoder that exhibits a low average complexity finally , in contrast to the overlaid alamouti codes , which lose their fast decodability property on time varying channels , we show that the golden code is fast decodable on both quasistatic and rapid time varying channels
in this paper , we consider a cognitive setting under the context of cooperative communications , where the cognitive radio \( cr \) user is assumed to be a self organized relay for the network the cr user and the pu are assumed to be energy harvesters the cr user cooperatively relays some of the undelivered packets of the primary user \( pu \) specifically , the cr user stores a fraction of the undelivered primary packets in a relaying queue \( buffer \) it manages the flow of the undelivered primary packets to its relaying queue using the appropriate actions over time slots moreover , it has the decision of choosing the used queue for channel accessing at idle time slots \( slots where the pu 's queue is empty \) it is assumed that one data packet transmission dissipates one energy packet the optimal policy changes according to the primary and cr users arrival rates to the data and energy queues as well as the channels connectivity the cr user saves energy for the pu by taking the responsibility of relaying the undelivered primary packets it optimally organizes its own energy packets to maximize its payoff as time progresses
linkedin search is deeply personalized for the same queries , different searchers expect completely different results this paper presents our approach to achieving this by mining various data sources available in linkedin to infer searchers' intents \( such as hiring , job seeking , etc \) , as well as extending the concept of homophily to capture the searcher result similarities on many aspects then , learning to rank \( ltr \) is applied to combine these signals with standard search features
in october 2007 , a research proposal for the university of sydney , australia , the author suggested that biovie physical phenomenon as `electrodynamic dependant biological vision' , is governed by relativistic quantum laws and biovision the phenomenon on the basis of `biovielectroluminescence' , satisfies man microbio megabio computer vision \( mmmcv \) , as a robust candidate for physical and visual sciences the general aim of this addendum is to present a refined text of sections 1 3 of that proposal and highlighting the contents of its appendix in form of a `mechanisms' section we then briefly remind in an article aimed for december 2007 , by appending two more equations into section 3 , a theoretical ii time scenario as a time model well proposed for the phenomenon the time model within the core of the proposal , plays a significant role in emphasizing the principle points on objectives no 1 8 , sub hypothesis 3 1 2 , mentioned in article arxiv 0710 0410 it also expresses the time concept in terms of causing quantized energy f \( e \) of time t , emit in regard to shortening the probability of particle loci as predictable patterns of particle 's un occurred motion , a solution to heisenberg 's uncertainty principle \( hup \) into a simplistic manner we conclude that , practical frames via a time algorithm to this model , fixates such predictable patterns of motion of scenery bodies onto recordable observation points of a mmmcv system it even suppresses predicts superposition phenomena coming from a human subject and or other bio subjects for any decision making event , e g , brainwave quantum patterns based on vision maintaining the existential probability of riemann surfaces of ii time scenarios in the context of biovielectroluminescence , makes motion prediction a possibility
explaining adaptive behavior is a central problem in artificial intelligence research here we formalize adaptive agents as mixture distributions over sequences of inputs and outputs \( i o \) each distribution of the mixture constitutes a `possible world' , but the agent does not know which of the possible worlds it is actually facing the problem is to adapt the i o stream in a way that is compatible with the true world a natural measure of adaptation can be obtained by the kullback leibler \( kl \) divergence between the i o distribution of the true world and the i o distribution expected by the agent that is uncertain about possible worlds in the case of pure input streams , the bayesian mixture provides a well known solution for this problem we show , however , that in the case of i o streams this solution breaks down , because outputs are issued by the agent itself and require a different probabilistic syntax as provided by intervention calculus based on this calculus , we obtain a bayesian control rule that allows modeling adaptive behavior with mixture distributions over i o streams this rule might allow for a novel approach to adaptive control based on a minimum kl principle
in this paper we propose a novel framework for decentralized , online learning by many learners at each moment of time , an instance characterized by a certain context may arrive to each learner based on the context , the learner can select one of its own actions \( which gives a reward and provides information \) or request assistance from another learner in the latter case , the requester pays a cost and receives the reward but the provider learns the information in our framework , learners are modeled as cooperative contextual bandits each learner seeks to maximize the expected reward from its arrivals , which involves trading off the reward received from its own actions , the information learned from its own actions , the reward received from the actions requested of others and the cost paid for these actions taking into account what it has learned about the value of assistance from each other learner we develop distributed online learning algorithms and provide analytic bounds to compare the efficiency of these with algorithms with the complete knowledge \( oracle \) benchmark \( in which the expected reward of every action in every context is known by every learner \) our estimates show that regret the loss incurred by the algorithm is sublinear in time our theoretical framework can be used in many practical applications including big data mining , event detection in surveillance sensor networks and distributed online recommendation systems
matrix completion under interval uncertainty can be cast as matrix completion with element wise box constraints we present an efficient alternating direction parallel coordinate descent method for the problem we show that the method outperforms any other known method on a benchmark in image in painting in terms of signal to noise ratio , and that it provides high quality solutions for an instance of collaborative filtering with 100 , 198 , 805 recommendations within 5 minutes
this paper presents a plan based architecture for response generation in collaborative consultation dialogues , with emphasis on cases in which the system \( consultant \) and user \( executing agent \) disagree our work contributes to an overall system for collaborative problem solving by providing a plan based framework that captures the em propose evaluate modify cycle of collaboration , and by allowing the system to initiate subdialogues to negotiate proposed additions to the shared plan and to provide support for its claims in addition , our system handles in a unified manner the negotiation of proposed domain actions , proposed problem solving actions , and beliefs proposed by discourse actions furthermore , it captures cooperative responses within the collaborative framework and accounts for why questions are sometimes never answered
bit retrieval is the problem of reconstructing a binary sequence from its periodic autocorrelation , with applications in cryptography and x ray crystallography after defining the problem , with and without noise , we describe and compare various algorithms for solving it a geometrical constraint satisfaction algorithm , relaxed reflect reflect , is currently the best algorithm for noisy bit retrieval
we describe information forests , an approach to classification that generalizes random forests by replacing the splitting criterion of non leaf nodes from a discriminative one based on the entropy of the label distribution to a generative one based on maximizing the information divergence between the class conditional distributions in the resulting partitions the basic idea consists of deferring classification until a measure of classification confidence is sufficiently high , and instead breaking down the data so as to maximize this measure in an alternative interpretation , information forests attempt to partition the data into subsets that are as informative as possible for the purpose of the task , which is to classify the data classification confidence , or informative content of the subsets , is quantified by the information divergence our approach relates to active learning , semi supervised learning , mixed generative discriminative learning
in a recent article \( proc natl acad sci , 110 \( 36 \) , 14557 14562 \) , el karoui et al study the distribution of robust regression estimators in the regime in which the number of parameters p is of the same order as the number of samples n using numerical simulations and `highly plausible' heuristic arguments , they unveil a striking new phenomenon namely , the regression coefficients contain an extra gaussian noise component that is not explained by classical concepts such as the fisher information matrix we show here that that this phenomenon can be characterized rigorously techniques that were developed by the authors to analyze the lasso estimator under high dimensional asymptotics we introduce an approximate message passing \( amp \) algorithm to compute m estimators and deploy state evolution to evaluate the operating characteristics of amp and so also m estimates our analysis clarifies that the `extra gaussian noise' encountered in this problem is fundamentally similar to phenomena already studied for regularized least squares in the setting n p
there is a recent surge of interest in developing algorithms for finding sparse solutions of underdetermined systems of linear equations y phi x in many applications , extremely large problem sizes are envisioned , with at least tens of thousands of equations and hundreds of thousands of unknowns for such problem sizes , low computational complexity is paramount the best studied ell 1 minimization algorithm is not fast enough to fulfill this need iterative thresholding algorithms have been proposed to address this problem in this paper we want to analyze two of these algorithms theoretically , and give sufficient conditions under which they recover the sparsest solution
s92 research was begun in 1987 to analyze word frequencies in present day spanish for making speech pathology evaluation tools 500 2 , 000 word samples of children , adolescents and adults' language were input between 1988 1991 , calculations done in 1992 statistical and lewandowski analyses were carried out in 1993
we develop a new lower bound method for analysing the complexity of the equality function \( eq \) in the simultaneous message passing \( smp \) model of communication complexity the new technique gives tight lower bounds of omega \( sqrt n \) for both eq and its negation ne in the non deterministic version of quantum classical smp , where merlin is also quantum this is the strongest known version of smp where the complexity of both eq and ne remain high \( previously known techniques seem to be insufficient for this \) besides , our analysis provides to a unified view of the communication complexity of eq and ne , allowing to obtain tight characterisation in all previously studied and a few newly introduced versions of smp , including all possible combination of either quantum or randomised alice , bob and merlin in the non deterministic case some of our results highlight that ne is easier than eq in the presence of classical proofs , whereas the problems have \( roughly \) the same complexity when a quantum proof is present
information dissemination is a fundamental problem in parallel and distributed computing in its simplest variant , the broadcasting problem , a message has to be spread among all nodes of a graph a prominent communication protocol for this problem is based on the random phone call model \( karp et al , focs 2000 \) in each step , every node opens a communication channel to a randomly chosen neighbor for bi directional communication motivated by replicated databases and peer to peer networks , berenbrink et al , icalp 2010 , considered the gossiping problem in the random phone call model there , each node starts with its own message and all messages have to be disseminated to all nodes in the network they showed that any o \( log n \) time algorithm in complete graphs requires omega \( log n \) message transmissions per node to complete gossiping , w h p , while for broadcasting the average number of transmissions per node is o \( log log n \) it is known that the o \( n log log n \) bound on the number of transmissions required for randomized broadcasting in complete graphs cannot be achieved in sparse graphs even if they have best expansion and connectivity properties in this paper , we analyze whether a similar influence of the graph density also holds w r t the performance of gossiping we study analytically and empirically the communication overhead generated by randomized gossiping in random graphs and consider simple modifications of the random phone call model in these graphs our results indicate that , unlike in broadcasting , there is no significant difference between the performance of randomized gossiping in complete graphs and sparse random graphs furthermore , our simulations indicate that by tuning the parameters of our algorithms , we can significantly reduce the communication overhead compared to the traditional push pull approach in the graphs we consider
in this paper , the problem of pilot beam pattern design for channel estimation in massive multiple input multiple output systems with a large number of transmit antennas at the base station is considered , and a new algorithm for pilot beam pattern design for optimal channel estimation is proposed under the assumption that the channel is a stationary gauss markov random process the proposed algorithm designs the pilot beam pattern sequentially by exploiting the properties of kalman filtering and the associated prediction error covariance matrices and also the channel statistics such as spatial and temporal channel correlation the resulting design generates a sequentially optimal sequence of pilot beam patterns with low complexity for a given set of system parameters numerical results show the effectiveness of the proposed algorithm
both small world models of random networks with occasional long range connections and gossip processes with occasional long range transmission of information have similar characteristic behaviour the long range elements appreciably reduce the effective distances , measured in space or in time , between pairs of typical points in this paper , we show that their common behaviour can be interpreted as a product of the locally branching nature of the models in particular , it is shown that both typical distances between points and the proportion of space that can be reached within a given distance or time can be approximated by formulae involving the limit random variable of the branching process
numerical algorithms have two kinds of costs arithmetic and communication , by which we mean either moving data between levels of a memory hierarchy \( in the sequential case \) or over a network connecting processors \( in the parallel case \) communication costs often dominate arithmetic costs , so it is of interest to design algorithms minimizing communication in this paper we first extend known lower bounds on the communication cost \( both for bandwidth and for latency \) of conventional \( o \( n 3 \) \) matrix multiplication to cholesky factorization , which is used for solving dense symmetric positive definite linear systems second , we compare the costs of various cholesky decomposition implementations to these lower bounds and identify the algorithms and data structures that attain them in the sequential case , we consider both the two level and hierarchical memory models combined with prior results in 13 , 14 , 15 , this gives a set of communication optimal algorithms for o \( n 3 \) implementations of the three basic factorizations of dense linear algebra lu with pivoting , qr and cholesky but it goes beyond this prior work on sequential lu by optimizing communication for any number of levels of memory hierarchy
we consider the task of learning mappings from sequential data to real valued responses we present and evaluate an approach to learning a type of hidden markov model \( hmm \) for regression the learning process involves inferring the structure and parameters of a conventional hmm , while simultaneously learning a regression model that maps features that characterize paths through the model to continuous responses our results , in both synthetic and biological domains , demonstrate the value of jointly learning the two components of our approach
transteg \( trancoding steganography \) is a fairly new ip telephony steganographic method that functions by compressing overt \( voice \) data to make space for the steganogram by means of transcoding it offers high steganographic bandwidth , retains good voice quality and is generally harder to detect than other existing voip steganographic methods in transteg , after the steganogram reaches the receiver , the hidden information is extracted and the speech data is practically restored to what was originally sent this is a huge advantage compared with other existing voip steganographic methods , where the hidden data can be extracted and removed but the original data cannot be restored because it was previously erased due to a hidden data insertion process in this paper we address the issue of steganalysis of transteg various transteg scenarios and possibilities of warden \( s \) localization are analyzed with regards to the transteg detection a steganalysis method based on mfcc \( mel frequency cepstral coefficients \) parameters and gmms \( gaussian mixture models \) was developed and tested for various overt covert codec pairs in a single warden scenario with double transcoding the proposed method allowed for efficient detection of some codec pairs \( e g , g 711 g 729 \) , whilst some others remained more resistant to detection \( e g , ilbc amr \)
modern software systems may exhibit a nondeterministic behavior due to many unpredictable factors in this work , we propose the node coverage game , a two player turn based game played on a finite game graph , as a formalization of the problem to test such systems each node in the graph represents a em functional equivalence class of the software under test \( sut \) one player , the tester , wants to maximize the node coverage , measured by the number of nodes visited when exploring the game graphs , while his opponent , the sut , wants to minimize it an optimal test would maximize the cover , and it is an interesting problem to find the maximal number of nodes that the tester can guarantee to visit , irrespective of the responses of the sut we show that the decision problem of whether the guarantee is less than a given number is np complete then we present techniques for testing nondeterministic suts with existing test suites for deterministic models finally , we report our implementation and experiments
we address the problem of social network de anonymization when relationships between people are described by scale free graphs in particular , we propose a rigorous , asymptotic mathematical analysis of the network de anonymization problem while capturing the impact of power law node degree distribution , which is a fundamental and quite ubiquitous feature of many complex systems such as social networks by applying bootstrap percolation and a novel graph slicing technique , we prove that large inhomogeneities in the node degree lead to a dramatic reduction of the initial set of nodes that must be known a priori \( the seeds \) in order to successfully identify all other users we characterize the size of this set when seeds are selected using different criteria , and we show that their number can be as small as n epsilon , for any small epsilon 0 our results are validated through simulation experiments on a real social network graph
we consider the problem of privately answering queries defined on databases which are collections of points belonging to some metric space we give simple , computationally efficient algorithms for answering distance queries defined over an arbitrary metric distance queries are specified by points in the metric space , and ask for the average distance from the query point to the points contained in the database , according to the specified metric our algorithms run efficiently in the database size and the dimension of the space , and operate in both the online query release setting , and the offline setting in which they must in polynomial time generate a fixed data structure which can answer all queries of interest this represents one of the first subclasses of linear queries for which efficient algorithms are known for the private query release problem , circumventing known hardness results for generic linear queries
in this paper , we focus our attention on the large capacities unsplittable flow problem in a game theoretic setting in this setting , there are selfish agents , which control some of the requests characteristics , and may be dishonest about them it is worth noting that in game theoretic settings many standard techniques , such as randomized rounding , violate certain monotonicity properties , which are imperative for truthfulness , and therefore cannot be employed in light of this state of affairs , we design a monotone deterministic algorithm , which is based on a primal dual machinery , which attains an approximation ratio of frac e e 1 , up to a disparity of epsilon away this implies an improvement on the current best truthful mechanism , as well as an improvement on the current best combinatorial algorithm for the problem under consideration surprisingly , we demonstrate that any algorithm in the family of reasonable iterative path minimizing algorithms , cannot yield a better approximation ratio consequently , it follows that in order to achieve a monotone ptas , if exists , one would have to exert different techniques we also consider the large capacities textit single minded multi unit combinatorial auction problem this problem is closely related to the unsplittable flow problem since one can formulate it as a special case of the integer linear program of the unsplittable flow problem accordingly , we obtain a comparable performance guarantee by refining the algorithm suggested for the unsplittable flow problem
we consider a wireless device to device \( d2d \) network where communication is restricted to be single hop users make arbitrary requests from a finite library of files and have pre cached information on their devices , subject to a per node storage capacity constraint a similar problem has already been considered in an ``infrastructure'' setting , where all users receive a common multicast \( coded \) message from a single omniscient server \( e g , a base station having all the files in the library \) through a shared bottleneck link in this work , we consider a d2d ``infrastructure less'' version of the problem we propose a caching strategy based on deterministic assignment of subpackets of the library files , and a coded delivery strategy where the users send linearly coded messages to each other in order to collectively satisfy their demands we also consider a random caching strategy , which is more suitable to a fully decentralized implementation under certain conditions , both approaches can achieve the information theoretic outer bound within a constant multiplicative factor in our previous work , we showed that a caching d2d wireless network with one hop communication , random caching , and uncoded delivery , achieves the same throughput scaling law of the infrastructure based coded multicasting scheme , in the regime of large number of users and files in the library this shows that the spatial reuse gain of the d2d network is order equivalent to the coded multicasting gain of single base station transmission it is therefore natural to ask whether these two gains are cumulative , i e , if a d2d network with both local communication \( spatial reuse \) and coded multicasting can provide an improved scaling law somewhat counterintuitively , we show that these gains do not cumulate \( in terms of throughput scaling law \)
we deterministically crack the secure , statistical key exchange protocol based on feedback proposed by pao lo liu j lightwave techology 27 \( 2009 \) pp 5230 34 the crack is ultimate and absolute because it works under idealized conditions , and produces much higher data visibility for the eavesdropper than the protocol provides for alice and bob even with the most idealistic driving noise spectrum stated by liu , during the most secure phase of the protocol , far away from the transients , where the system is already in its most secure steady state , the eavesdropper has 100 success rate in identifying the key bits , at the same time when alice and bob have less than 100 success rate while using the liu protocol no statistics is needed , eve can extract the secure bit from two samples of the signal in the two direction thus the liu protocol offers no security against the attack described in this paper
in this paper , code pairs based on trellis coded modulation are proposed over psk signal sets for a two user gaussian multiple access channel in order to provide unique decodability property to the receiver and to maximally enlarge the constellation constrained \( cc \) capacity region , a relative angle of rotation is introduced between the signal sets subsequently , the structure of the textit sum alphabet of two psk signal sets is exploited to prove that ungerboeck labelling on the trellis of each user maximizes the guaranteed minimum squared euclidean distance , d 2 g , min in the textit sum trellis hence , such a labelling scheme can be used systematically to construct trellis code pairs for a two user gmac to approach emph any rate pair within the capacity region
the problem of pi 1 separating the hierarchy of bounded arithmetic has been studied in the paper it is shown that the notion of herbrand consistency , in its full generality , cannot pi 1 separate the theory rm i delta 0 bigwedge j omega j from rm i delta 0 though it can pi 1 separate rm i delta 0 exp from rm i delta 0 this extends a result of l a ko l odziejczyk \( 2006 \) , by showing the unprovability of the herbrand consistency of rm i delta 0 in the theory rm i delta 0 bigwedge j omega j
this encyclopedic article gives a mini introduction into the theory of universal learning , founded by ray solomonoff in the 1960s and significantly developed and extended in the last decade it explains the spirit of universal learning , but necessarily glosses over technical subtleties
one of the most efficient methods of exploiting space diversity for portable wireless devices is cooperative communication utilizing space time block codes in cooperative communication , users besides communicating their own information , also relay the information of other users in this paper we investigate a scheme where cooperation is achieved using two methods , namely , distributed space time coding and network coding two cooperating users utilize alamouti space time code for inter user cooperation and in addition utilize a third relay which performs network coding the third relay does not have any of its information to be sent in this paper we propose a scheme utilizing convolutional code based network coding , instead of conventional xor based network code and utilize iterative joint network channel decoder for efficient decoding extrinsic information transfer \( exit \) chart analysis is performed to investigate the convergence property of the proposed decoder
a framework for virtual reality of engineering objects has been developed this framework may simulate different equipment related to virtual reality framework supports 6d dynamics , ordinary differential equations , finite formulas , vector and matrix operations the framework also supports embedding of external software
in this work we explain the implementation of event driven real time interpreters for the concurrent constraint programming \( ccp \) and non deterministic timed concurrent constraint \( ntcc \) for malisms the ccp interpreter was tested with a program to find , concurrently , paths in a graph and it will be used in the future to find musical sequences in the music improvisation software omax , developed by the french acoustics music research institute \( ircam \) in the other hand , the ntcc interpreter was tested with a music improvisation system based on ntcc \( ccfomi \) , developed by the avispa research group and ircam additionally , we present gecol 2 , a wrapper for the generic constraints development environment \( gecode \) to common lisp , de veloped to port the interpreters to common lisp in the future we concluded that using gecode for the concurrency control avoids the need of having threads and synchronizing them , leading to a simple and efficient implementation of ccp and ntcc we also noticed that the time units in ntcc interpreter do not represent discrete time units , because when we simulate the ntcc specifications in the interpreter , the time units have different durations in the future , we propose forcing the duration of each time unit to a fix time , that way we would be able to reason about ntcc time units as we do with discrete time units
previous work shows request tracing systems help understand and debug the performance problems of multi tier services however , for large scale data centers , more than hundreds of thousands of service instances provide online service at the same time previous work such as white box or black box tracing systems will produce large amount of log data , which would be correlated into large quantities of causal paths for performance debugging in this paper , we propose an innovative algorithm to eliminate valueless logs of multitiers services our experiment shows our method filters 84 valueless causal paths and is promising to be used in large scale data centers
with the explosive growth of internet technology , easy transfer of digital multimedia is feasible however , this kind of convenience with which authorized users can access information , turns out to be a mixed blessing due to information piracy the emerging field of digital rights management \( drm \) systems addresses issues related to the intellectual property rights of digital content in this paper , an object oriented \( oo \) drm system , called imaging system with watermarking and attack resilience \( iswar \) , is presented that generates and authenticates color images with embedded mechanisms for protection against infringement of ownership rights as well as security attacks in addition to the methods , in the object oriented sense , for performing traditional encryption and decryption , the system implements methods for visible and invisible watermarking this paper presents one visible and one invisible watermarking algorithm that have been integrated in the system the qualitative and quantitative results obtained for these two watermarking algorithms with several benchmark images indicate that high quality watermarked images are produced by the algorithms with the help of experimental results it is demonstrated that the presented invisible watermarking techniques are resilient to the well known benchmark attacks and hence a fail safe method for providing constant protection to ownership rights
in smart grid , a home appliance can adjust its power consumption level according to the realtime power price obtained from communication channels most studies on smart grid do not consider the cost of communications which cannot be ignored in many situations therefore , the total cost in smart grid should be jointly optimized with the communication cost in this paper , a probabilistic mechanism of locational margin price \( lmp \) is applied and a model for the stochastic evolution of the underlying load which determines the power price is proposed based on this framework of power price , the problem of determining when to inquire the power price is formulated as a markov decision process and the corresponding elements , namely the action space , system state and reward function , are defined dynamic programming is then applied to obtain the optimal strategy a simpler myopic approach is proposed by comparing the cost of communications and the penalty incurred by using the old value of power price numerical results show the significant performance gain of the optimal strategy of price inquiry , as well as the near optimality of the myopic approach
the scientific community is becoming more and more interested in the research that applies the mathematical formalism of quantum theory to model human decision making in this paper , we provide the theoretical foundations of the quantum approach to cognition that we developed in brussels these foundations rest on the results of two decade studies on the axiomatic and operational realistic approaches to the foundations of quantum physics the deep analogies between the foundations of physics and cognition lead us to investigate the validity of quantum theory as a general and unitary framework for cognitive processes , and the empirical success of the hilbert space models derived by such investigation provides a strong theoretical confirmation of this validity however , two situations in the cognitive realm , 'question order effects' and 'response replicability' , indicate that even the hilbert space framework could be insufficient to reproduce the collected data this does not mean that the mentioned operational realistic approach would be incorrect , but simply that a larger class of measurements would be in force in human cognition , so that an extended quantum formalism may be needed to deal with all of them as we will explain , the recently derived 'extended bloch representation' of quantum theory \( and the associated 'general tension reduction' model \) precisely provides such extended formalism , while remaining within the same unitary interpretative framework
in this paper , we give a causal solution to the problem of spline interpolation using h infinity optimal approximation generally speaking , spline interpolation requires filtering the whole sampled data , the past and the future , to reconstruct the inter sample values this leads to non causality of the filter , and this becomes a critical issue for real time applications our objective here is to derive a causal system which approximates spline interpolation by h infinity optimization for the filter the advantage of h infinity optimization is that it can address uncertainty in the input signals to be interpolated in design , and hence the optimized system has robustness property against signal uncertainty we give a closed form solution to the h infinity optimization in the case of the cubic splines for higher order splines , the optimal filter can be effectively solved by a numerical computation we also show that the optimal fir \( finite impulse response \) filter can be designed by an lmi \( linear matrix inequality \) , which can also be effectively solved numerically a design example is presented to illustrate the result
in many massively parallel data management platforms , programs are represented as small imperative pieces of code connected in a data flow this popular abstraction makes it hard to apply algebraic reordering techniques employed by relational dbmss and other systems that use an algebraic programming abstraction we present a code analysis technique based on reverse data and control flow analysis that discovers a set of properties from user code , which can be used to emulate algebraic optimizations in this setting
modern software systems often consist of many different components , each with a number of options although unit tests may reveal faulty options for individual components , functionally correct components may interact in unforeseen ways to cause a fault covering arrays are used to test for interactions among components systematically a two stage framework , providing a number of concrete algorithms , is developed for the efficient construction of covering arrays our framework divides the construction in two stages in the first stage , a time and memory efficient randomized algorithm covers most of the interactions in the second stage , a more sophisticated search covers the remainder in relatively few tests in this way , the storage limitations of the sophisticated search algorithms are avoided hence the range of the number of components for which the algorithm can be applied is extended , without increasing the number of tests many of the framework instantiations can be tuned to optimize a memory quality trade off , so that fewer tests can be achieved using more memory the algorithms developed outperform the currently best known methods when the number of components ranges from 20 to 60 , the number of options for each ranges from 3 to 6 , and t way interactions are covered for t in 5 , 6 in some cases a reduction in the number of tests by more than 50 is achieved
functional networks of complex systems are obtained from the analysis of the temporal activity of their components , and are often used to infer their unknown underlying connectivity we obtain the equations relating topology and function in a system of diffusively delay coupled elements in complex networks we solve exactly the resulting equations in motifs \( directed structures of three nodes \) , and in directed networks the mean field solution for directed uncorrelated networks shows that the clusterization of the activity is dominated by the in degree of the nodes , and that the locking frequency decreases with increasing average degree we find that the exponent of a power law degree distribution of the structural topology , b , is related to the exponent of the associated functional network as a 1 \( 2 b \) , for b 2
a polynomial time algorithm which detects all paths and cycles of all lengths in form of vertex pairs \( start , finish \)
in this paper , a novel framework based on trace norm minimization for audio segment is proposed in this framework , both the feature extraction and classification are obtained by solving corresponding convex optimization problem with trace norm regularization for feature extraction , robust principle component analysis \( robust pca \) via minimization a combination of the nuclear norm and the ell 1 norm is used to extract low rank features which are robust to white noise and gross corruption for audio segments these low rank features are fed to a linear classifier where the weight and bias are learned by solving similar trace norm constrained problems for this classifier , most methods find the weight and bias in batch mode learning , which makes them inefficient for large scale problems in this paper , we propose an online framework using accelerated proximal gradient method this framework has a main advantage in memory cost in addition , as a result of the regularization formulation of matrix classification , the lipschitz constant was given explicitly , and hence the step size estimation of general proximal gradient method was omitted in our approach experiments on real data sets for laugh non laugh and applause non applause classification indicate that this novel framework is effective and noise robust
in this paper , we propose a novel class of nash problems for cognitive radio \( cr \) networks composed of multiple primary users \( pus \) and secondary users \( sus \) wherein each su \( player \) competes against the others to maximize his own opportunistic throughput by choosing jointly the sensing duration , the detection thresholds , and the vector power allocation over a multichannel link in addition to power budget constraints , several \( deterministic or probabilistic \) interference constraints can be accommodated in the proposed general formulation , such as constraints on the maximum individual aggregate \( probabilistic \) interference tolerable from the pus to keep the optimization as decentralized as possible , global interference constraints , when present , are imposed via pricing the prices are thus additional variables to be optimized the resulting players' optimization problems are nonconvex and there are price clearance conditions associated with the nonconvex global interference constraints to be satisfied by the equilibria of the game , which make the analysis of the proposed game a challenging task none of classical results in the game theory literature can be successfully applied to deal with the nonconvexity of the game , we introduce a relaxed equilibrium concept , the quasi nash equilibrium \( qne \) , and study its main properties , performance , and connection with local nash equilibria quite interestingly , the proposed game theoretical formulations yield a considerable performance improvement with respect to current centralized and decentralized designs of cr systems , which validates the concept of qne
let f be a uniformly distributed random k sat formula with n variables and m clauses non constructive arguments show that f is satisfiable for clause variable ratios m n r \( k \) 2 k ln 2 with high probability yet no efficient algorithm is know to find a satisfying assignment for densities as low as m n r \( k \) ln \( k \) k with a non vanishing probability in fact , the density m n r \( k \) ln \( k \) k seems to form a barrier for a broad class of local search algorithms one of the very few algorithms that plausibly seemed capable of breaking this barrier is a message passing algorithm called belief propagation guided decimation it was put forward on the basis of deep but non rigorous statistical mechanics considerations experiments conducted for k 3 , 4 , 5 suggested that the algorithm might succeed for densities very close to r k furnishing the first rigorous analysis of bp decimation , the present paper shows that the algorithm fails to find a satisfying assignment already for m n c r \( k \) k , for a constant c 0 \( independent of k \)
in many robotic domains such as flexible automated manufacturing or personal assistance , a fundamental perception task is that of identifying and localizing objects whose 3d models are known canonical approaches to this problem include discriminative methods that find correspondences between feature descriptors computed over the model and observed data while these methods have been employed successfully , they can be unreliable when the feature descriptors cannot capture variations in observed data a classic example being occlusion as a step towards deliberative reasoning , we present perch perception via search , an algorithm that seeks to find the best explanation of the observed sensor data by hypothesizing possible scenes in a generative fashion our contributions are i \) formulating the multi object recognition and localization task as an optimization problem over the space of hypothesized scenes , ii \) exploiting structure in the optimization to cast it as a combinatorial search problem on what we call the monotone scene generation tree , and iii \) leveraging parallelization and recent advances in multi heuristic search in making combinatorial search tractable we prove that our system can guaranteeably produce the best explanation of the scene under the chosen cost function , and validate our claims on real world rgb d test data our experimental results show that we can identify and localize objects under heavy occlusion cases where state of the art methods struggle
semi non negative matrix factorization is a technique that learns a low dimensional representation of a dataset that lends itself to a clustering interpretation it is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower level hidden attributes , that classical one level clustering methodologies can not interpret in this work we propose a novel model , deep semi nmf , that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different , unknown attributes of a given dataset we also present a semi supervised version of the algorithm , named deep wsf , that allows the use of \( partial \) prior information for each of the known attributes of a dataset , that allows the model to be used on datasets with mixed attribute knowledge finally , we show that our models are able to learn low dimensional representations that are better suited for clustering , but also classification , outperforming semi non negative matrix factorization , but also other state of the art methodologies variants
we propose and study a new class of problem , called finding achievable region \( far \) let o be a set of n disjoint obstacles in r 2 , m be a moving object let s and l denote the starting point and maximum path length of the moving object m , respectively given a point p in r 2 , we say the point p is achievable for m such that pi \( s , p \) leq l , where pi \( cdot \) denotes the shortest path length in the presence of obstacles the far problem is to find a region mathscr r such that , for any point p in r 2 , if it is achievable for m , then p in mathscr r otherwise , p notin mathscr r the far problem has a variety of applications ranging from location based service , probabilistic answer on uncertain data , to moving target search in this paper , we restrict our attention to the case of line segment obstacles to tackle this problem , we develop three algorithms we first present a simpler version algorithm for the sake of intuition its basic idea is to reduce our problem to computing the union of a set of circular visibility regions \( cvrs \) in order to obtain the cvrs , we adopt the spatial pruning mechanism and incorporate the idea of the rotational plane sweep furthermore , for most cvrs , the circles used to construct them are unavailable beforehand we adopt the visibility graph technique to obtain those circles this algorithm takes o \( n 3 \) time by analysing its dominant steps , we break through its bottleneck by using the short path map \( spm \) technique to obtain those circles \( unavailable beforehand \) , yielding an o \( n 2 log n \) algorithm owing to the finding above , the third algorithm also uses the spm technique it however , does not continue to construct the cvrs instead , it directly traverses each region of the spm to trace the boundaries , the final algorithm obtains the o \( n log n \) worst case upper bound
as one of the newest members in artificial immune systems \( ais \) , the dendritic cell algorithm \( dca \) has been applied to a range of problems these applications mainly belong to the field of anomaly detection however , real time detection , a new challenge to anomaly detection , requires improvement on the real time capability of the dca to assess such capability , formal methods in the research of rea time systems can be employed the findings of the assessment can provide guideline for the future development of the algorithm therefore , in this paper we use an interval logic based method , named the duration calculus \( dc \) , to specify a simplified single cell model of the dca based on the dc specifications with further induction , we find that each individual cell in the dca can perform its function as a detector in real time since the dca can be seen as many such cells operating in parallel , it is potentially capable of performing real time detection however , the analysis process of the standard dca constricts its real time capability as a result , we conclude that the analysis process of the standard dca should be replaced by a real time analysis component , which can perform periodic analysis for the purpose of real time detection
a long haul transmission of 100 gb s without optical chromatic dispersion \( cd \) compensation provides a range of benefits regarding cost effectiveness , power budget , and nonlinearity tolerance the channel memory is largely dominated by cd in this case with an intersymbol interference spread of more than 100 symbol durations in this paper , we propose cd equalization technique based on nonmaximally decimated discrete fourier transform \( nmdft \) filter bank \( fb \) with non trivial prototype filter and complex valued infinite impulse response \( iir \) all pass filter per sub band the design of the sub band iir all pass filter is based on minimizing the mean square error \( mse \) in group delay and phase cost functions in an optimization framework necessary conditions are derived and incorporated in a multi step and multi band optimization framework to ensure the stability of the resulting iir filter it is shown that the complexity of the proposed method grows logarithmically with the channel memory , therefore , larger cd values can be tolerated with our approach
the most important aim in tool path generation methods is to increase the machining efficiency by minimizing the total length of tool paths while the error is kept under a prescribed tolerance this can be achieved by determining the moving direction of the cutting tool such that the machined stripe is the widest from a technical point of view it is recommended that the angle between the tool axis and the surface normal does not change too much along the tool path in order to ensure even abrasion of the tool in this paper a mathematical method for tool path generation in 3 axis milling is presented , which considers these requirements by combining the features of isophotic curves and principal curvatures it calculates the proposed moving direction of the tool at each point of the surface the proposed direction depends on the measurement of the tool and on the curvature values of the surface for triangulated surfaces a new local offset computation method is presented , which is suitable also for detecting tool collision with the target surface and self intersection in the offset mesh
dynamic bayesian networks provide a compact and natural representation for complex dynamic systems however , in many cases , there is no expert available from whom a model can be elicited learning provides an alternative approach for constructing models of dynamic systems in this paper , we address some of the crucial computational aspects of learning the structure of dynamic systems , particularly those where some relevant variables are partially observed or even entirely unknown our approach is based on the structural expectation maximization \( sem \) algorithm the main computational cost of the sem algorithm is the gathering of expected sufficient statistics we propose a novel approximation scheme that allows these sufficient statistics to be computed efficiently we also investigate the fundamental problem of discovering the existence of hidden variables without exhaustive and expensive search our approach is based on the observation that , in dynamic systems , ignoring a hidden variable typically results in a violation of the markov property thus , our algorithm searches for such violations in the data , and introduces hidden variables to explain them we provide empirical results showing that the algorithm is able to learn the dynamics of complex systems in a computationally tractable way
for decades , the growth and volume of digital data collection has made it challenging to digest large volumes of information and extract underlying structure coined 'big data' , massive amounts of information has quite often been gathered inconsistently \( e g from many sources , of various forms , at different rates , etc \) these factors impede the practices of not only processing data , but also analyzing and displaying it in an efficient manner to the user many efforts have been completed in the data mining and visual analytics community to create effective ways to further improve analysis and achieve the knowledge desired for better understanding our approach for improved big data visual analytics is two fold , focusing on both visualization and interaction given geo tagged information , we are exploring the benefits of visualizing datasets in the original geospatial domain by utilizing a virtual reality platform after running proven analytics on the data , we intend to represent the information in a more realistic 3d setting , where analysts can achieve an enhanced situational awareness and rely on familiar perceptions to draw in depth conclusions on the dataset in addition , developing a human computer interface that responds to natural user actions and inputs creates a more intuitive environment tasks can be performed to manipulate the dataset and allow users to dive deeper upon request , adhering to desired demands and intentions due to the volume and popularity of social media , we developed a 3d tool visualizing twitter on mit 's campus for analysis utilizing emerging technologies of today to create a fully immersive tool that promotes visualization and interaction can help ease the process of understanding and representing big data
in this paper , we present a factor 16 approximation algorithm for the following np hard distance fitting problem given a finite set x and a distance d on x , find a robinsonian distance dr on x minimizing the l infty error d dr infty maxx , y epsilonx d \( x , y \) dr \( x , y \) a distance dr on a finite set x is robinsonian if its matrix can be symmetrically permuted so that its elements do not decrease when moving away from the main diagonal along any row or column robinsonian distances generalize ultrametrics , line distances and occur in the seriation problems and in classification
quantum money is a cryptographic protocol in which a mint can produce a quantum state , no one else can copy the state , and anyone \( with a quantum computer \) can verify that the state came from the mint we present a concrete quantum money scheme based on superpositions of diagrams that encode oriented links with the same alexander polynomial we expect our scheme to be secure against computationally bounded adversaries
the use of free and open source software is gaining momentum due to the ever increasing availability and use of the internet organizations are also now adopting open source software , despite some reservations in particular regarding the provision and availability of support one of the greatest concerns about free and open source software is the availability of post release support and the handling of for support a common belief is that there is no appropriate support available for this class of software , while an alternative argument is that due to the active involvement of internet users in online forums , there is in fact a large resource available that communicates and manages the management of support requests the research model of this empirical investigation establishes and studies the relationship between open source software support requests and online public forums the results of this empirical study provide evidence about the realities of support that is present in open source software projects we used a dataset consisting of 616 open source software projects covering a broad range of categories in this investigation the results show that online forums play a significant role in managing support requests in open source software , thus becoming a major source of assistance in maintenance of the open source projects
we compare mechanisms for compensation handling and dynamic update in calculi for concurrency these mechanisms are increasingly relevant in the specification of reliable communicating systems compensations and updates are intuitively similar both specify how the behavior of a concurrent system changes at runtime in response to an exceptional event however , calculi with compensations and updates are technically quite different we investigate the relative expressiveness of these calculi we develop encodings of core process languages with compensations into a calculus of adaptable processes developed in prior work our encodings shed light on the \( intricate \) semantics of compensation handling and its key constructs they also enable the transference of existing verification and reasoning techniques for adaptable processes to core languages with compensation handling
we apply a tree based methodology to solve new , very broadly defined families of nested recursions of the general form r \( n \) sum i 1 k r \( n a i sum j 1 p r \( n b ij \) \) , where a i are integers , b ij are natural numbers , and k , p are natural numbers that we use to denote arity and order , respectively , and with some specified initial conditions the key idea of the tree based solution method is to associate such recursions with infinite labelled trees in a natural way so that the solution to the recursions solves a counting question relating to the corresponding trees we characterize certain recursion families within r \( n \) by introducing simultaneous parameters that appear both within the recursion itself and that also specify structural properties of the corresponding tree first , we extend and unify recently discovered results concerning two families of arity k 2 , order p 1 recursions next , we investigate the solution of nested recursion families by taking linear combinations of solution sequence frequencies for simpler nested recursions , which correspond to superpositions of the associated trees this leads us to identify and solve two new recursion families for arity k 2 and general order p finally , we extend these results to general arity k 2 we conclude with several related open problems
compact categories have lately seen renewed interest via applications to quantum physics being essentially finite dimensional , they cannot accomodate \( co \) limit based constructions for example , they cannot capture protocols such as quantum key distribution , that rely on the law of large numbers to overcome this limitation , we introduce the notion of a compactly accessible category , relying on the extra structure of a factorisation system this notion allows for infinite dimension while retaining key properties of compact categories the main technical result is that the choice of duals functor on the compact part extends canonically to the whole compactly accessible category as an example , we model a quantum key distribution protocol and prove its correctness categorically
we study the connection between the order of phase transitions in combinatorial problems and the complexity of decision algorithms for such problems we rigorously show that , for a class of random constraint satisfaction problems , a limited connection between the two phenomena indeed exists specifically , we extend the definition of the spine order parameter of bollobas et al to random constraint satisfaction problems , rigorously showing that for such problems a discontinuity of the spine is associated with a 2 omega \( n \) resolution complexity \( and thus a 2 omega \( n \) complexity of dpll algorithms \) on random instances the two phenomena have a common underlying cause the emergence of ``large'' \( linear size \) minimally unsatisfiable subformulas of a random formula at the satisfiability phase transition we present several further results that add weight to the intuition that random constraint satisfaction problems with a sharp threshold and a continuous spine are ``qualitatively similar to random 2 sat'' finally , we argue that it is the spine rather than the backbone parameter whose continuity has implications for the decision complexity of combinatorial problems , and we provide experimental evidence that the two parameters can behave in a different manner
we present a novel technique to remove spurious ambiguity from transition systems for dependency parsing our technique chooses a canonical sequence of transition operations \( computation \) for a given dependency tree our technique can be applied to a large class of bottom up transition systems , including for instance nivre \( 2004 \) and attardi \( 2006 \)
this paper examines the role and efficiency of the non convex loss functions for binary classification problems in particular , we investigate how to design a simple and effective boosting algorithm that is robust to the outliers in the data the analysis of the role of a particular non convex loss for prediction accuracy varies depending on the diminishing tail properties of the gradient of the loss the ability of the loss to efficiently adapt to the outlying data , the local convex properties of the loss and the proportion of the contaminated data in order to use these properties efficiently , we propose a new family of non convex losses named gamma robust losses moreover , we present a new boosting framework , it arch boost , designed for augmenting the existing work such that its corresponding classification algorithm is significantly more adaptable to the unknown data contamination along with the arch boosting framework , the non convex losses lead to the new class of boosting algorithms , named adaptive , robust , boosting \( arb \) furthermore , we present theoretical examples that demonstrate the robustness properties of the proposed algorithms in particular , we develop a new breakdown point analysis and a new influence function analysis that demonstrate gains in robustness moreover , we present new theoretical results , based only on local curvatures , which may be used to establish statistical and optimization properties of the proposed arch boosting algorithms with highly non convex loss functions extensive numerical calculations are used to illustrate these theoretical properties and reveal advantages over the existing boosting methods when data exhibits a number of outliers
entity linking is an indispensable operation of populating knowledge repositories for information extraction it studies on aligning a textual entity mention to its corresponding disambiguated entry in a knowledge repository in this paper , we propose a new paradigm named distantly supervised entity linking \( dsel \) , in the sense that the disambiguated entities that belong to a huge knowledge repository \( freebase \) are automatically aligned to the corresponding descriptive webpages \( wiki pages \) in this way , a large scale of weakly labeled data can be generated without manual annotation and fed to a classifier for linking more newly discovered entities compared with traditional paradigms based on solo knowledge base , dsel benefits more via jointly leveraging the respective advantages of freebase and wikipedia specifically , the proposed paradigm facilitates bridging the disambiguated labels \( freebase \) of entities and their textual descriptions \( wikipedia \) for web scale entities experiments conducted on a dataset of 140 , 000 items and 60 , 000 features achieve a baseline f1 measure of 0 517 furthermore , we analyze the feature performance and improve the f1 measure to 0 545
coding for the causal cognitive radio channel , with the cognitive source subjected to a half duplex constraint , is studied a discrete memoryless channel model incorporating the half duplex constraint is presented , and a new achievable rate region is derived for this channel it is proved that this rate region contains the previously known causal achievable rate region of cite devroye06 for gaussian channels
web page categorization is one of the challenging tasks in the world of ever increasing web technologies there are many ways of categorization of web pages based on different approach and features this paper proposes a new dimension in the way of categorization of web pages using artificial neural network \( ann \) through extracting the features automatically here eight major categories of web pages have been selected for categorization these are business economy , education , government , entertainment , sports , news media , job search , and science the whole process of the proposed system is done in three successive stages in the first stage , the features are automatically extracted through analyzing the source of the web pages the second stage includes fixing the input values of the neural network all the values remain between 0 and 1 the variations in those values affect the output finally the third stage determines the class of a certain web page out of eight predefined classes this stage is done using back propagation algorithm of artificial neural network the proposed concept will facilitate web mining , retrievals of information from the web and also the search engines
this study analyses turkish syntax from an informational point of view sign based linguistic representation and principles of hpsg \( head driven phrase structure grammar \) theory are adapted to turkish the basic informational elements are nested and inherently sorted feature structures called signs in the implementation , logic programming tool ale \( attribute logic engine \) which is primarily designed for implementing hpsg grammars is used a type and structure hierarchy of turkish language is designed syntactic phenomena such a s subcategorization , relative clauses , constituent order variation , adjuncts , nomina l predicates and complement modifier relations in turkish are analyzed a parser is designed and implemented in ale
plagiarism detection systems comprise various approaches that aim to create a fair environment for academic publications and appropriately acknowledge the authors' works while the need for a reliable and performant plagiarism detection system increases with an increasing amount of publications , current systems still have shortcomings particularly intelligent research plagiarism detection still leaves room for improvement an important factor for progress in research is a suitable evaluation framework in this paper , we give an overview on the evaluation of plagiarism detection we then use a taxonomy provided in former research , to classify recent approaches of plagiarism detection based on this , we asses the current research situation in the field of plagiarism detection and derive further research questions and approaches to be tackled in the future
obtaining accepting lassos , witnesses and winning strategies in omega automata and games with omega regular winning conditions is an integral part of many formal methods commonly found in practice today despite the fact that in most applications , the lassos , witnesses and strategies found should be as small as possible , little is known about the hardness of obtaining small such certificates in this paper , we survey the known hardness results and complete the complexity landscape for the cases not considered in the literature so far we pay particular attention to the approximation hardness of the problems as approximate small solutions usually suffice in practice
we study the distributed detection problem in a balanced binary relay tree , where the leaves of the tree are sensors generating binary messages the root of the tree is a fusion center that makes the overall decision every other node in the tree is a fusion node that fuses two binary messages from its child nodes into a new binary message and sends it to the parent node at the next level we assume that the fusion nodes at the same level use the same fusion rule we call a string of fusion rules used at different levels a fusion strategy we consider the problem of finding a fusion strategy that maximizes the reduction in the total error probability between the sensors and the fusion center we formulate this problem as a deterministic dynamic program and express the solution in terms of bellman 's equations we introduce the notion of stringsubmodularity and show that the reduction in the total error probability is a stringsubmodular function consequentially , we show that the greedy strategy , which only maximizes the level wise reduction in the total error probability , is within a factor of the optimal strategy in terms of reduction in the total error probability
consider a gaussian multiple input multiple output \( mimo \) multiple access channel \( mac \) with channel matrix mathbf h and a gaussian mimo broadcast channel \( bc \) with channel matrix mathbf h t for the mimo mac , the integer forcing architecture consists of first decoding integer linear combinations of the transmitted codewords , which are then solved for the original messages for the mimo bc , the integer forcing architecture consists of pre inverting the integer linear combinations at the transmitter so that each receiver can obtain its desired codeword by decoding an integer linear combination in both cases , integer forcing offers higher achievable rates than zero forcing while maintaining a similar implementation complexity this paper establishes an uplink downlink duality relationship for integer forcing , i e , any sum rate that is achievable via integer forcing on the mimo mac can be achieved via integer forcing on the mimo bc with the same sum power and vice versa recent work has also shown that integer forcing for the mimo mac can be enhanced via successive cancellation this paper introduces dirty paper integer forcing for the mimo bc and demonstrates that it satisfies an uplink downlink duality with successive integer forcing for the mimo mac
the minimum distance of a code is an important concept in information theory hence , computing the minimum distance of a code with a minimum computational cost is a crucial process to many problems in this area in this paper , we present and evaluate a family of algorithms and implementations to compute the minimum distance of a random linear code over mathbb f 2 that are faster than different current implementations in addition to the basic sequential implementations , we present parallel and vectorized implementations that render high performances on modern architectures the attained performance results show the benefits of the developed optimized algorithms , which obtain remarkable performance improvements compared with state of the art implementations widely used nowadays
the new york public library is participating in the chronicling america initiative to develop an online searchable database of historically significant newspaper articles microfilm copies of the newspapers are scanned and high resolution optical character recognition \( ocr \) software is run on them the text from the ocr provides a wealth of data and opinion for researchers and historians however , categorization of articles provided by the ocr engine is rudimentary and a large number of the articles are labeled editorial without further grouping manually sorting articles into fine grained categories is time consuming if not impossible given the size of the corpus this paper studies techniques for automatic categorization of newspaper articles so as to enhance search and retrieval on the archive we explore unsupervised \( e g kmeans \) and semi supervised \( e g constrained clustering \) learning algorithms to develop article categorization schemes geared towards the needs of end users a pilot study was designed to understand whether there was unanimous agreement amongst patrons regarding how articles can be categorized it was found that the task was very subjective and consequently automated algorithms that could deal with subjective labels were used while the small scale pilot study was extremely helpful in designing machine learning algorithms , a much larger system needs to be developed to collect annotations from users of the archive the bodhi system currently being developed is a step in that direction , allowing users to correct wrongly scanned ocr and providing keywords and tags for newspaper articles used frequently on successful implementation of the beta version of this system , we hope that it can be integrated with existing software being developed for the chronicling america project
difference sets and their generalisations to difference families arise from the study of designs and many other applications here we give a brief survey of some of these applications , noting in particular the diverse definitions of difference families and the variations in priorities in constructions we propose a definition of disjoint difference families that encompasses these variations and allows a comparison of the similarities and disparities we then focus on two constructions of disjoint difference families arising from frequency hopping sequences and showed that they are in fact the same we conclude with a discussion of the notion of equivalence for frequency hopping sequences and for disjoint difference families
sparse learning techniques have been routinely used for feature selection as the resulting model usually has a small number of non zero entries safe screening , which eliminates the features that are guaranteed to have zero coefficients for a certain value of the regularization parameter , is a technique for improving the computational efficiency safe screening is gaining increasing attention since 1 \) solving sparse learning formulations usually has a high computational cost especially when the number of features is large and 2 \) one needs to try several regularization parameters to select a suitable model in this paper , we propose an approach called sasvi \( safe screening with variational inequalities \) sasvi makes use of the variational inequality that provides the sufficient and necessary optimality condition for the dual problem several existing approaches for lasso screening can be casted as relaxed versions of the proposed sasvi , thus sasvi provides a stronger safe screening rule we further study the monotone properties of sasvi for lasso , based on which a sure removal regularization parameter can be identified for each feature experimental results on both synthetic and real data sets are reported to demonstrate the effectiveness of the proposed sasvi for lasso screening
the k otter nielsen h o holdt algorithm is a popular way to construct the bivariate interpolation polynomial in the guruswami sudan decoding algorithm for reed solomon codes in this paper , we show how one can use divide conquer techniques to provide an asymptotic speed up of the algorithm , rendering its complexity quasi linear in n several of our observations can also provide a practical speed up to the classical version of the algorithm
in this paper , we draw the specifications of a novel benchmark for comparing parallel processing frameworks in the context of big data applications hosted in the cloud we aim at filling several gaps in already existing cloud data processing benchmarks , which lack a real life context for their processes , thus losing relevance when trying to assess performance for real applications hence , we propose a fictitious news site hosted in the cloud that is to be managed by the framework under analysis , together with several objective use case scenarios and measures for evaluating system performance the main strengths of our benchmark are parallelization capabilities supporting cloud features and big data properties
a lot of research effort has been put into community detection from all corners of academic interest such as physics , mathematics and computer science in this paper i have proposed a bi objective genetic algorithm for community detection which maximizes modularity and community score then the results obtained for both benchmark and real life data sets are compared with other algorithms using the modularity and mni performance metrics the results show that the bocd algorithm is capable of successfully detecting community structure in both real life and synthetic datasets , as well as improving upon the performance of previous techniques
sparse code multiple access \( scma \) , a non orthogonal multiple access scheme , has been introduced as a key 5g technology to improve spectral efficiency in this work , we propose scma to enable open loop coordinated multipoint \( comp \) joint transmission \( jt \) the scheme combines comp techniques with multi user scma \( mu scma \) in downlink this scheme provides open loop user multiplexing and jt in power and code domains , with robustness to mobility and low overhead of channel state information \( csi \) acquisition the combined scheme is called mu scma comp , in which scma layers and transmit power of multiple transmit points \( tps \) are shared among multiple users while a user may receive multiple scma layers from multiple tps within a comp cluster the benefits of the proposed scheme includes i \) drastic overhead reduction of csi acquisition , ii \) significant increase in throughput and coverage , and iii \) robustness to channel aging various algorithms of mu scma comp are presented , including the detection strategy , power sharing optimization , and scheduling system level evaluation shows that the proposed schemes provide significant throughput and coverage gains over ofdma for both pedestrian and vehicular users
we study the centroid with respect to the class of information theoretic burbea rao divergences that generalize the celebrated jensen shannon divergence by measuring the non negative jensen difference induced by a strictly convex and differentiable function although those burbea rao divergences are symmetric by construction , they are not metric since they fail to satisfy the triangle inequality we first explain how a particular symmetrization of bregman divergences called jensen bregman distances yields exactly those burbea rao divergences we then proceed by defining skew burbea rao divergences , and show that skew burbea rao divergences amount in limit cases to compute bregman divergences we then prove that burbea rao centroids are unique , and can be arbitrarily finely approximated by a generic iterative concave convex optimization algorithm with guaranteed convergence property in the second part of the paper , we consider the bhattacharyya distance that is commonly used to measure overlapping degree of probability distributions we show that bhattacharyya distances on members of the same statistical exponential family amount to calculate a burbea rao divergence in disguise thus we get an efficient algorithm for computing the bhattacharyya centroid of a set of parametric distributions belonging to the same exponential families , improving over former specialized methods found in the literature that were limited to univariate or diagonal multivariate gaussians to illustrate the performance of our bhattacharyya burbea rao centroid algorithm , we present experimental performance results for k means and hierarchical clustering methods of gaussian mixture models
a key problem in verification of multi agent systems by model checking concerns the fact that the state space of the system grows exponentially with the number of agents present this makes practical model checking unfeasible whenever the system contains more than a few agents in this paper we put forward a technique to establish a cutoff result , thereby showing that all systems of arbitrary number of agents can be verified by model checking a single system containing a number of agents equal to the cutoff of the system while this problem is undecidable in general , we here define a class of parameterised interpreted systems and a parameterised temporal epistemic logic for which the result can be shown we exemplify the theoretical results on a robotic example and present an implementation of the technique on top of mcmas , an open source model checker for multi agent systems
following an early claim by nelson mcevoy cite nelson mcevoy 2007 suggesting that word associations can display `spooky action at a distance behaviour' , a serious investigation of the potentially quantum nature of such associations is currently underway this paper presents a simple quantum model of a word association system it is shown that a quantum model of word entanglement can recover aspects of both the spreading activation equation and the spooky activation at a distance equation , both of which are used to model the activation level of words in human memory
recently , several large scale rdf knowledge bases have been built and applied in many knowledge based applications to further increase the number of facts in rdf knowledge bases , logic rules can be used to predict new facts based on the existing ones therefore , how to automatically learn reliable rules from large scale knowledge bases becomes increasingly important in this paper , we propose a novel rule learning approach named rdf2rules for rdf knowledge bases rdf2rules first mines frequent predicate cycles \( fpcs \) , a kind of interesting frequent patterns in knowledge bases , and then generates rules from the mined fpcs because each fpc can produce multiple rules , and effective pruning strategy is used in the process of mining fpcs , rdf2rules works very efficiently another advantage of rdf2rules is that it uses the entity type information when generates and evaluates rules , which makes the learned rules more accurate experiments show that our approach outperforms the compared approach in terms of both efficiency and accuracy
graph based models form a fundamental aspect of data representation in data sciences and play a key role in modeling complex networked systems in particular , recently there is an ever increasing interest in modeling dynamic complex networks , i e networks in which the topological structure \( nodes and edges \) may vary over time in this context , we propose a novel model for representing finite discrete time varying graphs \( tvgs \) , which are typically used to model dynamic complex networked systems we analyze the data structures built from our proposed model and demonstrate that , for most practical cases , the asymptotic memory complexity of our model is in the order of the cardinality of the set of edges further , we show that our proposal is an unifying model that can represent several previous \( classes of \) models for dynamic networks found in the recent literature , which in general are unable to represent each other in contrast to previous models , our proposal is also able to intrinsically model cyclic \( i e periodic \) behavior in dynamic networks these representation capabilities attest the expressive power of our proposed unifying model for tvgs we thus believe our unifying model for tvgs is a step forward in the theoretical foundations for data analysis of complex networked systems
online social networking may be a way to support health professionals' need for continuous learning through interaction with peers and experts understanding and evaluating such learning is important but difficult , and social network analysis \( sna \) offers a solution this paper demonstrates how sna can be used to study levels of participation as well as the patterns of interactions that take place among health professionals in a large online professional learning network our analysis has shown that their learning network is highly centralised and loosely connected the level of participation is low in general , and most interactions are structured around a small set of users consisting of moderators and core members the structural patterns of interaction indicates there is a chance of small group learning occurring and requires further investigation to identify those potential learning groups this first stage of analysis , to be followed by longitudinal study of the dynamics of interaction and complemented by content analysis of their discussion , may contribute to greater sophistication in the analysis and utilisation of new environments for health professional learning
the central issue in direct sequence code division multiple access \( ds cdma \) ad hoc networks is the prevention of a near far problem this paper considers two types of guard zones that may be used to control the near far problem a fundamental exclusion zone and an additional csma guard zone that may be established by the carrier sense multiple access \( csma \) protocol in the exclusion zone , no mobiles are physically present , modeling the minimum physical separation among mobiles that is always present in actual networks potentially interfering mobiles beyond a transmitting mobile 's exclusion zone , but within its csma guard zone , are deactivated by the protocol this paper provides an analysis of ds csma networks with either or both types of guard zones a network of finite extent with a finite number of mobiles and uniform clustering as the spatial distribution is modeled the analysis applies a closed form expression for the outage probability in the presence of nakagami fading , conditioned on the network geometry the tradeoffs between exclusion zones and csma guard zones are explored for ds cdma and unspread networks the spreading factor and the guard zone radius provide design flexibility in achieving specified levels of average outage probability and transmission capacity the advantage of an exclusion zone over a csma guard zone is that since the network is not thinned , the number of active mobiles remains constant , and higher transmission capacities can be achieved
the proportionally fair sharing of the capacity of a poisson network using spatial aloha leads to closed form performance expressions in two extreme cases \( 1 \) the case without topology information , where the analysis boils down to a parametric optimization problem leveraging stochastic geometry \( 2 \) the case with full network topology information , which was recently solved using shot noise techniques we show that there exists a continuum of adaptive controls between these two extremes , based on local stopping sets , which can also be analyzed in closed form we also show that these control schemes are implementable , in contrast to the full information case which is not as local information increases , the performance levels of these schemes are shown to get arbitrarily close to those of the full information scheme the analytical results are combined with discrete event simulation to provide a detailed evaluation of the performance of this class of medium access controls
in this paper we answer the question of when circulant quantum spin networks with nearest neighbor couplings can give perfect state transfer the network is described by a circulant graph g , which is characterized by its circulant adjacency matrix a formally , we say that there exists a it perfect state transfer \( pst \) between vertices a , b in v \( g \) if f \( tau \) ab 1 , for some positive real number tau , where f \( t \) exp \( i at \) saxena , severini and shparlinski \( it international journal of quantum information 5 \( 2007 \) , 417 430 \) proved that f \( tau \) aa 1 for some a in v \( g \) and tau in r if and only if all eigenvalues of g are integer \( that is , the graph is integral \) the integral circulant graph icg n \( d \) has the vertex set z n 0 , 1 , 2 , , n 1 and vertices a and b are adjacent if gcd \( a b , n \) in d , where d subseteq d d mid n , 1 leq d n these graphs are highly symmetric and have important applications in chemical graph theory we show that icg n \( d \) has pst if and only if n in 4 n and d widetilde d 3 cup d 2 cup 2d 2 cup 4d 2 cup n 2 a , where widetilde d 3 d in d n d in 8 n , d 2 d in d n d in 8 n 4 setminus n 4 and a in 1 , 2 we have thus answered the question of complete characterization of perfect state transfer in integral circulant graphs raised in it quantum information and computation , vol 10 , no 3 4 \( 2010 \) 0325 0342 by angeles canul it et al furthermore , we also calculate perfect quantum communication distance \( distance between vertices where pst occurs \) and describe the spectra of integral circulant graphs having pst we conclude by giving a closed form expression calculating the number of integral circulant graphs of a given order having pst
we construct codes over the ring mathbb f 2 u mathbb f 2 with u 2 0 these code are designed for use in dna computing applications the codes obtained satisfy the reverse complement constraint , the gc content constraint and avoid the secondary structure they are derived from the cyclic complement reversible codes over the ring mathbb f 2 u mathbb f 2 we also construct an infinite family of bch dna codes
arikan has shown that systematic polar codes \( spc \) outperform nonsystematic polar codes \( nspc \) however , the performance gain comes at the price of elevated encoding complexity , i e , compared to nspc , the available encoding methods for spc require higher memory and computation in this letter , we propose an efficient encoding algorithm requiring only n bits of memory and having frac n 2 log 2n xor operations moreover , the auxiliary variables in the algorithm can share the memory to reduce extra memory requirement furthermore , a parallel 2 bit encoding algorithm is also presented to improve the encoding throughput remarkably , we show that parallel encoding can be implemented with the same number of xor operations and memory bits finally , the proposed encoding algorithm can be directly used for nspc with the same complexity
the overrelational manifesto \( below orm \) proposes a possible approach to creation of data storage systems of the next generation orm starts from the requirement that information in a relational database is represented by a set of relation values accordingly , it is assumed that the information about any entity of an enterprise must also be represented as a set of relation values \( the orm main requirement \) a system of types is introduced , which allows one to fulfill the main requirement the data are represented in the form of complex objects , and the state of any object is described as a set of relation values emphasize that the types describing the objects are encapsulated , inherited , and polymorphic then , it is shown that the data represented as a set of such objects may also be represented as a set of relational values defined on the set of scalar domains \( dual data representation \) in the general case , any class is associated with a set of relation variables \( r variables \) each one containing some data about all objects of this class existing in the system one of the key points is the fact that the usage of complex \( from the user 's viewpoint \) refined names of r variables and their attributes makes it possible to preserve the semantics of complex data structures represented in the form of a set of relation values the most important part of the data storage system created on the approach proposed is an object oriented translator operating over a relational dbms the expressiveness of such a system is comparable with that of oo programming languages
this paper investigates delay distortion power trade offs in transmission of quasi stationary sources over block fading channels by studying encoder and decoder buffering techniques to smooth out the source and channel variations four source and channel coding schemes that consider buffer and power constraints are presented to minimize the reconstructed source distortion the first one is a high performance scheme , which benefits from optimized source and channel rate adaptation in the second scheme , the channel coding rate is fixed and optimized along with transmission power with respect to channel and source variations hence this scheme enjoys simplicity of implementation the two last schemes have fixed transmission power with optimized adaptive or fixed channel coding rate for all the proposed schemes , closed form solutions for mean distortion , optimized rate and power are provided and in the high snr regime , the mean distortion exponent and the asymptotic mean power gains are derived the proposed schemes with buffering exploit the diversity due to source and channel variations specifically , when the buffer size is limited , fixed channel rate adaptive power scheme outperforms an adaptive rate fixed power scheme furthermore , analytical and numerical results demonstrate that with limited buffer size , the system performance in terms of reconstructed signal snr saturates as transmission power is increased , suggesting that appropriate buffer size selection is important to achieve a desired reconstruction quality
this paper gives an overview of recent progress in the brain inspired computing field with a focus on implementation using emerging memories as electronic synapses design considerations and challenges such as requirements and design targets on multilevel states , device variability , programming energy , array level connectivity , fan in fanout , wire energy , and ir drop are presented wires are increasingly important in design decisions , especially for large systems , and cycle to cycle variations have large impact on learning performance
we propose a process algebra for wireless mesh networks that combines novel treatments of local broadcast , conditional unicast and data structures in this framework , we model the ad hoc on demand distance vector \( aodv \) routing protocol and \( dis \) prove crucial properties such as loop freedom and packet delivery
determining the attachments of prepositions and subordinate conjunctions is a key problem in parsing natural language this paper presents a trainable approach to making these attachments through transformation sequences and error driven learning our approach is broad coverage , and accounts for roughly three times the attachment cases that have previously been handled by corpus based techniques in addition , our approach is based on a simplified model of syntax that is more consistent with the practice in current state of the art language processing systems this paper sketches syntactic and algorithmic details , and presents experimental results on data sets derived from the penn treebank we obtain an attachment accuracy of 75 4 for the general case , the first such corpus based result to be reported for the restricted cases previously studied with corpus based methods , our approach yields an accuracy comparable to current work \( 83 1 \)
we study the capacity regions of broadcast channels with binary inputs and symmetric outputs we study the partial order induced by the more capable ordering of broadcast channels for channels belonging to this class this study leads to some surprising connections regarding various notions of dominance of receivers the results here also help us isolate some classes of symmetric channels where the best known inner and outer bounds differ
detecting and analyzing directional structures in images is important in many applications since one dimensional patterns often correspond to important features such as object contours or trajectories classifying a structure as directional or non directional requires a measure to quantify the degree of directionality and a threshold , which needs to be chosen based on the statistics of the image in order to do this , we model the image as a random field so far , little research has been performed on analyzing directionality in random fields in this paper , we propose a measure to quantify the degree of directionality based on the random monogenic signal , which enables a unique decomposition of a 2d signal into local amplitude , local orientation , and local phase we investigate the second order statistical properties of the monogenic signal for isotropic , anisotropic , and unidirectional random fields we analyze our measure of directionality for finite size sample images , and determine a threshold to distinguish between unidirectional and non unidirectional random fields , which allows the automatic classification of images
memory can be defined as the ability to retain and recall information in a diverse range of forms it is a vital component of the way in which we as human beings operate on a day to day basis given a particular situation , decisions are made and actions undertaken in response to that situation based on our memory of related prior events and experiences by utilising our memory we can anticipate the outcome of our chosen actions to avoid unexpected or unwanted events in addition , as we subtly alter our actions and recognise altered outcomes we learn and create new memories , enabling us to improve the efficiency of our actions over time however , as this process occurs so naturally in the subconscious its importance is often overlooked
we consider staged self assembly systems , in which square shaped tiles can be added to bins in several stages within these bins , the tiles may connect to each other , depending on the glue types of their edges previous work by demaine et al showed that a relatively small number of tile types suffices to produce arbitrary shapes in this model however , these constructions were only based on a spanning tree of the geometric shape , so they did not produce full connectivity of the underlying grid graph in the case of shapes with holes designing fully connected assemblies with a polylogarithmic number of stages was left as a major open problem we resolve this challenge by presenting new systems for staged assembly that produce fully connected polyominoes in o \( log 2 n \) stages , for various scale factors and temperature tau 2 as well as tau 1 our constructions work even for shapes with holes and uses only a constant number of glues and tiles moreover , the underlying approach is more geometric in nature , implying that it promised to be more feasible for shapes with compact geometric description
we discuss quantum algorithms , based on the bernstein vazirani algorithm , for finding which variables a boolean function depends on there are 2 n possible linear boolean functions of n variables given a linear boolean function , the bernstein vazirani quantum algorithm can deterministically identify which one of these boolean functions we are given using just one single function query the same quantum algorithm can also be used to learn which input variables other types of boolean functions depend on , with a success probability that depends on the form of the boolean function that is tested , but does not depend on the total number of input variables we also outline a procedure to futher amplify the success probability , based on another quantum algorithm , the grover search
in this paper , we present ell 1 , p multi task structure learning for gaussian graphical models we analyze the sufficient number of samples for the correct recovery of the support union and edge signs we also analyze the necessary number of samples for any conceivable method by providing information theoretic lower bounds we compare the statistical efficiency of multi task learning versus that of single task learning for experiments , we use a block coordinate descent method that is provably convergent and generates a sequence of positive definite solutions we provide experimental validation on synthetic data as well as on two publicly available real world data sets , including functional magnetic resonance imaging and gene expression data
as techniques for graph query processing mature , the need for optimization is increasingly becoming an imperative indices are one of the key ingredients toward efficient query processing strategies via cost based optimization due to the apparent absence of a common representation model , it is difficult to make a focused effort toward developing access structures , metrics to evaluate query costs , and choose alternatives in this context , recent interests in covering based graph matching appears to be a promising direction of research in this paper , our goal is to formally introduce a new graph representation model , called minimum hub cover , and demonstrate that this representation offers interesting strategic advantages , facilitates construction of candidate graphs from graph fragments , and helps leverage indices in novel ways for query optimization however , similar to other covering problems , minimum hub cover is np hard , and thus is a natural candidate for optimization we claim that computing the minimum hub cover leads to substantial cost reduction for graph query processing we present a computational characterization of minimum hub cover based on integer programming to substantiate our claim and investigate its computational cost on various graph types
atkinson developed a strategy which splits solution of a pde system into homogeneous and particular solutions , where the former have to satisfy the boundary and governing equation , while the latter only need to satisfy the governing equation without concerning geometry since the particular solution can be solved irrespective of boundary shape , we can use a readily available fast fourier or orthogonal polynomial technique o \( nlogn \) to evaluate it in a regular box or sphere surrounding physical domain the distinction of this study is that we approximate homogeneous solution with nonsingular general solution rbf as in the boundary knot method the collocation method using general solution rbf has very high accuracy and spectral convergent speed and is a simple , truly meshfree approach for any complicated geometry more importantly , the use of nonsingular general solution avoids the controversial artificial boundary in the method of fundamental solution due to the singularity of fundamental solution
this paper investigates the energy efficient power allocation for a two tier , underlaid femtocell network the behaviors of the macrocell base station \( mbs \) and the femtocell users \( fus \) are modeled hierarchically as a stackelberg game the mbs guarantees its own qos requirement by charging the fus individually according to the cross tier interference , and the fus responds by controlling the local transmit power non cooperatively due to the limit of information exchange in intra and inter tiers , a self learning based strategy updating mechanism is proposed for each user to learn the equilibrium strategies in the same stackelberg game framework , two different scenarios based on the continuous and discrete power profiles for the fus are studied , respectively the self learning schemes in the two scenarios are designed based on the local best response by studying the properties of the proposed game in the two situations , the convergence property of the learning schemes is provided the simulation results are provided to support the theoretical finding in different situations of the proposed game , and the efficiency of the learning schemes is validated
given a graph g , and a spanning subgraph h of g , a circular q backbone k coloring of \( g , h \) is a proper k coloring c of g such that q le lvert c \( u \) c \( v \) rvert le k q , for every edge uv in e \( h \) the circular q backbone chromatic number of \( g , h \) , denoted by cbc q \( g , h \) , is the minimum integer k for which there exists a circular q backbone k coloring of \( g , h \) the four color theorem implies that whenever g is planar , we have cbc 2 \( g , h \) le 8 it is conjectured that this upper bound can be improved to 7 when h is a tree , and to 6 when h is a matching in this work , we show that 1 \) if g is planar and has no c 4 as subgraph , and h is a linear spanning forest of g , then cbc 2 \( g , h \) leq 7 2 \) if g is a plane graph having no two 3 faces sharing an edge , and h is a matching of g , then cbc 2 \( g , h \) leq 6 and 3 \) if g is planar and has no c 4 nor c 5 as subgraph , and h is a mathing of g , then cbc 2 \( g , h \) leq 5 these results partially answer questions posed by broersma , fujisawa and yoshimoto \( 2003 \) , and by broersma , fomin and golovach \( 2007 \) it also points towards a positive answer for the steinberg 's conjecture
imputation of missing data in large regions of satellite imagery is necessary when the acquired image has been damaged by shadows due to clouds , or information gaps produced by sensor failure the general approach for imputation of missing data , that could not be considered missed at random , suggests the use of other available data previous work , like local linear histogram matching , take advantage of a co registered older image obtained by the same sensor , yielding good results in filling homogeneous regions , but poor results if the scenes being combined have radical differences in target radiance due , for example , to the presence of sun glint or snow this study proposes three different alternatives for filling the data gaps the first two involves merging radiometric information from a lower resolution image acquired at the same time , in the fourier domain \( method a \) , and using linear regression \( method b \) the third method consider segmentation as the main target of processing , and propose a method to fill the gaps in the map of classes , avoiding direct imputation \( method c \) all the methods were compared by means of a large simulation study , evaluating performance with a multivariate response vector with four measures q , rmse , kappa and overall accuracy coefficients difference in performance were tested with a manova mixed model design with two main effects , imputation method and type of lower resolution extra data , and a blocking third factor with a nested sub factor , introduced by the real landsat image and the sub images that were used method b proved to be the best for all criteria
relay transmission can enhance coverage and throughput , while it can be vulnerable to eavesdropping attacks due to the additional transmission of the source message at the relay thus , whether or not one should use relay transmission for secure communication is an interesting and important problem in this paper , we consider the transmission of a confidential message from a source to a destination in a decentralized wireless network in the presence of randomly distributed eavesdroppers the source destination pair can be potentially assisted by randomly distributed relays for an arbitrary relay , we derive exact expressions of secure connection probability for both colluding and non colluding eavesdroppers we further obtain lower bound expressions on the secure connection probability , which are accurate when the eavesdropper density is small by utilizing these lower bound expressions , we propose a relay selection strategy to improve the secure connection probability by analytically comparing the secure connection probability for direct transmission and relay transmission , we address the important problem of whether or not to relay and discuss the condition for relay transmission in terms of the relay density and source destination distance these analytical results are accurate in the small eavesdropper density regime
support vector machines represent a promising development in machine learning research that is not widely used within the remote sensing community this paper reports the results of multispectral \( landsat 7 etm \) and hyperspectral dais \) data in which multi class svms are compared with maximum likelihood and artificial neural network methods in terms of classification accuracy our results show that the svm achieves a higher level of classification accuracy than either the maximum likelihood or the neural classifier , and that the support vector machine can be used with small training datasets and high dimensional data
brain signal variability in the measurements obtained from different subjects during different sessions significantly deteriorates the accuracy of most brain computer interface \( bci \) systems moreover these variabilities , also known as inter subject or inter session variabilities , require lengthy calibration sessions before the bci system can be used furthermore , the calibration session has to be repeated for each subject independently and before use of the bci due to the inter session variability in this study , we present an algorithm in order to minimize the above mentioned variabilities and to overcome the time consuming and usually error prone calibration time our algorithm is based on linear programming support vector machines and their extensions to a multiple kernel learning framework we tackle the inter subject or session variability in the feature spaces of the classifiers this is done by incorporating each subject or session specific feature spaces into much richer feature spaces with a set of optimal decision boundaries each decision boundary represents the subject or a session specific spatio temporal variabilities of neural signals consequently , a single classifier with multiple feature spaces will generalize well to new unseen test patterns even without the calibration steps we demonstrate that classifiers maintain good performances even under the presence of a large degree of bci variability the present study analyzes bci variability related to oxy hemoglobin neural signals measured using a functional near infrared spectroscopy
differentially 4 uniform permutations on gf 2 2k with high nonlinearity are often chosen as substitution boxes in both block and stream ciphers recently , qu et al introduced a class of functions , which are called preferred functions , to construct a lot of infinite families of such permutations cite qttl in this paper , we propose a particular type of boolean functions to characterize the preferred functions on the one hand , such boolean functions can be determined by solving linear equations , and they give rise to a huge number of differentially 4 uniform permutations over gf 2 2k hence they may provide more choices for the design of substitution boxes on the other hand , by investigating the number of these boolean functions , we show that the number of ccz inequivalent differentially 4 uniform permutations over gf 2 2k grows exponentially when k increases , which gives a positive answer to an open problem proposed in cite qttl
the fully quantum reverse shannon theorem establishes the optimal rate of noiseless classical communication required for simulating the action of many instances of a noisy quantum channel on an arbitrary input state , while also allowing for an arbitrary amount of shared entanglement of an arbitrary form turning this theorem around establishes a strong converse for the entanglement assisted classical capacity of any quantum channel this paper proves the strong converse for entanglement assisted capacity by a completely different approach and identifies a bound on the strong converse exponent for this task namely , we exploit the recent entanglement assisted meta converse theorem of matthews and wehner , several properties of the recently established sandwiched renyi relative entropy \( also referred to as the quantum renyi divergence \) , and the multiplicativity of completely bounded p norms due to devetak et al the proof here demonstrates the extent to which the arimoto approach can be helpful in proving strong converse theorems , it provides an operational relevance for the multiplicativity result of devetak et al , and it adds to the growing body of evidence that the sandwiched renyi relative entropy is the correct quantum generalization of the classical concept for all alpha 1
we consider the average consensus problem in a multi node network of finite size communication between nodes is modeled by a sequence of directed signals with arbitrary communication delays four distributed algorithms that achieve average consensus are proposed necessary and sufficient communication conditions are given for each algorithm to achieve average consensus resource costs for each algorithm are derived based on the number of scalar values that are required for communication and storage at each node numerical examples are provided to illustrate the empirical convergence rate of the four algorithms in comparison with a well known gossip algorithm as well as a randomized information spreading algorithm when assuming a fully connected random graph with instantaneous communication
the identification of the language of the script is an important stage in the process of recognition of the writing there are several works in this research area , which treat various languages most of the used methods are global or statistical in this present paper , we study the possibility of using the features of scripts to identify the language the identification of the language of the script by characteristics returns the identification in the case of multilingual documents less difficult we present by this work , a study on the possibility of using the structural features to identify the arabic language from an arabic latin text
in this paper we present a profile based approach to information filtering by an analysis of the content of text documents the wikipedia index database is created and used to automatically generate the user profile from the user document collection the problem oriented wikipedia subcorpora are created \( using knowledge extracted from the user profile \) for each topic of user interests the index databases of these subcorpora are applied to filtering information flow \( e g , mails , news \) thus , the analyzed texts are classified into several topics explicitly presented in the user profile the paper concentrates on the indexing part of the approach the architecture of an application implementing the wikipedia indexing is described the indexing method is evaluated using the russian and simple english wikipedia
we present a new model for time series classification , called the hidden unit logistic model , that uses binary stochastic hidden units to model latent structure in the data the hidden units are connected in a chain structure that models temporal dependencies in the data compared to the prior models for time series classification such as the hidden conditional random field , our model can model very complex decision boundaries because the number of latent states grows exponentially with the number of hidden units we demonstrate the strong performance of our model in experiments on a variety of \( computer vision \) tasks , including handwritten character recognition , speech recognition , facial expression , and action recognition we also present a state of the art system for facial action unit detection based on the hidden unit logistic model
this paper develops an analytic theory for the study of some polya urns with random rules the idea is to extend the isomorphism theorem in flajolet et al \( 2006 \) , which connects deterministic balanced urns to a differential system for the generating function the methodology is based upon adaptation of operators and use of a weighted probability generating function systems of differential equations are developed , and when they can be solved , they lead to characterization of the exact distributions underlying the urn evolution we give a few illustrative examples
in this research , we apply ensembles of fourier encoded spectra to capture and mine recurring concepts in a data stream environment previous research showed that compact versions of decision trees can be obtained by applying the discrete fourier transform to accurately capture recurrent concepts in a data stream however , in highly volatile environments where new concepts emerge often , the approach of encoding each concept in a separate spectrum is no longer viable due to memory overload and thus in this research we present an ensemble approach that addresses this problem our empirical results on real world data and synthetic data exhibiting varying degrees of recurrence reveal that the ensemble approach outperforms the single spectrum approach in terms of classification accuracy , memory and execution time
for a 0 , 1 valued matrix m let rm cc \( m \) denote the deterministic communication complexity of the boolean function associated with m the log rank conjecture of lov ' a sz and saks focs 1988 states that rm cc \( m \) leq log c \( rm rank \( m \) \) for some absolute constant c where rm rank \( m \) denotes the rank of m over the field of real numbers we show that rm cc \( m \) leq c cdot rm rank \( m \) log rm rank \( m \) for some absolute constant c , assuming a well known conjecture from additive combinatorics known as the polynomial freiman ruzsa \( pfr \) conjecture our proof is based on the study of the approximate duality conjecture which was recently suggested by ben sasson and zewi stoc 2011 and studied there in connection to the pfr conjecture first we improve the bounds on approximate duality assuming the pfr conjecture then we use the approximate duality conjecture \( with improved bounds \) to get the aforementioned upper bound on the communication complexity of low rank martices , where this part uses the methodology suggested by nisan and wigderson combinatorica 1995
we consider settings in which t multi antenna transmitters and k single antenna receivers concurrently utilize the available communication resources each transmitter sends useful information only to its intended receivers and can degrade the performance of unintended systems here , we assume the performance measures associated with each receiver are monotonic with the received power gains in general , the systems' joint operation is desired to be pareto optimal however , designing pareto optimal resource allocation schemes is known to be difficult in order to reduce the complexity of achieving efficient operating points , we show that it is sufficient to consider rank 1 transmit covariance matrices and propose a framework for determining the efficient beamforming vectors these beamforming vectors are thereby also parameterized by t \( k 1 \) real valued parameters each between zero and one the framework is based on analyzing each transmitter 's power gain region which is composed of all jointly achievable power gains at the receivers the efficient beamforming vectors are on a specific boundary section of the power gain region , and in certain scenarios it is shown that it is necessary to perform additional power allocation on the beamforming vectors two examples which include broadcast and multicast data as well as a cognitive radio application scenario illustrate the results
speech processing requires very efficient methods and algorithms finite state transducers have been shown recently both to constitute a very useful abstract model and to lead to highly efficient time and space algorithms in this field we present these methods and algorithms and illustrate them in the case of speech recognition in addition to classical techniques , we describe many new algorithms such as minimization , global and local on the fly determinization of weighted automata , and efficient composition of transducers these methods are currently used in large vocabulary speech recognition systems we then show how the same formalism and algorithms can be used in text to speech applications and related areas of language processing such as morphology , syntax , and local grammars , in a very efficient way the tutorial is self contained and requires no specific computational or linguistic knowledge other than classical results
we study the validity problem for propositional dependence logic , modal dependence logic and extended modal dependence logic we show that the validity problem for propositional dependence logic is nexptime complete in addition , we establish that the corresponding problem for modal dependence logic and extended modal dependence logic is nexptime hard and in nexptime np
this paper introduces the ongoing integration of contiki 's uip stack into the omnet port of the network simulation cradle \( nsc \) the nsc utilizes code from real world stack implementations and allows for an accurate simulation and comparison of different tcp ip stacks uip \( v6 \) provides resource constrained devices with an rfc compliant tcp ip stack and promotes the use of ipv6 in the vastly growing field of internet of things scenarios this work in progress report discusses our motivation to integrate uip into the nsc , our chosen approach and possible use cases for the simulation of uip in omnet
identifying measurable genetic indicators \( or biomarkers \) of a specific condition of a biological system is a key element of precision medicine indeed it allows to tailor diagnostic , prognostic and treatment choice to individual characteristics of a patient in machine learning terms , biomarker discovery can be framed as a feature selection problem on whole genome data sets however , classical feature selection methods are usually underpowered to process these data sets , which contain orders of magnitude more features than samples this can be addressed by making the assumption that genetic features that are linked on a biological network are more likely to work jointly towards explaining the phenotype of interest we review here three families of methods for feature selection that integrate prior knowledge in the form of networks
the perfect phylogeny is one of the most used models in different areas of computational biology in this paper we consider the problem of the persistent perfect phylogeny \( referred as p pp \) recently introduced to extend the perfect phylogeny model allowing persistent characters , that is characters can be gained and lost at most once we define a natural generalization of the p pp problem obtained by requiring that for some pairs \( character , species \) , neither the species nor any of its ancestors can have the character in other words , some characters cannot be persistent for some species this new problem is called constrained p pp \( cp pp \) based on a graph formulation of the cp pp problem , we are able to provide a polynomial time solution for the cp pp problem for matrices having an empty conflict graph in particular we show that all such matrices admit a persistent perfect phylogeny in the unconstrained case using this result , we develop a parameterized algorithm for solving the cp pp problem where the parameter is the number of characters a preliminary experimental analysis of the algorithm shows that it performs efficiently and it may analyze real haplotype data not conforming to the classical perfect phylogeny model
spectrum sensing is one of the enabling functionalities for cognitive radio \( cr \) systems to operate in the spectrum white space to protect the primary incumbent users from interference , the cr is required to detect incumbent signals at very low signal to noise ratio \( snr \) in this paper , we present a spectrum sensing technique based on correlating spectra for detection of television \( tv \) broadcasting signals the basic strategy is to correlate the periodogram of the received signal with the a priori known spectral features of the primary signal we show that according to the neyman pearson criterion , this spectral correlation based sensing technique is asymptotically optimal at very low snr and with a large sensing time from the system design perspective , we analyze the effect of the spectral features on the spectrum sensing performance through the optimization analysis , we obtain useful insights on how to choose effective spectral features to achieve reliable sensing simulation results show that the proposed sensing technique can reliably detect analog and digital tv signals at snr as low as 20 db
image compression is an important filed in image processing the science welcomes any tinny contribution that may increase the compression ratio by whichever insignificant percentage therefore , the essential contribution in this paper is to increase the compression ratio for the well known portable network graphics \( png \) image file format the contribution starts with converting the original png image into k modulus method \( k mm \) practically , taking k equals to ten , and then the pixels in the constructed image will be integers divisible by ten since png uses lempel ziv compression algorithm , then the ability to reduce file size will increase according to the repetition in pixels in each k by k window according to the transformation done by k mm experimental results show that the proposed technique \( k png \) produces high compression ratio with smaller file size in comparison to the original png file
we give efficient algorithms for ranking lyndon words of length n over an alphabet of size sigma the rank of a lyndon word is its position in the sequence of lexicographically ordered lyndon words of the same length the outputs are integers of exponential size , and complexity of arithmetic operations on such large integers cannot be ignored our model of computations is the word ram , in which basic arithmetic operations on \( large \) numbers of size at most sigma n take o \( n \) time our algorithm for ranking lyndon words makes o \( n 2 \) arithmetic operations \( this would imply directly cubic time on word ram \) however , using an algebraic approach we are able to reduce the total time complexity on the word ram to o \( n 2 log sigma \) we also present an o \( n 3 log 2 sigma \) time algorithm that generates the lyndon word of a given length and rank in lexicographic order finally we use the connections between lyndon words and lexicographically minimal de bruijn sequences \( theorem of fredricksen and maiorana \) to develop the first polynomial time algorithm for decoding minimal de bruijn sequence of any rank n \( it determines the position of an arbitrary word of length n within the de bruijn sequence \)
the characterization of the global maximum of energy efficiency \( ee \) problems in wireless networks is a challenging problem due to its nonconvex nature in interference channels the aim of this work is to develop a new and general framework to achieve globally optimal power control solutions first , the hidden monotonic structure of the most common ee maximization problems is exploited jointly with fractional programming theory to obtain globally optimal solutions with exponential complexity in the number of network links to overcome this issue , we also propose a framework to compute suboptimal power control strategies characterized by affordable complexity this is achieved by merging fractional programming and sequential optimization the proposed monotonic framework is used to shed light on the ultimate performance of wireless networks in terms of ee and also to benchmark the performance of the lower complexity framework based on sequential programming numerical evidence is provided to show that the sequential fractional programming achieves global optimality
quantum maximal distance separable \( mds \) codes form an important class of quantum codes it is very hard to construct quantum mds codes with relatively large minimum distance in this paper , based on classical constacyclic codes , we construct two classes of quantum mds codes with parameters lambda \( q 1 \) , lambda \( q 1 \) 2d 2 , d q where 2 leq d leq \( q 1 \) 2 lambda 1 , and q 1 lambda r with r even , and lambda \( q 1 \) , lambda \( q 1 \) 2d 2 , d q where 2 leq d leq \( q 1 \) 2 lambda 2 1 , and q 1 lambda r with r odd the quantum mds codes exhibited here have parameters better than the ones available in the literature
the cuckoo optimization algorithm \( coa \) is developed for solving single objective problems and it cannot be used for solving multi objective problems so the multi objective cuckoo optimization algorithm based on data envelopment analysis \( dea \) is developed in this paper and it can gain the efficient pareto frontiers this algorithm is presented by the ccr model of dea and the output oriented approach of it the selection criterion is higher efficiency for next iteration of the proposed hybrid method so the profit function of the coa is replaced by the efficiency value that is obtained from dea this algorithm is compared with other methods using some test problems the results shows using coa and dea approach for solving multi objective problems increases the speed and the accuracy of the generated solutions
we present a method of discrete modeling and analysis of multilevel dynamics of complex large scale hierarchical dynamic systems subject to external dynamic control mechanism architectural model of information system supporting simulation and analysis of dynamic processes and development scenarios \( strategies \) of complex large scale hierarchical systems is also proposed
we present a new computational technique to detect and analyze statistically significant geographic variation in language our meta analysis approach captures statistical properties of word usage across geographical regions and uses statistical methods to identify significant changes specific to regions while previous approaches have primarily focused on lexical variation between regions , our method identifies words that demonstrate semantic and syntactic variation as well we extend recently developed techniques for neural language models to learn word representations which capture differing semantics across geographical regions in order to quantify this variation and ensure robust detection of true regional differences , we formulate a null model to determine whether observed changes are statistically significant our method is the first such approach to explicitly account for random variation due to chance while detecting regional variation in word meaning to validate our model , we study and analyze two different massive online data sets millions of tweets from twitter spanning not only four different countries but also fifty states , as well as millions of phrases contained in the google book ngrams our analysis reveals interesting facets of language change at multiple scales of geographic resolution from neighboring states to distant continents finally , using our model , we propose a measure of semantic distance between languages our analysis of british and american english over a period of 100 years reveals that semantic variation between these dialects is shrinking
massive multi user \( mu \) multiple input multiple output \( mimo \) systems are one possible key technology for next generation wireless communication systems claims have been made that massive mu mimo will increase both the radiated energy efficiency as well as the sum rate capacity by orders of magnitude , because of the high transmit directivity however , due to the very large number of transceivers needed at each base station \( bs \) , a successful implementation of massive mu mimo will be contingent on of the availability of very cheap , compact and power efficient radio and digital processing hardware this may in turn impair the quality of the modulated radio frequency \( rf \) signal due to an increased amount of power amplifier distortion , phase noise , and quantization noise in this paper , we examine the effects of hardware impairments on a massive mu mimo single cell system by means of theory and simulation the simulations are performed using simplified , well established statistical hardware impairment models as well as more sophisticated and realistic models based upon measurements and electromagnetic antenna array simulations
recent developments in sensing technologies have enabled us to examine the nature of human social behavior in greater detail by applying an information theoretic method to the spatiotemporal data of cell phone locations , c song et al science 327 , 1018 \( 2010 \) found that human mobility patterns are remarkably predictable inspired by their work , we address a similar predictability question in a different kind of human social activity conversation events the predictability in the sequence of one 's conversation partners is defined as the degree to which one 's next conversation partner can be predicted given the current partner we quantify this predictability by using the mutual information we examine the predictability of conversation events for each individual using the longitudinal data of face to face interactions collected from two company offices in japan each subject wears a name tag equipped with an infrared sensor node , and conversation events are marked when signals are exchanged between sensor nodes in close proximity we find that the conversation events are predictable to some extent knowing the current partner decreases the uncertainty about the next partner by 28 4 on average much of the predictability is explained by long tailed distributions of interevent intervals however , a predictability also exists in the data , apart from the contribution of their long tailed nature in addition , an individual 's predictability is correlated with the position in the static social network derived from the data individuals confined in a community in the sense of an abundance of surrounding triangles tend to have low predictability , and those bridging different communities tend to have high predictability
the discontinuous petrov galerkin \( dpg \) methodology of demkowicz and gopalakrishnan 15 , 17 guarantees the optimality of the solution in an energy norm , and provides several features facilitating adaptive schemes a key question that has not yet been answered in general though there are some results for poisson , e g is how best to precondition the dpg system matrix , so that iterative solvers may be used to allow solution of large scale problems in this paper , we detail a strategy for preconditioning the dpg system matrix using geometric multigrid which we have implemented as part of camellia 26 , and demonstrate through numerical experiments its effectiveness in the context of several variational formulations we observe that in some of our experiments , the behavior of the preconditioner is closely tied to the discrete test space enrichment we include experiments involving adaptive meshes with hanging nodes for lid driven cavity flow , demonstrating that the preconditioners can be applied in the context of challenging problems we also include a scalability study demonstrating that the approach and our implementation scales well to many mpi ranks
this paper investigates the supervised learning problem with observations drawn from certain general stationary stochastic processes here by emph general , we mean that many stationary stochastic processes can be included we show that when the stochastic processes satisfy a generalized bernstein type inequality , a unified treatment on analyzing the learning schemes with various mixing processes can be conducted and a sharp oracle inequality for generic regularized empirical risk minimization schemes can be established the obtained oracle inequality is then applied to derive convergence rates for several learning schemes such as empirical risk minimization \( erm \) , least squares support vector machines \( ls svms \) using given generic kernels , and svms using gaussian kernels for both least squares and quantile regression it turns out that for i i d processes , our learning rates for erm recover the optimal rates on the other hand , for non i i d processes including geometrically alpha mixing markov processes , geometrically alpha mixing processes with restricted decay , phi mixing processes , and \( time reversed \) geometrically mathcal c mixing processes , our learning rates for svms with gaussian kernels match , up to some arbitrarily small extra term in the exponent , the optimal rates for the remaining cases , our rates are at least close to the optimal rates as a by product , the assumed generalized bernstein type inequality also provides an interpretation of the so called effective number of observations for various mixing processes
quantum stabilizer states over gf \( m \) can be represented as self dual additive codes over gf \( m 2 \) these codes can be represented as weighted graphs , and orbits of graphs under the generalized local complementation operation correspond to equivalence classes of codes we have previously used this fact to classify self dual additive codes over gf \( 4 \) in this paper we classify self dual additive codes over gf \( 9 \) , gf \( 16 \) , and gf \( 25 \) assuming that the classical mds conjecture holds , we are able to classify all self dual additive mds codes over gf \( 9 \) by using an extension technique we prove that the minimum distance of a self dual additive code is related to the minimum vertex degree in the associated graph orbit circulant graph codes are introduced , and a computer search reveals that this set contains many strong codes we show that some of these codes have highly regular graph representations
in this paper we propose distributed flooding based storage algorithms for large scale wireless sensor networks assume a wireless sensor network with n nodes that have limited power , memory , and bandwidth each node is capable of both sensing and storing data such sensor nodes might disappear from the network due to failures or battery depletion hence it is desired to design efficient schemes to collect data from these n nodes we propose two distributed storage algorithms \( dsa 's \) that utilize network flooding to solve this problem in the first algorithm , dsa i , we assume that every node utilizes network flooding to disseminate its data throughout the network using a mixing time of approximately o \( n \) we show that this algorithm is efficient in terms of the encoding and decoding operations in the second algorithm , dsa ii , we assume that the total number of nodes is not known to every sensor hence dissemination of the data does not depend on n the encoding operations in this case take o \( c mu 2 \) , where mu is the mean degree of the network graph and c is a system parameter we evaluate the performance of the proposed algorithms through analysis and simulation , and show that their performance matches the derived theoretical results
this paper presents a novel method for controlling swarms of unmanned aerial vehicles using stochastic optimal control \( soc \) theory the approach consists of a centralized high level controller that computes optimal state trajectories as velocity sequences , and a platform specific low level controller which ensures that these velocity sequences are met the high level control task is expressed as a centralized path integral control problem , for which optimal control computation corresponds to a probabilistic inference problem that can be solved by efficient sampling methods through simulation we show that our soc approach \( a \) has significant benefits compared to deterministic control and other soc methods in multi modal problems with noise dependent optimal solutions , \( b \) is capable of controlling a large number of platforms in real time , and \( c \) yields collective emergent behavior in the form of flight formations finally , we show that our approach works for real platforms , by controlling a swarm of three quadrotors
information estimates such as the ``direct method'' of strong et al \( 1998 \) sidestep the difficult problem of estimating the joint distribution of response and stimulus by instead estimating the difference between the marginal and conditional entropies of the response while this is an effective estimation strategy , it tempts the practitioner to ignore the role of the stimulus and the meaning of mutual information we show here that , as the number of trials increases indefinitely , the direct \( or ``plug in'' \) estimate of marginal entropy converges \( with probability 1 \) to the entropy of the time averaged conditional distribution of the response , and the direct estimate of the conditional entropy converges to the time averaged entropy of the conditional distribution of the response under joint stationarity and ergodicity of the response and stimulus , the difference of these quantities converges to the mutual information when the stimulus is deterministic or non stationary the direct estimate of information no longer estimates mutual information , which is no longer meaningful , but it remains a measure of variability of the response distribution across time
in this paper a large class of universal windows for gabor frames \( weyl heisenberg frames \) is constructed these windows have the fundamental property that every overcritical rectangular lattice generates a gabor frame likewise , every undercritical rectangular lattice generates a riesz sequence
a new method for estimating the relative positions of location unaware nodes from the location aware nodes and the received signal strength \( rss \) between the nodes , in a wireless sensor network \( wsn \) , is proposed in the method , a regularization term is incorporated in the optimization problem leading to significant improvement in the estimation accuracy even in the presence of position errors of the location aware nodes and distance errors between the nodes the regularization term is appropriated weighted on the basis of the degree of connectivity between the nodes in the network the method is formulated as a convex optimization problem using the semidefinite relaxation approach experimental comparisons with state of the art competing methods show that the proposed method yields node positions that are much more accurate even in the presence of measurement errors
the multiple input single output interference channel is considered each transmitter is assumed to know the channels between itself and all receivers perfectly and the receivers are assumed to treat interference as additive noise in this setting , noncooperative transmission does not take into account the interference generated at other receivers which generally leads to inefficient performance of the links to improve this situation , we study cooperation between the links using coalitional games the players \( links \) in a coalition either perform zero forcing transmission or wiener filter precoding to each other the epsilon core is a solution concept for coalitional games which takes into account the overhead required in coalition deviation we provide necessary and sufficient conditions for the strong and weak epsilon core of our coalitional game not to be empty with zero forcing transmission since , the epsilon core only considers the possibility of joint cooperation of all links , we study coalitional games in partition form in which several distinct coalitions can form we propose a polynomial time distributed coalition formation algorithm based on coalition merging and prove that its solution lies in the coalition structure stable set of our coalition formation game simulation results reveal the cooperation gains for different coalition formation complexities and deviation overhead models
low density parity check \( ldpc \) codes are an important class of codes with many applications two algebraic methods for constructing regular ldpc codes are derived one based on nonprimitive narrow sense bch codes and the other directly based on cyclotomic cosets the constructed codes have high rates and are free of cycles of length four consequently , they can be decoded using standard iterative decoding algorithms the exact dimension and bounds for the minimum distance and stopping distance are derived these constructed codes can be used to derive quantum error correcting codes
linear codes have been an interesting subject of study for many years , as linear codes with few weights have applications in secrete sharing , authentication codes , association schemes , and strongly regular graphs in this paper , a class of linear codes with a few weights over the finite field gf \( p \) are presented and their weight distributions are also determined , where p is an odd prime
the ability to control a complex network towards a desired behavior relies on our understanding of the complex nature of these social and technological networks the existence of numerous control schemes in a network promotes us to wonder what is the underlying principle of all control schemes and driver nodes \? here we introduce driver graph , a simple geometry that reveals the complex relationship between all control schemes and driver nodes we prove that the node adjacent to a driver node in the driver graph will appear in another control scheme and all control schemes are related by adjacent nodes in the driver graph furthermore , we find the connected nodes in driver graph have the same control role , and the giant components emerge in the driver graphs of many real networks , which provides a clear topological explanation of bifurcation phenomenon emerging in dense networks and promotes us to design an efficient method to alter the control roles of nodes the findings provide an insight into control principles of complex networks and offer a general mechanism to design a suitable control scheme for different purposes
by reformulating a learning process of a set system l as a game between teacher \( presenter of data \) and learner \( updater of the abstract independent set \) , we define the order type dim l of l to be the order type of the game tree the theory of this new order type and continuous , monotone function between set systems corresponds to the theory of well quasi orderings \( wqos \) as nash williams developed the theory of wqos to the theory of better quasi orderings \( bqos \) , we introduce a set system that has order type and corresponds to a bqo we prove that the class of set systems corresponding to bqos is closed by any monotone function in \( shinohara and arimura inductive inference of unbounded unions of pattern languages from positive data theoretical computer science , pp 191 209 , 2000 \) , for any set system l , they considered the class of arbitrary \( finite \) unions of members of l from viewpoint of wqos and bqos , we characterize the set systems l such that the class of arbitrary \( finite \) unions of members of l has order type the characterization shows that the order structure of the set system l with respect to the set inclusion is not important for the resulting set system having order type we point out continuous , monotone function of set systems is similar to positive reduction to jockusch owings' weakly semirecursive sets
to verify the correct operation of systems , engineers need to determine the set of configurations of a dynamical model that are able to safely reach a specified configuration under a control law unfortunately , constructing models for systems interacting in highly dynamic environments is difficult this paper addresses this challenge by presenting a convex optimization method to efficiently compute the set of configurations of a polynomial hybrid dynamical system that are able to safely reach a user defined target set despite parametric uncertainty in the model this class of models describes , for example , legged robots moving over uncertain terrains the presented approach utilizes the notion of occupation measures to describe the evolution of trajectories of a nonlinear hybrid dynamical system with parametric uncertainty as a linear equation over measures whose supports coincide with the trajectories under investigation this linear equation with user defined support constraints is approximated with vanishing conservatism using a hierarchy of semidefinite programs that are each proven to compute an inner outer approximation to the set of initial conditions that can reach the user defined target set safely in spite of uncertainty the efficacy of this method is illustrated on a collection of six representative examples
an n simplex is said to be n well centered if its circumcenter lies in its interior we introduce several other geometric conditions and an algebraic condition that can be used to determine whether a simplex is n well centered these conditions , together with some other observations , are used to describe restrictions on the local combinatorial structure of simplicial meshes in which every simplex is well centered in particular , it is shown that in a 3 well centered \( 2 well centered \) tetrahedral mesh there are at least 7 \( 9 \) edges incident to each interior vertex , and these bounds are sharp moreover , it is shown that , in stark contrast to the 2 dimensional analog , where there are exactly two vertex links that prevent a well centered triangle mesh in r 2 , there are infinitely many vertex links that prohibit a well centered tetrahedral mesh in r 3
we study a protocol in which many parties use quantum communication to transfer a shared state to a receiver without communicating with each other this protocol is a multiparty version of the fully quantum slepian wolf protocol for two senders and arises through the repeated application of the two sender protocol we describe bounds on the achievable rate region for the distributed compression problem the inner bound arises by expressing the achievable rate region for our protocol in terms of its vertices and extreme rays and , equivalently , in terms of facet inequalities we also prove an outer bound on all possible rates for distributed compression based on the multiparty squashed entanglement , a measure of multiparty entanglement
the multi frontal direct solver is the state of the art algorithm for the direct solution of sparse linear systems this paper provides computational complexity and memory usage estimates for the application of the multi frontal direct solver algorithm on linear systems resulting from b spline based isogeometric finite elements , where the mesh is a structured grid specifically we provide the estimates for systems resulting from c p 1 polynomial b spline spaces and compare them to those obtained using c 0 spaces
we present a novel approach to pseudo feedback based ad hoc retrieval that uses language models induced from both documents and clusters first , we treat the pseudo feedback documents produced in response to the original query as a set of pseudo queries that themselves can serve as input to the retrieval process observing that the documents returned in response to the pseudo queries can then act as pseudo queries for subsequent rounds , we arrive at a formulation of pseudo query based retrieval as an iterative process experiments show that several concrete instantiations of this idea , when applied in conjunction with techniques designed to heighten precision , yield performance results rivaling those of a number of previously proposed algorithms , including the standard language modeling approach the use of cluster based language models is a key contributing factor to our algorithms' success
in this paper , we study a model of quantum markov chains that is a quantum analogue of markov chains and is obtained by replacing probabilities in transition matrices with quantum operations we show that this model is very suited to describe hybrid systems that consist of a quantum component and a classical one , although it has the same expressive power as another quantum markov model proposed in the literature indeed , hybrid systems are often encountered in quantum information processing for example , both quantum programs and quantum protocols can be regarded as hybrid systems thus , we further propose a model called hybrid quantum automata \( hqa \) that can be used to describe these hybrid systems that receive inputs \( actions \) from the outer world we show the language equivalence problem of hqa is decidable in polynomial time furthermore , we apply this result to the trace equivalence problem of quantum markov chains , and thus it is also decidable in polynomial time finally , we discuss model checking linear time properties of quantum markov chains , and show the quantitative analysis of regular safety properties can be addressed successfully
we study the behavior of approximate message passing , a solver for linear sparse estimation problems such as compressed sensing , when the i i d matrices for which it has been specifically designed are replaced by structured operators , such as fourier and hadamard ones we show empirically that after proper randomization , the structure of the operators does not significantly affect the performances of the solver furthermore , for some specially designed spatially coupled operators , this allows a computationally fast and memory efficient reconstruction in compressed sensing up to the information theoretical limit we also show how this approach can be applied to sparse superposition codes , allowing the approximate message passing decoder to perform at large rates for moderate block length
an upper bound to the information capacity of a wavelength division multi plexed optical fiber communication system is derived in a model incorporating the nonlinear propagation effects of cross phase modulation \( xpm \) this work is based on the paper by mitra et al , finding lower bounds to the channel capacity , in which physical models for propagation are used to calculate statistical properties of the conditional probability distribution relating input and output in a single wdm channel in this paper we present a tractable channel model incorporating the effects of cross phase modulation using this model we find an upper bound to the information capacity of the fiber optical communication channel at high snr the results provide physical insight into the manner in which nonlinearities degrade the information capacity
the relay encoder is an unreliable probabilistic device which is aimed at helping the communication between the sender and the receiver in this work we show that in the quantum setting the probabilistic behavior can be completely eliminated we also show how to combine quantum polar encoding with superactivation assistance in order to achieve reliable and capacity achieving private communication over noisy quantum relay channels
in this work , we discuss the joint precoding with finite rate feedback in the so called network mimo where the txs share the knowledge of the data symbols to be transmitted we introduce a distributed channel state information \( dcsi \) model where each tx has its own local estimate of the overall multi user mimo channel and must make a precoding decision solely based on the available local csi we refer to this channel as the dcsi mimo channel and the precoding problem as distributed precoding we extend to the dcsi setting the work from jindal for the conventional mimo broadcast channel \( bc \) in which the number of degrees of freedom \( dofs \) achieved by zero forcing \( zf \) was derived as a function of the scaling in the logarithm of the signal to noise ratio \( snr \) of the number of quantizing bits particularly , we show the seemingly pessimistic result that the number of dofs at each user is limited by the worst csi across all users and across all txs this is in contrast to the conventional mimo bc where the number of dofs at one user is solely dependent on the quality of the estimation of his own feedback consequently , we provide precoding schemes improving on the achieved number of dofs for the two user case , the derived novel precoder achieves a number of dofs limited by the best csi accuracy across the txs instead of the worst with conventional zf we also advocate the use of hierarchical quantization of the csi , for which we show that considerable gains are possible finally , we use the previous analysis to derive the dofs optimal allocation of the feedback bits to the various txs under a constraint on the size of the aggregate feedback in the network , in the case where conventional zf is used
in this paper we study a natural generalization of both sc k path and sc k tree problems , namely , the sc subgraph isomorphism problem in the sc subgraph isomorphism problem we are given two graphs f and g on k and n vertices respectively as an input , and the question is whether there exists a subgraph of g isomorphic to f we show that if the treewidth of f is at most t , then there is a randomized algorithm for the sc subgraph isomorphism problem running in time co \( 2 k n 2t \) to do so , we associate a new multivariate homomorphism polynomial of degree at most k with the sc subgraph isomorphism problem and construct an arithmetic circuit of size at most n co \( t \) for this polynomial using this polynomial , we also give a deterministic algorithm to count the number of homomorphisms from f to g that takes n co \( t \) time and uses polynomial space for the counting version of the sc subgraph isomorphism problem , where the objective is to count the number of distinct subgraphs of g that are isomorphic to f , we give a deterministic algorithm running in time and space co \( n choose k 2 n 2p \) or n choose k 2 n co \( t log k \) we also give an algorithm running in time co \( 2 k n choose k 2 n 5p \) and taking space polynomial in n here p and t denote the pathwidth and the treewidth of f , respectively thus our work not only improves on known results on sc subgraph isomorphism but it also extends and generalize most of the known results on sc k path and sc k tree
in this letter , we study the ergodic capacity of a maximum ratio combining \( mrc \) rician fading channel with full channel state information \( csi \) at the transmitter and at the receiver we focus on the low signal to noise ratio \( snr \) regime and we show that the capacity scales as \( l omega \( k l \) \) snr log \( 1 snr \) , where omega is the expected channel gain per branch , k is the rician fading factor , and l is the number of diversity branches we show that one bit csi feedback at the transmitter is enough to achieve this capacity using an on off power control scheme our framework can be seen as a generalization of recently established results regarding the fading channels capacity characterization in the low snr regime
in this paper , we present a new power allocation scheme for a decode and forward \( df \) relaying enhanced cooperative wireless system while both source and relay nodes may have limited traditional brown power supply or fixed green energy storage , the hybrid source node can also draw power from the surrounding radio frequency \( rf \) signals in particular , we assume a deterministic rf energy harvesting \( eh \) model under which the signals transmitted by the relay serve as the renewable energy source for the source node the amount of harvested energy is known for a given transmission power of the forwarding signal and channel condition between the source and relay nodes to maximize the overall throughput while meeting the constraints imposed by the non sustainable energy sources and the renewable energy source , an optimization problem is formulated and solved based on different harvesting efficiency and channel condition , closed form solutions are derived to obtain the optimal source and relay power allocation jointly it is shown that instead of demanding high on grid power supply or high green energy availability , the system can achieve compatible or higher throughput by utilizing the harvested energy
in this report we include some derivations on a channel estimation method based on the use of dirty paper coding specifically , we analyze the proposed method , named dirty paper based channel estimation \( dpce \) , when the considered codebooks are high dimensional lattices
we study an economic decision problem where the actors are two firms and the antitrust authority whose main task is to monitor and prevent firms' potential anti competitive behaviour and its effect on the market the antitrust authority 's decision process is modelled using a bayesian network where both the relational structure and the parameters of the model are estimated from a data set provided by the authority itself a number of economic variables that influence this decision process are also included in the model we analyse how monitoring by the antitrust authority affects firms' strategies about cooperation firms' strategies are modelled as a repeated prisoner 's dilemma using object oriented bayesian networks we show how the integration of firms' decision process and external market information can be modelled in this way various decision scenarios and strategies are illustrated
this paper presents a new approach to solving n queen problems , which involves a model of distributed autonomous agents with artificial life \( alife \) and a method of representing n queen constraints in an agent environment the distributed agents locally interact with their living environment , i e , a chessboard , and execute their reactive behaviors by applying their behavioral rules for randomized motion , least conflict position searching , and cooperating with other agents etc the agent based n queen problem solving system evolves through selection and contest according to the rule of survival of the fittest , in which some agents will die or be eaten if their moving strategies are less efficient than others the experimental results have shown that this system is capable of solving large scale n queen problems this paper also provides a model of alife agents for solving general csps
we present a framework to study linear deterministic interference networks over finite fields unlike the popular linear deterministic models introduced to study gaussian networks , we consider networks where the channel coefficients are general scalars over some extension field ff p m \( scalar m th extension field models \) , m times m diagonal matrices over ff p \( m symbol extension ground field models \) , and m times m general non singular matrices \( mimo ground field models \) we use the companion matrix representation of the extension field to convert m th extension scalar models into mimo ground field models where the channel matrices have special algebraic structure for such models , we consider the 2 times 2 times 2 topology \( two hops two flow \) and the 3 user interference network topology we derive achievability results and feasibility conditions for certain schemes based on the precoding based network alignment \( pbna \) approach , where intermediate nodes use random linear network coding \( i e , propagate random linear combinations of their incoming messages \) and non trivial precoding decoding is performed only at the network edges , at the sources and destinations furthermore , we apply this approach to the scalar 2 times 2 times 2 complex gaussian ic with fixed channel coefficients , and show two competitive schemes outperforming other known approaches at any snr , where we combine finite field linear precoding decoding with lattice coding and the compute and forward approach at the signal level as a side result , we also show significant advantages of vector linear network coding both in terms of feasibility probability \( with random coding coefficients \) and in terms of coding latency , with respect to standard scalar linear network coding , in pbna schemes
in this paper , we investigate the maximum throughput of a saturated rechargeable secondary user \( su \) sharing the spectrum with a primary user \( pu \) the su harvests energy packets \( tokens \) from the environment with a certain harvesting rate all transmitters are assumed to have data buffers to store the incoming data packets in addition to its own traffic buffer , the su has a buffer for storing the admitted primary packets for relaying and a buffer for storing the energy tokens harvested from the environment we propose a new cooperative cognitive relaying protocol that allows the su to relay a fraction of the undelivered primary packets we consider an interference channel model \( or a multipacket reception \( mpr \) channel model \) , where concurrent transmissions can survive from interference with certain probability characterized by the complement of channel outages the proposed protocol exploits the primary queue burstiness and receivers' mpr capability in addition , it efficiently expends the secondary energy tokens under the objective of secondary throughput maximization our numerical results show the benefits of cooperation , receivers' mpr capability , and secondary energy queue arrival rate on the system performance from a network layer standpoint
the l th stopping redundancy rho l \( mathcal c \) of the binary n , k , d code mathcal c , 1 le l le d , is defined as the minimum number of rows in the parity check matrix of mathcal c , such that the smallest stopping set is of size at least l the stopping redundancy rho \( mathcal c \) is defined as rho d \( mathcal c \) in this work , we improve on the probabilistic analysis of stopping redundancy , proposed by han , siegel and vardy , which yields the best bounds known today in our approach , we judiciously select the first few rows in the parity check matrix , and then continue with the probabilistic method by using similar techniques , we improve also on the best known bounds on rho l \( mathcal c \) , for 1 le l le d our approach is compared to the existing methods by numerical computations
